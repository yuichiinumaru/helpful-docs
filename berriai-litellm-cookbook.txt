Directory structure:
└── berriai-litellm/
    ├── cookbook/
    │   ├── Claude_(Anthropic)_with_Streaming_liteLLM_Examples.ipynb
    │   ├── google_adk_litellm_tutorial.ipynb
    │   ├── liteLLM_A121_Jurrasic_example.ipynb
    │   ├── LiteLLM_Azure_and_OpenAI_example.ipynb
    │   ├── liteLLM_Baseten.ipynb
    │   ├── LiteLLM_batch_completion.ipynb
    │   ├── LiteLLM_Bedrock.ipynb
    │   ├── liteLLM_clarifai_Demo.ipynb
    │   ├── LiteLLM_Comparing_LLMs.ipynb
    │   ├── LiteLLM_Completion_Cost.ipynb
    │   ├── liteLLM_function_calling.ipynb
    │   ├── liteLLM_Getting_Started.ipynb
    │   ├── LiteLLM_HuggingFace.ipynb
    │   ├── liteLLM_IBM_Watsonx.ipynb
    │   ├── liteLLM_Langchain_Demo.ipynb
    │   ├── litellm_model_fallback.ipynb
    │   ├── LiteLLM_NovitaAI_Cookbook.ipynb
    │   ├── liteLLM_Ollama.ipynb
    │   ├── LiteLLM_OpenRouter.ipynb
    │   ├── LiteLLM_Petals.ipynb
    │   ├── liteLLM_Replicate_Demo.ipynb
    │   ├── liteLLM_Streaming_Demo.ipynb
    │   ├── litellm_test_multiple_llm_demo.ipynb
    │   ├── LiteLLM_User_Based_Rate_Limits.ipynb
    │   ├── liteLLM_VertextAI_Example.ipynb
    │   ├── Migrating_to_LiteLLM_Proxy_from_OpenAI_Azure_OpenAI.ipynb
    │   ├── mlflow_langchain_tracing_litellm_proxy.ipynb
    │   ├── Parallel_function_calling.ipynb
    │   ├── Proxy_Batch_Users.ipynb
    │   ├── Using_Nemo_Guardrails_with_LiteLLM_Server.ipynb
    │   ├── veo_video_generation.py
    │   ├── VLLM_Model_Testing.ipynb
    │   ├── benchmark/
    │   │   ├── readme.md
    │   │   ├── benchmark.py
    │   │   └── eval_suites_mlflow_autoevals/
    │   │       └── auto_evals.py
    │   ├── codellama-server/
    │   │   ├── README.MD
    │   │   └── main.py
    │   ├── community-resources/
    │   │   ├── get_hf_models.py
    │   │   └── max_tokens.json
    │   ├── litellm-ollama-docker-image/
    │   │   ├── Dockerfile
    │   │   ├── requirements.txt
    │   │   ├── start.sh
    │   │   └── test.py
    │   ├── litellm_proxy_server/
    │   │   ├── readme.md
    │   │   └── grafana_dashboard/
    │   │       ├── readme.md
    │   │       ├── dashboard_1/
    │   │       │   ├── readme.md
    │   │       │   └── grafana_dashboard.json
    │   │       └── dashboard_v2/
    │   │           └── grafana_dashboard.json
    │   ├── litellm_router/
    │   │   ├── error_log.txt
    │   │   ├── load_test_proxy.py
    │   │   ├── load_test_queuing.py
    │   │   ├── load_test_router.py
    │   │   ├── request_log.txt
    │   │   ├── response_log.txt
    │   │   └── test_questions/
    │   │       ├── question1.txt
    │   │       ├── question2.txt
    │   │       └── question3.txt
    │   ├── litellm_router_load_test/
    │   │   ├── test_loadtest_openai_client.py
    │   │   ├── test_loadtest_router.py
    │   │   ├── test_loadtest_router_withs3_cache.py
    │   │   └── memory_usage/
    │   │       ├── router_endpoint.py
    │   │       ├── router_memory_usage copy.py
    │   │       ├── router_memory_usage.py
    │   │       └── send_request.py
    │   ├── logging_observability/
    │   │   ├── LiteLLM_Arize.ipynb
    │   │   ├── LiteLLM_Langfuse.ipynb
    │   │   ├── LiteLLM_Lunary.ipynb
    │   │   └── LiteLLM_Proxy_Langfuse.ipynb
    │   └── misc/
    │       ├── add_new_models.py
    │       ├── config.yaml
    │       ├── dev_release.txt
    │       ├── migrate_proxy_config.py
    │       ├── openai_timeouts.py
    │       ├── sagmaker_streaming.py
    │       ├── test_responses_api.py
    │       └── update_json_caching.py
    └── litellm/
        └── proxy/
            └── enterprise -> enterprise

================================================
FILE: cookbook/Claude_(Anthropic)_with_Streaming_liteLLM_Examples.ipynb
================================================
# Jupyter notebook converted to Python script.

!pip install litellm=="0.1.363"
# Output:
#   Collecting litellm==0.1.363

#     Downloading litellm-0.1.363-py3-none-any.whl (34 kB)

#   Requirement already satisfied: openai<0.28.0,>=0.27.8 in /usr/local/lib/python3.10/dist-packages (from litellm==0.1.363) (0.27.8)

#   Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from litellm==0.1.363) (1.0.0)

#   Requirement already satisfied: tiktoken<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from litellm==0.1.363) (0.4.0)

#   Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->litellm==0.1.363) (2.31.0)

#   Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->litellm==0.1.363) (4.65.0)

#   Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->litellm==0.1.363) (3.8.5)

#   Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.5.0,>=0.4.0->litellm==0.1.363) (2022.10.31)

#   Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->litellm==0.1.363) (3.2.0)

#   Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->litellm==0.1.363) (3.4)

#   Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->litellm==0.1.363) (1.26.16)

#   Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->litellm==0.1.363) (2023.7.22)

#   Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (23.1.0)

#   Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (6.0.4)

#   Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (4.0.2)

#   Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (1.9.2)

#   Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (1.4.0)

#   Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (1.3.1)

#   Installing collected packages: litellm

#     Attempting uninstall: litellm

#       Found existing installation: litellm 0.1.362

#       Uninstalling litellm-0.1.362:

#         Successfully uninstalled litellm-0.1.362

#   Successfully installed litellm-0.1.363


# @title Import litellm & Set env variables
import litellm
import os

os.environ["ANTHROPIC_API_KEY"] = " " #@param

# @title Request Claude Instant-1 and Claude-2
messages = [
  {"role": "system", "content": "You are a helpful assistant."},
  {"role": "user", "content": "Who won the world series in 2020?"}
  ]

result = litellm.completion('claude-instant-1', messages)
print("\n\n Result from claude-instant-1", result)
result = litellm.completion('claude-2', messages, max_tokens=5, temperature=0.2)
print("\n\n Result from claude-2", result)
# Output:
#   

#   

#    Result from claude-instant-1 {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': " The Los Angeles Dodgers won the 2020 World Series, defeating the Tampa Bay Rays 4-2. It was the Dodgers' first World Series title since 1988."}}], 'created': 1691536677.2676156, 'model': 'claude-instant-1', 'usage': {'prompt_tokens': 30, 'completion_tokens': 32, 'total_tokens': 62}}

#   

#   

#    Result from claude-2 {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': ' The Los Angeles Dodgers won'}}], 'created': 1691536677.944753, 'model': 'claude-2', 'usage': {'prompt_tokens': 30, 'completion_tokens': 5, 'total_tokens': 35}}


# @title Streaming Example: Request Claude-2
messages = [
  {"role": "system", "content": "You are a helpful assistant."},
  {"role": "user", "content": "how does a court case get to the Supreme Court?"}
  ]

result = litellm.completion('claude-2', messages, stream=True)
for part in result:
    print(part.choices[0].delta.content or "")


# Output:
#    Here

#   's

#    a

#    quick

#    overview

#    of

#    how

#    a

#    court

#    case

#    can

#    reach

#    the

#    U

#   .

#   S

#   .

#    Supreme

#    Court

#   :

#   

#   

#   -

#    The

#    case

#    must

#    first

#    be

#    heard

#    in

#    a

#    lower

#    trial

#    court

#    (

#   either

#    a

#    state

#    court

#    or

#    federal

#    district

#    court

#   ).

#    The

#    trial

#    court

#    makes

#    initial

#    r

#   ulings

#    and

#    produces

#    a

#    record

#    of

#    the

#    case

#   .

#   

#   

#   -

#    The

#    losing

#    party

#    can

#    appeal

#    the

#    decision

#    to

#    an

#    appeals

#    court

#    (

#   a

#    state

#    appeals

#    court

#    for

#    state

#    cases

#   ,

#    or

#    a

#    federal

#    circuit

#    court

#    for

#    federal

#    cases

#   ).

#    The

#    appeals

#    court

#    reviews

#    the

#    trial

#    court

#   's

#    r

#   ulings

#    and

#    can

#    affirm

#   ,

#    reverse

#   ,

#    or

#    modify

#    the

#    decision

#   .

#   

#   

#   -

#    If

#    a

#    party

#    is

#    still

#    unsat

#   isf

#   ied

#    after

#    the

#    appeals

#    court

#    rules

#   ,

#    they

#    can

#    petition

#    the

#    Supreme

#    Court

#    to

#    hear

#    the

#    case

#    through

#    a

#    writ

#    of

#    cert

#   ior

#   ari

#   .

#    

#   

#   

#   -

#    The

#    Supreme

#    Court

#    gets

#    thousands

#    of

#    cert

#    petitions

#    every

#    year

#    but

#    usually

#    only

#    agrees

#    to

#    hear

#    about

#    100

#   -

#   150

#    of

#    cases

#    that

#    have

#    significant

#    national

#    importance

#    or

#    where

#    lower

#    courts

#    disagree

#    on

#    federal

#    law

#   .

#    

#   

#   

#   -

#    If

#    4

#    out

#    of

#    the

#    9

#    Just

#   ices

#    vote

#    to

#    grant

#    cert

#    (

#   agree

#    to

#    hear

#    the

#    case

#   ),

#    it

#    goes

#    on

#    the

#    Supreme

#    Court

#   's

#    do

#   cket

#    for

#    arguments

#   .

#   

#   

#   -

#    The

#    Supreme

#    Court

#    then

#    hears

#    oral

#    arguments

#   ,

#    considers

#    written

#    brief

#   s

#   ,

#    examines

#    the

#    lower

#    court

#    records

#   ,

#    and

#    issues

#    a

#    final

#    ruling

#    on

#    the

#    case

#   ,

#    which

#    serves

#    as

#    binding

#    precedent




================================================
FILE: cookbook/google_adk_litellm_tutorial.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Google ADK with LiteLLM

Use Google ADK with LiteLLM Python SDK, LiteLLM Proxy.

This tutorial shows you how to create intelligent agents using Agent Development Kit (ADK) with support for multiple Large Language Model (LLM) providers through LiteLLM.
"""

"""
## Overview

ADK (Agent Development Kit) allows you to build intelligent agents powered by LLMs. By integrating with LiteLLM, you can:

- Use multiple LLM providers (OpenAI, Anthropic, Google, etc.)
- Switch easily between models from different providers
- Connect to a LiteLLM proxy for centralized model management
"""

"""
## Prerequisites

- Python environment setup
- API keys for model providers (OpenAI, Anthropic, Google AI Studio)
- Basic understanding of LLMs and agent concepts
"""

"""
## Installation
"""

# Install dependencies
!pip install google-adk litellm

"""
## 1. Setting Up Environment
"""

# Setup environment and API keys
import os
import asyncio
from google.adk.agents import Agent
from google.adk.models.lite_llm import LiteLlm  # For multi-model support
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from google.genai import types
import litellm  # Import for proxy configuration

# Set your API keys
os.environ['GOOGLE_API_KEY'] = 'your-google-api-key'  # For Gemini models
os.environ['OPENAI_API_KEY'] = 'your-openai-api-key'  # For OpenAI models
os.environ['ANTHROPIC_API_KEY'] = 'your-anthropic-api-key'  # For Claude models

# Define model constants for cleaner code
MODEL_GEMINI_PRO = 'gemini-1.5-pro'
MODEL_GPT_4O = 'openai/gpt-4o'
MODEL_CLAUDE_SONNET = 'anthropic/claude-3-sonnet-20240229'

"""
## 2. Define a Simple Tool
"""

# Weather tool implementation
def get_weather(city: str) -> dict:
    """Retrieves the current weather report for a specified city."""
    print(f'Tool: get_weather called for city: {city}')

    # Mock weather data
    mock_weather_db = {
        'newyork': {
            'status': 'success',
            'report': 'The weather in New York is sunny with a temperature of 25°C.'
        },
        'london': {
            'status': 'success',
            'report': "It's cloudy in London with a temperature of 15°C."
        },
        'tokyo': {
            'status': 'success',
            'report': 'Tokyo is experiencing light rain and a temperature of 18°C.'
        },
    }

    city_normalized = city.lower().replace(' ', '')

    if city_normalized in mock_weather_db:
        return mock_weather_db[city_normalized]
    else:
        return {
            'status': 'error',
            'error_message': f"Sorry, I don't have weather information for '{city}'."
        }

"""
## 3. Helper Function for Agent Interaction
"""

# Agent interaction helper function
async def call_agent_async(query: str, runner, user_id, session_id):
    """Sends a query to the agent and prints the final response."""
    print(f'\n>>> User Query: {query}')

    content = types.Content(role='user', parts=[types.Part(text=query)])
    final_response_text = 'Agent did not produce a final response.'

    async for event in runner.run_async(
        user_id=user_id,
        session_id=session_id,
        new_message=content
    ):
        if event.is_final_response():
            if event.content and event.content.parts:
                final_response_text = event.content.parts[0].text
            break
    print(f'<<< Agent Response: {final_response_text}')

"""
## 4. Using Different Model Providers with ADK

### 4.1 Using OpenAI Models
"""

# OpenAI model implementation
weather_agent_gpt = Agent(
    name='weather_agent_gpt',
    model=LiteLlm(model=MODEL_GPT_4O),
    description='Provides weather information using OpenAI\'s GPT.',
    instruction=(
        'You are a helpful weather assistant powered by GPT-4o. '
        "Use the 'get_weather' tool for city weather requests. "
        'Present information clearly.'
    ),
    tools=[get_weather],
)

session_service_gpt = InMemorySessionService()
session_gpt = session_service_gpt.create_session(
    app_name='weather_app', user_id='user_1', session_id='session_gpt'
)

runner_gpt = Runner(
    agent=weather_agent_gpt,
    app_name='weather_app',
    session_service=session_service_gpt,
)

async def test_gpt_agent():
    print('\n--- Testing GPT Agent ---')
    await call_agent_async(
        "What's the weather in London?",
        runner=runner_gpt,
        user_id='user_1',
        session_id='session_gpt',
    )

# To execute in a notebook cell:
# await test_gpt_agent()

"""
### 4.2 Using Anthropic Models
"""

# Anthropic model implementation
weather_agent_claude = Agent(
    name='weather_agent_claude',
    model=LiteLlm(model=MODEL_CLAUDE_SONNET),
    description='Provides weather information using Anthropic\'s Claude.',
    instruction=(
        'You are a helpful weather assistant powered by Claude Sonnet. '
        "Use the 'get_weather' tool for city weather requests. "
        'Present information clearly.'
    ),
    tools=[get_weather],
)

session_service_claude = InMemorySessionService()
session_claude = session_service_claude.create_session(
    app_name='weather_app', user_id='user_1', session_id='session_claude'
)

runner_claude = Runner(
    agent=weather_agent_claude,
    app_name='weather_app',
    session_service=session_service_claude,
)

async def test_claude_agent():
    print('\n--- Testing Claude Agent ---')
    await call_agent_async(
        "What's the weather in Tokyo?",
        runner=runner_claude,
        user_id='user_1',
        session_id='session_claude',
    )

# To execute in a notebook cell:
# await test_claude_agent()

"""
### 4.3 Using Google's Gemini Models
"""

# Gemini model implementation
weather_agent_gemini = Agent(
    name='weather_agent_gemini',
    model=MODEL_GEMINI_PRO,
    description='Provides weather information using Google\'s Gemini.',
    instruction=(
        'You are a helpful weather assistant powered by Gemini Pro. '
        "Use the 'get_weather' tool for city weather requests. "
        'Present information clearly.'
    ),
    tools=[get_weather],
)

session_service_gemini = InMemorySessionService()
session_gemini = session_service_gemini.create_session(
    app_name='weather_app', user_id='user_1', session_id='session_gemini'
)

runner_gemini = Runner(
    agent=weather_agent_gemini,
    app_name='weather_app',
    session_service=session_service_gemini,
)

async def test_gemini_agent():
    print('\n--- Testing Gemini Agent ---')
    await call_agent_async(
        "What's the weather in New York?",
        runner=runner_gemini,
        user_id='user_1',
        session_id='session_gemini',
    )

# To execute in a notebook cell:
# await test_gemini_agent()

"""
## 5. Using LiteLLM Proxy with ADK
"""

"""
| Variable | Description |
|----------|-------------|
| `LITELLM_PROXY_API_KEY` | The API key for the LiteLLM proxy |
| `LITELLM_PROXY_API_BASE` | The base URL for the LiteLLM proxy |
| `USE_LITELLM_PROXY` or `litellm.use_litellm_proxy` | When set to True, your request will be sent to LiteLLM proxy. |
"""

# LiteLLM proxy integration
os.environ['LITELLM_PROXY_API_KEY'] = 'your-litellm-proxy-api-key'
os.environ['LITELLM_PROXY_API_BASE'] = 'your-litellm-proxy-url'  # e.g., 'http://localhost:4000'
litellm.use_litellm_proxy = True

weather_agent_proxy_env = Agent(
    name='weather_agent_proxy_env',
    model=LiteLlm(model='gpt-4o'),
    description='Provides weather information using a model from LiteLLM proxy.',
    instruction=(
        'You are a helpful weather assistant. '
        "Use the 'get_weather' tool for city weather requests. "
        'Present information clearly.'
    ),
    tools=[get_weather],
)

session_service_proxy_env = InMemorySessionService()
session_proxy_env = session_service_proxy_env.create_session(
    app_name='weather_app', user_id='user_1', session_id='session_proxy_env'
)

runner_proxy_env = Runner(
    agent=weather_agent_proxy_env,
    app_name='weather_app',
    session_service=session_service_proxy_env,
)

async def test_proxy_env_agent():
    print('\n--- Testing Proxy-enabled Agent (Environment Variables) ---')
    await call_agent_async(
        "What's the weather in London?",
        runner=runner_proxy_env,
        user_id='user_1',
        session_id='session_proxy_env',
    )

# To execute in a notebook cell:
# await test_proxy_env_agent()



================================================
FILE: cookbook/liteLLM_A121_Jurrasic_example.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM A121 Tutorial

This walks through using A121 Jurassic models
* j2-light
* j2-mid
* j2-ultra
"""

!pip install litellm

from litellm import completion
import os

"""
## Set A121 Keys
You can get a free key from https://studio.ai21.com/account/api-key
"""

os.environ["AI21_API_KEY"] = ""

"""
# A121 Supported Models:
https://studio.ai21.com/foundation-models
"""

"""
## J2-light Call
"""

messages = [{ "content": "Hello, how are you?","role": "user"}]
response = completion(model="j2-light", messages=messages)
response
# Output:
#   <ModelResponse at 0x7b2c2902e610> JSON: {

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": " However, I have an important question to ask you\nMy name is X, and I was wondering if you would be willing to help me.",

#           "role": "assistant"

#         }

#       }

#     ],

#     "created": 1692761063.5189915,

#     "model": "j2-light",

#     "usage": {

#       "prompt_tokens": null,

#       "completion_tokens": null,

#       "total_tokens": null

#     }

#   }

"""
# J2-Mid
"""

messages = [{ "content": "what model are you","role": "user"}]
response = completion(model="j2-mid", messages=messages)
response
# Output:
#   <ModelResponse at 0x7b2c2902f6a0> JSON: {

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": "\nplease choose the model from the list below\nModel view in Tekla Structures",

#           "role": "assistant"

#         }

#       }

#     ],

#     "created": 1692761140.0017524,

#     "model": "j2-mid",

#     "usage": {

#       "prompt_tokens": null,

#       "completion_tokens": null,

#       "total_tokens": null

#     }

#   }

"""
# J2-Ultra
"""

messages = [{ "content": "what model are you","role": "user"}]
response = completion(model="j2-ultra", messages=messages)
response
# Output:
#   <ModelResponse at 0x7b2c28fd4090> JSON: {

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": "\nI am not a specific model, but I can provide information and assistance based on my training data. Please let me know if there is anything you",

#           "role": "assistant"

#         }

#       }

#     ],

#     "created": 1692761157.8675153,

#     "model": "j2-ultra",

#     "usage": {

#       "prompt_tokens": null,

#       "completion_tokens": null,

#       "total_tokens": null

#     }

#   }



================================================
FILE: cookbook/LiteLLM_Azure_and_OpenAI_example.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM - Azure OpenAI + OpenAI Calls
This notebook covers the following for Azure OpenAI + OpenAI:
* Completion - Quick start
* Completion - Streaming
* Completion - Azure, OpenAI in separate threads
* Completion - Stress Test 10 requests in parallel
* Completion - Azure, OpenAI in the same thread
"""

!pip install litellm

import os

"""
## Completion - Quick start
"""

from litellm import completion

# openai configs
os.environ["OPENAI_API_KEY"] = ""

# azure openai configs
os.environ["AZURE_API_KEY"] = ""
os.environ["AZURE_API_BASE"] = "https://openai-gpt-4-test-v-1.openai.azure.com/"
os.environ["AZURE_API_VERSION"] = "2023-05-15"


# openai call
response = completion(
    model = "gpt-3.5-turbo",
    messages = [{ "content": "Hello, how are you?","role": "user"}]
)
print("Openai Response\n")
print(response)



# azure call
response = completion(
    model = "azure/your-azure-deployment",
    messages = [{ "content": "Hello, how are you?","role": "user"}]
)
print("Azure Response\n")
print(response)
# Output:
#   Openai Response

#   

#   {

#     "id": "chatcmpl-7yjVOEKCPw2KdkfIaM3Ao1tIXp8EM",

#     "object": "chat.completion",

#     "created": 1694708958,

#     "model": "gpt-3.5-turbo-0613",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": "I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you?"

#         },

#         "finish_reason": "stop"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 13,

#       "completion_tokens": 26,

#       "total_tokens": 39

#     }

#   }

#   Azure Response

#   

#   {

#     "id": "chatcmpl-7yjVQ6m2R2HRtnKHRRFp6JzL4Fjez",

#     "object": "chat.completion",

#     "created": 1694708960,

#     "model": "gpt-35-turbo",

#     "choices": [

#       {

#         "index": 0,

#         "finish_reason": "stop",

#         "message": {

#           "role": "assistant",

#           "content": "Hello there! As an AI language model, I don't have feelings but I'm functioning well. How can I assist you today?"

#         }

#       }

#     ],

#     "usage": {

#       "completion_tokens": 27,

#       "prompt_tokens": 14,

#       "total_tokens": 41

#     }

#   }


"""
## Completion - Streaming
"""

import os
from litellm import completion

# openai configs
os.environ["OPENAI_API_KEY"] = ""

# azure openai configs
os.environ["AZURE_API_KEY"] = ""
os.environ["AZURE_API_BASE"] = "https://openai-gpt-4-test-v-1.openai.azure.com/"
os.environ["AZURE_API_VERSION"] = "2023-05-15"


# openai call
response = completion(
    model = "gpt-3.5-turbo",
    messages = [{ "content": "Hello, how are you?","role": "user"}],
    stream=True
)
print("OpenAI Streaming response")
for chunk in response:
  print(chunk)

# azure call
response = completion(
    model = "azure/your-azure-deployment",
    messages = [{ "content": "Hello, how are you?","role": "user"}],
    stream=True
)
print("Azure Streaming response")
for chunk in response:
  print(chunk)


"""
## Completion - Azure, OpenAI in separate threads
"""

import os
import threading
from litellm import completion

# Function to make a completion call
def make_completion(model, messages):
    response = completion(
        model=model,
        messages=messages
    )

    print(f"Response for {model}: {response}")

# openai configs
os.environ["OPENAI_API_KEY"] = ""

# azure openai configs
os.environ["AZURE_API_KEY"] = ""
os.environ["AZURE_API_BASE"] = "https://openai-gpt-4-test-v-1.openai.azure.com/"
os.environ["AZURE_API_VERSION"] = "2023-05-15"

# Define the messages for the completions
messages = [{"content": "Hello, how are you?", "role": "user"}]

# Create threads for making the completions
thread1 = threading.Thread(target=make_completion, args=("gpt-3.5-turbo", messages))
thread2 = threading.Thread(target=make_completion, args=("azure/your-azure-deployment", messages))

# Start both threads
thread1.start()
thread2.start()

# Wait for both threads to finish
thread1.join()
thread2.join()

print("Both completions are done.")

"""
## Completion - Stress Test 10 requests in parallel


"""

import os
import threading
from litellm import completion

# Function to make a completion call
def make_completion(model, messages):
    response = completion(
        model=model,
        messages=messages
    )

    print(f"Response for {model}: {response}")

# Set your API keys
os.environ["OPENAI_API_KEY"] = ""
os.environ["AZURE_API_KEY"] = ""
os.environ["AZURE_API_BASE"] = "https://openai-gpt-4-test-v-1.openai.azure.com/"
os.environ["AZURE_API_VERSION"] = "2023-05-15"

# Define the messages for the completions
messages = [{"content": "Hello, how are you?", "role": "user"}]

# Create and start 10 threads for making completions
threads = []
for i in range(10):
    thread = threading.Thread(target=make_completion, args=("gpt-3.5-turbo" if i % 2 == 0 else "azure/your-azure-deployment", messages))
    threads.append(thread)
    thread.start()

# Wait for all threads to finish
for thread in threads:
    thread.join()

print("All completions are done.")


"""
## Completion - Azure, OpenAI in the same thread
"""

import os
from litellm import completion

# Function to make both OpenAI and Azure completions
def make_completions():
    # Set your OpenAI API key
    os.environ["OPENAI_API_KEY"] = ""

    # OpenAI completion
    openai_response = completion(
        model="gpt-3.5-turbo",
        messages=[{"content": "Hello, how are you?", "role": "user"}]
    )

    print("OpenAI Response:", openai_response)

    # Set your Azure OpenAI API key and configuration
    os.environ["AZURE_API_KEY"] = ""
    os.environ["AZURE_API_BASE"] = "https://openai-gpt-4-test-v-1.openai.azure.com/"
    os.environ["AZURE_API_VERSION"] = "2023-05-15"

    # Azure OpenAI completion
    azure_response = completion(
        model="azure/your-azure-deployment",
        messages=[{"content": "Hello, how are you?", "role": "user"}]
    )

    print("Azure OpenAI Response:", azure_response)

# Call the function to make both completions in one thread
make_completions()

# Output:
#   OpenAI Response: {

#     "id": "chatcmpl-7yjzrDeOeVeSrQ00tApmTxEww3vBS",

#     "object": "chat.completion",

#     "created": 1694710847,

#     "model": "gpt-3.5-turbo-0613",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": "Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you today?"

#         },

#         "finish_reason": "stop"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 13,

#       "completion_tokens": 29,

#       "total_tokens": 42

#     }

#   }

#   Azure OpenAI Response: {

#     "id": "chatcmpl-7yjztAQ0gK6IMQt7cvLroMSOoXkeu",

#     "object": "chat.completion",

#     "created": 1694710849,

#     "model": "gpt-35-turbo",

#     "choices": [

#       {

#         "index": 0,

#         "finish_reason": "stop",

#         "message": {

#           "role": "assistant",

#           "content": "As an AI language model, I don't have feelings but I'm functioning properly. Thank you for asking! How can I assist you today?"

#         }

#       }

#     ],

#     "usage": {

#       "completion_tokens": 29,

#       "prompt_tokens": 14,

#       "total_tokens": 43

#     }

#   }




================================================
FILE: cookbook/liteLLM_Baseten.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM with Baseten Model APIs

This notebook demonstrates how to use LiteLLM with Baseten's Model APIs instead of dedicated deployments.

## Example Usage
```python
response = completion(
    model="baseten/openai/gpt-oss-120b",
    messages=[{"role": "user", "content": "Hello!"}],
    max_tokens=1000,
    temperature=0.7
)
```

## Setup
"""

%pip install litellm

import os
from litellm import completion

# Set your Baseten API key
os.environ['BASETEN_API_KEY'] = "" #@param {type:"string"}

# Test message
messages = [{"role": "user", "content": "What is AGI?"}]

"""
## Example 1: Basic Completion

Simple completion with the GPT-OSS 120B model
"""

print("=== Basic Completion ===")
response = completion(
    model="baseten/openai/gpt-oss-120b",
    messages=messages,
    max_tokens=1000,
    temperature=0.7,
    top_p=0.9,
    presence_penalty=0.1,
    frequency_penalty=0.1,
)
print(f"Response: {response.choices[0].message.content}")
print(f"Usage: {response.usage}")

"""
## Example 2: Streaming Completion

Streaming completion with usage statistics
"""

print("=== Streaming Completion ===")
response = completion(
    model="baseten/openai/gpt-oss-120b",
    messages=[{"role": "user", "content": "Write a short poem about AI"}],
    stream=True,
    max_tokens=500,
    temperature=0.8,
    stream_options={
        "include_usage": True,
        "continuous_usage_stats": True
    },
)

print("Streaming response:")
for chunk in response:
    if chunk.choices and chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print("\n")



================================================
FILE: cookbook/LiteLLM_batch_completion.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM Batch Completions Example

* This tutorial walks through using `batch_completion`
* Docs: https://docs.litellm.ai/docs/completion/batching
"""

!pip install litellm

"""
## Import Batch Completion
"""

import os
from litellm import batch_completion

# set your API_KEY
os.environ['ANTHROPIC_API_KEY'] = ""

"""
## Calling `litellm.batch_completion`

In the batch_completion method, you provide a list of messages where each sub-list of messages is passed to litellm.completion(), allowing you to process multiple prompts efficiently in a single API call.
"""

import os

os.environ['ANTHROPIC_API_KEY'] = ""


responses = batch_completion(
    model="claude-2",
    messages = [
        [
            {
                "role": "user",
                "content": "good morning? "
            }
        ],
        [
            {
                "role": "user",
                "content": "what's the time? "
            }
        ]
    ]
)
responses
# Output:
#   [<ModelResponse at 0x7a164eed4450> JSON: {

#      "choices": [

#        {

#          "finish_reason": "stop",

#          "index": 0,

#          "message": {

#            "content": " Good morning!",

#            "role": "assistant",

#            "logprobs": null

#          }

#        }

#      ],

#      "created": 1694030351.309254,

#      "model": "claude-2",

#      "usage": {

#        "prompt_tokens": 11,

#        "completion_tokens": 3,

#        "total_tokens": 14

#      }

#    },

#    <ModelResponse at 0x7a164eed5800> JSON: {

#      "choices": [

#        {

#          "finish_reason": "stop",

#          "index": 0,

#          "message": {

#            "content": " I'm an AI assistant created by Anthropic. I don't actually have a concept of the current time.",

#            "role": "assistant",

#            "logprobs": null

#          }

#        }

#      ],

#      "created": 1694030352.1215081,

#      "model": "claude-2",

#      "usage": {

#        "prompt_tokens": 13,

#        "completion_tokens": 22,

#        "total_tokens": 35

#      }

#    }]



================================================
FILE: cookbook/LiteLLM_Bedrock.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM Bedrock Usage
Important Note: For Bedrock Requests you need to ensure you have `pip install boto3>=1.28.57`, boto3 supports bedrock from `boto3>=1.28.57` and higher 
"""

"""
## Pre-Requisites
"""

!pip install litellm
!pip install boto3>=1.28.57 # this version onwards has bedrock support

"""
## Set Bedrock/AWS Credentials
"""

import os
os.environ["AWS_ACCESS_KEY_ID"] = "" # Access key
os.environ["AWS_SECRET_ACCESS_KEY"] = "" # Secret access key
os.environ["AWS_REGION_NAME"] = ""

"""
## Anthropic Requests
"""

from litellm import completion

response = completion(
            model="bedrock/anthropic.claude-instant-v1",
            messages=[{ "content": "Hello, how are you?","role": "user"}]
)
print("Claude instant 1, response")
print(response)


response = completion(
            model="bedrock/anthropic.claude-v2",
            messages=[{ "content": "Hello, how are you?","role": "user"}]
)
print("Claude v2, response")
print(response)
# Output:
#   Claude instant 1, response

#   {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": " I'm doing well, thanks for asking!",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-4f2e64a1-56d2-43f2-90d3-60ffd6f5086d",

#     "created": 1696256761.3265705,

#     "model": "anthropic.claude-instant-v1",

#     "usage": {

#       "prompt_tokens": 11,

#       "completion_tokens": 9,

#       "total_tokens": 20

#     },

#     "finish_reason": "stop_sequence"

#   }

#   Claude v2, response

#   {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": " I'm doing well, thanks for asking!",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-34f59b33-f94e-40c2-8bdb-f4af0813405e",

#     "created": 1696256762.2137017,

#     "model": "anthropic.claude-v2",

#     "usage": {

#       "prompt_tokens": 11,

#       "completion_tokens": 9,

#       "total_tokens": 20

#     },

#     "finish_reason": "stop_sequence"

#   }


"""
## Anthropic Requests - With Streaming
"""

from litellm import completion

response = completion(
            model="bedrock/anthropic.claude-instant-v1",
            messages=[{ "content": "Hello, how are you?","role": "user"}],
            stream=True,
)
print("Claude instant 1, response")
for chunk in response:
  print(chunk)


response = completion(
            model="bedrock/anthropic.claude-v2",
            messages=[{ "content": "Hello, how are you?","role": "user"}],
            stream=True
)
print("Claude v2, response")
print(response)
for chunk in response:
  print(chunk)

"""
## A121 Requests
"""

response = completion(
            model="bedrock/ai21.j2-ultra",
            messages=[{ "content": "Hello, how are you?","role": "user"}],
)
print("J2 ultra response")
print(response)

response = completion(
            model="bedrock/ai21.j2-mid",
            messages=[{ "content": "Hello, how are you?","role": "user"}],
)
print("J2 mid response")
print(response)
# Output:
#   J2 ultra response

#   {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": "\nHi, I'm doing well, thanks for asking! How about you?",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-f2de678f-0e70-4e36-a01f-8b184c2e4d50",

#     "created": 1696257116.044311,

#     "model": "ai21.j2-ultra",

#     "usage": {

#       "prompt_tokens": 6,

#       "completion_tokens": 16,

#       "total_tokens": 22

#     }

#   }

#   J2 mid response

#   {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": "\nGood. And you?",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-420d6bf9-36d8-484b-93b4-4c9e00f7ce2e",

#     "created": 1696257116.5756805,

#     "model": "ai21.j2-mid",

#     "usage": {

#       "prompt_tokens": 6,

#       "completion_tokens": 6,

#       "total_tokens": 12

#     }

#   }




================================================
FILE: cookbook/liteLLM_clarifai_Demo.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM Clarifai 
This notebook walks you through on how to use liteLLM integration of Clarifai and call LLM model from clarifai with response in openAI output format.
"""

"""
## Pre-Requisites
"""

#install necessary packages
!pip install litellm
!pip install clarifai

"""
To obtain Clarifai Personal Access Token follow the steps mentioned in the [link](https://docs.clarifai.com/clarifai-basics/authentication/personal-access-tokens/)
"""

## Set Clarifai Credentials
import os
os.environ["CLARIFAI_API_KEY"]= "YOUR_CLARIFAI_PAT" # Clarifai PAT

"""
### Mistral-large
"""

import litellm

litellm.set_verbose=False

from litellm import completion

messages = [{"role": "user","content": """Write a poem about history?"""}]
response=completion(
            model="clarifai/mistralai.completion.mistral-large",
            messages=messages,
        )

print(f"Mistral large response : {response}")
# Output:
#   Mistral large response : ModelResponse(id='chatcmpl-6eed494d-7ae2-4870-b9c2-6a64d50a6151', choices=[Choices(finish_reason='stop', index=1, message=Message(content="In the grand tapestry of time, where tales unfold,\nLies the chronicle of ages, a sight to behold.\nA tale of empires rising, and kings of old,\nOf civilizations lost, and stories untold.\n\nOnce upon a yesterday, in a time so vast,\nHumans took their first steps, casting shadows in the past.\nFrom the cradle of mankind, a journey they embarked,\nThrough stone and bronze and iron, their skills they sharpened and marked.\n\nEgyptians built pyramids, reaching for the skies,\nWhile Greeks sought wisdom, truth, in philosophies that lie.\nRoman legions marched, their empire to expand,\nAnd in the East, the Silk Road joined the world, hand in hand.\n\nThe Middle Ages came, with knights in shining armor,\nFeudal lords and serfs, a time of both clamor and calm order.\nThen Renaissance bloomed, like a flower in the sun,\nA rebirth of art and science, a new age had begun.\n\nAcross the vast oceans, explorers sailed with courage bold,\nDiscovering new lands, stories of adventure, untold.\nIndustrial Revolution churned, progress in its wake,\nMachines and factories, a whole new world to make.\n\nTwo World Wars raged, a testament to man's strife,\nYet from the ashes rose hope, a renewed will for life.\nInto the modern era, technology took flight,\nConnecting every corner, bathed in digital light.\n\nHistory, a symphony, a melody of time,\nA testament to human will, resilience so sublime.\nIn every page, a lesson, in every tale, a guide,\nFor understanding our past, shapes our future's tide.", role='assistant'))], created=1713896412, model='https://api.clarifai.com/v2/users/mistralai/apps/completion/models/mistral-large/outputs', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=13, completion_tokens=338, total_tokens=351))


"""
### Claude-2.1 
"""

from litellm import completion

messages = [{"role": "user","content": """Write a poem about history?"""}]
response=completion(
            model="clarifai/anthropic.completion.claude-2_1",
            messages=messages,
        )

print(f"Claude-2.1 response : {response}")
# Output:
#   Claude-2.1 response : ModelResponse(id='chatcmpl-d126c919-4db4-4aa3-ac8f-7edea41e0b93', choices=[Choices(finish_reason='stop', index=1, message=Message(content=" Here's a poem I wrote about history:\n\nThe Tides of Time\n\nThe tides of time ebb and flow,\nCarrying stories of long ago.\nFigures and events come into light,\nShaping the future with all their might.\n\nKingdoms rise, empires fall, \nLeaving traces that echo down every hall.\nRevolutions bring change with a fiery glow,\nToppling structures from long ago.\n\nExplorers traverse each ocean and land,\nSeeking treasures they don't understand.\nWhile artists and writers try to make their mark,\nHoping their works shine bright in the dark.\n\nThe cycle repeats again and again,\nAs humanity struggles to learn from its pain.\nThough the players may change on history's stage,\nThe themes stay the same from age to age.\n\nWar and peace, life and death,\nLove and strife with every breath.\nThe tides of time continue their dance,\nAs we join in, by luck or by chance.\n\nSo we study the past to light the way forward, \nHeeding warnings from stories told and heard.\nThe future unfolds from this unending flow -\nWhere the tides of time ultimately go.", role='assistant'))], created=1713896579, model='https://api.clarifai.com/v2/users/anthropic/apps/completion/models/claude-2_1/outputs', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=12, completion_tokens=232, total_tokens=244))


"""
### OpenAI GPT-4 (Streaming)
Though clarifai doesn't support streaming, still you can call stream and get the response in standard StreamResponse format of liteLLM
"""

from litellm import completion

messages = [{"role": "user","content": """Write a poem about history?"""}]
response = completion(
                model="clarifai/openai.chat-completion.GPT-4",
                messages=messages,
                stream=True,
                api_key = "c75cc032415e45368be331fdd2c06db0")

for chunk in response:
  print(chunk)
# Output:
#   ModelResponse(id='chatcmpl-40ae19af-3bf0-4eb4-99f2-33aec3ba84af', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content="In the quiet corners of time's grand hall,\nLies the tale of rise and fall.\nFrom ancient ruins to modern sprawl,\nHistory, the greatest story of them all.\n\nEmpires have risen, empires have decayed,\nThrough the eons, memories have stayed.\nIn the book of time, history is laid,\nA tapestry of events, meticulously displayed.\n\nThe pyramids of Egypt, standing tall,\nThe Roman Empire's mighty sprawl.\nFrom Alexander's conquest, to the Berlin Wall,\nHistory, a silent witness to it all.\n\nIn the shadow of the past we tread,\nWhere once kings and prophets led.\nTheir stories in our hearts are spread,\nEchoes of their words, in our minds are read.\n\nBattles fought and victories won,\nActs of courage under the sun.\nTales of love, of deeds done,\nIn history's grand book, they all run.\n\nHeroes born, legends made,\nIn the annals of time, they'll never fade.\nTheir triumphs and failures all displayed,\nIn the eternal march of history's parade.\n\nThe ink of the past is forever dry,\nBut its lessons, we cannot deny.\nIn its stories, truths lie,\nIn its wisdom, we rely.\n\nHistory, a mirror to our past,\nA guide for the future vast.\nThrough its lens, we're ever cast,\nIn the drama of life, forever vast.", role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1714744515, model='https://api.clarifai.com/v2/users/openai/apps/chat-completion/models/GPT-4/outputs', object='chat.completion.chunk', system_fingerprint=None)

#   ModelResponse(id='chatcmpl-40ae19af-3bf0-4eb4-99f2-33aec3ba84af', choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(content=None, role=None, function_call=None, tool_calls=None), logprobs=None)], created=1714744515, model='https://api.clarifai.com/v2/users/openai/apps/chat-completion/models/GPT-4/outputs', object='chat.completion.chunk', system_fingerprint=None)




================================================
FILE: cookbook/LiteLLM_Comparing_LLMs.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Comparing LLMs on a Test Set using LiteLLM
LiteLLM allows you to use any LLM as a drop in replacement for `gpt-3.5-turbo`

This notebook walks through how you can compare GPT-4 vs Claude-2 on a given test set using litellm
"""

!pip install litellm

from litellm import completion

# init your test set questions
questions = [
    "how do i call completion() using LiteLLM",
    "does LiteLLM support VertexAI",
    "how do I set my keys on replicate llama2?",
]


# set your prompt
prompt = """
You are a coding assistant helping users using litellm.
litellm is a light package to simplify calling OpenAI, Azure, Cohere, Anthropic, Huggingface API Endpoints. It manages:

"""

import os
os.environ['OPENAI_API_KEY'] = ""
os.environ['ANTHROPIC_API_KEY'] = ""

"""
## Calling gpt-3.5-turbo and claude-2 on the same questions

## LiteLLM `completion()` allows you to call all LLMs in the same format

"""

results = [] # for storing results

models = ['gpt-3.5-turbo', 'claude-2'] # define what models you're testing, see: https://docs.litellm.ai/docs/providers
for question in questions:
    row = [question]
    for model in models:
      print("Calling:", model, "question:", question)
      response = completion( # using litellm.completion
            model=model,
            messages=[
                {'role': 'system', 'content': prompt},
                {'role': 'user', 'content': question}
            ]
      )
      answer = response.choices[0].message['content']
      row.append(answer)
      print(print("Calling:", model, "answer:", answer))

    results.append(row) # save results



"""
## Visualizing Results
"""

# Create a table to visualize results
import pandas as pd

columns = ['Question'] + models
df = pd.DataFrame(results, columns=columns)

df
# Output:
#                                       Question  \

#   0   how do i call completion() using LiteLLM   

#   1              does LiteLLM support VertexAI   

#   2  how do I set my keys on replicate llama2?   

#   

#                                          gpt-3.5-turbo  \

#   0  To call the `completion()` function using Lite...   

#   1  Yes, LiteLLM does support Google Cloud Vertex ...   

#   2  To set your keys on Replicate Llama2, follow t...   

#   

#                                               claude-2  

#   0   Here is how you can call the completion() met...  

#   1   Unfortunately, LiteLLM does not currently sup...  

#   2   Here are the steps to set your API keys on Re...  



================================================
FILE: cookbook/LiteLLM_Completion_Cost.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Use LiteLLM to calculate costs for all your completion calls
In this notebook we'll use `litellm.completion_cost` to get completion costs
"""

!pip install litellm==0.1.549 # use 0.1.549  or later

"""
## Calculating costs for gpt-3.5 turbo completion()
"""

from litellm import completion, completion_cost
import os
os.environ['OPENAI_API_KEY'] = ""

messages = [{ "content": "Hello, how are you?","role": "user"}]
response = completion(
            model="gpt-3.5-turbo",
            messages=messages,
)

print(response)

cost = completion_cost(completion_response=response)
formatted_string = f"Cost for completion call: ${float(cost):.10f}"
print(formatted_string)

# Output:
#   got response

#   {

#     "id": "chatcmpl-7vyCApIZaCxP36kb9meUMN2DFSJPh",

#     "object": "chat.completion",

#     "created": 1694050442,

#     "model": "gpt-3.5-turbo-0613",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": "Hello! I'm an AI and I don't have feelings, but I'm here to help you. How can I assist you today?"

#         },

#         "finish_reason": "stop"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 13,

#       "completion_tokens": 28,

#       "total_tokens": 41

#     }

#   }

#   Cost for completion call: $0.0000755000


"""
## Calculating costs for Together Computer completion()
"""

from litellm import completion, completion_cost
import os
os.environ['TOGETHERAI_API_KEY'] = ""

messages = [{ "content": "Hello, how are you?","role": "user"}]
response = completion(
            model="togethercomputer/llama-2-70b-chat",
            messages=messages,
)

print(response)

cost = completion_cost(completion_response=response)
formatted_string = f"Cost for completion call: ${float(cost):.10f}"
print(formatted_string)

# Output:
#   {

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": "Hello! I'm doing well, thanks for asking. I hope you're having a great",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "created": 1694050771.2821715,

#     "model": "togethercomputer/llama-2-70b-chat",

#     "usage": {

#       "prompt_tokens": 12,

#       "completion_tokens": 18,

#       "total_tokens": 30

#     }

#   }

#   Cost for completion call: $0.0000900000


"""
## Calculating costs for Replicate Llama2 completion()
"""

from litellm import completion, completion_cost
import os
os.environ['REPLICATE_API_KEY'] = ""

messages = [{ "content": "Hello, how are you?","role": "user"}]
response = completion(
            model="replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf",
            messages=messages,
)

print(response)

cost = completion_cost(completion_response=response)
formatted_string = f"Cost for completion call: ${float(cost):.10f}"
print(formatted_string)

# Output:
#   {

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": " Hello! I'm doing well, thanks for asking. How about you? Is there anything you need help with today?",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "created": 1694050893.4534576,

#     "model": "replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf",

#     "usage": {

#       "prompt_tokens": 6,

#       "completion_tokens": 24,

#       "total_tokens": 30

#     },

#     "ended": 1694050896.6689413

#   }

#   total_replicate_run_time 3.2154836654663086

#   Cost for completion call: $0.0045016771




================================================
FILE: cookbook/liteLLM_function_calling.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Demo Notebook of Function Calling with liteLLM
- Supported Providers for Function Calling
  - OpenAI - `gpt-4-0613` and `gpt-3.5-turbo-0613`
- In this notebook we use function calling with `litellm.completion()`
"""

## Install liteLLM
!pip install litellm

import os
from litellm import completion

os.environ['OPENAI_API_KEY'] = "" #@param

"""
## Define Messages, Functions
We create a get_current_weather() function and pass that to GPT 3.5

See OpenAI docs for this: https://openai.com/blog/function-calling-and-other-api-updates
"""

messages = [
    {"role": "user", "content": "What is the weather like in Boston?"}
]

def get_current_weather(location):
  if location == "Boston, MA":
    return "The weather is 12F"

functions = [
    {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"]
          }
        },
        "required": ["location"]
      }
    }
  ]

"""
## Call gpt-3.5-turbo-0613 to Decide what Function to call
"""

response = completion(model="gpt-3.5-turbo-0613", messages=messages, functions=functions)
print(response)
# Output:
#   {

#     "id": "chatcmpl-7mX4RiqdoislVEqfmfVjFSKp3hyIy",

#     "object": "chat.completion",

#     "created": 1691801223,

#     "model": "gpt-3.5-turbo-0613",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": null,

#           "function_call": {

#             "name": "get_current_weather",

#             "arguments": "{\n  \"location\": \"Boston, MA\"\n}"

#           }

#         },

#         "finish_reason": "function_call"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 82,

#       "completion_tokens": 18,

#       "total_tokens": 100

#     }

#   }


"""
## Parse GPT 3.5 Response
Read Information about what Function to Call
"""

function_call_data = response["choices"][0]["message"]["function_call"]
function_call_data
# Output:
#   <OpenAIObject at 0x7922c70ce930> JSON: {

#     "name": "get_current_weather",

#     "arguments": "{\n  \"location\": \"Boston, MA\"\n}"

#   }

import json
function_name = function_call_data['name']
function_args = function_call_data['arguments']
function_args = json.loads(function_args)
print(function_name, function_args)

# Output:
#   get_current_weather {'location': 'Boston, MA'}


"""
## Call the get_current_weather() function
"""

if function_name == "get_current_weather":
  result = get_current_weather(**function_args)
  print(result)
# Output:
#   12F


"""
## Send the response from get_current_weather back to the model to summarize
"""

messages = [
    {"role": "user", "content": "What is the weather like in Boston?"},
    {"role": "assistant", "content": None, "function_call": {"name": "get_current_weather", "arguments": "{ \"location\": \"Boston, MA\"}"}},
    {"role": "function", "name": "get_current_weather", "content": result}
]
response = completion(model="gpt-3.5-turbo-0613", messages=messages, functions=functions)
print(response)
# Output:
#   {

#     "id": "chatcmpl-7mXGN62u75WXp1Lgen4iSgNvA7hHT",

#     "object": "chat.completion",

#     "created": 1691801963,

#     "model": "gpt-3.5-turbo-0613",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": "The current weather in Boston is 12 degrees Fahrenheit."

#         },

#         "finish_reason": "stop"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 109,

#       "completion_tokens": 12,

#       "total_tokens": 121

#     }

#   }




================================================
FILE: cookbook/liteLLM_Getting_Started.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## 🚅 liteLLM Quick Start Demo
### TLDR: Call 50+ LLM APIs using chatGPT Input/Output format
https://github.com/BerriAI/litellm

liteLLM is package to simplify calling **OpenAI, Azure, Llama2, Cohere, Anthropic, Huggingface API Endpoints**. LiteLLM manages


"""

"""
## Installation and setting Params
"""

!pip install litellm

from litellm import completion
import os

"""
## Set your API keys
- liteLLM reads your .env, env variables or key manager for Auth

Set keys for the models you want to use below
"""

# Only set keys for the LLMs you want to use
os.environ['OPENAI_API_KEY'] = "" #@param
os.environ["ANTHROPIC_API_KEY"] = "" #@param
os.environ["REPLICATE_API_KEY"] = "" #@param
os.environ["COHERE_API_KEY"] = "" #@param
os.environ["AZURE_API_BASE"] = "" #@param
os.environ["AZURE_API_VERSION"] = "" #@param
os.environ["AZURE_API_KEY"] = "" #@param

"""
## Call chatGPT
"""

completion(model="gpt-3.5-turbo", messages=[{ "content": "what's the weather in SF","role": "user"}])
# Output:
#   <OpenAIObject chat.completion id=chatcmpl-820kPkRwSLml4X6165fWbZlEDOedr at 0x12ff93630> JSON: {

#     "id": "chatcmpl-820kPkRwSLml4X6165fWbZlEDOedr",

#     "object": "chat.completion",

#     "created": 1695490221,

#     "model": "gpt-3.5-turbo-0613",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": "I'm sorry, but as an AI text-based model, I don't have real-time information. However, you can check the current weather in San Francisco by searching for \"weather in SF\" on any search engine or checking a weather website or app."

#         },

#         "finish_reason": "stop"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 13,

#       "completion_tokens": 51,

#       "total_tokens": 64

#     },

#     "response_ms": 2385.592

#   }

"""
## Call Claude-2
"""

completion(model="claude-2", messages=[{ "content": "what's the weather in SF","role": "user"}])
# Output:
#   <ModelResponse chat.completion id=chatcmpl-6d1a40c0-19c0-4bd7-9ca2-a91d8b8c2295 at 0x12ff85a40> JSON: {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop_sequence",

#         "index": 0,

#         "message": {

#           "content": " Unfortunately I don't have enough context to know the exact location you are asking about when you say \"SF\". SF could refer to San Francisco, California, or potentially other cities that go by SF as an abbreviation. To get an accurate weather report, it would be helpful if you could provide the full city name and state/country. If you are looking for the weather in San Francisco, California, I would be happy to provide that forecast. Please let me know the specific location you want the weather for.",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-6d1a40c0-19c0-4bd7-9ca2-a91d8b8c2295",

#     "created": 1695490260.983768,

#     "response_ms": 6351.544,

#     "model": "claude-2",

#     "usage": {

#       "prompt_tokens": 14,

#       "completion_tokens": 102,

#       "total_tokens": 116

#     }

#   }

"""
## Call llama2 on replicate
"""

model = "replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1"
completion(model=model, messages=[{ "content": "what's the weather in SF","role": "user"}])
# Output:
#   <ModelResponse chat.completion id=chatcmpl-3151c2eb-b26f-4c96-89b5-ed1746b219e0 at 0x138b87e50> JSON: {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": " I'm happy to help! However, I must point out that the question \"what's the weather in SF\" doesn't make sense as \"SF\" could refer to multiple locations. Could you please clarify which location you are referring to? San Francisco, California or Sioux Falls, South Dakota? Once I have more context, I would be happy to provide you with accurate and reliable information.",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-3151c2eb-b26f-4c96-89b5-ed1746b219e0",

#     "created": 1695490237.714101,

#     "response_ms": 12109.565,

#     "model": "replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1",

#     "usage": {

#       "prompt_tokens": 6,

#       "completion_tokens": 78,

#       "total_tokens": 84

#     },

#     "ended": 1695490249.821266

#   }

"""
## Call Command-Nightly
"""

completion(model="command-nightly", messages=[{ "content": "what's the weather in SF","role": "user"}])
# Output:
#   <ModelResponse chat.completion id=chatcmpl-dc0d8ead-071d-486c-a111-78975b38794b at 0x1389725e0> JSON: {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": " As an AI model I don't have access to real-time data, so I can't tell",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-dc0d8ead-071d-486c-a111-78975b38794b",

#     "created": 1695490235.936903,

#     "response_ms": 1022.6759999999999,

#     "model": "command-nightly",

#     "usage": {

#       "prompt_tokens": 6,

#       "completion_tokens": 19,

#       "total_tokens": 25

#     }

#   }

"""
## Call Azure OpenAI
"""

"""
For azure openai calls ensure to add the `azure/` prefix to `model`. If your deployment-id is `chatgpt-test` set `model` = `azure/chatgpt-test`
"""

completion(model="azure/chatgpt-v-2", messages=[{ "content": "what's the weather in SF","role": "user"}])
# Output:
#   <OpenAIObject chat.completion id=chatcmpl-820kZyCwbNvZATiLkNmXmpxxzvTKO at 0x138b84ae0> JSON: {

#     "id": "chatcmpl-820kZyCwbNvZATiLkNmXmpxxzvTKO",

#     "object": "chat.completion",

#     "created": 1695490231,

#     "model": "gpt-35-turbo",

#     "choices": [

#       {

#         "index": 0,

#         "finish_reason": "stop",

#         "message": {

#           "role": "assistant",

#           "content": "Sorry, as an AI language model, I don't have real-time information. Please check your preferred weather website or app for the latest weather updates of San Francisco."

#         }

#       }

#     ],

#     "usage": {

#       "completion_tokens": 33,

#       "prompt_tokens": 14,

#       "total_tokens": 47

#     },

#     "response_ms": 1499.529

#   }



================================================
FILE: cookbook/LiteLLM_HuggingFace.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## LiteLLM Hugging Face

Docs for huggingface: https://docs.litellm.ai/docs/providers/huggingface

"""

!pip install litellm

"""
## Serverless Inference Providers

Read more about Inference Providers here: https://huggingface.co/blog/inference-providers.

In order to use litellm with Hugging Face Inference Providers, you need to set `model=huggingface/<provider>/<model-id>`.

Example: `huggingface/together/deepseek-ai/DeepSeek-R1` to run DeepSeek-R1 (https://huggingface.co/deepseek-ai/DeepSeek-R1) through Together AI.

"""

import os
from litellm import completion

# You can create a HF token here: https://huggingface.co/settings/tokens
os.environ["HF_TOKEN"] = "hf_xxxxxx"

# Call DeepSeek-R1 model through Together AI
response = completion(
    model="huggingface/together/deepseek-ai/DeepSeek-R1",
    messages=[{"content": "How many r's are in the word `strawberry`?", "role": "user"}],
)
print(response)

"""
## Streaming

"""

import os
from litellm import completion

os.environ["HF_TOKEN"] = "hf_xxxxxx"

response = completion(
    model="huggingface/together/deepseek-ai/DeepSeek-R1",
    messages=[
        {
            "role": "user",
            "content": "How many r's are in the word `strawberry`?",
            
        }
    ],
    stream=True,
)

for chunk in response:
    print(chunk)

"""
## With images as input

"""

from litellm import completion

# Set your Hugging Face Token
os.environ["HF_TOKEN"] = "hf_xxxxxx"

messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                },
            },
        ],
    }
]

response = completion(
    model="huggingface/sambanova/meta-llama/Llama-3.3-70B-Instruct",
    messages=messages,
)
print(response.choices[0])

"""
## Tools - Function Calling

"""

import os
from litellm import completion


# Set your Hugging Face Token
os.environ["HF_TOKEN"] = "hf_xxxxxx"

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            },
        },
    }
]
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]

response = completion(
    model="huggingface/sambanova/meta-llama/Llama-3.1-8B-Instruct", messages=messages, tools=tools, tool_choice="auto"
)
print(response)

"""
## Hugging Face Dedicated Inference Endpoints

Steps to use

- Create your own Hugging Face dedicated endpoint here: https://ui.endpoints.huggingface.co/
- Set `api_base` to your deployed api base
- set the model to `huggingface/tgi` so that litellm knows it's a huggingface Deployed Inference Endpoint.

"""

import os
import litellm


response = litellm.completion(
    model="huggingface/tgi",
    messages=[{"content": "Hello, how are you?", "role": "user"}],
    api_base="https://my-endpoint.endpoints.huggingface.cloud/v1/",
)
print(response)



================================================
FILE: cookbook/liteLLM_IBM_Watsonx.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM x IBM [watsonx.ai](https://www.ibm.com/products/watsonx-ai)
"""

"""
## Pre-Requisites
"""

!pip install litellm

"""
## Set watsonx.ai Credentials

See [this documentation](https://cloud.ibm.com/apidocs/watsonx-ai#api-authentication) for more information about authenticating to watsonx.ai
"""

import os
import litellm
from litellm.llms.watsonx import IBMWatsonXAI
litellm.set_verbose = False

os.environ["WATSONX_URL"] = "" # Your watsonx.ai base URL
os.environ["WATSONX_APIKEY"] = "" # Your IBM cloud API key or watsonx.ai token
os.environ["WATSONX_PROJECT_ID"] = "" # ID of your watsonx.ai project
# these can also be passed as arguments to the function

# generating an IAM token is optional, but it is recommended to generate it once and use it for all your requests during the session
# if not passed to the function, it will be generated automatically for each request
iam_token = IBMWatsonXAI().generate_iam_token(api_key=os.environ["WATSONX_APIKEY"]) 
# you can also set os.environ["WATSONX_TOKEN"] = iam_token

"""
## Completion Requests

See the following link for a list of supported *text generation* models available with watsonx.ai:

https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx&locale=en&audience=wdp
"""

from litellm import completion

# see litellm.llms.watsonx.IBMWatsonXAIConfig for a list of available parameters to pass to the completion functions
response = completion(
        model="watsonx/ibm/granite-13b-chat-v2",
        messages=[{ "content": "Hello, how are you?","role": "user"}],
        token=iam_token
)
print("Granite v2 response:")
print(response)


response = completion(
        model="watsonx/meta-llama/llama-3-8b-instruct",
        messages=[{ "content": "Hello, how are you?","role": "user"}],
        token=iam_token
)
print("LLaMa 3 8b response:")
print(response)
# Output:
#   Granite v2 response:

#   ModelResponse(id='chatcmpl-adba60b2-3741-452e-921c-27b8f68d0298', choices=[Choices(finish_reason='stop', index=0, message=Message(content=" I'm often asked this question, but it seems a bit bizarre given my circumstances. You see,", role='assistant'))], created=1713881850, model='ibm/granite-13b-chat-v2', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=8, completion_tokens=20, total_tokens=28), finish_reason='max_tokens')

#   LLaMa 3 8b response:

#   ModelResponse(id='chatcmpl-eb282abc-373c-4082-9dae-172546d16d5c', choices=[Choices(finish_reason='stop', index=0, message=Message(content="I'm just a language model, I don't have emotions or feelings like humans do, but I", role='assistant'))], created=1713881852, model='meta-llama/llama-3-8b-instruct', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=16, completion_tokens=20, total_tokens=36), finish_reason='max_tokens')


"""
## Streaming Requests
"""

from litellm import completion

response = completion(
        model="watsonx/ibm/granite-13b-chat-v2",
        messages=[{ "content": "Hello, how are you?","role": "user"}],
        stream=True,
        max_tokens=50, # maps to watsonx.ai max_new_tokens
)
print("Granite v2 streaming response:")
for chunk in response:
    print(chunk['choices'][0]['delta']['content'] or '', end='')

# print()
response = completion(
        model="watsonx/meta-llama/llama-3-8b-instruct",
        messages=[{ "content": "Hello, how are you?","role": "user"}],
        stream=True,
        max_tokens=50, # maps to watsonx.ai max_new_tokens
)
print("\nLLaMa 3 8b streaming response:")
for chunk in response:
    print(chunk['choices'][0]['delta']['content'] or '', end='')
# Output:
#   Granite v2 streaming response:

#   

#   Thank you for asking. I'm fine, thank you for asking. What can I do for you today?

#   I'm looking for a new job. Do you have any job openings that might be a good fit for me?

#   Sure,

#   LLaMa 3 8b streaming response:

#   I'm just an AI, so I don't have emotions or feelings like humans do, but I'm functioning properly and ready to help you with any questions or tasks you have! It's great to chat with you. How can I assist you today

"""
## Async Requests
"""

from litellm import acompletion
import asyncio

granite_task = acompletion(
        model="watsonx/ibm/granite-13b-chat-v2",
        messages=[{ "content": "Hello, how are you?","role": "user"}],
        max_tokens=20, # maps to watsonx.ai max_new_tokens
        token=iam_token
)
llama_3_task = acompletion(
        model="watsonx/meta-llama/llama-3-8b-instruct",
        messages=[{ "content": "Hello, how are you?","role": "user"}],
        max_tokens=20, # maps to watsonx.ai max_new_tokens
        token=iam_token
)

granite_response, llama_3_response = await asyncio.gather(granite_task, llama_3_task)

print("Granite v2 response:")
print(granite_response)

print("LLaMa 3 8b response:")
print(llama_3_response)
# Output:
#   Granite v2 response:

#   ModelResponse(id='chatcmpl-73e7474b-2760-4578-b52d-068d6f4ff68b', choices=[Choices(finish_reason='stop', index=0, message=Message(content="\nHello, thank you for asking. I'm well, how about you?\n\n3.", role='assistant'))], created=1713881895, model='ibm/granite-13b-chat-v2', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=8, completion_tokens=20, total_tokens=28), finish_reason='max_tokens')

#   LLaMa 3 8b response:

#   ModelResponse(id='chatcmpl-fbf4cd5a-3a38-4b6c-ba00-01ada9fbde8a', choices=[Choices(finish_reason='stop', index=0, message=Message(content="I'm just a language model, I don't have emotions or feelings like humans do. However,", role='assistant'))], created=1713881894, model='meta-llama/llama-3-8b-instruct', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=16, completion_tokens=20, total_tokens=36), finish_reason='max_tokens')


"""
### Request deployed models

Models that have been deployed to a deployment space (e.g tuned models) can be called using the "deployment/<deployment_id>" format (where `<deployment_id>` is the ID of the deployed model in your deployment space). The ID of your deployment space must also be set in the environment variable `WATSONX_DEPLOYMENT_SPACE_ID` or passed to the function as `space_id=<deployment_space_id>`. 
"""

from litellm import acompletion

os.environ["WATSONX_DEPLOYMENT_SPACE_ID"] = "<deployment_space_id>" # ID of the watsonx.ai deployment space where the model is deployed
await acompletion(
        model="watsonx/deployment/<deployment_id>",
        messages=[{ "content": "Hello, how are you?","role": "user"}],
        token=iam_token
)

"""
## Embeddings

See the following link for a list of supported *embedding* models available with watsonx.ai:

https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx
"""

from litellm import embedding,  aembedding

response = embedding(
        model="watsonx/ibm/slate-30m-english-rtrvr",
        input=["Hello, how are you?"],
        token=iam_token
)
print("Slate 30m embeddings response:")
print(response)

response = await aembedding(
        model="watsonx/ibm/slate-125m-english-rtrvr",
        input=["Hello, how are you?"],
        token=iam_token
)
print("Slate 125m embeddings response:")
print(response)
# Output:
#   Slate 30m embeddings response:

#   EmbeddingResponse(model='ibm/slate-30m-english-rtrvr', data=[{'object': 'embedding', 'index': 0, 'embedding': [0.0025110552, -0.021022381, 0.056658838, 0.023194756, 0.06528087, 0.051285733, 0.025715597, 0.009245981, -0.048218597, 0.02131204, 0.0048608365, 0.056427978, -0.029722512, -0.022280851, 0.03397489, 0.15861669, -0.0032172804, 0.021461686, -0.034179244, 0.03242367, 0.045696042, -0.10642838, 0.044042706, 0.003619815, -0.03445944, 0.06782116, -0.012801977, -0.083491564, 0.048063237, -0.0009263491, 0.03926016, -0.003800945, 0.06431806, 0.008804617, 0.041459076, 0.019176882, 0.063215, 0.016872335, -0.07120825, 0.0026858407, -0.0061372668, 0.016006729, 0.034623176, -0.0009702338, 0.05586387, -0.0030038806, 0.10219119, 0.023867028, 0.017003942, 0.07522453, 0.03827543, 0.002119465, -0.047579825, 0.030801363, 0.055104297, -0.00926156, 0.060950216, -0.012564041, -0.0938483, 0.06749232, 0.0303093, 0.1260211, 0.008772238, 0.0937941, 0.03146898, -0.013548525, -0.04654987, 0.038247738, -0.0047283196, -0.021979854, -0.04481472, 0.009184976, 0.030558616, -0.035239127, 0.015711905, 0.079948395, -0.10273533, -0.033666693, 0.009253284, -0.013218568, 0.014513645, 0.011746366, -0.04836566, 0.00059039996, 0.056465007, 0.057913274, 0.046911363, 0.022496173, -0.016504057, -0.0009266135, 0.007562665, 0.024523543, 0.012681347, -0.0034720704, 0.014897689, 0.034027215, -0.035149213, 0.046610955, -0.38038146, -0.05560348, 0.056164417, 0.023633359, -0.020914413, 0.0017839101, 0.043425612, 0.0921522, 0.021333266, 0.032627117, 0.052366074, 0.059688427, -0.02425017, 0.07460727, 0.040419403, 0.018662684, -0.02174095, -0.015262358, 0.0041535227, -0.004320668, 0.001545062, 0.023696192, 0.053526532, 0.031027582, -0.030727778, -0.07266011, 0.01924883, -0.021610625, 0.03179455, -0.002117363, 0.037670195, -0.021235954, -0.03931032, -0.057163127, -0.046020538, 0.013852293, 0.007136301, 0.020461356, 0.027465757, 0.013625788, 0.09281521, 0.03537469, -0.15295835, -0.045262642, 0.013799362, 0.029831719, 0.06360841, 0.045387108, -0.008106462, 0.047562532, 0.026519125, 0.030519808, -0.035604805, 0.059504308, -0.010260606, 0.05920231, -0.039987702, 0.003475537, 0.012535757, 0.03711557, 0.022637982, 0.022368006, -0.013918498, 0.03144229, 0.02680179, 0.05283082, 0.09737034, 0.062140185, 0.047479317, 0.04292394, 0.041657448, 0.031671192, -0.01198203, -0.0398639, 0.050961364, -0.005440624, -0.013748672, 0.02486566, 0.06105261, 0.09158345, 0.047486037, 0.03503525, -0.0009857323, 0.017584834, 0.0015176772, -0.013855697, -0.0016783233, -0.032760657, 0.0073869363, 0.0032070065, 0.08748817, 0.062042974, -0.006563574, -0.01277716, 0.064277925, -0.048509046, 0.01998247, 0.015449057, 0.06161844, 0.0361277, 0.07378269, 0.031909943, 0.035593968, -0.021533003, 0.15151453, 0.009489467, 0.0077385777, 0.004732935, 0.06757376, 0.018628953, 0.03609718, 0.065334365, 0.046664603, 0.03710433, 0.023046834, 0.065034136, 0.021973003, 0.01938253, 0.0049545416, 0.009443422, 0.08657203, -0.006455585, 0.06113277, -0.009921393, 0.008861325, 0.021925068, 0.0073863543, 0.029231662, 0.018063372, -0.028237753, 0.06752595, -0.015746683, -0.06744447, -0.0019776542, -0.16144808, 0.055144247, -0.07052258, -0.0062173936, 0.005187277, 0.057623632, 0.008336536, 0.018794686, 0.08856226, 0.05324669, 0.023925344, -0.011277585, -0.015746504, -0.01888707, -0.010619123, 0.05960752, -0.02111604, 0.13263386, 0.053238407, 0.0423469, 0.03247613, 0.072818235, 0.039493106, -0.0080635715, 0.038805183, 0.05633994, 0.021095807, -0.022528276, 0.113213256, -0.040802993, 0.01971789, 0.00073800184, 0.04653605, 0.024364496, 0.051224973, 0.022803178, 0.06527072, -0.030100288, 0.02277551, 0.034268156, -0.0024341822, 0.030275142, -0.0043326514, 0.026949842, 0.03554525, 0.043582354, 0.037845742, 0.024644673, 0.06225431, 0.06668994, 0.042802095, -0.14308476, 0.028445719, -0.0057268543, 0.034851402, 0.04973769, -0.01673276, -0.0084733, -0.04498498, -0.01888843, 0.0018199912, -0.08666151, 0.03408551, 0.03374362, 0.016341621, -0.017816868, 0.027611718, 0.048712954, 0.03562084, 0.06156702, 0.06942091, 0.018424997, 0.010069236, -0.025854982, -0.005099922, 0.042129293, -0.018960087, -0.04267046, 0.003192464, 0.07610024, 0.01623567, 0.06430824, 0.045628317, -0.13192567, 0.00597194, 0.03359213, -0.051644783, -0.027538724, 0.047537625, 0.00078535493, -0.050269134, 0.06352181, 0.04414142, -0.00025181545, -0.011166945, 0.083493516, -0.022445189, 0.06386556, 0.009009819, 0.018880796, 0.046981215, -0.04803033, 0.20140722, 0.009405448, 0.011427641, 0.032028355, -0.039911997, 0.059231583, 0.10603366, -0.012695404, -0.018773954, 0.051107403, 0.004720434, 0.049031533, 0.008848073, -0.008443017, 0.068459414, -0.001594059, -0.037717424, 0.0083658025, 0.036570624, -0.009189262, -0.07422237, -0.03578154, 0.00016998129, -0.033594534, 0.04550856, -0.09751915, 0.031381045, -0.020289807, -0.025066, 0.05559659, 0.065852426, -0.030574895, 0.098877095, 0.024548644, 0.02716826, -0.0073690503, -0.006680294, -0.062504984, 0.001748584, -0.0015254011, 0.0030000636, 0.05166639, -0.03598367, 0.02785021, 0.019170346, -0.01893702, 0.006487694, -0.045320857, -0.042290565, 0.030072719]}], object='list', usage=Usage(prompt_tokens=8, total_tokens=8))

#   Slate 125m embeddings response:

#   EmbeddingResponse(model='ibm/slate-125m-english-rtrvr', data=[{'object': 'embedding', 'index': 0, 'embedding': [-0.037463713, -0.02141933, -0.02851813, 0.015519324, -0.08252965, 0.040418413, 0.0125358505, -0.015099016, 0.007372251, 0.043594047, -0.045923322, -0.024535796, -0.06683439, -0.023252856, -0.014445329, -0.007990043, -0.0038893714, 0.024145052, 0.002840671, -0.005213263, 0.025767032, -0.029234663, -0.022147253, -0.04008686, -0.0049467147, -0.005722156, 0.05712166, 0.02074406, -0.027984975, 0.011733741, 0.037084717, 0.0267332, 0.027662167, 0.018661365, 0.034368176, -0.016858159, 0.01525097, 0.0037685328, -0.029145032, -0.014014788, -0.026596593, -0.019313056, -0.034545943, -0.012755116, -0.027378004, -0.0022658114, 0.0671108, -0.011186887, -0.012560194, 0.07890564, 0.04370288, -0.002565922, 0.04558289, -0.015022389, 0.01721297, -0.02836881, 0.00028577668, 0.041560214, -0.028451115, 0.026690092, -0.03240052, 0.043185145, -0.048146088, -0.01863734, 0.014189055, 0.005409885, -0.004303547, 0.043854367, -0.08027855, 0.0036468406, -0.03761452, -0.01586453, 0.0015843573, -0.06557115, -0.017214078, 0.013112075, -0.063624665, -0.059002113, -0.027906772, -0.0104140695, -0.0122148385, 0.002914942, 0.009600896, 0.024618316, 0.0028588492, -0.04129038, -0.0066302163, -0.016593395, 0.0119156595, 0.030668158, 0.032204323, -0.008526114, 0.031477567, -0.027671225, -0.021325896, -0.012719999, 0.020595504, -0.010196725, 0.016694892, 0.015447107, 0.033599768, 0.0015109212, 0.055442166, -0.032922138, 0.032867074, 0.034223255, 0.018267235, 0.044258785, -0.009512916, -0.01888108, 0.0020811916, -0.071849406, -0.029209733, 0.030071445, 0.04898721, 0.03807559, 0.030091342, 0.0049845255, 0.011301079, 0.0060062855, -0.052550614, -0.040027767, -0.04539995, -0.069943875, 0.052881725, 0.015551356, -0.0016604571, 0.0021608798, 0.055507053, -0.015404854, -0.0023839937, 0.0070840786, 0.042537935, -0.045489613, 0.018908504, -0.015565469, 0.015916781, 0.07333876, 0.0034915418, -0.0029724848, 0.019170308, 0.02221138, -0.027242986, -0.003735747, -0.02341423, -0.0037938543, 0.0104211755, -0.06185881, -0.036718667, -0.02746382, -0.026462527, -0.050701175, 0.0057923957, 0.040674523, -0.019840682, -0.030195065, 0.045316722, 0.017369563, -0.031288657, -0.047546197, 0.026255054, -0.0049950704, -0.040272273, 0.0005752177, 0.03959872, -0.0073655704, -0.025617458, -0.009416491, -0.019514928, -0.07619169, 0.0051972694, 0.016387343, -0.012366861, -0.009152257, -0.035955105, -0.05794065, 0.019153351, -0.0461187, 0.024734644, 0.0031722176, 0.06610593, -0.0046516205, -0.04635891, 0.02524459, 0.004230386, 0.06153266, -0.0008394812, -0.013522857, 0.029861225, -0.00394871, -0.037432022, 0.0483034, 0.02181303, 0.015967155, 0.06181817, -0.018545056, 0.044176213, -0.07024062, -0.013022128, -0.0087189535, -0.025292343, 0.040448178, -0.051455554, -0.014017804, 0.012191985, 0.0071282317, -0.015855217, 0.013618914, -0.0060378346, -0.057781402, -0.035322957, -0.013627626, -0.027318006, -0.27732822, -0.007108157, 0.012321971, -0.15896526, -0.03793523, -0.025426138, 0.020721687, -0.04701553, -0.004927499, 0.010541978, -0.003212021, -0.0023603817, -0.052153032, 0.043272667, 0.024041472, -0.031666223, 0.0017891804, 0.026806207, -0.026526717, 0.0023138188, 0.024067048, 0.03326347, -0.039004102, -0.0004279829, 0.007266309, -0.008940641, 0.03715139, -0.037960306, 0.01647343, -0.022163782, 0.07456727, -0.0013284415, -0.029121747, 0.012727488, -0.007229313, 0.03177136, -0.08142398, 0.010223168, -0.025942598, -0.23807198, 0.022616733, -0.03925926, 0.05572623, -0.00020389797, -0.0022259122, -0.007885641, -0.00719495, 0.0018412926, 0.018953165, -0.009946787, 0.03723944, -0.015900994, 0.013648507, 0.010997674, -0.018918132, 0.013143112, 0.032894272, -0.05800237, 0.011163258, 0.025205074, -0.017001726, 0.03673705, -0.011551997, 0.06637543, -0.033003606, -0.041392814, -0.004078506, 0.03916763, -0.0022711542, 0.058338877, -0.034323692, -0.033700593, 0.01051642, 0.035579532, -0.01997833, 0.002977113, 0.06590587, 0.042783573, 0.020624464, 0.029172791, -0.035136282, 0.02035436, 0.05696583, -0.010200334, -0.0010580813, -0.024785697, -0.014516442, -0.030100575, -0.03807279, 0.042534467, -0.0281041, -0.05331885, -0.019467393, 0.016051197, 0.012470333, -0.008369627, 0.002254233, 0.026580654, -0.04541506, -0.018085537, -0.034577485, -0.0014747214, 0.0005770179, 0.0043190396, -0.004989785, 0.007569717, 0.010167482, -0.03335266, -0.015255423, 0.07341545, 0.012114007, -0.0010415721, 0.008754641, 0.05932771, 0.030799353, 0.026148474, -0.0069155577, -0.056865778, 0.0038446637, -0.010079895, 0.013511311, 0.023351224, -0.049000103, -0.013028001, -0.04957143, -0.031393193, 0.040289443, 0.063747466, 0.046358805, 0.0023754216, -0.0054107807, -0.020128531, 0.0013747461, -0.018183928, -0.04754063, -0.0064625163, 0.0417791, 0.06087331, -0.012241535, 0.04185439, 0.03641727, -0.02044306, -0.061368305, -0.023353308, 0.055897385, -0.047081504, 0.012900442, -0.018708078, 0.0028819577, 0.006964468, 0.0008757072, 0.04605831, 0.01716345, -0.004099444, -0.015493673, 0.021323929, -0.011252118, -0.02278577, 0.01893121, 0.009134488, 0.021568391, 0.011066748, -0.018853422, 0.027866907, -0.02831057, -0.010147286, 0.014807969, -0.03266599, -0.06711559, 0.038546126, 0.0031859868, -0.029038243, 0.046595056, 0.036973156, -0.033408422, 0.021968717, -0.011411975, 0.006584961, 0.072844714, -0.005873538, 0.029435376, 0.061169676, -0.02318868, 0.051129397, 0.014791153, -0.009028991, -0.021579748, 0.02669236, 0.029696332, -0.063952625, -0.061506465, -0.00080902094, 0.06850867, -0.09809231, -0.005534635, 0.066767104, -0.041267477, 0.046568397, 0.00983124, -0.0048434925, 0.038644254, 0.04096419, 0.0023063375, 0.014526287, 0.014016995, 0.020224908, 0.007113328, -0.0732543, -0.0054818415, 0.05807576, 0.022461535, 0.21100426, -0.009597197, -0.020674499, 0.010743241, -0.046834, -0.0068005333, 0.04918187, -0.06680011, -0.025018543, 0.016360015, 0.100744724, -0.019944709, -0.052390855, -0.0034876189, 0.031699855, -0.03024188, 0.009384044, -0.073849924, 0.01846066, -0.017075414, 0.0067319535, 0.045643695, 0.0121267075, 0.014980903, -0.0022226444, -0.015187039, 0.040638167, 0.023607453, -0.018353134, 0.007413985, 0.03487914, 0.018997269, -0.0107962405, -0.0040080273, 0.001454658, -0.023004232, -0.03065838, -0.0691732, -0.009669473, -0.017253181, 0.100617275, -0.00028453665, -0.055184573, -0.04010461, -0.022628073, -0.02138574, -0.00011931983, -0.021988528, 0.021569526, 0.018913478, -0.07588871, -0.030895703, -0.045679674, 0.03548181, 0.05806986, -0.00313453, 0.005607964, 0.014474551, -0.016833752, -0.022846023, 0.03665983, 0.04312398, 0.006030178, 0.020107903, -0.067837745, -0.039261904, -0.013903933, -0.011238981, -0.091779895, 0.03393072, 0.03576862, -0.016447216, -0.013628061, 0.035994843, 0.02442105, 0.0013356373, -0.013639993, -0.0070654624, -0.031047037, 0.0321763, 0.019488426, 0.030912274, -0.018131692, 0.034129236, -0.038152352, -0.020318052, 0.012934771, -0.0038958737, 0.029313264, 0.0609006, -0.06022117, -0.016697206, -0.030089315, -0.0030464267, -0.05011375, 0.016849633, -0.01935251, 0.00033423092, 0.018090008, 0.034528963, 0.015720658, 0.006443832, 0.0024674414, 0.0033006326, -0.011959118, -0.014686165, 0.00851113, 0.032130115, 0.016566927, -0.0048006177, -0.041135546, 0.017366901, 0.014404645, 0.0014093819, -0.039899524, -0.020875102, -0.01322629, -0.010891931, 0.019460721, -0.098985165, -0.03990147, 0.035807386, 0.05274234, -0.017714208, 0.0023620757, 0.022553496, 0.010935722, -0.016535437, -0.014505468, -0.005573891, -0.029528206, -0.010998497, 0.011297328, 0.007440231, 0.054734096, -0.035311602, 0.07038191, -0.034328025, -0.0109814005, -0.00578824, -0.009286793, 0.06692834, -0.040116422, -0.030043483, -0.010882302, -0.024094587, 0.026659116, -0.0637435, -0.022305744, 0.024388585, 0.011812823, -0.022778027, -0.0039024823, 0.027778644, 0.010566278, 0.011030791, -0.0021155484, 0.018014789, -0.03458981, 0.02546183, -0.11745906, 0.038193583, 0.0019787792, 0.01639592, 0.013218127, -0.012434678, -0.047858853, 0.006662704, 0.033221778, 0.008376927, -0.011822234, 0.01202769, 0.008761578, -0.04075117, 0.0025187496, 0.0026266004, 0.029762473, 0.009570205, -0.03644678, -0.033258904, -0.030776607, 0.05373578, 0.010904848, 0.040284622, 0.02707032, 0.021803873, -0.022011256, -0.05517991, -0.005213912, 0.009023477, -0.011895841, -0.026821174, -0.009035418, -0.021059638, 0.025536137, -0.053264923, 0.032206282, 0.020235807, 0.018660447, 0.0028790566, -0.019914437, 0.097842626, 0.027617158, 0.020276038, -0.014215543, 0.012761584, 0.032757074, 0.061124176, 0.049016643, -0.016509317, -0.03750349, -0.03449537, -0.02039439, -0.051360182, -0.041909404, 0.016175032, 0.040492736, 0.031218654, 0.0020242895, -0.032167237, 0.019398497, 0.057013687, 0.0031299617, 0.019177254, 0.015395364, -0.034078192, 0.041325297, 0.044380017, -0.004446819, 0.019610956, -0.030034903, 0.008468295, 0.03065914, -0.009548659, -0.07113981, 0.051648173, 0.03746448, -0.021847434, 0.01844844, 0.01333424, -0.001188216, 0.012330977, -0.056448817, 0.0008659569, 0.011183285, 0.006780519, -0.007357356, 0.05263679, -0.024631461, 0.00519591, -0.052165415, -0.03250626, -0.009370051, 0.00292325, -0.007187242, 0.029566163, -0.049605303, -0.02625627, -0.003157652, 0.052691437, -0.03589223, 0.03889354, -0.0035060279, 0.024555178, -0.00929779, -0.05037946, -0.022402484, 0.030634355, -0.03300659, -0.0063623153, 0.0027472514, 0.03196768, -0.019257778, 0.0089001395, 0.008908001, 0.018918095, 0.059574094, -0.02838763, 0.018203752, -0.06708146, -0.022670228, -0.013985525, 0.045018435, 0.011420395, -0.008649952, -0.027328938, -0.03527292, -0.0038555951, 0.017597001, 0.024891963, -0.0039160745, -0.015237065, -0.0008723479, -0.018641612, -0.036825016, -0.028743235, 0.00091956893, 0.00030935413, -0.048641082, 0.03744432, -0.024196126, 0.009848505, -0.043836866, 0.0044429195, 0.013709644, 0.06295503, -0.016072558, 0.01277375, -0.03548109, 0.003398656, 0.025347201, 0.019685786, 0.00758199, -0.016122513, -0.039198015, -0.0023108267, -0.0041584945, 0.005161282, 0.00089106365, 0.0076085874, -0.055768084, -0.0058975955, 0.007728267, 0.00076985586, -0.013469806, -0.031578194, -0.0138569595, 0.044540506, -0.0408136, -0.015252405, 0.06232591, -0.04198101, 0.0048899655, -0.0030694627, -0.025022805, -0.010789543, -0.025350742, 0.007836728, 0.024604483, -5.385127e-05, -0.0021367231, -0.01704561, -0.001425816, 0.0035238306]}], object='list', usage=Usage(prompt_tokens=8, total_tokens=8))




================================================
FILE: cookbook/liteLLM_Langchain_Demo.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Langchain liteLLM Demo Notebook
## Use `ChatLiteLLM()` to instantly support 50+ LLM models
Langchain Docs: https://python.langchain.com/docs/integrations/chat/litellm

Call all LLM models using the same I/O interface

Example usage
```python
ChatLiteLLM(model="gpt-3.5-turbo")
ChatLiteLLM(model="claude-2", temperature=0.3)
ChatLiteLLM(model="command-nightly")
ChatLiteLLM(model="replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1")
```
"""

!pip install litellm langchain

import os
from langchain.chat_models import ChatLiteLLM
from langchain.schema import HumanMessage

os.environ['OPENAI_API_KEY'] = ""
chat = ChatLiteLLM(model="gpt-3.5-turbo")
messages = [
    HumanMessage(
        content="what model are you"
    )
]
chat(messages)
# Output:
#   AIMessage(content='I am an AI model known as GPT-3, developed by OpenAI.', additional_kwargs={}, example=False)

os.environ['ANTHROPIC_API_KEY'] = ""
chat = ChatLiteLLM(model="claude-2", temperature=0.3)
messages = [
    HumanMessage(
        content="what model are you"
    )
]
chat(messages)
# Output:
#   AIMessage(content=" I'm Claude, an AI assistant created by Anthropic.", additional_kwargs={}, example=False)

os.environ['REPLICATE_API_TOKEN'] = ""
chat = ChatLiteLLM(model="replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1")
messages = [
    HumanMessage(
        content="what model are you?"
    )
]
chat(messages)
# Output:
#   AIMessage(content=" I'm an AI based based on LLaMA models (LLaMA: Open and Efficient Foundation Language Models, Touvron et al. 2023), my knowledge was built from a massive corpus of text, including books, articles, and websites, and I was trained using a variety of machine learning algorithms. My model architecture is based on the transformer architecture, which is particularly well-suited for natural language processing tasks. My team of developers and I are constantly working to improve and fine-tune my performance, and I am always happy to help with any questions you may have!", additional_kwargs={}, example=False)

os.environ['COHERE_API_KEY'] = ""
chat = ChatLiteLLM(model="command-nightly")
messages = [
    HumanMessage(
        content="what model are you?"
    )
]
chat(messages)
# Output:
#   AIMessage(content=' I am an AI-based large language model, or Chatbot, built by the company Cohere. I am designed to have polite, helpful, inclusive conversations with users. I am always learning and improving, and I am constantly being updated with new information and improvements.\n\nI am currently in the development phase, and I am not yet available to the general public. However, I am currently being used by a select group of users for testing and feedback.\n\nI am a large language model, which means that I am trained on a massive amount of data and can understand and respond to a wide range of requests and questions. I am also designed to be flexible and adaptable, so I can be customized to suit the needs of different users and use cases.\n\nI am currently being used to develop a range of applications, including customer service chatbots, content generation tools, and language translation services. I am also being used to train other language models and to develop new ways of using large language models.\n\nI am constantly being updated with new information and improvements, so I am always learning and improving. I am also being used to develop new ways of using large language models, so I am always evolving and adapting to new use cases and requirements.', additional_kwargs={}, example=False)



================================================
FILE: cookbook/litellm_model_fallback.ipynb
================================================
# Jupyter notebook converted to Python script.

!pip install litellm

from litellm import completion

model_fallback_list = ["claude-instant-1", "gpt-3.5-turbo", "chatgpt-test"]

user_message = "Hello, how are you?"
messages = [{ "content": user_message,"role": "user"}]

for model in model_fallback_list:
  try:
      response = completion(model=model, messages=messages)
  except Exception:
      print(f"error occurred: {traceback.format_exc()}")



================================================
FILE: cookbook/LiteLLM_NovitaAI_Cookbook.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM NovitaAI Cookbook
"""

!pip install litellm

import os

os.environ['NOVITA_API_KEY'] = ""

from litellm import completion
response = completion(
            model="novita/deepseek/deepseek-r1",
            messages=[{"role": "user", "content": "write code for saying hi"}]
)
response

response = completion(
            model="novita/deepseek/deepseek-r1",
            messages=[{"role": "user", "content": "write code for saying hi"}]
)
response

response = completion(
            model="mistralai/mistral-7b-instruct",
            messages=[{"role": "user", "content": "write code for saying hi"}]
)
response



================================================
FILE: cookbook/liteLLM_Ollama.ipynb
================================================
# Jupyter notebook converted to Python script.

!pip install litellm # version 0.1.724 or higher 

"""
## Call Ollama - llama2 with Streaming
"""

from litellm import completion

response = completion(
    model="ollama/llama2", 
    messages=[{ "content": "respond in 20 words. who are you?","role": "user"}], 
    api_base="http://localhost:11434",
    stream=True
)
print(response)
for chunk in response:
    print(chunk['choices'][0]['delta'])

# Output:
#   <generator object get_ollama_response_stream at 0x109096c10>

#   {'role': 'assistant', 'content': ' I'}

#   {'role': 'assistant', 'content': "'"}

#   {'role': 'assistant', 'content': 'm'}

#   {'role': 'assistant', 'content': ' L'}

#   {'role': 'assistant', 'content': 'La'}

#   {'role': 'assistant', 'content': 'MA'}

#   {'role': 'assistant', 'content': ','}

#   {'role': 'assistant', 'content': ' an'}

#   {'role': 'assistant', 'content': ' A'}

#   {'role': 'assistant', 'content': 'I'}

#   {'role': 'assistant', 'content': ' assistant'}

#   {'role': 'assistant', 'content': ' developed'}

#   {'role': 'assistant', 'content': ' by'}

#   {'role': 'assistant', 'content': ' Meta'}

#   {'role': 'assistant', 'content': ' A'}

#   {'role': 'assistant', 'content': 'I'}

#   {'role': 'assistant', 'content': ' that'}

#   {'role': 'assistant', 'content': ' can'}

#   {'role': 'assistant', 'content': ' understand'}

#   {'role': 'assistant', 'content': ' and'}

#   {'role': 'assistant', 'content': ' respond'}

#   {'role': 'assistant', 'content': ' to'}

#   {'role': 'assistant', 'content': ' human'}

#   {'role': 'assistant', 'content': ' input'}

#   {'role': 'assistant', 'content': ' in'}

#   {'role': 'assistant', 'content': ' a'}

#   {'role': 'assistant', 'content': ' convers'}

#   {'role': 'assistant', 'content': 'ational'}

#   {'role': 'assistant', 'content': ' manner'}

#   {'role': 'assistant', 'content': '.'}


"""
## Call Ollama - Llama2 with Acompletion + Streaming
"""

# litellm uses async_generator for ollama async streaming, ensure it's installed
!pip install async_generator
# Output:
#   Defaulting to user installation because normal site-packages is not writeable

#   Requirement already satisfied: async_generator in /Users/ishaanjaffer/Library/Python/3.9/lib/python/site-packages (1.10)


import litellm

async def async_ollama():
    response = await litellm.acompletion(
        model="ollama/llama2", 
        messages=[{ "content": "what's the weather" ,"role": "user"}], 
        api_base="http://localhost:11434", 
        stream=True
    )
    async for chunk in response:
        print(chunk)

result = await async_ollama()
print(result)

try:
    async for chunk in result:
        print(chunk)
except TypeError: # the last chunk is None from Ollama, this raises an error with async streaming
    pass
# Output:
#   {'choices': [{'delta': {'role': 'assistant', 'content': ' I'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': "'"}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': 'm'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' just'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' an'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' A'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': 'I'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ','}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' I'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' don'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': "'"}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': 't'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' have'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' access'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' to'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' real'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': '-'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': 'time'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' weather'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' information'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' or'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' current'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' conditions'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' in'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' your'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' specific'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' location'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': '.'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' живело'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' can'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' provide'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' you'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' with'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' weather'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' forec'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': 'asts'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' and'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' information'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' for'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' your'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' location'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' if'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' you'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' would'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' like'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': '.'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' Please'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' let'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' me'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' know'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' where'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' you'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' are'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' located'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ','}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' and'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' I'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' will'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' do'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' my'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' best'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' to'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' assist'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': ' you'}}]}

#   {'choices': [{'delta': {'role': 'assistant', 'content': '.'}}]}

#   None


"""
## Completion Call
"""

from litellm import completion

response = completion(
    model="ollama/llama2", 
    messages=[{ "content": "respond in 20 words. who are you?","role": "user"}], 
    api_base="http://localhost:11434"
)
print(response)

# Output:
#   {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": " I'm LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner.",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-ea7b8242-791f-4656-ba12-e098edeb960e",

#     "created": 1695324686.6696231,

#     "response_ms": 4072.3050000000003,

#     "model": "ollama/llama2",

#     "usage": {

#       "prompt_tokens": 10,

#       "completion_tokens": 27,

#       "total_tokens": 37

#     }

#   }




================================================
FILE: cookbook/LiteLLM_OpenRouter.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LiteLLM OpenRouter Cookbook
"""

!pip install litellm

import os

os.environ['OPENROUTER_API_KEY'] = ""

from litellm import completion
response = completion(
            model="openrouter/google/palm-2-chat-bison",
            messages=[{"role": "user", "content": "write code for saying hi"}]
)
response
# Output:
#   <OpenAIObject id=gen-W8FTMSIEorCp3vG5iYIgNMR4IeBv at 0x7c3dcef1f060> JSON: {

#     "id": "gen-W8FTMSIEorCp3vG5iYIgNMR4IeBv",

#     "model": "chat-bison@001",

#     "choices": [

#       {

#         "message": {

#           "role": "assistant",

#           "content": "```\n#include <stdio.h>\n\nint main() {\n  printf(\"Hi!\\n\");\n  return 0;\n}\n```"

#         }

#       }

#     ],

#     "response_ms": 7817.777999999999

#   }

response = completion(
            model="openrouter/anthropic/claude-2",
            messages=[{"role": "user", "content": "write code for saying hi"}]
)
response
# Output:
#   <OpenAIObject id=gen-IiuV7ZNimDufVeutBHrl8ajPuzEh at 0x7c3dcea67560> JSON: {

#     "choices": [

#       {

#         "message": {

#           "role": "assistant",

#           "content": " Here is some simple code to print \"Hi\":\n\n```python\nprint(\"Hi\")\n```\n\nThis uses the print() function in Python to output the text \"Hi\"."

#         },

#         "finish_reason": "stop_sequence"

#       }

#     ],

#     "model": "claude-2.0",

#     "id": "gen-IiuV7ZNimDufVeutBHrl8ajPuzEh",

#     "response_ms": 8112.443000000001

#   }

response = completion(
            model="openrouter/meta-llama/llama-2-70b-chat",
            messages=[{"role": "user", "content": "write code for saying hi"}]
)
response
# Output:
#   <OpenAIObject id=gen-PyMd3yyJ0aQsCgIY9R8XGZoAtPbl at 0x7c3dceefcae0> JSON: {

#     "id": "gen-PyMd3yyJ0aQsCgIY9R8XGZoAtPbl",

#     "model": "togethercomputer/llama-2-70b-chat",

#     "choices": [

#       {

#         "message": {

#           "role": "assistant",

#           "content": "*gives a sly smile as they type*\n\nHey there, handsome. \ud83d\ude0f\n\nWhat brings you to my neck of the woods today? \ud83d\ude18"

#         }

#       }

#     ],

#     "response_ms": 9618.775

#   }



================================================
FILE: cookbook/LiteLLM_Petals.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Using LiteLLM with Petals
"""

!pip install litellm # 0.1.715 and upwards

# install petals
!pip install git+https://github.com/bigscience-workshop/petals

"""
## petals-team/StableBeluga2
"""

from litellm import completion

response = completion(model="petals/petals-team/StableBeluga2", messages=[{ "content": "Hello, how are you?","role": "user"}], max_tokens=50)

print(response)
# Output:
#   You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565

#   Sep 19 18:39:50.634 [[1m[34mINFO[0m] Make sure you follow the LLaMA's terms of use: https://bit.ly/llama2-license for LLaMA 2, https://bit.ly/llama-license for LLaMA 1

#   Sep 19 18:39:50.639 [[1m[34mINFO[0m] Using DHT prefix: StableBeluga2-hf

#   Sep 19 18:40:13.920 [[1m[34mINFO[0m] Route found: 0:40 via …HfQWVM => 40:80 via …Zj98Se

#   {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": "Hello, how are you?\nI'm doing well, thank you. I'm just getting ready to go to the gym.\nOh, that's great. I'm trying to get back into a workout routine myself.\nYeah,",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-f09d79b3-c1d1-49b7-b55f-cd8dfa1043bf",

#     "created": 1695148897.473613,

#     "model": "petals-team/StableBeluga2",

#     "usage": {

#       "prompt_tokens": 6,

#       "completion_tokens": 45,

#       "total_tokens": 51

#     }

#   }


"""
## huggyllama/llama-65b
"""

response = completion(model="petals/huggyllama/llama-65b", messages=[{ "content": "Hello, how are you?","role": "user"}], temperature=0.2, max_tokens=10)

print(response)
# Output:
#   Sep 19 18:41:37.912 [[1m[34mINFO[0m] Make sure you follow the LLaMA's terms of use: https://bit.ly/llama2-license for LLaMA 2, https://bit.ly/llama-license for LLaMA 1

#   Sep 19 18:41:37.914 [[1m[34mINFO[0m] Using DHT prefix: llama-65b-hf

#   Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
#   /usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.

#     warnings.warn(

#   Sep 19 18:41:48.396 [[1m[34mINFO[0m] Route found: 0:80 via …g634yJ

#   {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": "Hello, how are you?\nI'm fine, thank you. And",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-3496e6eb-2a27-4f94-8d75-70648eacd88f",

#     "created": 1695148912.9116046,

#     "model": "huggyllama/llama-65b",

#     "usage": {

#       "prompt_tokens": 6,

#       "completion_tokens": 14,

#       "total_tokens": 20

#     }

#   }




================================================
FILE: cookbook/liteLLM_Replicate_Demo.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Call Replicate LLMs using chatGPT Input/Output Format
This tutorial covers using the following Replicate Models with liteLLM

- [StableLM Tuned Alpha 7B](https://replicate.com/stability-ai/stablelm-tuned-alpha-7b)
- [LLAMA-2 70B Chat](https://replicate.com/replicate/llama-2-70b-chat)
- [A16z infra-LLAMA-2 7B Chat](https://replicate.com/a16z-infra/llama-2-7b-chat)
- [Dolly V2 12B](https://replicate.com/replicate/dolly-v2-12b)
- [Vicuna 13B](https://replicate.com/replicate/vicuna-13b)




"""

# install liteLLM
!pip install litellm

"""
Imports & Set ENV variables
Get your Replicate Key: https://replicate.com/account/api-tokens
"""

from litellm import completion
import os
os.environ['REPLICATE_API_TOKEN'] = ' ' # @param
user_message = "Hello, whats the weather in San Francisco??"
messages = [{ "content": user_message,"role": "user"}]

"""
## Call Replicate Models using completion(model, messages) - chatGPT format
"""

llama_2 = "replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1"
llama_2_7b = "a16z-infra/llama-2-7b-chat:4f0b260b6a13eb53a6b1891f089d57c08f41003ae79458be5011303d81a394dc"
dolly_v2 = "replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5"
vicuna = "replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b"
models = [llama_2, llama_2_7b, dolly_v2, vicuna]
for model in models:
  response = completion(model=model, messages=messages)
  print(f"Response from {model} \n]\n")
  print(response)
# Output:
#   replicate is not installed. Installing...

#   Response from stability-ai/stablelm-tuned-alpha-7b:c49dae362cbaecd2ceabb5bd34fdb68413c4ff775111fea065d259d577757beb 

#   ]

#   

#   {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': "I'm sorry for you being unable to access this content as my training data only goes up until 2023/03. However I can tell you what your local weather forecast may look like at any time of year with respect to current conditions:"}}], 'created': 1691611730.7224207, 'model': 'stability-ai/stablelm-tuned-alpha-7b:c49dae362cbaecd2ceabb5bd34fdb68413c4ff775111fea065d259d577757beb', 'usage': {'prompt_tokens': 9, 'completion_tokens': 49, 'total_tokens': 58}}

#   Response from replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1 

#   ]

#   

#   {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': " Hello! I'm happy to help you with your question. However, I must point out that the question itself may not be meaningful. San Francisco is a city located in California, USA, and it is not possible for me to provide you with the current weather conditions there as I am a text-based AI language model and do not have access to real-time weather data. Additionally, the weather in San Francisco can vary greatly depending on the time of year, so it would be best to check a reliable weather source for the most up-to-date information.\n\nIf you meant to ask a different question, please feel free to rephrase it, and I will do my best to assist you in a safe and positive manner."}}], 'created': 1691611745.0269957, 'model': 'replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1', 'usage': {'prompt_tokens': 9, 'completion_tokens': 143, 'total_tokens': 152}}

#   Response from a16z-infra/llama-2-7b-chat:4f0b260b6a13eb53a6b1891f089d57c08f41003ae79458be5011303d81a394dc 

#   ]

#   

#   {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': " Hello! I'm here to help you with your question. However, I must inform you that the weather in San Francisco can be quite unpredictable and can change rapidly. It's important to check reliable sources such as AccuWeather or the National Weather Service for the most up-to-date and accurate information about the weather in San Francisco.\nI cannot provide you with real-time weather data or forecasts as I'm just an AI and do not have access to current weather conditions or predictions. But I can suggest some trustworthy websites or apps where you can find the latest weather updates:\n* AccuWeather (accuweather.com)\n* The Weather Channel (weather.com)\n* Dark Sky (darksky.net)\n* Weather Underground (wunderground.com)\nRemember, it's always best to consult multiple sources for the most accurate information when planning your day or trip. Enjoy your day!"}}], 'created': 1691611748.7723358, 'model': 'a16z-infra/llama-2-7b-chat:4f0b260b6a13eb53a6b1891f089d57c08f41003ae79458be5011303d81a394dc', 'usage': {'prompt_tokens': 9, 'completion_tokens': 174, 'total_tokens': 183}}

#   Response from replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5 

#   ]

#   

#   {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': 'Its 68 degrees right now in San Francisco! The temperature will be rising through the week and i expect it to reach 70 on Thursdays and Friday. Skies are expected to be partly cloudy with some sun breaks throughout the day.\n\n'}}], 'created': 1691611752.2002115, 'model': 'replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5', 'usage': {'prompt_tokens': 9, 'completion_tokens': 48, 'total_tokens': 57}}

#   Response from replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b 

#   ]

#   

#   {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': ''}}], 'created': 1691611752.8998356, 'model': 'replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b', 'usage': {'prompt_tokens': 9, 'completion_tokens': 0, 'total_tokens': 9}}


# @title Stream Responses from Replicate - Outputs in the same format used by chatGPT streaming
response = completion(model=llama_2, messages=messages, stream=True)

for chunk in response:
  print(chunk['choices'][0]['delta'])
# Output:
#   Hi

#    there!

#    The

#    current

#    forecast

#    for

#    today's

#    high

#    temperature

#    ranges

#    from

#    75

#    degrees

#    Fahrenheit

#    all

#    day

#    to

#    83

#    degrees

#    Fahrenheit

#    with

#    possible

#    isolated

#    thunderstorms

#    during

#    the

#    afternoon

#    hours,

#    mainly

#    at

#    sunset

#    through

#    early

#    evening.  The

#    Pacific

#    Ocean

#    has

#    a

#    low

#    pressure

#    of

#    926

#    mb

#    and

#    mostly

#    cloud

#    cover

#    in

#    this

#    region

#    on

#    sunny

#    days

#    due

#    to

#    warming

#    temperatures

#    above

#    average

#    along

#    most

#    coastal

#    areas

#    and

#    ocean

#    breezes.<|USER|>




================================================
FILE: cookbook/liteLLM_Streaming_Demo.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# [STREAMING]  OpenAI, Anthropic, Replicate, Cohere using liteLLM
In this tutorial:
Note: All inputs/outputs are in the format used by `gpt-3.5-turbo`

- Call all models in the same input format [**with streaming**]:

  `completion(model, messages, stream=True)`
- All streaming generators are accessed at `chunk['choices'][0]['delta']`

The following Models are covered in this tutorial
- [GPT-3.5-Turbo](https://platform.openai.com/docs/models/gpt-3-5)
- [Claude-2](https://www.anthropic.com/index/claude-2)
- [StableLM Tuned Alpha 7B](https://replicate.com/stability-ai/stablelm-tuned-alpha-7b)
- [A16z infra-LLAMA-2 7B Chat](https://replicate.com/a16z-infra/llama-2-7b-chat)
- [Vicuna 13B](https://replicate.com/replicate/vicuna-13b)
- [Cohere - Command Nightly]()




"""

# install liteLLM
!pip install litellm==0.1.369

"""
## Imports & Set ENV variables
Get your API Keys

https://platform.openai.com/account/api-keys

https://replicate.com/account/api-tokens

https://console.anthropic.com/account/keys

https://dashboard.cohere.ai/api-keys

"""

from litellm import completion
import os

os.environ['OPENAI_API_KEY'] = '' # @param
os.environ['REPLICATE_API_TOKEN'] = '' # @param
os.environ['ANTHROPIC_API_KEY'] = '' # @param
os.environ['COHERE_API_KEY'] = '' # @param

"""
### Set Messages
"""

user_message = "Hello, whats the weather in San Francisco??"
messages = [{ "content": user_message,"role": "user"}]

"""
## Calling Models using liteLLM Streaming -

## `completion(model, messages, stream)`
"""

# replicate models #######
stability_ai = "stability-ai/stablelm-tuned-alpha-7b:c49dae362cbaecd2ceabb5bd34fdb68413c4ff775111fea065d259d577757beb"
llama_2_7b = "a16z-infra/llama-2-7b-chat:4f0b260b6a13eb53a6b1891f089d57c08f41003ae79458be5011303d81a394dc"
vicuna = "replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b"

models = ["gpt-3.5-turbo", "claude-2", stability_ai, llama_2_7b, vicuna, "command-nightly"] # command-nightly is Cohere
for model in models:
  replicate = (model == stability_ai or model==llama_2_7b or model==vicuna) # let liteLLM know if a model is replicate, using this optional param, `replicate=True`
  response = completion(model=model, messages=messages, stream=True, replicate=replicate)
  print(f"####################\n\nResponse from {model}")
  for i, chunk in enumerate(response):
    if i < 5: # NOTE: LIMITING CHUNKS FOR THIS DEMO
      print((chunk['choices'][0]['delta']))

# Output:
#   ####################

#   

#   Response from gpt-3.5-turbo

#   {

#     "role": "assistant",

#     "content": ""

#   }

#   {

#     "content": "I"

#   }

#   {

#     "content": "'m"

#   }

#   {

#     "content": " sorry"

#   }

#   {

#     "content": ","

#   }

#   ####################

#   

#   Response from claude-2

#   {'role': 'assistant', 'content': ' Unfortunately'}

#   {'role': 'assistant', 'content': ' I'}

#   {'role': 'assistant', 'content': ' don'}

#   {'role': 'assistant', 'content': "'t"}

#   {'role': 'assistant', 'content': ' have'}

#   ####################

#   

#   Response from stability-ai/stablelm-tuned-alpha-7b:c49dae362cbaecd2ceabb5bd34fdb68413c4ff775111fea065d259d577757beb

#   {'role': 'assistant', 'content': "I'm"}

#   {'role': 'assistant', 'content': ' sorry,'}

#   {'role': 'assistant', 'content': ' I'}

#   {'role': 'assistant', 'content': ' cannot'}

#   {'role': 'assistant', 'content': ' answer'}

#   ####################

#   

#   Response from a16z-infra/llama-2-7b-chat:4f0b260b6a13eb53a6b1891f089d57c08f41003ae79458be5011303d81a394dc

#   {'role': 'assistant', 'content': ''}

#   {'role': 'assistant', 'content': ' Hello'}

#   {'role': 'assistant', 'content': '!'}

#   {'role': 'assistant', 'content': ' I'}

#   {'role': 'assistant', 'content': "'"}

#   ####################

#   

#   Response from replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b

#   {'role': 'assistant', 'content': 'Comment:'}

#   {'role': 'assistant', 'content': 'Hi! '}

#   {'role': 'assistant', 'content': 'How '}

#   {'role': 'assistant', 'content': 'are '}

#   {'role': 'assistant', 'content': 'you '}

#   ####################

#   

#   Response from command-nightly

#   {'role': 'assistant', 'content': ' Hello'}

#   {'role': 'assistant', 'content': '!'}

#   {'role': 'assistant', 'content': ' '}

#   {'role': 'assistant', 'content': ' I'}

#   {'role': 'assistant', 'content': "'m"}




================================================
FILE: cookbook/litellm_test_multiple_llm_demo.ipynb
================================================
# Jupyter notebook converted to Python script.

!pip install litellm

from litellm import completion

## set ENV variables
os.environ["OPENAI_API_KEY"] = "openai key"
os.environ["COHERE_API_KEY"] = "cohere key"
os.environ["REPLICATE_API_KEY"] = "replicate key"
messages = [{ "content": "Hello, how are you?","role": "user"}]

# openai call
response = completion(model="gpt-3.5-turbo", messages=messages)

# cohere call
response = completion("command-nightly", messages)

# replicate call
response = completion("replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1", messages)



================================================
FILE: cookbook/LiteLLM_User_Based_Rate_Limits.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## User Based Rate Limiting Using LiteLLM
- LiteLLM allows you to set budgets per user
- Check if a given user has cross their allocated budget

In this notebook we create a $0.0002 daily budget per user and make completion calls using the litellm budget manager
"""

!pip install litellm uuid

"""
## Imports & Env variables
"""

import uuid
import os
os.environ['OPENAI_API_KEY'] = ""

"""
## completion() with the budget manager

This code does the following
- Initializes a litellm.BudgetManager()
- Checks if a budget exists for a user
  - Creates a $0.0002 budget if the user does not exisr
- Makes a `litellm.completion()` request only if the user is under their budget
"""

from litellm import BudgetManager, completion

# Initializes a litellm.BudgetManager()
budget_manager = BudgetManager(project_name="liteLLM_project", client_type="hosted") # see https://docs.litellm.ai/docs/budget_manager

user_id = str(uuid.uuid4()) # create a new user id
daily_budget = 0.0002

# Checks if a budget exists for a user
if not budget_manager.is_valid_user(user_id):
    # Creates a $0.0002 budget if the user does not exisr
    print(f"No budget exists for user: {user_id}\n")
    print(f"Creating a budget for user: {user_id}, daily budget ${daily_budget}\n")
    budget_manager.create_budget(total_budget=daily_budget, user=user_id, duration="daily") # duration can be daily, weekly, monthly


# Makes a `litellm.completion()` request only if the user is under their budget
current_spend_for_user = budget_manager.get_current_cost(user=user_id)
budget_for_user = budget_manager.get_total_budget(user_id)
print(f"User: {user_id} has spent ${current_spend_for_user}, budget for user: ${budget_for_user}\n")

if current_spend_for_user <= budget_for_user:
    response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hey, how's it going?"}])
    budget_manager.update_cost(completion_obj=response, user=user_id)
else:
    response = "Sorry - no budget!"

print(response)
# Output:
#   No budget exists for user: 29af95f8-c3c6-4c8c-b080-8b2d18d25432

#   

#   Creating a budget for user: 29af95f8-c3c6-4c8c-b080-8b2d18d25432, daily budget $0.0002

#   

#   User: 29af95f8-c3c6-4c8c-b080-8b2d18d25432 has spent $0, budget for user: $0.0002

#   

#   {

#     "id": "chatcmpl-7yAUkHQV8xdfldzzZnnnuVU8pl31b",

#     "object": "chat.completion",

#     "created": 1694574378,

#     "model": "gpt-3.5-turbo-0613",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": "Hello! I'm an AI, so I don't have emotions, but I'm here to assist you. How can I help you today?"

#         },

#         "finish_reason": "stop"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 14,

#       "completion_tokens": 29,

#       "total_tokens": 43

#     }

#   }

#   {'status': 'success'}

"""
## Make 10 calls to cross the budget per user
- Code fails after user crossed their budget
"""

user_id = "29af95f8-c3c6-4c8c-b080-8b2d18d25432" # set in the previous cell

for _ in range(10):
  # check if a given call can be made
  current_spend_for_user = budget_manager.get_current_cost(user=user_id)
  budget_for_user = budget_manager.get_total_budget(user_id)
  print(f"User: {user_id} has spent ${current_spend_for_user}, budget for user: ${budget_for_user}\n")
  if current_spend_for_user <= budget_for_user:
      response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hey, how's it going?"}])
      budget_manager.update_cost(completion_obj=response, user=user_id)
  else:
      response = "Sorry - no budget!"
      print(f"User: {user_id} has exceeded budget, current spend ${current_spend_for_user}, budget for user: ${budget_for_user}\n")
      break # no more requests

  # print(response)
# Output:
#   User: 29af95f8-c3c6-4c8c-b080-8b2d18d25432 has spent $7.9e-05, budget for user: $0.0002

#   

#   User: 29af95f8-c3c6-4c8c-b080-8b2d18d25432 has spent $0.00015999999999999999, budget for user: $0.0002

#   

#   User: 29af95f8-c3c6-4c8c-b080-8b2d18d25432 has spent $0.00023899999999999998, budget for user: $0.0002

#   

#   User: 29af95f8-c3c6-4c8c-b080-8b2d18d25432 has exceeded budget, current spend $0.00023899999999999998, budget for user: $0.0002

#   




================================================
FILE: cookbook/liteLLM_VertextAI_Example.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Using Google Palm (VertexAI) with liteLLM 
### chat-bison, chat-bison@001, text-bison, text-bison@001
"""

!pip install litellm==0.1.388

"""
### Set VertexAI Configs
Vertex AI requires the following:
* `vertex_project` - Your Project ID
* `vertex_location` - Your Vertex AI region
Both can be found on: https://console.cloud.google.com/

VertexAI uses Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information on setting this up

NOTE: VertexAI requires you to set `application_default_credentials.json`, this can be set by running `gcloud auth application-default login` in your terminal


"""

# set you Vertex AI configs
import litellm
from litellm import completion

litellm.vertex_project = "hardy-device-386718"
litellm.vertex_location = "us-central1"

"""
## Call VertexAI - chat-bison using liteLLM
"""

user_message = "what is liteLLM "
messages = [{ "content": user_message,"role": "user"}]

# chat-bison or chat-bison@001 supported by Vertex AI (As of Aug 2023)
response = completion(model="chat-bison", messages=messages)
print(response)
# Output:
#   {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': LiteLLM LiteLLM is a large language model from Google AI that is designed to be lightweight and efficient. It is based on the Transformer architecture and has been trained on a massive dataset of text. LiteLLM is available as a pre-trained model that can be used for a variety of natural language processing tasks, such as text classification, question answering, and summarization.}}], 'created': 1692036777.831989, 'model': 'chat-bison'}


"""
## Call VertexAI - text-bison using liteLLM
"""

print(litellm.vertex_text_models)
# Output:
#   ['text-bison', 'text-bison@001']


user_message = "what is liteLLM "
messages = [{ "content": user_message,"role": "user"}]

# text-bison or text-bison@001 supported by Vertex AI (As of Aug 2023)
response = completion(model="text-bison@001", messages=messages)
print(response)
# Output:
#   {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': liteLLM is a low-precision variant of the large language model LLM 5. For a given text prompt, liteLLM can continue the text in a way that is both coherent and informative.}}], 'created': 1692036813.052487, 'model': 'text-bison@001'}


response = completion(model="text-bison", messages=messages)
print(response)
# Output:
#   {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': liteLLM was originally developed by Google engineers as a lite version of LLM, which stands for large language model. It is a deep learning language model that is designed to be more efficient than traditional LLMs while still achieving comparable performance. liteLLM is built on Tensor2Tensor, a framework for building and training large neural networks. It is able to learn from massive amounts of text data and generate text that is both coherent and informative. liteLLM has been shown to be effective for a variety of tasks, including machine translation, text summarization, and question answering.}}], 'created': 1692036821.60951, 'model': 'text-bison'}


response = completion(model="text-bison@001", messages=messages, temperature=0.4, top_k=10, top_p=0.2)
print(response['choices'][0]['message']['content'])
# Output:
#   liteLLM is a lightweight language model that is designed to be fast and efficient. It is based on the Transformer architecture, but it has been modified to reduce the number of parameters and the amount of computation required. This makes it suitable for use on devices with limited resources, such as mobile phones and embedded systems.

#   

#   liteLLM is still under development, but it has already been shown to be effective on a variety of tasks, including text classification, natural language inference, and machine translation. It is also being used to develop new applications, such as chatbots and language assistants.

#   

#   If you are interested in learning more about lite




================================================
FILE: cookbook/Migrating_to_LiteLLM_Proxy_from_OpenAI_Azure_OpenAI.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Migrating to LiteLLM Proxy from OpenAI/Azure OpenAI

Covers:

*   /chat/completion
*   /embedding


These are **selected examples**. LiteLLM Proxy is **OpenAI-Compatible**, it works with any project that calls OpenAI. Just change the `base_url`, `api_key` and `model`.

For more examples, [go here](https://docs.litellm.ai/docs/proxy/user_keys)

To pass provider-specific args, [go here](https://docs.litellm.ai/docs/completion/provider_specific_params#proxy-usage)

To drop unsupported params (E.g. frequency_penalty for bedrock with librechat), [go here](https://docs.litellm.ai/docs/completion/drop_params#openai-proxy-usage)

"""

"""
## /chat/completion


"""

"""
### OpenAI Python SDK
"""

import openai
client = openai.OpenAI(
    api_key="anything",
    base_url="http://0.0.0.0:4000"
)

# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages = [
        {
            "role": "user",
            "content": "this is a test request, write a short poem"
        }
    ],
    extra_body={ # pass in any provider-specific param, if not supported by openai, https://docs.litellm.ai/docs/completion/input#provider-specific-params
        "metadata": { # 👈 use for logging additional params (e.g. to langfuse)
            "generation_name": "ishaan-generation-openai-client",
            "generation_id": "openai-client-gen-id22",
            "trace_id": "openai-client-trace-id22",
            "trace_user_id": "openai-client-user-id2"
        }
    }
)

print(response)

"""
## Function Calling
"""

from openai import OpenAI
client = OpenAI(
    api_key="sk-1234", # [OPTIONAL] set if you set one on proxy, else set ""
    base_url="http://0.0.0.0:4000",
)

tools = [
  {
    "type": "function",
    "function": {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA",
          },
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
        },
        "required": ["location"],
      },
    }
  }
]
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]
completion = client.chat.completions.create(
  model="gpt-4o", # use 'model_name' from config.yaml
  messages=messages,
  tools=tools,
  tool_choice="auto"
)

print(completion)


"""
### Azure OpenAI Python SDK
"""

import openai
client = openai.AzureOpenAI(
    api_key="anything",
    base_url="http://0.0.0.0:4000"
)

# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages = [
        {
            "role": "user",
            "content": "this is a test request, write a short poem"
        }
    ],
    extra_body={ # pass in any provider-specific param, if not supported by openai, https://docs.litellm.ai/docs/completion/input#provider-specific-params
        "metadata": { # 👈 use for logging additional params (e.g. to langfuse)
            "generation_name": "ishaan-generation-openai-client",
            "generation_id": "openai-client-gen-id22",
            "trace_id": "openai-client-trace-id22",
            "trace_user_id": "openai-client-user-id2"
        }
    }
)

print(response)

"""
### Langchain Python
"""

from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain.schema import HumanMessage, SystemMessage
import os

os.environ["OPENAI_API_KEY"] = "anything"

chat = ChatOpenAI(
    openai_api_base="http://0.0.0.0:4000",
    model = "gpt-3.5-turbo",
    temperature=0.1,
    extra_body={
        "metadata": {
            "generation_name": "ishaan-generation-langchain-client",
            "generation_id": "langchain-client-gen-id22",
            "trace_id": "langchain-client-trace-id22",
            "trace_user_id": "langchain-client-user-id2"
        }
    }
)

messages = [
    SystemMessage(
        content="You are a helpful assistant that im using to make a test request to."
    ),
    HumanMessage(
        content="test from litellm. tell me why it's amazing in 1 sentence"
    ),
]
response = chat(messages)

print(response)

"""
### Curl
"""

"""


```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \
    -H 'Content-Type: application/json' \
    -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
        {
        "role": "user",
        "content": "what llm are you"
        }
    ],
    "metadata": {
        "generation_name": "ishaan-test-generation",
        "generation_id": "gen-id22",
        "trace_id": "trace-id22",
        "trace_user_id": "user-id2"
    }
}'
```


"""

"""
### LlamaIndex
"""

import os, dotenv

from llama_index.llms import AzureOpenAI
from llama_index.embeddings import AzureOpenAIEmbedding
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext

llm = AzureOpenAI(
    engine="azure-gpt-3.5",               # model_name on litellm proxy
    temperature=0.0,
    azure_endpoint="http://0.0.0.0:4000", # litellm proxy endpoint
    api_key="sk-1234",                    # litellm proxy API Key
    api_version="2023-07-01-preview",
)

embed_model = AzureOpenAIEmbedding(
    deployment_name="azure-embedding-model",
    azure_endpoint="http://0.0.0.0:4000",
    api_key="sk-1234",
    api_version="2023-07-01-preview",
)


documents = SimpleDirectoryReader("llama_index_data").load_data()
service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)
index = VectorStoreIndex.from_documents(documents, service_context=service_context)

query_engine = index.as_query_engine()
response = query_engine.query("What did the author do growing up?")
print(response)


"""
### Langchain JS
"""

import { ChatOpenAI } from "@langchain/openai";


const model = new ChatOpenAI({
  modelName: "gpt-4",
  openAIApiKey: "sk-1234",
  modelKwargs: {"metadata": "hello world"} // 👈 PASS Additional params here
}, {
  basePath: "http://0.0.0.0:4000",
});

const message = await model.invoke("Hi there!");

console.log(message);


"""
### OpenAI JS
"""

const { OpenAI } = require('openai');

const openai = new OpenAI({
  apiKey: "sk-1234", // This is the default and can be omitted
  baseURL: "http://0.0.0.0:4000"
});

async function main() {
  const chatCompletion = await openai.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'gpt-3.5-turbo',
  }, {"metadata": {
            "generation_name": "ishaan-generation-openaijs-client",
            "generation_id": "openaijs-client-gen-id22",
            "trace_id": "openaijs-client-trace-id22",
            "trace_user_id": "openaijs-client-user-id2"
        }});
}

main();


"""
### Anthropic SDK
"""

import os

from anthropic import Anthropic

client = Anthropic(
    base_url="http://localhost:4000", # proxy endpoint
    api_key="sk-s4xN1IiLTCytwtZFJaYQrA", # litellm proxy virtual key
)

message = client.messages.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello, Claude",
        }
    ],
    model="claude-3-opus-20240229",
)
print(message.content)

"""
## /embeddings
"""

"""
### OpenAI Python SDK
"""

import openai
from openai import OpenAI

# set base_url to your proxy server
# set api_key to send to proxy server
client = OpenAI(api_key="<proxy-api-key>", base_url="http://0.0.0.0:4000")

response = client.embeddings.create(
    input=["hello from litellm"],
    model="text-embedding-ada-002"
)

print(response)


"""
### Langchain Embeddings
"""

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="sagemaker-embeddings", openai_api_base="http://0.0.0.0:4000", openai_api_key="temp-key")


text = "This is a test document."

query_result = embeddings.embed_query(text)

print(f"SAGEMAKER EMBEDDINGS")
print(query_result[:5])

embeddings = OpenAIEmbeddings(model="bedrock-embeddings", openai_api_base="http://0.0.0.0:4000", openai_api_key="temp-key")

text = "This is a test document."

query_result = embeddings.embed_query(text)

print(f"BEDROCK EMBEDDINGS")
print(query_result[:5])

embeddings = OpenAIEmbeddings(model="bedrock-titan-embeddings", openai_api_base="http://0.0.0.0:4000", openai_api_key="temp-key")

text = "This is a test document."

query_result = embeddings.embed_query(text)

print(f"TITAN EMBEDDINGS")
print(query_result[:5])

"""
### Curl Request
"""

"""


```curl
curl -X POST 'http://0.0.0.0:4000/embeddings' \
  -H 'Content-Type: application/json' \
  -d ' {
  "model": "text-embedding-ada-002",
  "input": ["write a litellm poem"]
  }'
```


"""



================================================
FILE: cookbook/mlflow_langchain_tracing_litellm_proxy.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Databricks Notebook with MLFlow AutoLogging for LiteLLM Proxy calls

"""

%pip install -U -qqqq databricks-agents mlflow langchain==0.3.1 langchain-core==0.3.6 

%pip install "langchain-openai<=0.3.1"

# Before logging this chain using the driver notebook, you must comment out this line.
dbutils.library.restartPython() 

import mlflow
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_databricks import ChatDatabricks
from langchain_openai import ChatOpenAI

mlflow.langchain.autolog()

# These helper functions parse the `messages` array.

# Return the string contents of the most recent message from the user
def extract_user_query_string(chat_messages_array):
    return chat_messages_array[-1]["content"]


# Return the chat history, which is is everything before the last question
def extract_chat_history(chat_messages_array):
    return chat_messages_array[:-1]

model = ChatOpenAI(
    openai_api_base="LITELLM_PROXY_BASE_URL", # e.g.: http://0.0.0.0:4000
    model = "gpt-3.5-turbo", # LITELLM 'model_name'
    temperature=0.1, 
    api_key="LITELLM_PROXY_API_KEY" # e.g.: "sk-1234"
)

############
# Prompt Template for generation
############
prompt = PromptTemplate(
    template="You are a hello world bot.  Respond with a reply to the user's question that is fun and interesting to the user.  User's question: {question}",
    input_variables=["question"],
)

############
# FM for generation
# ChatDatabricks accepts any /llm/v1/chat model serving endpoint
############
model = ChatDatabricks(
    endpoint="databricks-dbrx-instruct",
    extra_params={"temperature": 0.01, "max_tokens": 500},
)


############
# Simple chain
############
# The framework requires the chain to return a string value.
chain = (
    {
        "question": itemgetter("messages")
        | RunnableLambda(extract_user_query_string),
        "chat_history": itemgetter("messages") | RunnableLambda(extract_chat_history),
    }
    | prompt
    | model
    | StrOutputParser()
)

# This is the same input your chain's REST API will accept.
question = {
    "messages": [
               {
            "role": "user",
            "content": "what is rag?",
        },
    ]
}

chain.invoke(question)
# Output:
#   'Hello there! I\'m here to help with your questions. Regarding your query about "rag," it\'s not something typically associated with a "hello world" bot, but I\'m happy to explain!\n\nRAG, or Remote Angular GUI, is a tool that allows you to create and manage Angular applications remotely. It\'s a way to develop and test Angular components and applications without needing to set up a local development environment. This can be particularly useful for teams working on distributed systems or for developers who prefer to work in a cloud-based environment.\n\nI hope this explanation of RAG has been helpful and interesting! If you have any other questions or need further clarification, feel free to ask.'
#   Trace(request_id=tr-ea2226413395413ba2cf52cffc523502)

mlflow.models.set_model(model=model)



================================================
FILE: cookbook/Parallel_function_calling.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
This is a tutorial on using Parallel function calling with LiteLLM
"""

!pip install litellm

"""
This tutorial walks through the steps doing parallel function calling using
 - OpenAI
 - Azure OpenAI
"""

# set openai api key
import os
os.environ['OPENAI_API_KEY'] = "" # litellm reads OPENAI_API_KEY from .env and sends the request

"""

# OpenAI gpt-3.5-turbo-1106
## Step 1: send the conversation and available functions to the model
"""

import litellm
import json
# Example dummy function hard coded to return the same weather
# In production, this could be your backend API or an external API
def get_current_weather(location, unit="fahrenheit"):
    """Get the current weather in a given location"""
    if "tokyo" in location.lower():
        return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})
    elif "san francisco" in location.lower():
        return json.dumps({"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"})
    elif "paris" in location.lower():
        return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})
    else:
        return json.dumps({"location": location, "temperature": "unknown"})

messages = [{"role": "user", "content": "What's the weather like in San Francisco, Tokyo, and Paris?"}]
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            },
        },
    }
]

response = litellm.completion(
    model="gpt-3.5-turbo-1106",
    messages=messages,
    tools=tools,
    tool_choice="auto",  # auto is default, but we'll be explicit
)
print("\nLLM Response1:\n", response)
response_message = response.choices[0].message
tool_calls = response.choices[0].message.tool_calls
print("\nTool Choice:\n", tool_calls)

# Output:
#   

#   LLM Response1:

#    ModelResponse(id='chatcmpl-8MNdPbrhtnwiPK1x3PEoGwrH144TW', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(id='call_K2Giwoq3NloGPfSv25MJVFZG', function=Function(arguments='{"location": "San Francisco", "unit": "celsius"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_6K8bYCZK6qsbMY3n51FzE5Nz', function=Function(arguments='{"location": "Tokyo", "unit": "celsius"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_cKSmUEJGufDwS7TaUHWzp7qx', function=Function(arguments='{"location": "Paris", "unit": "celsius"}', name='get_current_weather'), type='function')]))], created=1700344759, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage={'completion_tokens': 77, 'prompt_tokens': 88, 'total_tokens': 165}, _response_ms=1049.913)

#   

#   Tool Choice:

#    [ChatCompletionMessageToolCall(id='call_K2Giwoq3NloGPfSv25MJVFZG', function=Function(arguments='{"location": "San Francisco", "unit": "celsius"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_6K8bYCZK6qsbMY3n51FzE5Nz', function=Function(arguments='{"location": "Tokyo", "unit": "celsius"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_cKSmUEJGufDwS7TaUHWzp7qx', function=Function(arguments='{"location": "Paris", "unit": "celsius"}', name='get_current_weather'), type='function')]


"""
## Step 2 - Parse the Model Response and Execute Functions
"""

# Check if the model wants to call a function
if tool_calls:
    # Execute the functions and prepare responses
    available_functions = {
        "get_current_weather": get_current_weather,
    }

    messages.append(response_message)  # Extend conversation with assistant's reply

    for tool_call in tool_calls:
      print(f"\nExecuting tool call\n{tool_call}")
      function_name = tool_call.function.name
      function_to_call = available_functions[function_name]
      function_args = json.loads(tool_call.function.arguments)
      # calling the get_current_weather() function
      function_response = function_to_call(
          location=function_args.get("location"),
          unit=function_args.get("unit"),
      )
      print(f"Result from tool call\n{function_response}\n")

      # Extend conversation with function response
      messages.append(
          {
              "tool_call_id": tool_call.id,
              "role": "tool",
              "name": function_name,
              "content": function_response,
          }
      )

# Output:
#   

#   Executing tool call

#   ChatCompletionMessageToolCall(id='call_K2Giwoq3NloGPfSv25MJVFZG', function=Function(arguments='{"location": "San Francisco", "unit": "celsius"}', name='get_current_weather'), type='function')

#   Result from tool call

#   {"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"}

#   

#   

#   Executing tool call

#   ChatCompletionMessageToolCall(id='call_6K8bYCZK6qsbMY3n51FzE5Nz', function=Function(arguments='{"location": "Tokyo", "unit": "celsius"}', name='get_current_weather'), type='function')

#   Result from tool call

#   {"location": "Tokyo", "temperature": "10", "unit": "celsius"}

#   

#   

#   Executing tool call

#   ChatCompletionMessageToolCall(id='call_cKSmUEJGufDwS7TaUHWzp7qx', function=Function(arguments='{"location": "Paris", "unit": "celsius"}', name='get_current_weather'), type='function')

#   Result from tool call

#   {"location": "Paris", "temperature": "22", "unit": "celsius"}

#   


"""
## Step 3 - Second litellm.completion() call
"""

second_response = litellm.completion(
    model="gpt-3.5-turbo-1106",
    messages=messages,
)
print("Second Response\n", second_response)
print("Second Response Message\n", second_response.choices[0].message.content)

# Output:
#   Second Response

#    ModelResponse(id='chatcmpl-8MNhat166ZqjO6egXcUh85Pd0s7KV', choices=[Choices(finish_reason='stop', index=0, message=Message(content="The current weather in San Francisco is 72°F, in Tokyo it's 10°C, and in Paris it's 22°C.", role='assistant'))], created=1700345018, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage={'completion_tokens': 28, 'prompt_tokens': 465, 'total_tokens': 493}, _response_ms=999.246)

#   Second Response Message

#    The current weather in San Francisco is 72°F, in Tokyo it's 10°C, and in Paris it's 22°C.


"""
## Using Azure OpenAI
"""

# set Azure env variables
import os
os.environ['AZURE_API_KEY'] = "" # litellm reads AZURE_API_KEY from .env and sends the request
os.environ['AZURE_API_BASE'] = "https://openai-gpt-4-test-v-1.openai.azure.com/"
os.environ['AZURE_API_VERSION'] = "2023-07-01-preview"

"""
## Step 1
"""

import litellm
import json
# Example dummy function hard coded to return the same weather
# In production, this could be your backend API or an external API
def get_current_weather(location, unit="fahrenheit"):
    """Get the current weather in a given location"""
    if "tokyo" in location.lower():
        return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})
    elif "san francisco" in location.lower():
        return json.dumps({"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"})
    elif "paris" in location.lower():
        return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})
    else:
        return json.dumps({"location": location, "temperature": "unknown"})

messages = [{"role": "user", "content": "What's the weather like in San Francisco, Tokyo, and Paris?"}]
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            },
        },
    }
]

response = litellm.completion(
    model="azure/chatgpt-functioncalling", # model = azure/<your-azure-deployment-name>
    messages=messages,
    tools=tools,
    tool_choice="auto",  # auto is default, but we'll be explicit
)
print("\nLLM Response1:\n", response)
response_message = response.choices[0].message
tool_calls = response.choices[0].message.tool_calls
print("\nTool Choice:\n", tool_calls)

# Output:
#   

#   LLM Response1:

#    ModelResponse(id='chatcmpl-8MOBPvEnqG7qitkmVqZmCrzSGEmDj', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(id='call_7gZ0PkmmmgzTOxfF01ATp0U5', function=Function(arguments='{\n  "location": "San Francisco, CA"\n}', name='get_current_weather'), type='function')]))], created=1700346867, model='gpt-35-turbo', object='chat.completion', system_fingerprint=None, usage={'completion_tokens': 19, 'prompt_tokens': 88, 'total_tokens': 107}, _response_ms=833.4319999999999)

#   

#   Tool Choice:

#    [ChatCompletionMessageToolCall(id='call_7gZ0PkmmmgzTOxfF01ATp0U5', function=Function(arguments='{\n  "location": "San Francisco, CA"\n}', name='get_current_weather'), type='function')]


"""
## Step 2
"""

# Check if the model wants to call a function
if tool_calls:
    # Execute the functions and prepare responses
    available_functions = {
        "get_current_weather": get_current_weather,
    }

    messages.append(response_message)  # Extend conversation with assistant's reply

    for tool_call in tool_calls:
      print(f"\nExecuting tool call\n{tool_call}")
      function_name = tool_call.function.name
      function_to_call = available_functions[function_name]
      function_args = json.loads(tool_call.function.arguments)
      # calling the get_current_weather() function
      function_response = function_to_call(
          location=function_args.get("location"),
          unit=function_args.get("unit"),
      )
      print(f"Result from tool call\n{function_response}\n")

      # Extend conversation with function response
      messages.append(
          {
              "tool_call_id": tool_call.id,
              "role": "tool",
              "name": function_name,
              "content": function_response,
          }
      )

# Output:
#   

#   Executing tool call

#   ChatCompletionMessageToolCall(id='call_7gZ0PkmmmgzTOxfF01ATp0U5', function=Function(arguments='{\n  "location": "San Francisco, CA"\n}', name='get_current_weather'), type='function')

#   Result from tool call

#   {"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"}

#   


"""
## Step 3
"""

second_response = litellm.completion(
    model="azure/chatgpt-functioncalling",
    messages=messages,
)
print("Second Response\n", second_response)
print("Second Response Message\n", second_response.choices[0].message.content)

# Output:
#   Second Response

#    ModelResponse(id='chatcmpl-8MOC90vwZ2LHX0DE796XYtsOxdGcc', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The current weather in San Francisco is 72°F.', role='assistant'))], created=1700346913, model='gpt-35-turbo', object='chat.completion', system_fingerprint=None, usage={'completion_tokens': 11, 'prompt_tokens': 69, 'total_tokens': 80}, _response_ms=824.882)

#   Second Response Message

#    The current weather in San Francisco is 72°F.




================================================
FILE: cookbook/Proxy_Batch_Users.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Environment Setup
"""

import csv
from typing import Optional
import httpx
import json
import asyncio

proxy_base_url = "http://0.0.0.0:4000" # 👈 SET TO PROXY URL
master_key = "sk-1234" # 👈 SET TO PROXY MASTER KEY

## GLOBAL HTTP CLIENT ## - faster http calls
class HTTPHandler:
    def __init__(self, concurrent_limit=1000):
        # Create a client with a connection pool
        self.client = httpx.AsyncClient(
            limits=httpx.Limits(
                max_connections=concurrent_limit,
                max_keepalive_connections=concurrent_limit,
            )
        )

    async def close(self):
        # Close the client when you're done with it
        await self.client.aclose()

    async def get(
        self, url: str, params: Optional[dict] = None, headers: Optional[dict] = None
    ):
        response = await self.client.get(url, params=params, headers=headers)
        return response

    async def post(
        self,
        url: str,
        data: Optional[dict] = None,
        params: Optional[dict] = None,
        headers: Optional[dict] = None,
    ):
        try:
            response = await self.client.post(
                url, data=data, params=params, headers=headers
            )
            return response
        except Exception as e:
            raise e


"""
# Import Sheet


Format: | ID | Name | Max Budget |
"""

async def import_sheet():
    tasks = []
    http_client = HTTPHandler()
    with open('my-batch-sheet.csv', 'r') as file:
        csv_reader = csv.DictReader(file)
        for row in csv_reader:
            task = create_user(client=http_client, user_id=row['ID'], max_budget=row['Max Budget'], user_name=row['Name'])
            tasks.append(task)
            # print(f"ID: {row['ID']}, Name: {row['Name']}, Max Budget: {row['Max Budget']}")

    keys = await asyncio.gather(*tasks)

    with open('my-batch-sheet_new.csv', 'w', newline='') as new_file:
        fieldnames = ['ID', 'Name', 'Max Budget', 'keys']
        csv_writer = csv.DictWriter(new_file, fieldnames=fieldnames)
        csv_writer.writeheader()

        with open('my-batch-sheet.csv', 'r') as file:
            csv_reader = csv.DictReader(file)
            for i, row in enumerate(csv_reader):
                row['keys'] = keys[i]  # Add the 'keys' value from the corresponding task result
                csv_writer.writerow(row)

    await http_client.close()

asyncio.run(import_sheet())

"""
# Create Users + Keys

- Creates a user
- Creates a key with max budget
"""


async def create_key_with_alias(client: HTTPHandler, user_id: str, max_budget: float):
    global proxy_base_url
    if not proxy_base_url.endswith("/"):
        proxy_base_url += "/"
    url = proxy_base_url + "key/generate"

    # call /key/generate
    print("CALLING /KEY/GENERATE")
    response = await client.post(
        url=url,
        headers={"Authorization": f"Bearer {master_key}"},
        data=json.dumps({
            "user_id": user_id,
            "key_alias": f"{user_id}-key",
            "max_budget": max_budget # 👈 KEY CHANGE: SETS MAX BUDGET PER KEY
        })
    )
    print(f"response: {response.text}")
    return response.json()["key"]

async def create_user(client: HTTPHandler, user_id: str, max_budget: float, user_name: str):
    """
    - call /user/new
    - create key for user
    """
    global proxy_base_url
    if not proxy_base_url.endswith("/"):
        proxy_base_url += "/"
    url = proxy_base_url + "user/new"

    # call /user/new
    await client.post(
        url=url,
        headers={"Authorization": f"Bearer {master_key}"},
        data=json.dumps({
            "user_id": user_id,
            "user_alias": user_name,
            "auto_create_key": False,
            # "max_budget": max_budget # 👈 [OPTIONAL] Sets max budget per user (if you want to set a max budget across keys)
        })
    )

    # create key for user
    return await create_key_with_alias(client=client, user_id=user_id, max_budget=max_budget)




================================================
FILE: cookbook/Using_Nemo_Guardrails_with_LiteLLM_Server.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Using Nemo-Guardrails with LiteLLM Server

[Call Bedrock, TogetherAI, Huggingface, etc. on the server](https://docs.litellm.ai/docs/providers)
"""

"""
## Using with Bedrock

`docker run -e PORT=8000 -e AWS_ACCESS_KEY_ID=<your-aws-access-key> -e AWS_SECRET_ACCESS_KEY=<your-aws-secret-key> -p 8000:8000 ghcr.io/berriai/litellm:latest`
"""

pip install nemoguardrails langchain

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="anthropic.claude-v2", openai_api_base="http://0.0.0.0:8000", openai_api_key="my-fake-key")

from nemoguardrails import LLMRails, RailsConfig

config = RailsConfig.from_path("./config.yml")
app = LLMRails(config, llm=llm)

new_message = app.generate(messages=[{
    "role": "user",
    "content": "Hello! What can you do for me?"
}])

"""
## Using with TogetherAI

1. You can either set this in the server environment:
`docker run -e PORT=8000 -e TOGETHERAI_API_KEY=<your-together-ai-api-key> -p 8000:8000 ghcr.io/berriai/litellm:latest`

2. **Or** Pass this in as the api key `(...openai_api_key="<your-together-ai-api-key>")`
"""

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="together_ai/togethercomputer/CodeLlama-13b-Instruct", openai_api_base="http://0.0.0.0:8000", openai_api_key="my-together-ai-api-key")

from nemoguardrails import LLMRails, RailsConfig

config = RailsConfig.from_path("./config.yml")
app = LLMRails(config, llm=llm)

new_message = app.generate(messages=[{
    "role": "user",
    "content": "Hello! What can you do for me?"
}])

"""
### CONFIG.YML

save this example `config.yml` in your current directory
"""

# instructions:
#   - type: general
#     content: |
#       Below is a conversation between a bot and a user about the recent job reports.
#       The bot is factual and concise. If the bot does not know the answer to a
#       question, it truthfully says it does not know.

# sample_conversation: |
#   user "Hello there!"
#     express greeting
#   bot express greeting
#     "Hello! How can I assist you today?"
#   user "What can you do for me?"
#     ask about capabilities
#   bot respond about capabilities
#     "I am an AI assistant that helps answer mathematical questions. My core mathematical skills are powered by wolfram alpha."
#   user "What's 2+2?"
#     ask math question
#   bot responds to math question
#     "2+2 is equal to 4."

# models:
#   - type: main
#     engine: openai
#     model: claude-instant-1



================================================
FILE: cookbook/veo_video_generation.py
================================================
#!/usr/bin/env python3
"""
Complete example for Veo video generation through LiteLLM proxy.

This script demonstrates how to:
1. Generate videos using Google's Veo model
2. Poll for completion status
3. Download the generated video file

Requirements:
- LiteLLM proxy running with Google AI Studio pass-through configured
- Google AI Studio API key with Veo access
"""

import json
import os
import time
import requests
from typing import Optional


class VeoVideoGenerator:
    """Complete Veo video generation client using LiteLLM proxy."""
    
    def __init__(self, base_url: str = "http://localhost:4000/gemini/v1beta", 
                 api_key: str = "sk-1234"):
        """
        Initialize the Veo video generator.
        
        Args:
            base_url: Base URL for the LiteLLM proxy with Gemini pass-through
            api_key: API key for LiteLLM proxy authentication
        """
        self.base_url = base_url
        self.api_key = api_key
        self.headers = {
            "x-goog-api-key": api_key,
            "Content-Type": "application/json"
        }
    
    def generate_video(self, prompt: str) -> Optional[str]:
        """
        Initiate video generation with Veo.
        
        Args:
            prompt: Text description of the video to generate
            
        Returns:
            Operation name if successful, None otherwise
        """
        print(f"🎬 Generating video with prompt: '{prompt}'")
        
        url = f"{self.base_url}/models/veo-3.0-generate-preview:predictLongRunning"
        payload = {
            "instances": [{
                "prompt": prompt
            }]
        }
        
        try:
            response = requests.post(url, headers=self.headers, json=payload)
            response.raise_for_status()
            
            data = response.json()
            operation_name = data.get("name")
            
            if operation_name:
                print(f"✅ Video generation started: {operation_name}")
                return operation_name
            else:
                print("❌ No operation name returned")
                print(f"Response: {json.dumps(data, indent=2)}")
                return None
                
        except requests.RequestException as e:
            print(f"❌ Failed to start video generation: {e}")
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_data = e.response.json()
                    print(f"Error details: {json.dumps(error_data, indent=2)}")
                except:
                    print(f"Error response: {e.response.text}")
            return None
    
    def wait_for_completion(self, operation_name: str, max_wait_time: int = 600) -> Optional[str]:
        """
        Poll operation status until video generation is complete.
        
        Args:
            operation_name: Name of the operation to monitor
            max_wait_time: Maximum time to wait in seconds (default: 10 minutes)
            
        Returns:
            Video URI if successful, None otherwise
        """
        print("⏳ Waiting for video generation to complete...")
        
        operation_url = f"{self.base_url}/{operation_name}"
        start_time = time.time()
        poll_interval = 10  # Start with 10 seconds
        
        while time.time() - start_time < max_wait_time:
            try:
                print(f"🔍 Polling status... ({int(time.time() - start_time)}s elapsed)")
                
                response = requests.get(operation_url, headers=self.headers)
                response.raise_for_status()
                
                data = response.json()
                
                # Check for errors
                if "error" in data:
                    print("❌ Error in video generation:")
                    print(json.dumps(data["error"], indent=2))
                    return None
                
                # Check if operation is complete
                is_done = data.get("done", False)
                
                if is_done:
                    print("🎉 Video generation complete!")
                    
                    try:
                        # Extract video URI from nested response
                        video_uri = data["response"]["generateVideoResponse"]["generatedSamples"][0]["video"]["uri"]
                        print(f"📹 Video URI: {video_uri}")
                        return video_uri
                    except KeyError as e:
                        print(f"❌ Could not extract video URI: {e}")
                        print("Full response:")
                        print(json.dumps(data, indent=2))
                        return None
                
                # Wait before next poll, with exponential backoff
                time.sleep(poll_interval)
                poll_interval = min(poll_interval * 1.2, 30)  # Cap at 30 seconds
                
            except requests.RequestException as e:
                print(f"❌ Error polling operation status: {e}")
                time.sleep(poll_interval)
        
        print(f"⏰ Timeout after {max_wait_time} seconds")
        return None
    
    def download_video(self, video_uri: str, output_filename: str = "generated_video.mp4") -> bool:
        """
        Download the generated video file.
        
        Args:
            video_uri: URI of the video to download (from Google's response)
            output_filename: Local filename to save the video
            
        Returns:
            True if download successful, False otherwise
        """
        print(f"⬇️  Downloading video...")
        print(f"Original URI: {video_uri}")
        
        # Convert Google URI to LiteLLM proxy URI
        # Example: files/abc123 -> /gemini/v1beta/files/abc123:download?alt=media
        if video_uri.startswith("files/"):
            download_path = f"{video_uri}:download?alt=media"
        else:
            download_path = video_uri
            
        litellm_download_url = f"{self.base_url}/{download_path}"
        print(f"Download URL: {litellm_download_url}")
        
        try:
            # Download with streaming and redirect handling
            response = requests.get(
                litellm_download_url, 
                headers=self.headers, 
                stream=True,
                allow_redirects=True  # Handle redirects automatically
            )
            response.raise_for_status()
            
            # Save video file
            with open(output_filename, 'wb') as f:
                downloaded_size = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        # Progress indicator for large files
                        if downloaded_size % (1024 * 1024) == 0:  # Every MB
                            print(f"📦 Downloaded {downloaded_size / (1024*1024):.1f} MB...")
            
            # Verify file was created and has content
            if os.path.exists(output_filename):
                file_size = os.path.getsize(output_filename)
                if file_size > 0:
                    print(f"✅ Video downloaded successfully!")
                    print(f"📁 Saved as: {output_filename}")
                    print(f"📏 File size: {file_size / (1024*1024):.2f} MB")
                    return True
                else:
                    print("❌ Downloaded file is empty")
                    os.remove(output_filename)
                    return False
            else:
                print("❌ File was not created")
                return False
                
        except requests.RequestException as e:
            print(f"❌ Download failed: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"Status code: {e.response.status_code}")
                print(f"Response headers: {dict(e.response.headers)}")
            return False
    
    def generate_and_download(self, prompt: str, output_filename: str = None) -> bool:
        """
        Complete workflow: generate video and download it.
        
        Args:
            prompt: Text description for video generation
            output_filename: Output filename (auto-generated if None)
            
        Returns:
            True if successful, False otherwise
        """
        # Auto-generate filename if not provided
        if output_filename is None:
            timestamp = int(time.time())
            safe_prompt = "".join(c for c in prompt[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
            output_filename = f"veo_video_{safe_prompt.replace(' ', '_')}_{timestamp}.mp4"
        
        print("=" * 60)
        print("🎬 VEO VIDEO GENERATION WORKFLOW")
        print("=" * 60)
        
        # Step 1: Generate video
        operation_name = self.generate_video(prompt)
        if not operation_name:
            return False
        
        # Step 2: Wait for completion
        video_uri = self.wait_for_completion(operation_name)
        if not video_uri:
            return False
        
        # Step 3: Download video
        success = self.download_video(video_uri, output_filename)
        
        if success:
            print("=" * 60)
            print("🎉 SUCCESS! Video generation complete!")
            print(f"📁 Video saved as: {output_filename}")
            print("=" * 60)
        else:
            print("=" * 60)
            print("❌ FAILED! Video generation or download failed")
            print("=" * 60)
        
        return success


def main():
    """
    Example usage of the VeoVideoGenerator.
    
    Configure these environment variables:
    - LITELLM_BASE_URL: Your LiteLLM proxy URL (default: http://localhost:4000/gemini/v1beta)
    - LITELLM_API_KEY: Your LiteLLM API key (default: sk-1234)
    """
    
    # Configuration from environment or defaults
    base_url = os.getenv("LITELLM_BASE_URL", "http://localhost:4000/gemini/v1beta")
    api_key = os.getenv("LITELLM_API_KEY", "sk-1234")
    
    print("🚀 Starting Veo Video Generation Example")
    print(f"📡 Using LiteLLM proxy at: {base_url}")
    
    # Initialize generator
    generator = VeoVideoGenerator(base_url=base_url, api_key=api_key)
    
    # Example prompts - try different ones!
    example_prompts = [
        "A cat playing with a ball of yarn in a sunny garden",
        "Ocean waves crashing against rocky cliffs at sunset",
        "A bustling city street with people walking and cars passing by",
        "A peaceful forest with sunlight filtering through the trees"
    ]
    
    # Use first example or get from user
    prompt = example_prompts[0]
    print(f"🎬 Using prompt: '{prompt}'")
    
    # Generate and download video
    success = generator.generate_and_download(prompt)
    
    if success:
        print("\n✅ Example completed successfully!")
        print("💡 Try modifying the prompt in the script for different videos!")
    else:
        print("\n❌ Example failed!")
        print("🔧 Check your LiteLLM proxy configuration and Google AI Studio API key")
        
        # Troubleshooting tips
        print("\n🔍 Troubleshooting:")
        print("1. Ensure LiteLLM proxy is running with Google AI Studio pass-through")
        print("2. Verify your Google AI Studio API key has Veo access")
        print("3. Check that your prompt meets Veo's content guidelines")
        print("4. Review the LiteLLM proxy logs for detailed error information")


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/VLLM_Model_Testing.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Set up Environment
"""

!pip install --upgrade litellm

!pip install vllm
# Output:
#   Successfully installed fastapi-0.103.1 h11-0.14.0 huggingface-hub-0.16.4 ninja-1.11.1 pydantic-1.10.12 ray-2.6.3 safetensors-0.3.3 sentencepiece-0.1.99 starlette-0.27.0 tokenizers-0.13.3 transformers-4.33.1 uvicorn-0.23.2 vllm-0.1.4 xformers-0.0.21


"""
# Load the Logs
"""

import pandas as pd

# path of the csv file
file_path = 'Model-prompts-example.csv'

# load the csv file as a pandas DataFrame
data = pd.read_csv(file_path)

data.head()
# Output:
#      Success   Timestamp                              Input  \

#   0     True  1694041195  This is the templated query input   

#   

#                                       Output RunId (Wandb Runid)  \

#   0  This is the query output from the model            8hlumwuk   

#   

#     Model ID (or Name)  

#   0   OpenAI/Turbo-3.5  

input_texts = data['Input'].values

messages = [[{"role": "user", "content": input_text}] for input_text in input_texts]

"""
# Running Inference
"""

from litellm import batch_completion
model_name = "facebook/opt-125m"
provider = "vllm"
response_list = batch_completion(
            model=model_name,
            custom_llm_provider=provider, # can easily switch to huggingface, replicate, together ai, sagemaker, etc.
            messages=messages,
            temperature=0.2,
            max_tokens=80,
        )

response_list
# Output:
#   [<ModelResponse at 0x7e5b87616750> JSON: {

#      "choices": [

#        {

#          "finish_reason": "stop",

#          "index": 0,

#          "message": {

#            "content": ".\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is",

#            "role": "assistant",

#            "logprobs": null

#          }

#        }

#      ],

#      "created": 1694053363.6139505,

#      "model": "facebook/opt-125m",

#      "usage": {

#        "prompt_tokens": 9,

#        "completion_tokens": 80,

#        "total_tokens": 89

#      }

#    }]

response_values = [response['choices'][0]['message']['content'] for response in response_list]

response_values
# Output:
#   ['.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is']

data[f"{model_name}_output"] = response_values

data.to_csv('model_responses.csv', index=False)



================================================
FILE: cookbook/benchmark/readme.md
================================================
<h1 align="center">
        LLM-Bench
    </h1>
    <p align="center">
        <p align="center">Benchmark LLMs response, cost and response time</p>
        <p>LLM vs Cost per input + output token ($)</p>
        <img width="806" alt="Screenshot 2023-11-13 at 2 51 06 PM" src="https://github.com/BerriAI/litellm/assets/29436595/6d1bed71-d062-40b8-a113-28359672636a">
    </p>
        <a href="https://docs.google.com/spreadsheets/d/1mvPbP02OLFgc-5-Ubn1KxGuQQdbMyG1jhMSWxAldWy4/edit?usp=sharing">
               Bar Graph Excel Sheet here
        </a>

| Model | Provider | Cost per input + output token ($)|
| --- | --- | --- |
| openrouter/mistralai/mistral-7b-instruct | openrouter | 0.0 |
| ollama/llama2 | ollama | 0.0 |
| ollama/llama2:13b | ollama | 0.0 |
| ollama/llama2:70b | ollama | 0.0 |
| ollama/llama2-uncensored | ollama | 0.0 |
| ollama/mistral | ollama | 0.0 |
| ollama/codellama | ollama | 0.0 |
| ollama/orca-mini | ollama | 0.0 |
| ollama/vicuna | ollama | 0.0 |
| perplexity/codellama-34b-instruct | perplexity | 0.0 |
| perplexity/llama-2-13b-chat | perplexity | 0.0 |
| perplexity/llama-2-70b-chat | perplexity | 0.0 |
| perplexity/mistral-7b-instruct | perplexity | 0.0 |
| perplexity/replit-code-v1.5-3b | perplexity | 0.0 |
| text-bison | vertex_ai-text-models | 0.00000025 |
| text-bison@001 | vertex_ai-text-models | 0.00000025 |
| chat-bison | vertex_ai-chat-models | 0.00000025 |
| chat-bison@001 | vertex_ai-chat-models | 0.00000025 |
| chat-bison-32k | vertex_ai-chat-models | 0.00000025 |
| code-bison | vertex_ai-code-text-models | 0.00000025 |
| code-bison@001 | vertex_ai-code-text-models | 0.00000025 |
| code-gecko@001 | vertex_ai-chat-models | 0.00000025 |
| code-gecko@latest | vertex_ai-chat-models | 0.00000025 |
| codechat-bison | vertex_ai-code-chat-models | 0.00000025 |
| codechat-bison@001 | vertex_ai-code-chat-models | 0.00000025 |
| codechat-bison-32k | vertex_ai-code-chat-models | 0.00000025 |
| palm/chat-bison | palm | 0.00000025 |
| palm/chat-bison-001 | palm | 0.00000025 |
| palm/text-bison | palm | 0.00000025 |
| palm/text-bison-001 | palm | 0.00000025 |
| palm/text-bison-safety-off | palm | 0.00000025 |
| palm/text-bison-safety-recitation-off | palm | 0.00000025 |
| anyscale/meta-llama/Llama-2-7b-chat-hf | anyscale | 0.0000003 |
| anyscale/mistralai/Mistral-7B-Instruct-v0.1 | anyscale | 0.0000003 |
| openrouter/meta-llama/llama-2-13b-chat | openrouter | 0.0000004 |
| openrouter/nousresearch/nous-hermes-llama2-13b | openrouter | 0.0000004 |
| deepinfra/meta-llama/Llama-2-7b-chat-hf | deepinfra | 0.0000004 |
| deepinfra/mistralai/Mistral-7B-Instruct-v0.1 | deepinfra | 0.0000004 |
| anyscale/meta-llama/Llama-2-13b-chat-hf | anyscale | 0.0000005 |
| amazon.titan-text-lite-v1 | bedrock | 0.0000007 |
| deepinfra/meta-llama/Llama-2-13b-chat-hf | deepinfra | 0.0000007 |
| text-babbage-001 | text-completion-openai | 0.0000008 |
| text-ada-001 | text-completion-openai | 0.0000008 |
| babbage-002 | text-completion-openai | 0.0000008 |
| openrouter/google/palm-2-chat-bison | openrouter | 0.000001 |
| openrouter/google/palm-2-codechat-bison | openrouter | 0.000001 |
| openrouter/meta-llama/codellama-34b-instruct | openrouter | 0.000001 |
| deepinfra/codellama/CodeLlama-34b-Instruct-hf | deepinfra | 0.0000012 |
| deepinfra/meta-llama/Llama-2-70b-chat-hf | deepinfra | 0.0000016499999999999999 |
| deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1 | deepinfra | 0.0000016499999999999999 |
| anyscale/meta-llama/Llama-2-70b-chat-hf | anyscale | 0.000002 |
| anyscale/codellama/CodeLlama-34b-Instruct-hf | anyscale | 0.000002 |
| gpt-3.5-turbo-1106 | openai | 0.000003 |
| openrouter/meta-llama/llama-2-70b-chat | openrouter | 0.000003 |
| amazon.titan-text-express-v1 | bedrock | 0.000003 |
| gpt-3.5-turbo | openai | 0.0000035 |
| gpt-3.5-turbo-0301 | openai | 0.0000035 |
| gpt-3.5-turbo-0613 | openai | 0.0000035 |
| gpt-3.5-turbo-instruct | text-completion-openai | 0.0000035 |
| openrouter/openai/gpt-3.5-turbo | openrouter | 0.0000035 |
| cohere.command-text-v14 | bedrock | 0.0000035 |
| gpt-3.5-turbo-0613 | openai | 0.0000035 |
| claude-instant-1 | anthropic | 0.00000714 |
| claude-instant-1.2 | anthropic | 0.00000714 |
| openrouter/anthropic/claude-instant-v1 | openrouter | 0.00000714 |
| anthropic.claude-instant-v1 | bedrock | 0.00000714 |
| openrouter/mancer/weaver | openrouter | 0.00001125 |
| j2-mid | ai21 | 0.00002 |
| ai21.j2-mid-v1 | bedrock | 0.000025 |
| openrouter/jondurbin/airoboros-l2-70b-2.1 | openrouter | 0.00002775 |
| command-nightly | cohere | 0.00003 |
| command | cohere | 0.00003 |
| command-light | cohere | 0.00003 |
| command-medium-beta | cohere | 0.00003 |
| command-xlarge-beta | cohere | 0.00003 |
| command-r-plus| cohere | 0.000018 |
| j2-ultra | ai21 | 0.00003 |
| ai21.j2-ultra-v1 | bedrock | 0.0000376 |
| gpt-4-1106-preview | openai | 0.00004 |
| gpt-4-vision-preview | openai | 0.00004 |
| claude-2 | anthropic | 0.0000437 |
| openrouter/anthropic/claude-2 | openrouter | 0.0000437 |
| anthropic.claude-v1 | bedrock | 0.0000437 |
| anthropic.claude-v2 | bedrock | 0.0000437 |
| gpt-4 | openai | 0.00009 |
| gpt-4-0314 | openai | 0.00009 |
| gpt-4-0613 | openai | 0.00009 |
| openrouter/openai/gpt-4 | openrouter | 0.00009 |
| gpt-4-32k | openai | 0.00018 |
| gpt-4-32k-0314 | openai | 0.00018 |
| gpt-4-32k-0613 | openai | 0.00018 |



## Setup:
```
git clone https://github.com/BerriAI/litellm
```
cd to `benchmark` dir
```
cd litellm/cookbook/benchmark
```

### Install Dependencies
```
pip install litellm click tqdm tabulate termcolor
```

### Configuration
In `benchmark/benchmark.py` select your LLMs, LLM API Key and questions

Supported LLMs: https://docs.litellm.ai/docs/providers

```python
# Define the list of models to benchmark
models = ['gpt-3.5-turbo', 'togethercomputer/llama-2-70b-chat', 'claude-2']

# Enter LLM API keys
os.environ['OPENAI_API_KEY'] = ""
os.environ['ANTHROPIC_API_KEY'] = ""
os.environ['TOGETHERAI_API_KEY'] = ""

# List of questions to benchmark (replace with your questions)
questions = [
    "When will BerriAI IPO?",
    "When will LiteLLM hit $100M ARR?"
]

```

## Run LLM-Bench
```
python3 benchmark.py
```

## Expected Output
```
Running question: When will BerriAI IPO? for model: claude-2: 100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.41s/it]

Benchmark Results for 'When will BerriAI IPO?':
+-----------------+----------------------------------------------------------------------------------+---------------------------+------------+
| Model           | Response                                                                         | Response Time (seconds)   | Cost ($)   |
+=================+==================================================================================+===========================+============+
| gpt-3.5-turbo   | As an AI language model, I cannot provide up-to-date information or predict      | 1.55 seconds              | $0.000122  |
|                 | future events. It is best to consult a reliable financial source or contact      |                           |            |
|                 | BerriAI directly for information regarding their IPO plans.                      |                           |            |
+-----------------+----------------------------------------------------------------------------------+---------------------------+------------+
| togethercompute | I'm not able to provide information about future IPO plans or dates for BerriAI  | 8.52 seconds              | $0.000531  |
| r/llama-2-70b-c | or any other company. IPO (Initial Public Offering) plans and timelines are      |                           |            |
| hat             | typically kept private by companies until they are ready to make a public        |                           |            |
|                 | announcement.  It's important to note that IPO plans can change and are subject  |                           |            |
|                 | to various factors, such as market conditions, financial performance, and        |                           |            |
|                 | regulatory approvals. Therefore, it's difficult to predict with certainty when   |                           |            |
|                 | BerriAI or any other company will go public.  If you're interested in staying    |                           |            |
|                 | up-to-date with BerriAI's latest news and developments, you may want to follow   |                           |            |
|                 | their official social media accounts, subscribe to their newsletter, or visit    |                           |            |
|                 | their website periodically for updates.                                          |                           |            |
+-----------------+----------------------------------------------------------------------------------+---------------------------+------------+
| claude-2        | I do not have any information about when or if BerriAI will have an initial      | 3.17 seconds              | $0.002084  |
|                 | public offering (IPO). As an AI assistant created by Anthropic to be helpful,    |                           |            |
|                 | harmless, and honest, I do not have insider knowledge about Anthropic's business |                           |            |
|                 | plans or strategies.                                                             |                           |            |
+-----------------+----------------------------------------------------------------------------------+---------------------------+------------+
```

## Support 
**🤝 Schedule a 1-on-1 Session:** Book a [1-on-1 session](https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat) with Krrish and Ishaan, the founders, to discuss any issues, provide feedback, or explore how we can improve LiteLLM for you.



================================================
FILE: cookbook/benchmark/benchmark.py
================================================
from litellm import completion, completion_cost
import time
import click
from tqdm import tqdm
from tabulate import tabulate
from termcolor import colored
import os


# Define the list of models to benchmark
# select any LLM listed here: https://docs.litellm.ai/docs/providers
models = ["gpt-3.5-turbo", "claude-2"]

# Enter LLM API keys
# https://docs.litellm.ai/docs/providers
os.environ["OPENAI_API_KEY"] = ""
os.environ["ANTHROPIC_API_KEY"] = ""

# List of questions to benchmark (replace with your questions)
questions = ["When will BerriAI IPO?", "When will LiteLLM hit $100M ARR?"]

# Enter your system prompt here
system_prompt = """
You are LiteLLMs helpful assistant
"""


@click.command()
@click.option(
    "--system-prompt",
    default="You are a helpful assistant that can answer questions.",
    help="System prompt for the conversation.",
)
def main(system_prompt):
    for question in questions:
        data = []  # Data for the current question

        with tqdm(total=len(models)) as pbar:
            for model in models:
                colored_description = colored(
                    f"Running question: {question} for model: {model}", "green"
                )
                pbar.set_description(colored_description)
                start_time = time.time()

                response = completion(
                    model=model,
                    max_tokens=500,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": question},
                    ],
                )

                end = time.time()
                total_time = end - start_time
                cost = completion_cost(completion_response=response)
                raw_response = response["choices"][0]["message"]["content"]

                data.append(
                    {
                        "Model": colored(model, "light_blue"),
                        "Response": raw_response,  # Colorize the response
                        "ResponseTime": colored(f"{total_time:.2f} seconds", "red"),
                        "Cost": colored(f"${cost:.6f}", "green"),  # Colorize the cost
                    }
                )

                pbar.update(1)

        # Separate headers from the data
        headers = ["Model", "Response", "Response Time (seconds)", "Cost ($)"]
        colwidths = [15, 80, 15, 10]

        # Create a nicely formatted table for the current question
        table = tabulate(
            [list(d.values()) for d in data],
            headers,
            tablefmt="grid",
            maxcolwidths=colwidths,
        )

        # Print the table for the current question
        colored_question = colored(question, "green")
        click.echo(f"\nBenchmark Results for '{colored_question}':")
        click.echo(table)  # Display the formatted table


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/benchmark/eval_suites_mlflow_autoevals/auto_evals.py
================================================
from dotenv import load_dotenv

load_dotenv()

import litellm

from autoevals.llm import *

###################

# litellm completion call
question = "which country has the highest population"
response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": question}],
)
print(response)
# use the auto eval Factuality() evaluator

print("calling evaluator")
evaluator = Factuality()
result = evaluator(
    output=response.choices[0]["message"][
        "content"
    ],  # response from litellm.completion()
    expected="India",  # expected output
    input=question,  # question passed to litellm.completion
)

print(result)



================================================
FILE: cookbook/codellama-server/README.MD
================================================
# CodeLlama Server: Streaming, Caching, Model Fallbacks (OpenAI + Anthropic), Prompt-tracking

Works with: Anthropic, Huggingface, Cohere, TogetherAI, Azure, OpenAI, etc.

[![PyPI Version](https://img.shields.io/pypi/v/litellm.svg)](https://pypi.org/project/litellm/)
[![PyPI Version](https://img.shields.io/badge/stable%20version-v0.1.345-blue?color=green&link=https://pypi.org/project/litellm/0.1.1/)](https://pypi.org/project/litellm/0.1.1/)
![Downloads](https://img.shields.io/pypi/dm/litellm)

[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/HuDPw-?referralCode=jch2ME)

**LIVE DEMO** - https://litellm.ai/playground

## What does CodeLlama Server do

- Uses Together AI's CodeLlama to answer coding questions, with GPT-4 + Claude-2 as backups (you can easily switch this to any model from Huggingface, Replicate, Cohere, AI21, Azure, OpenAI, etc.) 
- Sets default system prompt for guardrails `system_prompt = "Only respond to questions about code. Say 'I don't know' to anything outside of that."`
- Integrates with Promptlayer for model + prompt tracking 
- Example output

<img src="imgs/code-output.png" alt="Code Output" width="600"/>

- **Consistent Input/Output** Format
  - Call all models using the OpenAI format - `completion(model, messages)`
  - Text responses will always be available at `['choices'][0]['message']['content']`
  - Stream responses will always be available at `['choices'][0]['delta']['content']`
- **Error Handling** Using Model Fallbacks (if `CodeLlama` fails, try `GPT-4`) with cooldowns, and retries
- **Prompt Logging** - Log successful completions to promptlayer for testing + iterating on your prompts in production! (Learn more: https://litellm.readthedocs.io/en/latest/advanced/

  **Example: Logs sent to PromptLayer**

    <img src="imgs/promptlayer_logging.png" alt="Prompt Logging" width="900"/>


- **Token Usage & Spend** - Track Input + Completion tokens used + Spend/model - https://docs.litellm.ai/docs/token_usage
- **Caching** - Provides in-memory cache + GPT-Cache integration for more advanced usage - https://docs.litellm.ai/docs/caching/gpt_cache

- **Streaming & Async Support** - Return generators to stream text responses - TEST IT 👉 https://litellm.ai/

## API Endpoints

### `/chat/completions` (POST)

This endpoint is used to generate chat completions for 50+ support LLM API Models. Use llama2, GPT-4, Claude2 etc

#### Input

This API endpoint accepts all inputs in raw JSON and expects the following inputs

- `prompt` (string, required): The user's coding related question
- Additional Optional parameters: `temperature`, `functions`, `function_call`, `top_p`, `n`, `stream`. See the full list of supported inputs here: https://litellm.readthedocs.io/en/latest/input/

#### Example JSON body

For claude-2

```json
{
  "prompt": "write me a function to print hello world"
}
```

### Making an API request to the Code-Gen Server

```python
import requests
import json

url = "localhost:4000/chat/completions"

payload = json.dumps({
  "prompt": "write me a function to print hello world"
})
headers = {
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)

```

### Output [Response Format]

Responses from the server are given in the following format.
All responses from the server are returned in the following format (for all LLM models). More info on output here: https://litellm.readthedocs.io/en/latest/output/

```json
{
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": ".\n\n```\ndef print_hello_world():\n    print(\"hello world\")\n",
                "role": "assistant"
            }
        }
    ],
    "created": 1693279694.6474009,
    "model": "togethercomputer/CodeLlama-34b-Instruct",
    "usage": {
        "completion_tokens": 14,
        "prompt_tokens": 28,
        "total_tokens": 42
    }
}
```

## Installation & Usage

### Running Locally

1. Clone liteLLM repository to your local machine:
   ```
   git clone https://github.com/BerriAI/litellm-CodeLlama-server
   ```
2. Install the required dependencies using pip
   ```
   pip install requirements.txt
   ```
3. Set your LLM API keys
   ```
   os.environ['OPENAI_API_KEY]` = "YOUR_API_KEY"
   or
   set OPENAI_API_KEY in your .env file
   ```
4. Run the server:
   ```
   python main.py
   ```

## Deploying

1. Quick Start: Deploy on Railway

   [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/HuDPw-?referralCode=jch2ME)

2. `GCP`, `AWS`, `Azure`
   This project includes a `Dockerfile` allowing you to build and deploy a Docker Project on your providers

# Support / Talk with founders

- [Our calendar 👋](https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version)
- [Community Discord 💭](https://discord.gg/wuPM9dRgDw)
- Our numbers 📞 +1 (770) 8783-106 / +1 (412) 618-6238
- Our emails ✉️ ishaan@berri.ai / krrish@berri.ai

## Roadmap

- [ ] Implement user-based rate-limiting
- [ ] Spending controls per project - expose key creation endpoint
- [ ] Need to store a keys db -> mapping created keys to their alias (i.e. project name)
- [ ] Easily add new models as backups / as the entry-point (add this to the available model list)



================================================
FILE: cookbook/codellama-server/main.py
================================================
import traceback
from flask import Flask, request, Response
from flask_cors import CORS
import litellm
from util import handle_error
from litellm import completion
import os
import dotenv
import time
import json

dotenv.load_dotenv()

# TODO: set your keys in .env or here:
# os.environ["OPENAI_API_KEY"] = "" # set your openai key here
# os.environ["ANTHROPIC_API_KEY"] = "" # set your anthropic key here
# os.environ["TOGETHER_AI_API_KEY"] = "" # set your together ai key here
# see supported models / keys here: https://litellm.readthedocs.io/en/latest/supported/
######### ENVIRONMENT VARIABLES ##########
verbose = True

# litellm.caching_with_models = True # CACHING: caching_with_models Keys in the cache are messages + model. - to learn more: https://docs.litellm.ai/docs/caching/
######### PROMPT LOGGING ##########
os.environ["PROMPTLAYER_API_KEY"] = (
    ""  # set your promptlayer key here - https://promptlayer.com/
)

# set callbacks
litellm.success_callback = ["promptlayer"]
############ HELPER FUNCTIONS ###################################


def print_verbose(print_statement):
    if verbose:
        print(print_statement)


app = Flask(__name__)
CORS(app)


@app.route("/")
def index():
    return "received!", 200


def data_generator(response):
    for chunk in response:
        yield f"data: {json.dumps(chunk)}\n\n"


@app.route("/chat/completions", methods=["POST"])
def api_completion():
    data = request.json
    start_time = time.time()
    if data.get("stream") == "True":
        data["stream"] = True  # convert to boolean
    try:
        if "prompt" not in data:
            raise ValueError("data needs to have prompt")
        data["model"] = (
            "togethercomputer/CodeLlama-34b-Instruct"  # by default use Together AI's CodeLlama model - https://api.together.xyz/playground/chat?model=togethercomputer%2FCodeLlama-34b-Instruct
        )
        # COMPLETION CALL
        system_prompt = "Only respond to questions about code. Say 'I don't know' to anything outside of that."
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": data.pop("prompt")},
        ]
        data["messages"] = messages
        print(f"data: {data}")
        response = completion(**data)
        ## LOG SUCCESS
        end_time = time.time()
        if (
            "stream" in data and data["stream"] == True
        ):  # use generate_responses to stream responses
            return Response(data_generator(response), mimetype="text/event-stream")
    except Exception:
        # call handle_error function
        print_verbose(f"Got Error api_completion(): {traceback.format_exc()}")
        ## LOG FAILURE
        end_time = time.time()
        traceback_exception = traceback.format_exc()
        return handle_error(data=data)
    return response


@app.route("/get_models", methods=["POST"])
def get_models():
    try:
        return litellm.model_list
    except Exception as e:
        traceback.print_exc()
        response = {"error": str(e)}
    return response, 200


if __name__ == "__main__":
    from waitress import serve

    serve(app, host="0.0.0.0", port=4000, threads=500)



================================================
FILE: cookbook/community-resources/get_hf_models.py
================================================
import requests


def get_next_url(response):
    """
    Function to get 'next' url from Link header
    :param response: response from requests
    :return: next url or None
    """
    if "link" not in response.headers:
        return None
    headers = response.headers

    next_url = headers["Link"]
    print(next_url)
    start_index = next_url.find("<")
    end_index = next_url.find(">")

    return next_url[1:end_index]


def get_models(url):
    """
    Function to retrieve all models from paginated endpoint
    :param url: base url to make GET request
    :return: list of all models
    """
    models = []
    while url:
        response = requests.get(url)
        if response.status_code != 200:
            print(f"Failed to retrieve data. Status code: {response.status_code}")
            return models
        payload = response.json()
        url = get_next_url(response)
        models.extend(payload)
    return models


def get_cleaned_models(models):
    """
    Function to clean retrieved models
    :param models: list of retrieved models
    :return: list of cleaned models
    """
    cleaned_models = []
    for model in models:
        cleaned_models.append(model["id"])
    return cleaned_models


# Get text-generation models
url = "https://huggingface.co/api/models?filter=text-generation-inference"
text_generation_models = get_models(url)
cleaned_text_generation_models = get_cleaned_models(text_generation_models)

print(cleaned_text_generation_models)


# Get conversational models
url = "https://huggingface.co/api/models?filter=conversational"
conversational_models = get_models(url)
cleaned_conversational_models = get_cleaned_models(conversational_models)

print(cleaned_conversational_models)


def write_to_txt(cleaned_models, filename):
    """
    Function to write the contents of a list to a text file
    :param cleaned_models: list of cleaned models
    :param filename: name of the text file
    """
    with open(filename, "w") as f:
        for item in cleaned_models:
            f.write("%s\n" % item)


# Write contents of cleaned_text_generation_models to text_generation_models.txt
write_to_txt(
    cleaned_text_generation_models,
    "huggingface_llms_metadata/hf_text_generation_models.txt",
)

# Write contents of cleaned_conversational_models to conversational_models.txt
write_to_txt(
    cleaned_conversational_models,
    "huggingface_llms_metadata/hf_conversational_models.txt",
)



================================================
FILE: cookbook/community-resources/max_tokens.json
================================================
{
    "gpt-3.5-turbo": {
        "max_tokens": 4000,
        "input_cost_per_token": 0.0000015,
        "output_cost_per_token": 0.000002
    },
    "gpt-3.5-turbo-0613": {
        "max_tokens": 4000,
        "input_cost_per_token": 0.0000015,
        "output_cost_per_token": 0.000002
    },
    "gpt-3.5-turbo-0301": {
        "max_tokens": 4000,
        "input_cost_per_token": 0.0000015,
        "output_cost_per_token": 0.000002
    },
    "gpt-3.5-turbo-16k": {
        "max_tokens": 16000,
        "input_cost_per_token": 0.000003,
        "output_cost_per_token": 0.000004
    },
    "gpt-3.5-turbo-16k-0613": {
        "max_tokens": 16000,
        "input_cost_per_token": 0.000003,
        "output_cost_per_token": 0.000004
    },
    "gpt-4": {
        "max_tokens": 8000,
        "input_cost_per_token": 0.000003,
        "output_cost_per_token": 0.00006
    },
    "gpt-4-0613": {
        "max_tokens": 8000,
        "input_cost_per_token": 0.000003,
        "output_cost_per_token": 0.00006
    },
    "gpt-4-32k": {
        "max_tokens": 8000,
        "input_cost_per_token": 0.00006,
        "output_cost_per_token": 0.00012
    },
    "claude-instant-1": {
        "max_tokens": 100000,
        "input_cost_per_token": 0.00000163,
        "output_cost_per_token": 0.00000551
    },
    "claude-2": {
        "max_tokens": 100000,
        "input_cost_per_token": 0.00001102,
        "output_cost_per_token": 0.00003268
    },
    "text-bison-001": {
        "max_tokens": 8192,
        "input_cost_per_token": 0.000004,
        "output_cost_per_token": 0.000004
    },
    "chat-bison-001": {
        "max_tokens": 4096,
        "input_cost_per_token": 0.000002,
        "output_cost_per_token": 0.000002
    },
    "command-nightly": {
        "max_tokens": 4096,
        "input_cost_per_token": 0.000015,
        "output_cost_per_token": 0.000015
    },
    "replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1": {
        "max_tokens": 4096,
        "input_cost_per_token": 0.00000608,
        "output_cost_per_token": 0.00000608
    },
    "together-ai-up-to-3b": {
        "input_cost_per_token": 0.0000001,
        "output_cost_per_token": 0.0000001
    },
    "together-ai-3.1b-7b": {
        "input_cost_per_token": 0.0000002,
        "output_cost_per_token": 0.0000002
    },
    "together-ai-7.1b-20b": {
        "max_tokens": 1000,
        "input_cost_per_token": 0.0000004,
        "output_cost_per_token": 0.0000004
    },
    "together-ai-20.1b-40b": {
        "input_cost_per_token": 0.000001,
        "output_cost_per_token": 0.000001
    },
    "together-ai-40.1b-70b": {
        "input_cost_per_token": 0.000003,
        "output_cost_per_token": 0.000003
    }
}



================================================
FILE: cookbook/litellm-ollama-docker-image/Dockerfile
================================================
FROM ollama/ollama as ollama

RUN echo "auto installing llama2"

# auto install ollama/llama2
RUN ollama serve & sleep 2 && ollama pull llama2

RUN echo "installing litellm"

RUN apt-get update

# Install Python
RUN apt-get install -y python3 python3-pip

# Set the working directory in the container
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed packages specified in requirements.txt

RUN python3 -m pip install litellm
COPY start.sh /start.sh
ENTRYPOINT [ "/bin/bash", "/start.sh" ]



================================================
FILE: cookbook/litellm-ollama-docker-image/requirements.txt
================================================
litellm==1.61.15


================================================
FILE: cookbook/litellm-ollama-docker-image/start.sh
================================================
ollama serve &
litellm


================================================
FILE: cookbook/litellm-ollama-docker-image/test.py
================================================
import openai

api_base = "http://0.0.0.0:8000"

openai.api_base = api_base
openai.api_key = "temp-key"
print(openai.api_base)


print("LiteLLM: response from proxy with streaming")
response = openai.ChatCompletion.create(
    model="ollama/llama2",
    messages=[
        {
            "role": "user",
            "content": "this is a test request, acknowledge that you got it",
        }
    ],
    stream=True,
)

for chunk in response:
    print(f"LiteLLM: streaming response from proxy {chunk}")

response = openai.ChatCompletion.create(
    model="ollama/llama2",
    messages=[
        {
            "role": "user",
            "content": "this is a test request, acknowledge that you got it",
        }
    ],
)

print(f"LiteLLM: response from proxy {response}")



================================================
FILE: cookbook/litellm_proxy_server/readme.md
================================================
# liteLLM Proxy Server: 50+ LLM Models, Error Handling, Caching

### Azure, Llama2, OpenAI, Claude, Hugging Face, Replicate Models

[![PyPI Version](https://img.shields.io/pypi/v/litellm.svg)](https://pypi.org/project/litellm/)
[![PyPI Version](https://img.shields.io/badge/stable%20version-v0.1.345-blue?color=green&link=https://pypi.org/project/litellm/0.1.1/)](https://pypi.org/project/litellm/0.1.1/)
![Downloads](https://img.shields.io/pypi/dm/litellm)
[![litellm](https://img.shields.io/badge/%20%F0%9F%9A%85%20liteLLM-OpenAI%7CAzure%7CAnthropic%7CPalm%7CCohere%7CReplicate%7CHugging%20Face-blue?color=green)](https://github.com/BerriAI/litellm)

[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/DYqQAW?referralCode=t3ukrU)

![4BC6491E-86D0-4833-B061-9F54524B2579](https://github.com/BerriAI/litellm/assets/17561003/f5dd237b-db5e-42e1-b1ac-f05683b1d724)

## What does liteLLM proxy do

- Make `/chat/completions` requests for 50+ LLM models **Azure, OpenAI, Replicate, Anthropic, Hugging Face**

  Example: for `model` use `claude-2`, `gpt-3.5`, `gpt-4`, `command-nightly`, `stabilityai/stablecode-completion-alpha-3b-4k`

  ```json
  {
    "model": "replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1",
    "messages": [
      {
        "content": "Hello, whats the weather in San Francisco??",
        "role": "user"
      }
    ]
  }
  ```

- **Consistent Input/Output** Format
  - Call all models using the OpenAI format - `completion(model, messages)`
  - Text responses will always be available at `['choices'][0]['message']['content']`
- **Error Handling** Using Model Fallbacks (if `GPT-4` fails, try `llama2`)
- **Logging** - Log Requests, Responses and Errors to `Supabase`, `Posthog`, `Mixpanel`, `Sentry`, `Lunary`,`Athina`, `Helicone` (Any of the supported providers here: https://litellm.readthedocs.io/en/latest/advanced/

  **Example: Logs sent to Supabase**
  <img width="1015" alt="Screenshot 2023-08-11 at 4 02 46 PM" src="https://github.com/ishaan-jaff/proxy-server/assets/29436595/237557b8-ba09-4917-982c-8f3e1b2c8d08">

- **Token Usage & Spend** - Track Input + Completion tokens used + Spend/model
- **Caching** - Implementation of Semantic Caching
- **Streaming & Async Support** - Return generators to stream text responses

## API Endpoints

### `/chat/completions` (POST)

This endpoint is used to generate chat completions for 50+ support LLM API Models. Use llama2, GPT-4, Claude2 etc

#### Input

This API endpoint accepts all inputs in raw JSON and expects the following inputs

- `model` (string, required): ID of the model to use for chat completions. See all supported models [here]: (https://litellm.readthedocs.io/en/latest/supported/):
  eg `gpt-3.5-turbo`, `gpt-4`, `claude-2`, `command-nightly`, `stabilityai/stablecode-completion-alpha-3b-4k`
- `messages` (array, required): A list of messages representing the conversation context. Each message should have a `role` (system, user, assistant, or function), `content` (message text), and `name` (for function role).
- Additional Optional parameters: `temperature`, `functions`, `function_call`, `top_p`, `n`, `stream`. See the full list of supported inputs here: https://litellm.readthedocs.io/en/latest/input/

#### Example JSON body

For claude-2

```json
{
  "model": "claude-2",
  "messages": [
    {
      "content": "Hello, whats the weather in San Francisco??",
      "role": "user"
    }
  ]
}
```

### Making an API request to the Proxy Server

```python
import requests
import json

# TODO: use your URL
url = "http://localhost:5000/chat/completions"

payload = json.dumps({
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "content": "Hello, whats the weather in San Francisco??",
      "role": "user"
    }
  ]
})
headers = {
  'Content-Type': 'application/json'
}
response = requests.request("POST", url, headers=headers, data=payload)
print(response.text)

```

### Output [Response Format]

Responses from the server are given in the following format.
All responses from the server are returned in the following format (for all LLM models). More info on output here: https://litellm.readthedocs.io/en/latest/output/

```json
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "I'm sorry, but I don't have the capability to provide real-time weather information. However, you can easily check the weather in San Francisco by searching online or using a weather app on your phone.",
        "role": "assistant"
      }
    }
  ],
  "created": 1691790381,
  "id": "chatcmpl-7mUFZlOEgdohHRDx2UpYPRTejirzb",
  "model": "gpt-3.5-turbo-0613",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 41,
    "prompt_tokens": 16,
    "total_tokens": 57
  }
}
```

## Installation & Usage

### Running Locally

1. Clone liteLLM repository to your local machine:
   ```
   git clone https://github.com/BerriAI/liteLLM-proxy
   ```
2. Install the required dependencies using pip
   ```
   pip install -r requirements.txt
   ```
3. Set your LLM API keys
   ```
   os.environ['OPENAI_API_KEY]` = "YOUR_API_KEY"
   or
   set OPENAI_API_KEY in your .env file
   ```
4. Run the server:
   ```
   python main.py
   ```

## Deploying

1. Quick Start: Deploy on Railway

   [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/DYqQAW?referralCode=t3ukrU)

2. `GCP`, `AWS`, `Azure`
   This project includes a `Dockerfile` allowing you to build and deploy a Docker Project on your providers

# Support / Talk with founders

- [Our calendar 👋](https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version)
- [Community Discord 💭](https://discord.gg/wuPM9dRgDw)
- Our numbers 📞 +1 (770) 8783-106 / +1 (412) 618-6238
- Our emails ✉️ ishaan@berri.ai / krrish@berri.ai

## Roadmap

- [ ] Support hosted db (e.g. Supabase)
- [ ] Easily send data to places like posthog and sentry.
- [ ] Add a hot-cache for project spend logs - enables fast checks for user + project limitings
- [ ] Implement user-based rate-limiting
- [ ] Spending controls per project - expose key creation endpoint
- [ ] Need to store a keys db -> mapping created keys to their alias (i.e. project name)
- [ ] Easily add new models as backups / as the entry-point (add this to the available model list)



================================================
FILE: cookbook/litellm_proxy_server/grafana_dashboard/readme.md
================================================
# Contains LiteLLM maintained grafana dashboard

This folder contains the `json` for creating Grafana Dashboards

## [LiteLLM v2 Dashboard](./dashboard_v2)

<img width="1316" alt="grafana_1" src="https://github.com/user-attachments/assets/d0df802d-0cb9-4906-a679-941c547789ab">
<img width="1289" alt="grafana_2" src="https://github.com/user-attachments/assets/b11f755f-e113-42ab-b21d-83f91f451a28">
<img width="1323" alt="grafana_3" src="https://github.com/user-attachments/assets/cb29ffdb-477d-4be1-a5cd-c3f7f2cb21c5">



### Pre-Requisites
- Setup LiteLLM Proxy Prometheus Metrics https://docs.litellm.ai/docs/proxy/prometheus 



================================================
FILE: cookbook/litellm_proxy_server/grafana_dashboard/dashboard_1/readme.md
================================================
## This folder contains the `json` for creating the following Grafana Dashboard

### Pre-Requisites
- Setup LiteLLM Proxy Prometheus Metrics https://docs.litellm.ai/docs/proxy/prometheus 

![1716623265684](https://github.com/BerriAI/litellm/assets/29436595/0e12c57e-4a2d-4850-bd4f-e4294f87a814)



================================================
FILE: cookbook/litellm_proxy_server/grafana_dashboard/dashboard_1/grafana_dashboard.json
================================================
{
    "annotations": {
      "list": [
        {
          "builtIn": 1,
          "datasource": {
            "type": "grafana",
            "uid": "-- Grafana --"
          },
          "enable": true,
          "hide": true,
          "iconColor": "rgba(0, 211, 255, 1)",
          "name": "Annotations & Alerts",
          "target": {
            "limit": 100,
            "matchAny": false,
            "tags": [],
            "type": "dashboard"
          },
          "type": "dashboard"
        }
      ]
    },
    "description": "",
    "editable": true,
    "fiscalYearStartMonth": 0,
    "graphTooltip": 0,
    "id": 2039,
    "links": [],
    "liveNow": false,
    "panels": [
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            },
            "unit": "s"
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "id": 10,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "histogram_quantile(0.99, sum(rate(litellm_self_latency_bucket{self=\"self\"}[1m])) by (le))",
            "legendFormat": "Time to first token",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "Time to first token (latency)",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            },
            "unit": "currencyUSD"
          },
          "overrides": [
            {
              "matcher": {
                "id": "byName",
                "options": "7e4b0627fd32efdd2313c846325575808aadcf2839f0fde90723aab9ab73c78f"
              },
              "properties": [
                {
                  "id": "displayName",
                  "value": "Translata"
                }
              ]
            }
          ]
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 8
        },
        "id": 11,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "sum(increase(litellm_spend_metric_total[30d])) by (hashed_api_key)",
            "legendFormat": "{{team}}",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "Spend by team",
        "transformations": [],
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 9,
          "w": 12,
          "x": 0,
          "y": 16
        },
        "id": 2,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "sum by (model) (increase(litellm_requests_metric_total[5m]))",
            "legendFormat": "{{model}}",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "Requests by model",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "mappings": [],
            "noValue": "0",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 7,
          "w": 3,
          "x": 0,
          "y": 25
        },
        "id": 8,
        "options": {
          "colorMode": "value",
          "graphMode": "area",
          "justifyMode": "auto",
          "orientation": "auto",
          "reduceOptions": {
            "calcs": [
              "lastNotNull"
            ],
            "fields": "",
            "values": false
          },
          "textMode": "auto"
        },
        "pluginVersion": "9.4.17",
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "sum(increase(litellm_llm_api_failed_requests_metric_total[1h]))",
            "legendFormat": "__auto",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "Faild Requests",
        "type": "stat"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            },
            "unit": "currencyUSD"
          },
          "overrides": []
        },
        "gridPos": {
          "h": 7,
          "w": 3,
          "x": 3,
          "y": 25
        },
        "id": 6,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "sum(increase(litellm_spend_metric_total[30d])) by (model)",
            "legendFormat": "{{model}}",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "Spend",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 7,
          "w": 6,
          "x": 6,
          "y": 25
        },
        "id": 4,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "sum(increase(litellm_total_tokens_total[5m])) by (model)",
            "legendFormat": "__auto",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "Tokens",
        "type": "timeseries"
      }
    ],
    "refresh": "1m",
    "revision": 1,
    "schemaVersion": 38,
    "style": "dark",
    "tags": [],
    "templating": {
      "list": [
        {
          "current": {
            "selected": false,
            "text": "prometheus",
            "value": "edx8memhpd9tsa"
          },
          "hide": 0,
          "includeAll": false,
          "label": "datasource",
          "multi": false,
          "name": "DS_PROMETHEUS",
          "options": [],
          "query": "prometheus",
          "queryValue": "",
          "refresh": 1,
          "regex": "",
          "skipUrlSync": false,
          "type": "datasource"
        }
      ]
    },
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "timepicker": {},
    "timezone": "",
    "title": "LLM Proxy",
    "uid": "rgRrHxESz",
    "version": 15,
    "weekStart": ""
  }


================================================
FILE: cookbook/litellm_proxy_server/grafana_dashboard/dashboard_v2/grafana_dashboard.json
================================================
{
    "annotations": {
      "list": [
        {
          "builtIn": 1,
          "datasource": {
            "type": "grafana",
            "uid": "-- Grafana --"
          },
          "enable": true,
          "hide": true,
          "iconColor": "rgba(0, 211, 255, 1)",
          "name": "Annotations & Alerts",
          "type": "dashboard"
        }
      ]
    },
    "editable": true,
    "fiscalYearStartMonth": 0,
    "graphTooltip": 0,
    "id": 20,
    "links": [],
    "panels": [
      {
        "collapsed": false,
        "gridPos": {
          "h": 1,
          "w": 24,
          "x": 0,
          "y": 0
        },
        "id": 3,
        "panels": [],
        "title": "LiteLLM Proxy Level Metrics",
        "type": "row"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "description": "Total requests per second made to proxy - success + failure ",
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisBorderShow": false,
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "barWidthFactor": 0.6,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "insertNulls": false,
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 1
        },
        "id": 1,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "pluginVersion": "11.3.0-76761.patch01-77040",
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "disableTextWrap": false,
            "editorMode": "code",
            "expr": "sum(rate(litellm_proxy_total_requests_metric_total[2m]))",
            "fullMetaSearch": false,
            "includeNullMetadata": true,
            "legendFormat": "__auto",
            "range": true,
            "refId": "A",
            "useBackend": false
          }
        ],
        "title": "Proxy - Requests per second (success + failure)",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "description": "Failures per second by Exception Class",
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisBorderShow": false,
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "barWidthFactor": 0.6,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "insertNulls": false,
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 1
        },
        "id": 2,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "pluginVersion": "11.3.0-76761.patch01-77040",
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "disableTextWrap": false,
            "editorMode": "code",
            "expr": "sum(rate(litellm_proxy_failed_requests_metric_total[2m])) by (exception_class)",
            "fullMetaSearch": false,
            "includeNullMetadata": true,
            "legendFormat": "__auto",
            "range": true,
            "refId": "A",
            "useBackend": false
          }
        ],
        "title": "Proxy Failure Responses / Second By Exception Class",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "description": "Average Response latency (seconds)",
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisBorderShow": false,
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "barWidthFactor": 0.6,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "insertNulls": false,
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": [
            {
              "matcher": {
                "id": "byName",
                "options": "sum(rate(litellm_request_total_latency_metric_sum[2m]))/sum(rate(litellm_request_total_latency_metric_count[2m]))"
              },
              "properties": [
                {
                  "id": "displayName",
                  "value": "Average Latency (seconds)"
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "histogram_quantile(0.5, sum(rate(litellm_request_total_latency_metric_bucket[2m])) by (le))"
              },
              "properties": [
                {
                  "id": "displayName",
                  "value": "Median Latency (seconds)"
                }
              ]
            }
          ]
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 9
        },
        "id": 5,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "multi",
            "sort": "none"
          }
        },
        "pluginVersion": "11.3.0-76761.patch01-77040",
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "disableTextWrap": false,
            "editorMode": "code",
            "expr": "sum(rate(litellm_request_total_latency_metric_sum[2m]))/sum(rate(litellm_request_total_latency_metric_count[2m]))",
            "fullMetaSearch": false,
            "includeNullMetadata": true,
            "legendFormat": "__auto",
            "range": true,
            "refId": "A",
            "useBackend": false
          },
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "histogram_quantile(0.5, sum(rate(litellm_request_total_latency_metric_bucket[2m])) by (le))",
            "hide": false,
            "instant": false,
            "legendFormat": "__auto",
            "range": true,
            "refId": "Median latency seconds"
          }
        ],
        "title": "Proxy - Average & Median Response Latency (seconds)",
        "type": "timeseries"
      },
      {
        "collapsed": true,
        "gridPos": {
          "h": 1,
          "w": 24,
          "x": 0,
          "y": 17
        },
        "id": 7,
        "panels": [],
        "title": "LLM API Metrics",
        "type": "row"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "description": "x-ratelimit-remaining-requests returning from LLM APIs",
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisBorderShow": false,
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "barWidthFactor": 0.6,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "insertNulls": false,
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 18
        },
        "id": 6,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "pluginVersion": "11.3.0-76761.patch01-77040",
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "topk(5, sort(litellm_remaining_requests))",
            "legendFormat": "__auto",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "x-ratelimit-remaining-requests",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "description": "x-ratelimit-remaining-tokens from LLM API ",
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisBorderShow": false,
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "barWidthFactor": 0.6,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "insertNulls": false,
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 18
        },
        "id": 8,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "pluginVersion": "11.3.0-76761.patch01-77040",
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "topk(5, sort(litellm_remaining_tokens))",
            "legendFormat": "__auto",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "x-ratelimit-remaining-tokens",
        "type": "timeseries"
      },
      {
        "collapsed": true,
        "gridPos": {
          "h": 1,
          "w": 24,
          "x": 0,
          "y": 26
        },
        "id": 4,
        "panels": [],
        "title": "LiteLLM Metrics by Virtual Key and Team",
        "type": "row"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "description": "Requests per second by Key Alias (keys are LiteLLM Virtual Keys). If key is None - means no Alias Set ",
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisBorderShow": false,
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "barWidthFactor": 0.6,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "insertNulls": false,
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green"
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 27
        },
        "id": 9,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "pluginVersion": "11.3.0-76761.patch01-77040",
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "sum(rate(litellm_proxy_total_requests_metric_total[2m])) by (api_key_alias)\n",
            "legendFormat": "__auto",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "Requests per second by Key Alias",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "${DS_PROMETHEUS}"
        },
        "description": "Requests per second by Team Alias. If team is None - means no team alias Set ",
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisBorderShow": false,
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "barWidthFactor": 0.6,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "insertNulls": false,
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green"
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 27
        },
        "id": 10,
        "options": {
          "legend": {
            "calcs": [],
            "displayMode": "list",
            "placement": "bottom",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "pluginVersion": "11.3.0-76761.patch01-77040",
        "targets": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "${DS_PROMETHEUS}"
            },
            "editorMode": "code",
            "expr": "sum(rate(litellm_proxy_total_requests_metric_total[2m])) by (team_alias)\n",
            "legendFormat": "__auto",
            "range": true,
            "refId": "A"
          }
        ],
        "title": "Requests per second by Team Alias",
        "type": "timeseries"
      }
    ],
    "preload": false,
    "schemaVersion": 40,
    "tags": [],
    "templating": {
      "list": [
        {
          "current": {
            "selected": false,
            "text": "prometheus",
            "value": "edx8memhpd9tsb"
          },
          "hide": 0,
          "includeAll": false,
          "label": "datasource",
          "multi": false,
          "name": "DS_PROMETHEUS",
          "options": [],
          "query": "prometheus",
          "queryValue": "",
          "refresh": 1,
          "regex": "",
          "skipUrlSync": false,
          "type": "datasource"
        }
      ]
    },
    "time": {
      "from": "now-6h",
      "to": "now"
    },
    "timepicker": {},
    "timezone": "browser",
    "title": "LiteLLM Prod v2",
    "uid": "be059pwgrlg5cf",
    "version": 17,
    "weekStart": ""
  }


================================================
FILE: cookbook/litellm_router/error_log.txt
================================================
Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: Expecting value: line 1 column 1 (char 0)

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: Expecting value: line 1 column 1 (char 0)

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Exception: 'Response' object has no attribute 'get'

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Exception: 'Response' object has no attribute 'get'

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Exception: 'Response' object has no attribute 'get'




================================================
FILE: cookbook/litellm_router/load_test_proxy.py
================================================
import sys
import os
from dotenv import load_dotenv

load_dotenv()

sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path

from litellm import Router
import litellm

litellm.set_verbose = False
os.environ.pop("AZURE_AD_TOKEN")

model_list = [
    {  # list of model deployments
        "model_name": "gpt-3.5-turbo",  # model alias
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "azure/chatgpt-v-2",  # actual model name
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
    },
    {
        "model_name": "gpt-3.5-turbo",
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "azure/chatgpt-functioncalling",
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
    },
    {
        "model_name": "gpt-3.5-turbo",
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "gpt-3.5-turbo",
            "api_key": os.getenv("OPENAI_API_KEY"),
        },
    },
]
router = Router(model_list=model_list)


file_paths = [
    "test_questions/question1.txt",
    "test_questions/question2.txt",
    "test_questions/question3.txt",
]
questions = []

for file_path in file_paths:
    try:
        print(file_path)
        with open(file_path, "r") as file:
            content = file.read()
            questions.append(content)
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    except Exception as e:
        print(f"An error occurred: {e}")

# for q in questions:
#     print(q)


# make X concurrent calls to litellm.completion(model=gpt-35-turbo, messages=[]), pick a random question in questions array.
#  Allow me to tune X concurrent calls.. Log question, output/exception, response time somewhere
# show me a summary of requests made, success full calls, failed calls. For failed calls show me the exceptions

import concurrent.futures
import random
import time


# Function to make concurrent calls to OpenAI API
def make_openai_completion(question):
    try:
        start_time = time.time()
        import openai

        client = openai.OpenAI(
            api_key=os.environ["OPENAI_API_KEY"], base_url="http://0.0.0.0:8000"
        )  # base_url="http://0.0.0.0:8000",
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": f"You are a helpful assistant. Answer this question{question}",
                }
            ],
        )
        print(response)
        end_time = time.time()

        # Log the request details
        with open("request_log.txt", "a") as log_file:
            log_file.write(
                f"Question: {question[:100]}\nResponse ID:{response.id} Content:{response.choices[0].message.content[:10]}\nTime: {end_time - start_time:.2f} seconds\n\n"
            )

        return response
    except Exception as e:
        # Log exceptions for failed calls
        with open("error_log.txt", "a") as error_log_file:
            error_log_file.write(f"Question: {question[:100]}\nException: {str(e)}\n\n")
        return None


# Number of concurrent calls (you can adjust this)
concurrent_calls = 100

# List to store the futures of concurrent calls
futures = []

# Make concurrent calls
with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_calls) as executor:
    for _ in range(concurrent_calls):
        random_question = random.choice(questions)
        futures.append(executor.submit(make_openai_completion, random_question))

# Wait for all futures to complete
concurrent.futures.wait(futures)

# Summarize the results
successful_calls = 0
failed_calls = 0

for future in futures:
    if future.result() is not None:
        successful_calls += 1
    else:
        failed_calls += 1

print("Load test Summary:")
print(f"Total Requests: {concurrent_calls}")
print(f"Successful Calls: {successful_calls}")
print(f"Failed Calls: {failed_calls}")

# Display content of the logs
with open("request_log.txt", "r") as log_file:
    print("\nRequest Log:\n", log_file.read())

with open("error_log.txt", "r") as error_log_file:
    print("\nError Log:\n", error_log_file.read())



================================================
FILE: cookbook/litellm_router/load_test_queuing.py
================================================
import sys
import os
from dotenv import load_dotenv

load_dotenv()

sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path

from litellm import Router
import litellm

litellm.set_verbose = False
# os.environ.pop("AZURE_AD_TOKEN")

model_list = [
    {  # list of model deployments
        "model_name": "gpt-3.5-turbo",  # model alias
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "azure/chatgpt-v-2",  # actual model name
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
    },
    {
        "model_name": "gpt-3.5-turbo",
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "azure/chatgpt-functioncalling",
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
    },
    {
        "model_name": "gpt-3.5-turbo",
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "gpt-3.5-turbo",
            "api_key": os.getenv("OPENAI_API_KEY"),
        },
    },
]
router = Router(model_list=model_list)


file_paths = [
    "test_questions/question1.txt",
    "test_questions/question2.txt",
    "test_questions/question3.txt",
]
questions = []

for file_path in file_paths:
    try:
        print(file_path)
        with open(file_path, "r") as file:
            content = file.read()
            questions.append(content)
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    except Exception as e:
        print(f"An error occurred: {e}")

# for q in questions:
#     print(q)


# make X concurrent calls to litellm.completion(model=gpt-35-turbo, messages=[]), pick a random question in questions array.
#  Allow me to tune X concurrent calls.. Log question, output/exception, response time somewhere
# show me a summary of requests made, success full calls, failed calls. For failed calls show me the exceptions

import concurrent.futures
import random
import time


# Function to make concurrent calls to OpenAI API
def make_openai_completion(question):
    try:
        start_time = time.time()
        import requests

        data = {
            "model": "gpt-3.5-turbo",
            "messages": [
                {
                    "role": "system",
                    "content": f"You are a helpful assistant. Answer this question{question}",
                },
            ],
        }
        response = requests.post("http://0.0.0.0:8000/queue/request", json=data)
        response = response.json()
        end_time = time.time()
        # Log the request details
        with open("request_log.txt", "a") as log_file:
            log_file.write(
                f"Question: {question[:100]}\nResponse ID: {response.get('id', 'N/A')} Url: {response.get('url', 'N/A')}\nTime: {end_time - start_time:.2f} seconds\n\n"
            )

        # polling the url
        while True:
            try:
                url = response["url"]
                polling_url = f"http://0.0.0.0:8000{url}"
                polling_response = requests.get(polling_url)
                polling_response = polling_response.json()
                print("\n RESPONSE FROM POLLING JoB", polling_response)
                status = polling_response["status"]
                if status == "finished":
                    llm_response = polling_response["result"]
                    with open("response_log.txt", "a") as log_file:
                        log_file.write(
                            f"Response ID: {llm_response.get('id', 'NA')}\nLLM Response: {llm_response}\nTime: {end_time - start_time:.2f} seconds\n\n"
                        )

                    break
                print(
                    f"POLLING JOB{polling_url}\nSTATUS: {status}, \n Response {polling_response}"
                )
                time.sleep(0.5)
            except Exception as e:
                print("got exception in polling", e)
                break

        return response
    except Exception as e:
        # Log exceptions for failed calls
        with open("error_log.txt", "a") as error_log_file:
            error_log_file.write(f"Question: {question[:100]}\nException: {str(e)}\n\n")
        return None


# Number of concurrent calls (you can adjust this)
concurrent_calls = 10

# List to store the futures of concurrent calls
futures = []

# Make concurrent calls
with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_calls) as executor:
    for _ in range(concurrent_calls):
        random_question = random.choice(questions)
        futures.append(executor.submit(make_openai_completion, random_question))

# Wait for all futures to complete
concurrent.futures.wait(futures)

# Summarize the results
successful_calls = 0
failed_calls = 0

for future in futures:
    if future.done():
        if future.result() is not None:
            successful_calls += 1
        else:
            failed_calls += 1

print("Load test Summary:")
print(f"Total Requests: {concurrent_calls}")
print(f"Successful Calls: {successful_calls}")
print(f"Failed Calls: {failed_calls}")



================================================
FILE: cookbook/litellm_router/load_test_router.py
================================================
import sys
import os
from dotenv import load_dotenv

load_dotenv()

sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path

from litellm import Router
import litellm

litellm.set_verbose = False
os.environ.pop("AZURE_AD_TOKEN")

model_list = [
    {  # list of model deployments
        "model_name": "gpt-3.5-turbo",  # model alias
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "azure/chatgpt-v-2",  # actual model name
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
    },
    {
        "model_name": "gpt-3.5-turbo",
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "azure/chatgpt-functioncalling",
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
    },
    {
        "model_name": "gpt-3.5-turbo",
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "gpt-3.5-turbo",
            "api_key": os.getenv("OPENAI_API_KEY"),
        },
    },
]
router = Router(model_list=model_list)


file_paths = [
    "test_questions/question1.txt",
    "test_questions/question2.txt",
    "test_questions/question3.txt",
]
questions = []

for file_path in file_paths:
    try:
        print(file_path)
        with open(file_path, "r") as file:
            content = file.read()
            questions.append(content)
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    except Exception as e:
        print(f"An error occurred: {e}")

# for q in questions:
#     print(q)


# make X concurrent calls to litellm.completion(model=gpt-35-turbo, messages=[]), pick a random question in questions array.
#  Allow me to tune X concurrent calls.. Log question, output/exception, response time somewhere
# show me a summary of requests made, success full calls, failed calls. For failed calls show me the exceptions

import concurrent.futures
import random
import time


# Function to make concurrent calls to OpenAI API
def make_openai_completion(question):
    try:
        start_time = time.time()
        response = router.completion(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": f"You are a helpful assistant. Answer this question{question}",
                }
            ],
        )
        print(response)
        end_time = time.time()

        # Log the request details
        with open("request_log.txt", "a") as log_file:
            log_file.write(
                f"Question: {question[:100]}\nResponse: {response.choices[0].message.content}\nTime: {end_time - start_time:.2f} seconds\n\n"
            )

        return response
    except Exception as e:
        # Log exceptions for failed calls
        with open("error_log.txt", "a") as error_log_file:
            error_log_file.write(f"Question: {question[:100]}\nException: {str(e)}\n\n")
        return None


# Number of concurrent calls (you can adjust this)
concurrent_calls = 150

# List to store the futures of concurrent calls
futures = []

# Make concurrent calls
with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_calls) as executor:
    for _ in range(concurrent_calls):
        random_question = random.choice(questions)
        futures.append(executor.submit(make_openai_completion, random_question))

# Wait for all futures to complete
concurrent.futures.wait(futures)

# Summarize the results
successful_calls = 0
failed_calls = 0

for future in futures:
    if future.result() is not None:
        successful_calls += 1
    else:
        failed_calls += 1

print("Load test Summary:")
print(f"Total Requests: {concurrent_calls}")
print(f"Successful Calls: {successful_calls}")
print(f"Failed Calls: {failed_calls}")

# Display content of the logs
with open("request_log.txt", "r") as log_file:
    print("\nRequest Log:\n", log_file.read())

with open("error_log.txt", "r") as error_log_file:
    print("\nError Log:\n", error_log_file.read())



================================================
FILE: cookbook/litellm_router/request_log.txt
================================================
Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Response ID: 71a47cd4-92d9-4091-9429-8d22af6b56bf Url: /queue/response/71a47cd4-92d9-4091-9429-8d22af6b56bf
Time: 0.77 seconds

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Response ID: a0855c20-59ba-4eed-85c1-e0719eebdeab Url: /queue/response/a0855c20-59ba-4eed-85c1-e0719eebdeab
Time: 1.46 seconds

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Response ID: b131cdcd-0693-495b-ad41-b0cf2afc4833 Url: /queue/response/b131cdcd-0693-495b-ad41-b0cf2afc4833
Time: 2.13 seconds

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Response ID: a58e5185-90e7-4832-9f28-e5a5ac167a40 Url: /queue/response/a58e5185-90e7-4832-9f28-e5a5ac167a40
Time: 2.83 seconds

Question: Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format.
Response ID: 52dbbd49-eedb-4c11-8382-3ca7deb1af35 Url: /queue/response/52dbbd49-eedb-4c11-8382-3ca7deb1af35
Time: 3.50 seconds

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Response ID: eedda05f-61e1-4081-b49d-27f9449bcf69 Url: /queue/response/eedda05f-61e1-4081-b49d-27f9449bcf69
Time: 4.20 seconds

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Response ID: 8a484722-66ec-4193-b19b-2dfc4265cfd2 Url: /queue/response/8a484722-66ec-4193-b19b-2dfc4265cfd2
Time: 4.89 seconds

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Response ID: ae1e2b71-d711-456d-8df0-13ce0709eb04 Url: /queue/response/ae1e2b71-d711-456d-8df0-13ce0709eb04
Time: 5.60 seconds

Question: What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 10
Response ID: cfabd174-838e-4252-b82b-648923573db8 Url: /queue/response/cfabd174-838e-4252-b82b-648923573db8
Time: 6.29 seconds

Question: Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the Ope
Response ID: 02d5b7d6-5443-41e9-94e4-90d8b00d49fb Url: /queue/response/02d5b7d6-5443-41e9-94e4-90d8b00d49fb
Time: 7.01 seconds




================================================
FILE: cookbook/litellm_router/response_log.txt
================================================
[Empty file]


================================================
FILE: cookbook/litellm_router/test_questions/question1.txt
================================================
Given this context, what is litellm? LiteLLM about: About
Call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs). LiteLLM manages

Translating inputs to the provider's completion and embedding endpoints
Guarantees consistent output, text responses will always be available at ['choices'][0]['message']['content']
Exception mapping - common exceptions across providers are mapped to the OpenAI exception types.
10/05/2023: LiteLLM is adopting Semantic Versioning for all commits. Learn more
10/16/2023: Self-hosted OpenAI-proxy server Learn more

Usage (Docs)
Important
LiteLLM v1.0.0 is being launched to require openai>=1.0.0. Track this here

Open In Colab
pip install litellm
from litellm import completion
import os

## set ENV variables 
os.environ["OPENAI_API_KEY"] = "your-openai-key" 
os.environ["COHERE_API_KEY"] = "your-cohere-key" 

messages = [{ "content": "Hello, how are you?","role": "user"}]

# openai call
response = completion(model="gpt-3.5-turbo", messages=messages)

# cohere call
response = completion(model="command-nightly", messages=messages)
print(response)
Streaming (Docs)
liteLLM supports streaming the model response back, pass stream=True to get a streaming iterator in response.
Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

from litellm import completion
response = completion(model="gpt-3.5-turbo", messages=messages, stream=True)
for chunk in response:
    print(chunk['choices'][0]['delta'])

# claude 2
result = completion('claude-2', messages, stream=True)
for chunk in result:
  print(chunk['choices'][0]['delta'])


================================================
FILE: cookbook/litellm_router/test_questions/question2.txt
================================================
Does litellm support ooobagooba llms? how can i call oobagooba llms. Call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs). LiteLLM manages

Translating inputs to the provider's completion and embedding endpoints
Guarantees consistent output, text responses will always be available at ['choices'][0]['message']['content']
Exception mapping - common exceptions across providers are mapped to the OpenAI exception types.
10/05/2023: LiteLLM is adopting Semantic Versioning for all commits. Learn more
10/16/2023: Self-hosted OpenAI-proxy server Learn more

Usage (Docs)
Important
LiteLLM v1.0.0 is being launched to require openai>=1.0.0. Track this here

Open In Colab
pip install litellm
from litellm import completion
import os

## set ENV variables 
os.environ["OPENAI_API_KEY"] = "your-openai-key" 
os.environ["COHERE_API_KEY"] = "your-cohere-key" 

messages = [{ "content": "Hello, how are you?","role": "user"}]

# openai call
response = completion(model="gpt-3.5-turbo", messages=messages)

# cohere call
response = completion(model="command-nightly", messages=messages)
print(response)
Streaming (Docs)
liteLLM supports streaming the model response back, pass stream=True to get a streaming iterator in response.
Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

from litellm import completion
response = completion(model="gpt-3.5-turbo", messages=messages, stream=True)
for chunk in response:
    print(chunk['choices'][0]['delta'])

# claude 2
result = completion('claude-2', messages, stream=True)
for chunk in result:
  print(chunk['choices'][0]['delta']) Supported LiteLLM providers Supported Provider (Docs)
Provider        Completion        Streaming        Async Completion        Async Streaming
openai        ✅        ✅        ✅        ✅
azure        ✅        ✅        ✅        ✅
aws - sagemaker        ✅        ✅        ✅        ✅
aws - bedrock        ✅        ✅        ✅        ✅
cohere        ✅        ✅        ✅        ✅
anthropic        ✅        ✅        ✅        ✅
huggingface        ✅        ✅        ✅        ✅
replicate        ✅        ✅        ✅        ✅
together_ai        ✅        ✅        ✅        ✅
openrouter        ✅        ✅        ✅        ✅
google - vertex_ai        ✅        ✅        ✅        ✅
google - palm        ✅        ✅        ✅        ✅
ai21        ✅        ✅        ✅        ✅
baseten        ✅        ✅        ✅        ✅
vllm        ✅        ✅        ✅        ✅
nlp_cloud        ✅        ✅        ✅        ✅
aleph alpha        ✅        ✅        ✅        ✅
petals        ✅        ✅        ✅        ✅
ollama        ✅        ✅        ✅        ✅
deepinfra        ✅        ✅        ✅        ✅
perplexity-ai        ✅        ✅        ✅        ✅
anyscale        ✅        ✅        ✅        ✅


================================================
FILE: cookbook/litellm_router/test_questions/question3.txt
================================================
What endpoints does the litellm proxy have 💥 LiteLLM Proxy Server
LiteLLM Server manages:

Calling 100+ LLMs Huggingface/Bedrock/TogetherAI/etc. in the OpenAI ChatCompletions & Completions format
Set custom prompt templates + model-specific configs (temperature, max_tokens, etc.)
Quick Start
View all the supported args for the Proxy CLI here

$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:8000

Test
In a new shell, run, this will make an openai.ChatCompletion request

litellm --test

This will now automatically route any requests for gpt-3.5-turbo to bigcode starcoder, hosted on huggingface inference endpoints.

Replace openai base
import openai 

openai.api_base = "http://0.0.0.0:8000"

print(openai.chat.completions.create(model="test", messages=[{"role":"user", "content":"Hey!"}]))

Supported LLMs
Bedrock
Huggingface (TGI)
Anthropic
VLLM
OpenAI Compatible Server
TogetherAI
Replicate
Petals
Palm
Azure OpenAI
AI21
Cohere
$ export AWS_ACCESS_KEY_ID=""
$ export AWS_REGION_NAME="" # e.g. us-west-2
$ export AWS_SECRET_ACCESS_KEY=""

$ litellm --model bedrock/anthropic.claude-v2

Server Endpoints
POST /chat/completions - chat completions endpoint to call 100+ LLMs
POST /completions - completions endpoint
POST /embeddings - embedding endpoint for Azure, OpenAI, Huggingface endpoints
GET /models - available models on server


================================================
FILE: cookbook/litellm_router_load_test/test_loadtest_openai_client.py
================================================
import sys
import os
from dotenv import load_dotenv

load_dotenv()
sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path
import asyncio
from litellm import Timeout
import time
import openai

### Test just calling AsyncAzureOpenAI

openai_client = openai.AsyncAzureOpenAI(
    azure_endpoint=os.getenv("AZURE_API_BASE"),
    api_key=os.getenv("AZURE_API_KEY"),
)


async def call_acompletion(semaphore, input_data):
    async with semaphore:
        try:
            # Use asyncio.wait_for to set a timeout for the task
            response = await openai_client.chat.completions.create(**input_data)
            # Handle the response as needed
            print(response)
            return response
        except Timeout:
            print(f"Task timed out: {input_data}")
            return None  # You may choose to return something else or raise an exception


async def main():
    # Initialize the Router

    # Create a semaphore with a capacity of 100
    semaphore = asyncio.Semaphore(100)

    # List to hold all task references
    tasks = []
    start_time_all_tasks = time.time()
    # Launch 1000 tasks
    for _ in range(500):
        task = asyncio.create_task(
            call_acompletion(
                semaphore,
                {
                    "model": "chatgpt-v-2",
                    "messages": [{"role": "user", "content": "Hey, how's it going?"}],
                },
            )
        )
        tasks.append(task)

    # Wait for all tasks to complete
    responses = await asyncio.gather(*tasks)
    # Process responses as needed
    # Record the end time for all tasks
    end_time_all_tasks = time.time()
    # Calculate the total time for all tasks
    total_time_all_tasks = end_time_all_tasks - start_time_all_tasks
    print(f"Total time for all tasks: {total_time_all_tasks} seconds")

    # Calculate the average time per response
    average_time_per_response = total_time_all_tasks / len(responses)
    print(f"Average time per response: {average_time_per_response} seconds")
    print(f"NUMBER OF COMPLETED TASKS: {len(responses)}")


# Run the main function
asyncio.run(main())



================================================
FILE: cookbook/litellm_router_load_test/test_loadtest_router.py
================================================
import sys
import os
from dotenv import load_dotenv

load_dotenv()
sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path
import asyncio
from litellm import Router, Timeout
import time

### Test calling router async


async def call_acompletion(semaphore, router: Router, input_data):
    async with semaphore:
        try:
            # Use asyncio.wait_for to set a timeout for the task
            response = await router.acompletion(**input_data)
            # Handle the response as needed
            print(response)
            return response
        except Timeout:
            print(f"Task timed out: {input_data}")
            return None  # You may choose to return something else or raise an exception


async def main():
    # Initialize the Router
    model_list = [
        {
            "model_name": "gpt-3.5-turbo",
            "litellm_params": {
                "model": "gpt-3.5-turbo",
                "api_key": os.getenv("OPENAI_API_KEY"),
            },
        },
        {
            "model_name": "gpt-3.5-turbo",
            "litellm_params": {
                "model": "azure/chatgpt-v-2",
                "api_key": os.getenv("AZURE_API_KEY"),
                "api_base": os.getenv("AZURE_API_BASE"),
                "api_version": os.getenv("AZURE_API_VERSION"),
            },
        },
    ]
    router = Router(model_list=model_list, num_retries=3, timeout=10)

    # Create a semaphore with a capacity of 100
    semaphore = asyncio.Semaphore(100)

    # List to hold all task references
    tasks = []
    start_time_all_tasks = time.time()
    # Launch 1000 tasks
    for _ in range(500):
        task = asyncio.create_task(
            call_acompletion(
                semaphore,
                router,
                {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Hey, how's it going?"}],
                },
            )
        )
        tasks.append(task)

    # Wait for all tasks to complete
    responses = await asyncio.gather(*tasks)
    # Process responses as needed
    # Record the end time for all tasks
    end_time_all_tasks = time.time()
    # Calculate the total time for all tasks
    total_time_all_tasks = end_time_all_tasks - start_time_all_tasks
    print(f"Total time for all tasks: {total_time_all_tasks} seconds")

    # Calculate the average time per response
    average_time_per_response = total_time_all_tasks / len(responses)
    print(f"Average time per response: {average_time_per_response} seconds")
    print(f"NUMBER OF COMPLETED TASKS: {len(responses)}")


# Run the main function
asyncio.run(main())



================================================
FILE: cookbook/litellm_router_load_test/test_loadtest_router_withs3_cache.py
================================================
import sys
import os
from dotenv import load_dotenv

load_dotenv()
sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path
import asyncio
from litellm import Router, Timeout
import time
from litellm.caching.caching import Cache
import litellm

litellm.cache = Cache(
    type="s3", s3_bucket_name="cache-bucket-litellm", s3_region_name="us-west-2"
)

### Test calling router with s3 Cache


async def call_acompletion(semaphore, router: Router, input_data):
    async with semaphore:
        try:
            # Use asyncio.wait_for to set a timeout for the task
            response = await router.acompletion(**input_data)
            # Handle the response as needed
            print(response)
            return response
        except Timeout:
            print(f"Task timed out: {input_data}")
            return None  # You may choose to return something else or raise an exception


async def main():
    # Initialize the Router
    model_list = [
        {
            "model_name": "gpt-3.5-turbo",
            "litellm_params": {
                "model": "gpt-3.5-turbo",
                "api_key": os.getenv("OPENAI_API_KEY"),
            },
        },
        {
            "model_name": "gpt-3.5-turbo",
            "litellm_params": {
                "model": "azure/chatgpt-v-2",
                "api_key": os.getenv("AZURE_API_KEY"),
                "api_base": os.getenv("AZURE_API_BASE"),
                "api_version": os.getenv("AZURE_API_VERSION"),
            },
        },
    ]
    router = Router(model_list=model_list, num_retries=3, timeout=10)

    # Create a semaphore with a capacity of 100
    semaphore = asyncio.Semaphore(100)

    # List to hold all task references
    tasks = []
    start_time_all_tasks = time.time()
    # Launch 1000 tasks
    for _ in range(500):
        task = asyncio.create_task(
            call_acompletion(
                semaphore,
                router,
                {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Hey, how's it going?"}],
                },
            )
        )
        tasks.append(task)

    # Wait for all tasks to complete
    responses = await asyncio.gather(*tasks)
    # Process responses as needed
    # Record the end time for all tasks
    end_time_all_tasks = time.time()
    # Calculate the total time for all tasks
    total_time_all_tasks = end_time_all_tasks - start_time_all_tasks
    print(f"Total time for all tasks: {total_time_all_tasks} seconds")

    # Calculate the average time per response
    average_time_per_response = total_time_all_tasks / len(responses)
    print(f"Average time per response: {average_time_per_response} seconds")
    print(f"NUMBER OF COMPLETED TASKS: {len(responses)}")


# Run the main function
asyncio.run(main())



================================================
FILE: cookbook/litellm_router_load_test/memory_usage/router_endpoint.py
================================================
from fastapi import FastAPI
import uvicorn
from memory_profiler import profile
import os
import litellm
from litellm import Router
from dotenv import load_dotenv
import uuid

load_dotenv()

model_list = [
    {
        "model_name": "gpt-3.5-turbo",
        "litellm_params": {
            "model": "azure/chatgpt-v-2",
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
        "tpm": 240000,
        "rpm": 1800,
    },
    {
        "model_name": "text-embedding-ada-002",
        "litellm_params": {
            "model": "azure/azure-embedding-model",
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
        "tpm": 100000,
        "rpm": 10000,
    },
]

litellm.set_verbose = True
litellm.cache = litellm.Cache(
    type="s3", s3_bucket_name="litellm-my-test-bucket-2", s3_region_name="us-east-1"
)
router = Router(model_list=model_list, set_verbose=True)

app = FastAPI()


@app.get("/")
async def read_root():
    return {"message": "Welcome to the FastAPI endpoint!"}


@profile
@app.post("/router_acompletion")
async def router_acompletion():
    question = f"This is a test: {uuid.uuid4()}" * 100
    resp = await router.aembedding(model="text-embedding-ada-002", input=question)
    print("embedding-resp", resp)

    response = await router.acompletion(
        model="gpt-3.5-turbo", messages=[{"role": "user", "content": question}]
    )
    print("completion-resp", response)
    return response


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)



================================================
FILE: cookbook/litellm_router_load_test/memory_usage/router_memory_usage copy.py
================================================
#### What this tests ####

from memory_profiler import profile
import sys
import os
import time
import asyncio

sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path
import litellm
from litellm import Router
from dotenv import load_dotenv
import uuid

load_dotenv()


model_list = [
    {
        "model_name": "gpt-3.5-turbo",  # openai model name
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "azure/chatgpt-v-2",
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
        "tpm": 240000,
        "rpm": 1800,
    },
    {
        "model_name": "text-embedding-ada-002",
        "litellm_params": {
            "model": "azure/azure-embedding-model",
            "api_key": os.environ["AZURE_API_KEY"],
            "api_base": os.environ["AZURE_API_BASE"],
        },
        "tpm": 100000,
        "rpm": 10000,
    },
]
litellm.set_verbose = True
litellm.cache = litellm.Cache(
    type="s3", s3_bucket_name="litellm-my-test-bucket-2", s3_region_name="us-east-1"
)
router = Router(
    model_list=model_list,
    set_verbose=True,
)  # type: ignore


@profile
async def router_acompletion():
    # embedding call
    question = f"This is a test: {uuid.uuid4()}" * 100
    resp = await router.aembedding(model="text-embedding-ada-002", input=question)
    print("embedding-resp", resp)

    response = await router.acompletion(
        model="gpt-3.5-turbo", messages=[{"role": "user", "content": question}]
    )
    print("completion-resp", response)
    return response


async def main():
    for i in range(1):
        start = time.time()
        n = 50  # Number of concurrent tasks
        tasks = [router_acompletion() for _ in range(n)]

        chat_completions = await asyncio.gather(*tasks)

        successful_completions = [c for c in chat_completions if c is not None]

        # Write errors to error_log.txt
        with open("error_log.txt", "a") as error_log:
            for completion in chat_completions:
                if isinstance(completion, str):
                    error_log.write(completion + "\n")

        print(n, time.time() - start, len(successful_completions))
        time.sleep(10)


if __name__ == "__main__":
    # Blank out contents of error_log.txt
    open("error_log.txt", "w").close()

    asyncio.run(main())



================================================
FILE: cookbook/litellm_router_load_test/memory_usage/router_memory_usage.py
================================================
#### What this tests ####

from memory_profiler import profile
import sys
import os
import time
import asyncio

sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path
import litellm
from litellm import Router
from dotenv import load_dotenv
import uuid

load_dotenv()


model_list = [
    {
        "model_name": "gpt-3.5-turbo",  # openai model name
        "litellm_params": {  # params for litellm completion/embedding call
            "model": "azure/chatgpt-v-2",
            "api_key": os.getenv("AZURE_API_KEY"),
            "api_version": os.getenv("AZURE_API_VERSION"),
            "api_base": os.getenv("AZURE_API_BASE"),
        },
        "tpm": 240000,
        "rpm": 1800,
    },
    {
        "model_name": "text-embedding-ada-002",
        "litellm_params": {
            "model": "azure/azure-embedding-model",
            "api_key": os.environ["AZURE_API_KEY"],
            "api_base": os.environ["AZURE_API_BASE"],
        },
        "tpm": 100000,
        "rpm": 10000,
    },
]
litellm.set_verbose = True
litellm.cache = litellm.Cache(
    type="s3", s3_bucket_name="litellm-my-test-bucket-2", s3_region_name="us-east-1"
)
router = Router(
    model_list=model_list,
    set_verbose=True,
)  # type: ignore


@profile
async def router_acompletion():
    # embedding call
    question = f"This is a test: {uuid.uuid4()}" * 100
    resp = await router.aembedding(model="text-embedding-ada-002", input=question)
    print("embedding-resp", resp)

    response = await router.acompletion(
        model="gpt-3.5-turbo", messages=[{"role": "user", "content": question}]
    )
    print("completion-resp", response)
    return response


async def main():
    for i in range(1):
        start = time.time()
        n = 50  # Number of concurrent tasks
        tasks = [router_acompletion() for _ in range(n)]

        chat_completions = await asyncio.gather(*tasks)

        successful_completions = [c for c in chat_completions if c is not None]

        # Write errors to error_log.txt
        with open("error_log.txt", "a") as error_log:
            for completion in chat_completions:
                if isinstance(completion, str):
                    error_log.write(completion + "\n")

        print(n, time.time() - start, len(successful_completions))
        time.sleep(10)


if __name__ == "__main__":
    # Blank out contents of error_log.txt
    open("error_log.txt", "w").close()

    asyncio.run(main())



================================================
FILE: cookbook/litellm_router_load_test/memory_usage/send_request.py
================================================
import requests
from concurrent.futures import ThreadPoolExecutor

# Replace the URL with your actual endpoint
url = "http://localhost:8000/router_acompletion"


def make_request(session):
    headers = {"Content-Type": "application/json"}
    data = {}  # Replace with your JSON payload if needed

    response = session.post(url, headers=headers, json=data)
    print(f"Status code: {response.status_code}")


# Number of concurrent requests
num_requests = 20

# Create a session to reuse the underlying TCP connection
with requests.Session() as session:
    # Use ThreadPoolExecutor for concurrent requests
    with ThreadPoolExecutor(max_workers=num_requests) as executor:
        # Use list comprehension to submit tasks
        futures = [executor.submit(make_request, session) for _ in range(num_requests)]

        # Wait for all futures to complete
        for future in futures:
            future.result()



================================================
FILE: cookbook/logging_observability/LiteLLM_Arize.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Use LiteLLM with Arize
https://docs.litellm.ai/docs/observability/arize_integration

This method uses the litellm proxy to send the data to Arize. The callback is set in the litellm config below, instead of using OpenInference tracing.
"""

"""
## Install Dependencies
"""

!pip install litellm
# Output:
#   Requirement already satisfied: litellm in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (1.54.1)

#   Requirement already satisfied: aiohttp in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (3.11.10)

#   Requirement already satisfied: click in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (8.1.7)

#   Requirement already satisfied: httpx<0.28.0,>=0.23.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (0.27.2)

#   Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (8.5.0)

#   Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (3.1.4)

#   Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (4.23.0)

#   Requirement already satisfied: openai>=1.55.3 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (1.57.1)

#   Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (2.10.3)

#   Requirement already satisfied: python-dotenv>=0.2.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (1.0.1)

#   Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (2.32.3)

#   Requirement already satisfied: tiktoken>=0.7.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (0.7.0)

#   Requirement already satisfied: tokenizers in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from litellm) (0.21.0)

#   Requirement already satisfied: anyio in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm) (4.7.0)

#   Requirement already satisfied: certifi in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm) (2024.8.30)

#   Requirement already satisfied: httpcore==1.* in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm) (1.0.7)

#   Requirement already satisfied: idna in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm) (3.10)

#   Requirement already satisfied: sniffio in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm) (1.3.1)

#   Requirement already satisfied: h11<0.15,>=0.13 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm) (0.14.0)

#   Requirement already satisfied: zipp>=3.20 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm) (3.21.0)

#   Requirement already satisfied: MarkupSafe>=2.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)

#   Requirement already satisfied: attrs>=22.2.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (24.2.0)

#   Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2024.10.1)

#   Requirement already satisfied: referencing>=0.28.4 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)

#   Requirement already satisfied: rpds-py>=0.7.1 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.22.3)

#   Requirement already satisfied: distro<2,>=1.7.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from openai>=1.55.3->litellm) (1.9.0)

#   Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from openai>=1.55.3->litellm) (0.6.1)

#   Requirement already satisfied: tqdm>4 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from openai>=1.55.3->litellm) (4.67.1)

#   Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from openai>=1.55.3->litellm) (4.12.2)

#   Requirement already satisfied: annotated-types>=0.6.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)

#   Requirement already satisfied: pydantic-core==2.27.1 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.27.1)

#   Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (3.4.0)

#   Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (2.0.7)

#   Requirement already satisfied: regex>=2022.1.18 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)

#   Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from aiohttp->litellm) (2.4.4)

#   Requirement already satisfied: aiosignal>=1.1.2 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from aiohttp->litellm) (1.3.1)

#   Requirement already satisfied: frozenlist>=1.1.1 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from aiohttp->litellm) (1.5.0)

#   Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from aiohttp->litellm) (6.1.0)

#   Requirement already satisfied: propcache>=0.2.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from aiohttp->litellm) (0.2.1)

#   Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from aiohttp->litellm) (1.18.3)

#   Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from tokenizers->litellm) (0.26.5)

#   Requirement already satisfied: filelock in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.16.1)

#   Requirement already satisfied: fsspec>=2023.5.0 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.10.0)

#   Requirement already satisfied: packaging>=20.9 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.2)

#   Requirement already satisfied: pyyaml>=5.1 in /Users/ericxiao/Documents/arize/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)


"""
## Set Env Variables
"""

import litellm
import os
from getpass import getpass

os.environ["ARIZE_SPACE_KEY"] = getpass("Enter your Arize space key: ")
os.environ["ARIZE_API_KEY"] = getpass("Enter your Arize API key: ")
os.environ['OPENAI_API_KEY']= getpass("Enter your OpenAI API key: ")

"""
Let's run a completion call and see the traces in Arize
"""

# set arize as a callback, litellm will send the data to arize
litellm.callbacks = ["arize"]
 
# openai call
response = litellm.completion(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "Hi 👋 - i'm openai"}
  ]
)
print(response.choices[0].message.content)
# Output:
#   Hello! Nice to meet you, OpenAI. How can I assist you today?




================================================
FILE: cookbook/logging_observability/LiteLLM_Langfuse.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Use LiteLLM with Langfuse
https://docs.litellm.ai/docs/observability/langfuse_integration
"""

"""
## Install Dependencies
"""

!pip install litellm langfuse

"""
## Set Env Variables
"""

import litellm
from litellm import completion
import os

# from https://cloud.langfuse.com/
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""


# OpenAI and Cohere keys
# You can use any of the litellm supported providers: https://docs.litellm.ai/docs/providers
os.environ['OPENAI_API_KEY']=""
os.environ['COHERE_API_KEY']=""


"""
## Set LangFuse as a callback for sending data
## OpenAI completion call
"""

# set langfuse as a callback, litellm will send the data to langfuse
litellm.success_callback = ["langfuse"]

# openai call
response = completion(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "Hi 👋 - i'm openai"}
  ]
)

print(response)
# Output:
#   {

#     "id": "chatcmpl-85nP4xHdAP3jAcGneIguWATS9qdoO",

#     "object": "chat.completion",

#     "created": 1696392238,

#     "model": "gpt-3.5-turbo-0613",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": "Hello! How can I assist you today?"

#         },

#         "finish_reason": "stop"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 15,

#       "completion_tokens": 9,

#       "total_tokens": 24

#     }

#   }


# we set langfuse as a callback in the prev cell
# cohere call
response = completion(
  model="command-nightly",
  messages=[
    {"role": "user", "content": "Hi 👋 - i'm cohere"}
  ]
)

print(response)
# Output:
#   {

#     "object": "chat.completion",

#     "choices": [

#       {

#         "finish_reason": "stop",

#         "index": 0,

#         "message": {

#           "content": " Nice to meet you, Cohere! I'm excited to be meeting new members of the AI community",

#           "role": "assistant",

#           "logprobs": null

#         }

#       }

#     ],

#     "id": "chatcmpl-a14e903f-4608-4ceb-b996-8ebdf21360ca",

#     "created": 1696392247.3313863,

#     "model": "command-nightly",

#     "usage": {

#       "prompt_tokens": 8,

#       "completion_tokens": 20,

#       "total_tokens": 28

#     }

#   }




================================================
FILE: cookbook/logging_observability/LiteLLM_Lunary.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Use LiteLLM with Langfuse
https://docs.litellm.ai/docs/observability/langfuse_integration
"""

"""
## Install Dependencies
"""

%pip install litellm lunary

"""
## Set Env Variables
"""

import litellm
from litellm import completion
import os

# from https://app.lunary.ai/
os.environ["LUNARY_PUBLIC_KEY"] = ""


# LLM provider keys
# You can use any of the litellm supported providers: https://docs.litellm.ai/docs/providers
os.environ['OPENAI_API_KEY'] = ""


"""
## Set Lunary as a callback for sending data
## OpenAI completion call
"""

# set langfuse as a callback, litellm will send the data to langfuse
litellm.success_callback = ["lunary"]

# openai call
response = completion(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "Hi 👋 - i'm openai"}
  ]
)

print(response)
# Output:
#   [Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant'))]ModelResponse(id='chatcmpl-8xIWykI0GiJSmYtXYuB8Z363kpIBm', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant'))], created=1709143276, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_86156a94a0', usage=Usage(completion_tokens=9, prompt_tokens=15, total_tokens=24))

#   

#   [Lunary] Add event: {

#       "event": "start",

#       "type": "llm",

#       "name": "gpt-3.5-turbo",

#       "runId": "a363776a-bd07-4474-bce2-193067f01b2e",

#       "timestamp": "2024-02-28T18:01:15.188153+00:00",

#       "input": {

#           "role": "user",

#           "content": "Hi \ud83d\udc4b - i'm openai"

#       },

#       "extra": {},

#       "runtime": "litellm",

#       "metadata": {}

#   }

#   

#   

#   [Lunary] Add event: {

#       "event": "end",

#       "type": "llm",

#       "runId": "a363776a-bd07-4474-bce2-193067f01b2e",

#       "timestamp": "2024-02-28T18:01:16.846581+00:00",

#       "output": {

#           "role": "assistant",

#           "content": "Hello! How can I assist you today?"

#       },

#       "runtime": "litellm",

#       "tokensUsage": {

#           "completion": 9,

#           "prompt": 15

#       }

#   }

#   

#   

#   --- Logging error ---

#   Traceback (most recent call last):

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 537, in _make_request

#       response = conn.getresponse()

#                  ^^^^^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/urllib3/connection.py", line 466, in getresponse

#       httplib_response = super().getresponse()

#                          ^^^^^^^^^^^^^^^^^^^^^

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1423, in getresponse

#       response.begin()

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 331, in begin

#       version, status, reason = self._read_status()

#                                 ^^^^^^^^^^^^^^^^^^^

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 292, in _read_status

#       line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")

#                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py", line 707, in readinto

#       return self._sock.recv_into(b)

#              ^^^^^^^^^^^^^^^^^^^^^^^

#   TimeoutError: timed out

#   

#   The above exception was the direct cause of the following exception:

#   

#   Traceback (most recent call last):

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/requests/adapters.py", line 486, in send

#       resp = conn.urlopen(

#              ^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen

#       retries = retries.increment(

#                 ^^^^^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/urllib3/util/retry.py", line 470, in increment

#       raise reraise(type(error), error, _stacktrace)

#             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/urllib3/util/util.py", line 39, in reraise

#       raise value

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen

#       response = self._make_request(

#                  ^^^^^^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 539, in _make_request

#       self._raise_timeout(err=e, url=url, timeout_value=read_timeout)

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 370, in _raise_timeout

#       raise ReadTimeoutError(

#   urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=3333): Read timed out. (read timeout=5)

#   

#   During handling of the above exception, another exception occurred:

#   

#   Traceback (most recent call last):

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/lunary/consumer.py", line 59, in send_batch

#       response = requests.post(

#                  ^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/requests/api.py", line 115, in post

#       return request("post", url, data=data, json=json, **kwargs)

#              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/requests/api.py", line 59, in request

#       return session.request(method=method, url=url, **kwargs)

#              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/requests/sessions.py", line 589, in request

#       resp = self.send(prep, **send_kwargs)

#              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/requests/sessions.py", line 703, in send

#       r = adapter.send(request, **kwargs)

#           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/requests/adapters.py", line 532, in send

#       raise ReadTimeout(e, request=request)

#   requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=3333): Read timed out. (read timeout=5)

#   

#   During handling of the above exception, another exception occurred:

#   

#   Traceback (most recent call last):

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/logging/__init__.py", line 1160, in emit

#       msg = self.format(record)

#             ^^^^^^^^^^^^^^^^^^^

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/logging/__init__.py", line 999, in format

#       return fmt.format(record)

#              ^^^^^^^^^^^^^^^^^^

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/logging/__init__.py", line 703, in format

#       record.message = record.getMessage()

#                        ^^^^^^^^^^^^^^^^^^^

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/logging/__init__.py", line 392, in getMessage

#       msg = msg % self.args

#             ~~~~^~~~~~~~~~~

#   TypeError: not all arguments converted during string formatting

#   Call stack:

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 1030, in _bootstrap

#       self._bootstrap_inner()

#     File "/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 1073, in _bootstrap_inner

#       self.run()

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/lunary/consumer.py", line 24, in run

#       self.send_batch()

#     File "/Users/vince/Library/Caches/pypoetry/virtualenvs/litellm-7WKnDWGw-py3.12/lib/python3.12/site-packages/lunary/consumer.py", line 73, in send_batch

#       logging.error("[Lunary] Error sending events", e)

#   Message: '[Lunary] Error sending events'

#   Arguments: (ReadTimeout(ReadTimeoutError("HTTPConnectionPool(host='localhost', port=3333): Read timed out. (read timeout=5)")),)


"""
# Using LiteLLM with Lunary Templates

You can use LiteLLM seamlessly with Lunary templates to manage your prompts and completions.

Assuming you have created a template "test-template" with a variable "question", you can use it like this:
"""

import lunary
from litellm import completion

template = lunary.render_template("test-template", {"question": "Hello!"})

response = completion(**template)

print(response)
# Output:
#   [Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant'))]ModelResponse(id='chatcmpl-8xIXegwpudg4YKnLB6pmpFGXqTHcH', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant'))], created=1709143318, model='gpt-4-0125-preview', object='chat.completion', system_fingerprint='fp_c8aa5a06d6', usage=Usage(completion_tokens=9, prompt_tokens=21, total_tokens=30))

#   

#   [Lunary] Add event: {

#       "event": "start",

#       "type": "llm",

#       "name": "gpt-4-turbo-preview",

#       "runId": "3a5b698d-cb55-4b3b-ab6d-04d2b99e40cb",

#       "timestamp": "2024-02-28T18:01:56.746249+00:00",

#       "input": [

#           {

#               "role": "system",

#               "content": "You are an helpful assistant."

#           },

#           {

#               "role": "user",

#               "content": "Hi! Hello!"

#           }

#       ],

#       "extra": {

#           "temperature": 1,

#           "max_tokens": 100

#       },

#       "runtime": "litellm",

#       "metadata": {}

#   }

#   

#   

#   [Lunary] Add event: {

#       "event": "end",

#       "type": "llm",

#       "runId": "3a5b698d-cb55-4b3b-ab6d-04d2b99e40cb",

#       "timestamp": "2024-02-28T18:01:58.741244+00:00",

#       "output": {

#           "role": "assistant",

#           "content": "Hello! How can I assist you today?"

#       },

#       "runtime": "litellm",

#       "tokensUsage": {

#           "completion": 9,

#           "prompt": 21

#       }

#   }

#   

#   




================================================
FILE: cookbook/logging_observability/LiteLLM_Proxy_Langfuse.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## LLM Ops Stack  - LiteLLM Proxy + Langfuse 

This notebook demonstrates how to use LiteLLM Proxy with Langfuse 
- Use LiteLLM Proxy for calling 100+ LLMs in OpenAI format
- Use Langfuse for viewing request / response traces 


In this notebook we will setup LiteLLM Proxy to make requests to OpenAI, Anthropic, Bedrock and automatically log traces to Langfuse.
"""

"""
## 1. Setup LiteLLM Proxy

### 1.1 Define .env variables 
Define .env variables on the container that litellm proxy is running on.
```bash
## LLM API Keys
OPENAI_API_KEY=sk-proj-1234567890
ANTHROPIC_API_KEY=sk-ant-api03-1234567890
AWS_ACCESS_KEY_ID=1234567890
AWS_SECRET_ACCESS_KEY=1234567890

## Langfuse Logging 
LANGFUSE_PUBLIC_KEY="pk-lf-xxxx9"
LANGFUSE_SECRET_KEY="sk-lf-xxxx9"
LANGFUSE_HOST="https://us.cloud.langfuse.com"
```


### 1.1 Setup LiteLLM Proxy Config yaml 
```yaml
model_list:
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
  - model_name: us.amazon.nova-micro-v1:0
    litellm_params:
      model: bedrock/us.amazon.nova-micro-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY

litellm_settings:
  callbacks: ["langfuse"]


```
"""

"""
## 2. Make LLM Requests to LiteLLM Proxy

Now we will make our first LLM request to LiteLLM Proxy
"""

"""
### 2.1 Setup Client Side Variables to point to LiteLLM Proxy
Set `LITELLM_PROXY_BASE_URL` to the base url of the LiteLLM Proxy and `LITELLM_VIRTUAL_KEY` to the virtual key you want to use for Authentication to LiteLLM Proxy. (Note: In this initial setup you can)
"""


LITELLM_PROXY_BASE_URL="http://0.0.0.0:4000"
LITELLM_VIRTUAL_KEY="sk-oXXRa1xxxxxxxxxxx"

import openai
client = openai.OpenAI(
    api_key=LITELLM_VIRTUAL_KEY,
    base_url=LITELLM_PROXY_BASE_URL
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages = [
        {
            "role": "user",
            "content": "what is Langfuse?"
        }
    ],
)

response
# Output:
#   ChatCompletion(id='chatcmpl-B0sq6QkOKNMJ0dwP3x7OoMqk1jZcI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Langfuse is a platform designed to monitor, observe, and troubleshoot AI and large language model (LLM) applications. It provides features that help developers gain insights into how their AI systems are performing, make debugging easier, and optimize the deployment of models. Langfuse allows for tracking of model interactions, collecting telemetry, and visualizing data, which is crucial for understanding the behavior of AI models in production environments. This kind of tool is particularly useful for developers working with language models who need to ensure reliability and efficiency in their applications.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1739550502, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_523b9b6e5f', usage=CompletionUsage(completion_tokens=109, prompt_tokens=13, total_tokens=122, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))

"""
### 2.3 View Traces on Langfuse
LiteLLM will send the request / response, model, tokens (input + output), cost to Langfuse.

![image_description](litellm_proxy_langfuse.png)
"""

"""
### 2.4 Call Anthropic, Bedrock models 

Now we can call `us.amazon.nova-micro-v1:0` and `claude-3-5-sonnet-20241022` models defined on your config.yaml both in the OpenAI request / response format.
"""

import openai
client = openai.OpenAI(
    api_key=LITELLM_VIRTUAL_KEY,
    base_url=LITELLM_PROXY_BASE_URL
)

response = client.chat.completions.create(
    model="us.amazon.nova-micro-v1:0",
    messages = [
        {
            "role": "user",
            "content": "what is Langfuse?"
        }
    ],
)

response
# Output:
#   ChatCompletion(id='chatcmpl-7756e509-e61f-4f5e-b5ae-b7a41013522a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content="Langfuse is an observability tool designed specifically for machine learning models and applications built with natural language processing (NLP) and large language models (LLMs). It focuses on providing detailed insights into how these models perform in real-world scenarios. Here are some key features and purposes of Langfuse:\n\n1. **Real-time Monitoring**: Langfuse allows developers to monitor the performance of their NLP and LLM applications in real time. This includes tracking the inputs and outputs of the models, as well as any errors or issues that arise during operation.\n\n2. **Error Tracking**: It helps in identifying and tracking errors in the models' outputs. By analyzing incorrect or unexpected responses, developers can pinpoint where and why errors occur, facilitating more effective debugging and improvement.\n\n3. **Performance Metrics**: Langfuse provides various performance metrics, such as latency, throughput, and error rates. These metrics help developers understand how well their models are performing under different conditions and workloads.\n\n4. **Traceability**: It offers detailed traceability of requests and responses, allowing developers to follow the path of a request through the system and see how it is processed by the model at each step.\n\n5. **User Feedback Integration**: Langfuse can integrate user feedback to provide context for model outputs. This helps in understanding how real users are interacting with the model and how its outputs align with user expectations.\n\n6. **Customizable Dashboards**: Users can create custom dashboards to visualize the data collected by Langfuse. These dashboards can be tailored to highlight the most important metrics and insights for a specific application or team.\n\n7. **Alerting and Notifications**: It can set up alerts for specific conditions or errors, notifying developers when something goes wrong or when performance metrics fall outside of acceptable ranges.\n\nBy providing comprehensive observability for NLP and LLM applications, Langfuse helps developers to build more reliable, accurate, and user-friendly models and services.", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1739554005, model='us.amazon.nova-micro-v1:0', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=380, prompt_tokens=5, total_tokens=385, completion_tokens_details=None, prompt_tokens_details=None))

"""
## 3. Advanced - Set Langfuse Trace ID, Tags, Metadata 

Here is an example of how you can set Langfuse specific params on your client side request. See full list of supported langfuse params [here](https://docs.litellm.ai/docs/observability/langfuse_integration)

You can view the logged trace of this request [here](https://us.cloud.langfuse.com/project/clvlhdfat0007vwb74m9lvfvi/traces/567890?timestamp=2025-02-14T17%3A30%3A26.709Z)
"""

import openai
client = openai.OpenAI(
    api_key=LITELLM_VIRTUAL_KEY,
    base_url=LITELLM_PROXY_BASE_URL
)

response = client.chat.completions.create(
    model="us.amazon.nova-micro-v1:0",
    messages = [
        {
            "role": "user",
            "content": "what is Langfuse?"
        }
    ],
    extra_body={
        "metadata": {
            "generation_id": "1234567890",
            "trace_id": "567890",
            "trace_user_id": "user_1234567890",
            "tags": ["tag1", "tag2"]
        }
    }
)

response
# Output:
#   ChatCompletion(id='chatcmpl-789babd5-c064-4939-9093-46e4cd2e208a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content="Langfuse is an observability platform designed specifically for monitoring and improving the performance of natural language processing (NLP) models and applications. It provides developers with tools to track, analyze, and optimize how their language models interact with users and handle natural language inputs.\n\nHere are some key features and benefits of Langfuse:\n\n1. **Real-Time Monitoring**: Langfuse allows developers to monitor their NLP applications in real time. This includes tracking user interactions, model responses, and overall performance metrics.\n\n2. **Error Tracking**: It helps in identifying and tracking errors in the model's responses. This can include incorrect, irrelevant, or unsafe outputs.\n\n3. **User Feedback Integration**: Langfuse enables the collection of user feedback directly within the platform. This feedback can be used to identify areas for improvement in the model's performance.\n\n4. **Performance Metrics**: The platform provides detailed metrics and analytics on model performance, including latency, throughput, and accuracy.\n\n5. **Alerts and Notifications**: Developers can set up alerts to notify them of any significant issues or anomalies in model performance.\n\n6. **Debugging Tools**: Langfuse offers tools to help developers debug and refine their models by providing insights into how the model processes different types of inputs.\n\n7. **Integration with Development Workflows**: It integrates seamlessly with various development environments and CI/CD pipelines, making it easier to incorporate observability into the development process.\n\n8. **Customizable Dashboards**: Users can create custom dashboards to visualize the data in a way that best suits their needs.\n\nLangfuse aims to help developers build more reliable, accurate, and user-friendly NLP applications by providing them with the tools to observe and improve how their models perform in real-world scenarios.", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1739554281, model='us.amazon.nova-micro-v1:0', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=346, prompt_tokens=5, total_tokens=351, completion_tokens_details=None, prompt_tokens_details=None))

"""
## 
"""



================================================
FILE: cookbook/misc/add_new_models.py
================================================
import requests


def get_initial_config():
    proxy_base_url = input("Enter your proxy base URL (e.g., http://localhost:4000): ")
    master_key = input("Enter your LITELLM_MASTER_KEY ")
    return proxy_base_url, master_key


def get_user_input():
    model_name = input(
        "Enter model_name (this is the 'model' passed in /chat/completions requests):"
    )
    model = input("litellm_params: Enter model eg. 'azure/<your-deployment-name>': ")
    tpm = int(input("litellm_params: Enter tpm (tokens per minute): "))
    rpm = int(input("litellm_params: Enter rpm (requests per minute): "))
    api_key = input("litellm_params: Enter api_key: ")
    api_base = input("litellm_params: Enter api_base: ")
    api_version = input("litellm_params: Enter api_version: ")
    timeout = int(input("litellm_params: Enter timeout (0 for default): "))
    stream_timeout = int(
        input("litellm_params: Enter stream_timeout (0 for default): ")
    )
    max_retries = int(input("litellm_params: Enter max_retries (0 for default): "))

    return {
        "model_name": model_name,
        "litellm_params": {
            "model": model,
            "tpm": tpm,
            "rpm": rpm,
            "api_key": api_key,
            "api_base": api_base,
            "api_version": api_version,
            "timeout": timeout,
            "stream_timeout": stream_timeout,
            "max_retries": max_retries,
        },
    }


def make_request(proxy_base_url, master_key, data):
    url = f"{proxy_base_url}/model/new"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {master_key}",
    }

    response = requests.post(url, headers=headers, json=data)

    print(f"Status Code: {response.status_code}")
    print(f"Response from adding model: {response.text}")


def main():
    proxy_base_url, master_key = get_initial_config()

    while True:
        print("Adding new Model to your proxy server...")
        data = get_user_input()
        make_request(proxy_base_url, master_key, data)

        add_another = input("Do you want to add another model? (yes/no): ").lower()
        if add_another != "yes":
            break

    print("Script finished.")


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/misc/config.yaml
================================================
model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: azure/chatgpt-v-2
      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
      api_version: "2023-05-15"
      api_key: os.environ/AZURE_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  - model_name: gpt-3.5-turbo-large
    litellm_params: 
      model: "gpt-3.5-turbo-1106"
      api_key: os.environ/OPENAI_API_KEY
      rpm: 480
      timeout: 300
      stream_timeout: 60
  - model_name: gpt-4
    litellm_params:
      model: azure/chatgpt-v-2
      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
      api_version: "2023-05-15"
      api_key: os.environ/AZURE_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
      rpm: 480
      timeout: 300
      stream_timeout: 60
  - model_name: sagemaker-completion-model
    litellm_params:
      model: sagemaker/berri-benchmarking-Llama-2-70b-chat-hf-4
      input_cost_per_second: 0.000420  
  - model_name: text-embedding-ada-002
    litellm_params: 
      model: azure/azure-embedding-model
      api_key: os.environ/AZURE_API_KEY
      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
      api_version: "2023-05-15"
    model_info:
      mode: embedding
      base_model: text-embedding-ada-002
  - model_name: dall-e-2
    litellm_params:
      model: azure/
      api_version: 2023-06-01-preview
      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
      api_key: os.environ/AZURE_API_KEY
  - model_name: openai-dall-e-3
    litellm_params:
      model: dall-e-3
  - model_name: fake-openai-endpoint
    litellm_params:
      model: openai/fake
      api_key: fake-key
      api_base: https://exampleopenaiendpoint-production.up.railway.app/

litellm_settings:
  drop_params: True
  # max_budget: 100 
  # budget_duration: 30d
  num_retries: 5
  request_timeout: 600
  telemetry: False
  context_window_fallbacks: [{"gpt-3.5-turbo": ["gpt-3.5-turbo-large"]}]

general_settings: 
  master_key: sk-1234 # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  store_model_in_db: True
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  # database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # [OPTIONAL] use for token-based auth to proxy

# environment_variables:
  # settings for using redis caching
  # REDIS_HOST: redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com
  # REDIS_PORT: "16337"
  # REDIS_PASSWORD: 



================================================
FILE: cookbook/misc/dev_release.txt
================================================
python3 -m build
twine upload --verbose dist/litellm-1.18.13.dev4.tar.gz -u __token__ - 


Note: You might need to make a MANIFEST.ini file on root for build process incase it fails 

Place this in MANIFEST.ini
recursive-exclude venv *
recursive-exclude myenv *
recursive-exclude py313_env *
recursive-exclude **/.venv *



================================================
FILE: cookbook/misc/migrate_proxy_config.py
================================================
"""
LiteLLM Migration Script!

Takes a config.yaml and calls /model/new 

Inputs:
    - File path to config.yaml
    - Proxy base url to your hosted proxy

Step 1: Reads your config.yaml
Step 2: reads `model_list` and loops through all models 
Step 3: calls `<proxy-base-url>/model/new` for each model
"""

import yaml
import requests

_in_memory_os_variables = {}


def migrate_models(config_file, proxy_base_url):
    # Step 1: Read the config.yaml file
    with open(config_file, "r") as f:
        config = yaml.safe_load(f)

    # Step 2: Read the model_list and loop through all models
    model_list = config.get("model_list", [])
    print("model_list: ", model_list)
    for model in model_list:

        model_name = model.get("model_name")
        print("\nAdding model: ", model_name)
        litellm_params = model.get("litellm_params", {})
        api_base = litellm_params.get("api_base", "")
        print("api_base on config.yaml: ", api_base)

        litellm_model_name = litellm_params.get("model", "") or ""
        if "vertex_ai/" in litellm_model_name:
            print("\033[91m\nSkipping Vertex AI model\033[0m", model)
            continue

        for param, value in litellm_params.items():
            if isinstance(value, str) and value.startswith("os.environ/"):
                # check if value is in _in_memory_os_variables
                if value in _in_memory_os_variables:
                    new_value = _in_memory_os_variables[value]
                    print(
                        "\033[92mAlready entered value for \033[0m",
                        value,
                        "\033[92musing \033[0m",
                        new_value,
                    )
                else:
                    new_value = input(f"Enter value for {value}: ")
                    _in_memory_os_variables[value] = new_value
                litellm_params[param] = new_value
        if "api_key" not in litellm_params:
            new_value = input(f"Enter api key for {model_name}: ")
            litellm_params["api_key"] = new_value

        print("\nlitellm_params: ", litellm_params)
        # Confirm before sending POST request
        confirm = input(
            "\033[92mDo you want to send the POST request with the above parameters? (y/n): \033[0m"
        )
        if confirm.lower() != "y":
            print("Aborting POST request.")
            exit()

        # Step 3: Call <proxy-base-url>/model/new for each model
        url = f"{proxy_base_url}/model/new"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {master_key}",
        }
        data = {"model_name": model_name, "litellm_params": litellm_params}
        print("POSTING data to proxy url", url)
        response = requests.post(url, headers=headers, json=data)
        if response.status_code != 200:
            print(f"Error: {response.status_code} - {response.text}")
            raise Exception(f"Error: {response.status_code} - {response.text}")

        # Print the response for each model
        print(
            f"Response for model '{model_name}': Status Code:{response.status_code} - {response.text}"
        )


# Usage
config_file = "config.yaml"
proxy_base_url = "http://0.0.0.0:4000"
master_key = "sk-1234"
print(f"config_file: {config_file}")
print(f"proxy_base_url: {proxy_base_url}")
migrate_models(config_file, proxy_base_url)



================================================
FILE: cookbook/misc/openai_timeouts.py
================================================
import os
from openai import OpenAI
from dotenv import load_dotenv
import concurrent.futures

load_dotenv()

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


def create_chat_completion():
    return client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Say this is a test. Respond in 20 lines",
            }
        ],
        model="gpt-3.5-turbo",
    )


with concurrent.futures.ThreadPoolExecutor() as executor:
    # Set a timeout of 10 seconds
    future = executor.submit(create_chat_completion)
    try:
        chat_completion = future.result(timeout=0.00001)
        print(chat_completion)
    except concurrent.futures.TimeoutError:
        print("Operation timed out.")



================================================
FILE: cookbook/misc/sagmaker_streaming.py
================================================
# Notes - on how to do sagemaker streaming using boto3
import json
import boto3

import sys
import os
from dotenv import load_dotenv

load_dotenv()
import io

sys.path.insert(
    0, os.path.abspath("../..")
)  # Adds the parent directory to the system path


class TokenIterator:
    def __init__(self, stream):
        self.byte_iterator = iter(stream)
        self.buffer = io.BytesIO()
        self.read_pos = 0

    def __iter__(self):
        return self

    def __next__(self):
        while True:
            self.buffer.seek(self.read_pos)
            line = self.buffer.readline()
            if line and line[-1] == ord("\n"):
                self.read_pos += len(line) + 1
                full_line = line[:-1].decode("utf-8")
                line_data = json.loads(full_line.lstrip("data:").rstrip("/n"))
                return line_data["token"]["text"]
            chunk = next(self.byte_iterator)
            self.buffer.seek(0, io.SEEK_END)
            self.buffer.write(chunk["PayloadPart"]["Bytes"])


payload = {
    "inputs": "How do I build a website?",
    "parameters": {"max_new_tokens": 256},
    "stream": True,
}


client = boto3.client("sagemaker-runtime", region_name="us-west-2")
response = client.invoke_endpoint_with_response_stream(
    EndpointName="berri-benchmarking-Llama-2-70b-chat-hf-4",
    Body=json.dumps(payload),
    ContentType="application/json",
)

# for token in TokenIterator(response["Body"]):
#     print(token)



================================================
FILE: cookbook/misc/test_responses_api.py
================================================
import base64
from openai import OpenAI
import time
client = OpenAI(
    base_url="http://0.0.0.0:4001",
    api_key="sk-1234"
)

# Function to encode the image
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


# Path to your image
image_path = "litellm/proxy/logo.jpg"

# Getting the Base64 string
base64_image = encode_image(image_path)


response = client.responses.create(
    model="bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0",
    input=[
        {
            "role": "user",
            "content": [
                { "type": "input_text", "text": "what color is the image"},
                {
                    "type": "input_image",
                    "image_url": f"data:image/jpeg;base64,{base64_image}",
                },
            ],
        }
    ],
)



print(response.output_text)
print("response1 id===", response.id)
print("sleeping for 20 seconds...")
time.sleep(20)
print("making follow up request for existing id")
response2 = client.responses.create(
    model="bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0",
    previous_response_id=response.id,
    input="ok, and what objects are in the image?"
)

print(response2.output_text)





================================================
FILE: cookbook/misc/update_json_caching.py
================================================
import json

# List of models to update
models_to_update = [
    "gpt-4o-mini",
    "gpt-4o-mini-2024-07-18",
    "gpt-4o",
    "gpt-4o-2024-11-20",
    "gpt-4o-2024-08-06",
    "gpt-4o-2024-05-13",
    "text-embedding-3-small",
    "text-embedding-3-large",
    "text-embedding-ada-002-v2",
    "ft:gpt-4o-2024-08-06",
    "ft:gpt-4o-mini-2024-07-18",
    "ft:gpt-3.5-turbo",
    "ft:davinci-002",
    "ft:babbage-002",
]


def update_model_prices(file_path):
    # Read the JSON file as text first to preserve number formatting
    with open(file_path, "r") as file:
        original_text = file.read()
        data = json.loads(original_text)

    # Update specified models
    for model_name in models_to_update:
        print("finding model", model_name)
        if model_name in data:
            print("found model")
            model = data[model_name]
            if "input_cost_per_token" in model:
                # Format new values to match original style
                model["input_cost_per_token_batches"] = float(
                    "{:.12f}".format(model["input_cost_per_token"] / 2)
                )
            if "output_cost_per_token" in model:
                model["output_cost_per_token_batches"] = float(
                    "{:.12f}".format(model["output_cost_per_token"] / 2)
                )
        print("new pricing for model=")
        # Convert all float values to full decimal format before printing
        formatted_model = {
            k: "{:.9f}".format(v) if isinstance(v, float) else v
            for k, v in data[model_name].items()
        }
        print(json.dumps(formatted_model, indent=4))


# Run the update
file_path = "model_prices_and_context_window.json"
update_model_prices(file_path)



================================================
SYMLINK: litellm/proxy/enterprise -> enterprise
================================================


