Directory structure:
└── mem0ai-mem0/
    └── docs/
        ├── README.md
        ├── api-reference.mdx
        ├── changelog.mdx
        ├── docs.json
        ├── examples.mdx
        ├── faqs.mdx
        ├── integrations.mdx
        ├── introduction.mdx
        ├── llms.txt
        ├── quickstart.mdx
        ├── _snippets/
        │   ├── async-memory-add.mdx
        │   ├── blank-notif.mdx
        │   ├── get-help.mdx
        │   └── paper-release.mdx
        ├── api-reference/
        │   ├── entities/
        │   │   ├── delete-user.mdx
        │   │   └── get-users.mdx
        │   ├── memory/
        │   │   ├── add-memories.mdx
        │   │   ├── batch-delete.mdx
        │   │   ├── batch-update.mdx
        │   │   ├── create-memory-export.mdx
        │   │   ├── delete-memories.mdx
        │   │   ├── delete-memory.mdx
        │   │   ├── feedback.mdx
        │   │   ├── get-memory-export.mdx
        │   │   ├── get-memory.mdx
        │   │   ├── history-memory.mdx
        │   │   ├── update-memory.mdx
        │   │   ├── v1-get-memories.mdx
        │   │   ├── v1-search-memories.mdx
        │   │   ├── v2-get-memories.mdx
        │   │   └── v2-search-memories.mdx
        │   ├── organization/
        │   │   ├── add-org-member.mdx
        │   │   ├── create-org.mdx
        │   │   ├── delete-org.mdx
        │   │   ├── get-org-members.mdx
        │   │   ├── get-org.mdx
        │   │   └── get-orgs.mdx
        │   ├── project/
        │   │   ├── add-project-member.mdx
        │   │   ├── create-project.mdx
        │   │   ├── delete-project.mdx
        │   │   ├── get-project-members.mdx
        │   │   ├── get-project.mdx
        │   │   └── get-projects.mdx
        │   └── webhook/
        │       ├── create-webhook.mdx
        │       ├── delete-webhook.mdx
        │       ├── get-webhook.mdx
        │       └── update-webhook.mdx
        ├── components/
        │   ├── embedders/
        │   │   ├── config.mdx
        │   │   ├── overview.mdx
        │   │   └── models/
        │   │       ├── aws_bedrock.mdx
        │   │       ├── azure_openai.mdx
        │   │       ├── google_AI.mdx
        │   │       ├── huggingface.mdx
        │   │       ├── langchain.mdx
        │   │       ├── lmstudio.mdx
        │   │       ├── ollama.mdx
        │   │       ├── openai.mdx
        │   │       ├── together.mdx
        │   │       └── vertexai.mdx
        │   ├── llms/
        │   │   ├── config.mdx
        │   │   ├── overview.mdx
        │   │   └── models/
        │   │       ├── anthropic.mdx
        │   │       ├── aws_bedrock.mdx
        │   │       ├── azure_openai.mdx
        │   │       ├── deepseek.mdx
        │   │       ├── google_AI.mdx
        │   │       ├── groq.mdx
        │   │       ├── langchain.mdx
        │   │       ├── litellm.mdx
        │   │       ├── lmstudio.mdx
        │   │       ├── mistral_AI.mdx
        │   │       ├── ollama.mdx
        │   │       ├── openai.mdx
        │   │       ├── sarvam.mdx
        │   │       ├── together.mdx
        │   │       ├── vllm.mdx
        │   │       └── xAI.mdx
        │   └── vectordbs/
        │       ├── config.mdx
        │       ├── overview.mdx
        │       └── dbs/
        │           ├── azure.mdx
        │           ├── baidu.mdx
        │           ├── chroma.mdx
        │           ├── databricks.mdx
        │           ├── elasticsearch.mdx
        │           ├── faiss.mdx
        │           ├── langchain.mdx
        │           ├── milvus.mdx
        │           ├── mongodb.mdx
        │           ├── opensearch.mdx
        │           ├── pgvector.mdx
        │           ├── pinecone.mdx
        │           ├── qdrant.mdx
        │           ├── redis.mdx
        │           ├── s3_vectors.mdx
        │           ├── supabase.mdx
        │           ├── upstash-vector.mdx
        │           ├── vectorize.mdx
        │           ├── vertex_ai.mdx
        │           └── weaviate.mdx
        ├── contributing/
        │   ├── development.mdx
        │   └── documentation.mdx
        ├── core-concepts/
        │   ├── memory-types.mdx
        │   └── memory-operations/
        │       ├── add.mdx
        │       ├── delete.mdx
        │       ├── search.mdx
        │       └── update.mdx
        ├── examples/
        │   ├── ai_companion_js.mdx
        │   ├── aws_example.mdx
        │   ├── chrome-extension.mdx
        │   ├── collaborative-task-agent.mdx
        │   ├── customer-support-agent.mdx
        │   ├── eliza_os.mdx
        │   ├── email_processing.mdx
        │   ├── llama-index-mem0.mdx
        │   ├── llamaindex-multiagent-learning-system.mdx
        │   ├── mem0-agentic-tool.mdx
        │   ├── mem0-demo.mdx
        │   ├── mem0-google-adk-healthcare-assistant.mdx
        │   ├── mem0-mastra.mdx
        │   ├── mem0-openai-voice-demo.mdx
        │   ├── mem0-with-ollama.mdx
        │   ├── memory-guided-content-writing.mdx
        │   ├── multimodal-demo.mdx
        │   ├── openai-inbuilt-tools.mdx
        │   ├── personal-ai-tutor.mdx
        │   ├── personal-travel-assistant.mdx
        │   ├── personalized-deep-research.mdx
        │   ├── personalized-search-tavily-mem0.mdx
        │   └── youtube-assistant.mdx
        ├── integrations/
        │   ├── agentops.mdx
        │   ├── agno.mdx
        │   ├── autogen.mdx
        │   ├── aws-bedrock.mdx
        │   ├── crewai.mdx
        │   ├── dify.mdx
        │   ├── elevenlabs.mdx
        │   ├── flowise.mdx
        │   ├── google-ai-adk.mdx
        │   ├── keywords.mdx
        │   ├── langchain-tools.mdx
        │   ├── langchain.mdx
        │   ├── langgraph.mdx
        │   ├── livekit.mdx
        │   ├── llama-index.mdx
        │   ├── mastra.mdx
        │   ├── openai-agents-sdk.mdx
        │   ├── pipecat.mdx
        │   ├── raycast.mdx
        │   └── vercel-ai-sdk.mdx
        ├── open-source/
        │   ├── multimodal-support.mdx
        │   ├── node-quickstart.mdx
        │   ├── overview.mdx
        │   ├── python-quickstart.mdx
        │   ├── features/
        │   │   ├── async-memory.mdx
        │   │   ├── custom-fact-extraction-prompt.mdx
        │   │   ├── custom-update-memory-prompt.mdx
        │   │   ├── multimodal-support.mdx
        │   │   ├── openai_compatibility.mdx
        │   │   ├── overview.mdx
        │   │   └── rest-api.mdx
        │   └── graph_memory/
        │       ├── features.mdx
        │       └── overview.mdx
        ├── openmemory/
        │   ├── integrations.mdx
        │   ├── overview.mdx
        │   └── quickstart.mdx
        └── platform/
            ├── advanced-memory-operations.mdx
            ├── overview.mdx
            ├── quickstart.mdx
            └── features/
                ├── advanced-retrieval.mdx
                ├── async-client.mdx
                ├── contextual-add.mdx
                ├── criteria-retrieval.mdx
                ├── custom-categories.mdx
                ├── custom-instructions.mdx
                ├── direct-import.mdx
                ├── expiration-date.mdx
                ├── feedback-mechanism.mdx
                ├── graph-memory.mdx
                ├── group-chat.mdx
                ├── memory-export.mdx
                ├── multimodal-support.mdx
                ├── platform-overview.mdx
                ├── selective-memory.mdx
                ├── timestamp.mdx
                └── webhooks.mdx

================================================
FILE: docs/README.md
================================================
# Mintlify Starter Kit

Click on `Use this template` to copy the Mintlify starter kit. The starter kit contains examples including

- Guide pages
- Navigation
- Customizations
- API Reference pages
- Use of popular components

### Development

Install the [Mintlify CLI](https://www.npmjs.com/package/mintlify) to preview the documentation changes locally. To install, use the following command

```
npm i -g mintlify
```

Run the following command at the root of your documentation (where mint.json is)

```
mintlify dev
```

### Publishing Changes

Install our Github App to auto propagate changes from your repo to your deployment. Changes will be deployed to production automatically after pushing to the default branch. Find the link to install on your dashboard. 

#### Troubleshooting

- Mintlify dev isn't running - Run `mintlify install` it'll re-install dependencies.
- Page loads as a 404 - Make sure you are running in a folder with `mint.json`



================================================
FILE: docs/api-reference.mdx
================================================
---
title: Overview
icon: "info"
iconType: "solid"
---

Mem0 provides a powerful set of APIs that allow you to integrate advanced memory management capabilities into your applications. Our APIs are designed to be intuitive, efficient, and scalable, enabling you to create, retrieve, update, and delete memories across various entities such as users, agents, apps, and runs.

## Key Features

- **Memory Management**: Add, retrieve, update, and delete memories with ease.
- **Entity-based Operations**: Perform operations on memories associated with specific users, agents, apps, or runs.
- **Advanced Search**: Utilize our search API to find relevant memories based on various criteria.
- **History Tracking**: Access the history of memory interactions for comprehensive analysis.
- **User Management**: Manage user entities and their associated memories.

## API Structure

Our API is organized into several main categories:

1. **Memory APIs**: Core operations for managing individual memories and collections.
2. **Entities APIs**: Manage different entity types (users, agents, etc.) and their associated memories.
3. **Search API**: Advanced search functionality to retrieve relevant memories.
4. **History API**: Track and retrieve the history of memory interactions.

## Authentication

All API requests require authentication using HTTP Basic Auth. Ensure you include your API key in the Authorization header of each request.

## Organizations and projects (optional)

Organizations and projects provide the following capabilities:

- **Multi-org/project Support**: Specify organization and project when initializing the Mem0 client to attribute API usage appropriately
- **Member Management**: Control access to data through organization and project membership
- **Access Control**: Only members can access memories and data within their organization/project scope
- **Team Isolation**: Maintain data separation between different teams and projects for secure collaboration

Example with the mem0 Python package:

<Tabs>
  <Tab title="Python">

```python
from mem0 import MemoryClient
client = MemoryClient(org_id='YOUR_ORG_ID', project_id='YOUR_PROJECT_ID')
```

  </Tab>

  <Tab title="Node.js">

```javascript
import { MemoryClient } from "mem0ai";
const client = new MemoryClient({organizationId: "YOUR_ORG_ID", projectId: "YOUR_PROJECT_ID"});
```

  </Tab>
</Tabs>

### Project Management Methods

The Mem0 client provides comprehensive project management capabilities through the `client.project` interface:

#### Get Project Details

Retrieve information about the current project:

```python
# Get all project details
project_info = client.project.get()

# Get specific fields only
project_info = client.project.get(fields=["name", "description", "custom_categories"])
```

#### Create a New Project

Create a new project within your organization:

```python
# Create a project with name and description
new_project = client.project.create(
    name="My New Project",
    description="A project for managing customer support memories"
)
```

#### Update Project Settings

Modify project configuration including custom instructions, categories, and graph settings:

```python
# Update project with custom categories
client.project.update(
    custom_categories=[
        {"customer_preferences": "Customer likes, dislikes, and preferences"},
        {"support_history": "Previous support interactions and resolutions"}
    ]
)

# Update project with custom instructions
client.project.update(
    custom_instructions="..."
)

# Enable graph memory for the project
client.project.update(enable_graph=True)

# Update multiple settings at once
client.project.update(
    custom_instructions="...",
    custom_categories=[
        {"personal_info": "User personal information and preferences"},
        {"work_context": "Professional context and work-related information"}
    ],
    enable_graph=True
)
```

#### Delete Project

<Note>
This action will remove all memories, messages, and other related data in the project. This operation is irreversible.
</Note>

Remove a project and all its associated data:

```python
# Delete the current project (irreversible)
result = client.project.delete()
```

#### Member Management

Manage project members and their access levels:

```python
# Get all project members
members = client.project.get_members()

# Add a new member as a reader
client.project.add_member(
    email="colleague@company.com",
    role="READER"  # or "OWNER"
)

# Update a member's role
client.project.update_member(
    email="colleague@company.com",
    role="OWNER"
)

# Remove a member from the project
client.project.remove_member(email="colleague@company.com")
```

#### Member Roles

- **READER**: Can view and search memories, but cannot modify project settings or manage members
- **OWNER**: Full access including project modification, member management, and all reader permissions

#### Async Support

All project methods are also available in async mode:

```python
from mem0 import AsyncMemoryClient

async def manage_project():
    client = AsyncMemoryClient(org_id='YOUR_ORG_ID', project_id='YOUR_PROJECT_ID')

    # All methods support async/await
    project_info = await client.project.get()
    await client.project.update(enable_graph=True)
    members = await client.project.get_members()

# To call the async function properly
import asyncio
asyncio.run(manage_project())
```

## Getting Started

To begin using the Mem0 API, you'll need to:

1. Sign up for a [Mem0 account](https://app.mem0.ai) and obtain your API key.
2. Familiarize yourself with the API endpoints and their functionalities.
3. Make your first API call to add or retrieve a memory.

Explore the detailed documentation for each API endpoint to learn more about request/response formats, parameters, and example usage.



================================================
FILE: docs/changelog.mdx
================================================
---
title: "Product Updates"
mode: "wide"
---

 
<Tabs>
<Tab title="Python">

<Update label="2025-09-03" description="v0.1.117">

**New Features & Updates:**
- **OpenMemory:**
  - Added memory export / import feature
  - Added vector store integrations: Weaviate, FAISS, PGVector, Chroma, Redis, Elasticsearch, Milvus
  - Added `export_openmemory.sh` migration script
- **Vector Stores:**
  - Added Amazon S3 Vectors support
  - Added Databricks Mosaic AI vector store support
  - Added support for OpenAI Store
- **Graph Memory:** Added support for graph memory using Kuzu
- **Azure:** Added Azure Identity for Azure OpenAI and Azure AI Search authentication
- **Elasticsearch:** Added headers configuration support

**Improvements:**
  - Added custom connection client to enable connecting to local containers for Weaviate
  - Updated configuration AWS Bedrock
  - Fixed dependency issues and tests; updated docstrings
- **Documentation:**
  - Fixed Graph Docs page missing in sidebar
  - Updated integration documentation
  - Added version param in Search V2 API documentation
  - Updated Databricks documentation and refactored docs
  - Updated favicon logo
  - Fixed typos and Typescript docs

**Bug Fixes:**
- Baidu: Added missing provider for Baidu vector DB
- MongoDB: Replaced `query_vector` args in search method
- Fixed new memory mistaken for current
- AsyncMemory._add_to_vector_store: handled edge case when no facts found
- Fixed missing commas in Kuzu graph INSERT queries
- Fixed inconsistent created and updated properties for Graph
- Fixed missing `app_id` on client for Neptune Analytics
- Correctly pick AWS region from environment variable
- Fixed Ollama model existence check

**Refactoring:**
- **PGVector:** Use internal connection pools and context managers

</Update>

<Update label="2025-08-14" description="v0.1.116">

**New Features & Updates:**
- **Pinecone:** Added namespace support and improved type safety
- **Milvus:** Added db_name field to MilvusDBConfig
- **Vector Stores:** Added multi-id filters support
- **Vercel AI SDK:** Migration to AI SDK V5.0
- **Python Support:** Added Python 3.12 support
- **Graph Memory:** Added sanitizer methods for nodes and relationships
- **LLM Monitoring:** Added monitoring callback support

**Improvements:**
- **Performance:**
  - Improved async handling in AsyncMemory class
- **Documentation:**
  - Added async add announcement
  - Added personalized search docs
  - Added Neptune examples
  - Added V5 migration docs
- **Configuration:**
  - Refactored base class config for LLMs
  - Added sslmode for pgvector
- **Dependencies:**
  - Updated psycopg to version 3
  - Updated Docker compose

**Bug Fixes:**
- **Tests:**
  - Fixed failing tests
  - Restricted package versions
- **Memgraph:**
  - Fixed async attribute errors
  - Fixed n_embeddings usage
  - Fixed indexing issues
- **Vector Stores:**
  - Fixed Qdrant cloud indexing
  - Fixed Neo4j Cypher syntax
  - Fixed LLM parameters
- **Graph Store:**
  - Fixed LM config prioritization
- **Dependencies:**
  - Fixed JSON import for psycopg

**Refactoring:**
- **Google AI:** Refactored from Gemini to Google AI
- **Base Classes:** Refactored LLM base class configuration

</Update>

<Update label="2025-07-24" description="v0.1.115">

**New Features & Updates:**
- Enhanced project management via `client.project` and `AsyncMemoryClient.project` interfaces
- Full support for project CRUD operations (create, read, update, delete)
- Project member management: add, update, remove, and list members
- Manage project settings including custom instructions, categories, retrieval criteria, and graph enablement
- Both sync and async support for all project management operations

**Improvements:**
- **Documentation:**
  - Added detailed API reference and usage examples for new project management methods.
  - Updated all docs to use `client.project.get()` and `client.project.update()` instead of deprecated methods.
  
- **Deprecation:**
  - Marked `get_project()` and `update_project()` as deprecated (these methods were already present); added warnings to guide users to the new API.

**Bug Fixes:**
- **Tests:**
  - Fixed Gemini embedder and LLM test mocks for correct error handling and argument structure.
- **vLLM:**
  - Fixed duplicate import in vLLM module.

</Update>

<Update label="2025-07-05" description="v0.1.114">

**New Features:**
- **OpenAI Agents:** Added OpenAI agents SDK support
- **Amazon Neptune:** Added Amazon Neptune Analytics graph_store configuration and integration
- **vLLM:** Added vLLM support

**Improvements:**
- **Documentation:** 
  - Added SOC2 and HIPAA compliance documentation
  - Enhanced group chat feature documentation for platform
  - Added Google AI ADK Integration documentation
  - Fixed documentation images and links
- **Setup:** Fixed Mem0 setup, logging, and documentation issues

**Bug Fixes:**
- **MongoDB:** Fixed MongoDB Vector Store misaligned strings and classes
- **vLLM:** Fixed missing OpenAI import in vLLM module and call errors
- **Dependencies:** Fixed CI issues related to missing dependencies
- **Installation:** Reverted pip install changes

</Update>

<Update label="2025-06-30" description="v0.1.113">

**Bug Fixes:**
- **Gemini:** Fixed Gemini embedder configuration

</Update>

<Update label="2025-06-27" description="v0.1.112">

**New Features:**
- **Memory:** Added immutable parameter to add method
- **OpenMemory:** Added async_mode parameter support

**Improvements:**
- **Documentation:** 
  - Enhanced platform feature documentation
  - Fixed documentation links
  - Added async_mode documentation
- **MongoDB:** Fixed MongoDB configuration name

**Bug Fixes:**
- **Bedrock:** Fixed Bedrock LLM, embeddings, tools, and temporary credentials
- **Memory:** Fixed memory categorization by updating dependencies and correcting API usage
- **Gemini:** Fixed Gemini Embeddings and LLM issues

</Update>

<Update label="2025-06-23" description="v0.1.111">

**New Features:**
- **OpenMemory:** 
  - Added OpenMemory augment support
  - Added OpenMemory Local Support using new library
- **vLLM:** Added vLLM support integration

**Improvements:**
- **Documentation:** 
  - Added MCP Client Integration Guide and updated installation commands
  - Improved Agent Id documentation for Mem0 OSS Graph Memory
- **Core:** Added JSON parsing to solve hallucination errors

**Bug Fixes:**
- **Gemini:** Fixed Gemini Embeddings migration

</Update>

<Update label="2025-06-20" description="v0.1.110">

**New Features:**
- **Baidu:** Added Baidu vector database integration

**Improvements:**
- **Documentation:** 
  - Updated changelog
  - Fixed example in quickstart page
  - Updated client.update() method documentation in OpenAPI specification
- **OpenSearch:** Updated logger warning

**Bug Fixes:**
- **CI:** Fixed failing CI pipeline

</Update>

<Update label="2025-06-19" description="v0.1.109">

**New Features:**
- **AgentOps:** Added AgentOps integration
- **LM Studio:** Added response_format parameter for LM Studio configuration
- **Examples:** Added Memory agent powered by voice (Cartesia + Agno)

**Improvements:**
- **AI SDK:** Added output_format parameter
- **Client:** Enhanced update method to support metadata
- **Google:** Added Google Genai library support

**Bug Fixes:**
- **Build:** Fixed Build CI failure
- **Pinecone:** Fixed pinecone for async memory

</Update>

<Update label="2025-06-14" description="v0.1.108">

**New Features:**
- **MongoDB:** Added MongoDB Vector Store support
- **Client:** Added client support for summary functionality

**Improvements:**
- **Pinecone:** Fixed pinecone version issues
- **OpenSearch:** Added logger support
- **Testing:** Added python version test environments

</Update>

<Update label="2025-06-11" description="v0.1.107">

**Improvements:**
- **Documentation:**
  - Updated Livekit documentation migration
  - Updated OpenMemory hosted version documentation
- **Core:** Updated categorization flow
- **Storage:** Fixed migration issues

</Update>

<Update label="2025-06-09" description="v0.1.106">

**New Features:**
- **Cloudflare:** Added Cloudflare vector store support
- **Search:** Added threshold parameter to search functionality
- **API:** Added wildcard character support for v2 Memory APIs

**Improvements:**
- **Documentation:** Updated README docs for OpenMemory environment setup
- **Core:** Added support for unique user IDs

**Bug Fixes:**
- **Core:** Fixed error handling exceptions

</Update>

<Update label="2025-06-03" description="v0.1.104">

**Bug Fixes:**
- **Vector Stores:** Fixed GET_ALL functionality for FAISS and OpenSearch

</Update>

<Update label="2025-06-02" description="v0.1.103">

**New Features:**
- **LLM:** Added support for OpenAI compatible LLM providers with baseUrl configuration

**Improvements:**
- **Documentation:**
  - Fixed broken links
  - Improved Graph Memory features documentation clarity
  - Updated enable_graph documentation
- **TypeScript SDK:** Updated Google SDK peer dependency version
- **Client:** Added async mode parameter

</Update>

<Update label="2025-05-26" description="v0.1.102">

**New Features:**
- **Examples:** Added Neo4j example
- **AI SDK:** Added Google provider support
- **OpenMemory:** Added LLM and Embedding Providers support

**Improvements:**
- **Documentation:**
  - Updated memory export documentation
  - Enhanced role-based memory attribution rules documentation
  - Updated API reference and messages documentation
  - Added Mastra and Raycast documentation
  - Added NOT filter documentation for Search and GetAll V2
  - Announced Claude 4 support
- **Core:**
  - Removed support for passing string as input in client.add()
  - Added support for sarvam-m model
- **TypeScript SDK:** Fixed types from message interface

**Bug Fixes:**
- **Memory:** Prevented saving prompt artifacts as memory when no new facts are present
- **OpenMemory:** Fixed typos in MCP tool description

</Update>

<Update label="2025-05-15" description="v0.1.101">

**New Features:**
- **Neo4j:** Added base label configuration support

**Improvements:**
- **Documentation:**
  - Updated Healthcare example index
  - Enhanced collaborative task agent documentation clarity
  - Added criteria-based filtering documentation
- **OpenMemory:** Added cURL command for easy installation
- **Build:** Migrated to Hatch build system

</Update>

<Update label="2025-05-10" description="v0.1.100">

**New Features:**
- **Memory:** Added Group Chat Memory Feature support
- **Examples:** Added Healthcare assistant using Mem0 and Google ADK

**Bug Fixes:**
- **SSE:** Fixed SSE connection issues
- **MCP:** Fixed memories not appearing in MCP clients added from Dashboard

</Update>

<Update label="2025-05-07" description="v0.1.99">

**New Features:**
- **OpenMemory:** Added OpenMemory support
- **Neo4j:** Added weights to Neo4j model
- **AWS:** Added support for Opsearch Serverless
- **Examples:** Added ElizaOS Example

**Improvements:**
- **Documentation:** Updated Azure AI documentation
- **AI SDK:** Added missing parameters and updated demo application
- **OSS:** Fixed AOSS and AWS BedRock LLM

</Update>

<Update label="2025-04-30" description="v0.1.98">

**New Features:**
- **Neo4j:** Added support for Neo4j database
- **AWS:** Added support for AWS Bedrock Embeddings

**Improvements:**
- **Client:** Updated delete_users() to use V2 API endpoints
- **Documentation:** Updated timestamp and dual-identity memory management docs
- **Neo4j:** Improved Neo4j queries and removed warnings
- **AI SDK:** Added support for graceful failure when services are down

**Bug Fixes:**
- Fixed AI SDK filters
- Fixed new memories wrong type
- Fixed duplicated metadata issue while adding/updating memories

</Update>

<Update label="2025-04-23" description="v0.1.97">

**New Features:**
- **HuggingFace:** Added support for HF Inference

**Bug Fixes:**
- Fixed proxy for Mem0

</Update>

<Update label="2025-04-16" description="v0.1.96">

**New Features:**
- **Vercel AI SDK:** Added Graph Memory support

**Improvements:**
- **Documentation:** Fixed timestamp and README links
- **Client:** Updated TS client to use proper types for deleteUsers
- **Dependencies:** Removed unnecessary dependencies from base package

</Update>

<Update label="2025-04-09" description="v0.1.95">

**Improvements:**
- **Client:** Fixed Ping Method for using default org_id and project_id
- **Documentation:** Updated documentation

**Bug Fixes:**
- Fixed mem0-migrations issue

</Update>

<Update label="2025-04-26" description="v0.1.94">

**New Features:**
- **Integrations:** Added Memgraph integration
- **Memory:** Added timestamp support
- **Vector Stores:** Added reset function for VectorDBs

**Improvements:**
- **Documentation:**
  - Updated timestamp and expiration_date documentation
  - Fixed v2 search documentation
  - Added "memory" in EC "Custom config" section
  - Fixed typos in the json config sample

</Update>

<Update label="2025-04-21" description="v0.1.93">

**Improvements:**
- **Vector Stores:** Initialized embedding_model_dims in all vectordbs

**Bug Fixes:**
- **Documentation:** Fixed agno link

</Update>

<Update label="2025-04-18" description="v0.1.92">

**New Features:**
- **Memory:** Added Memory Reset functionality
- **Client:** Added support for Custom Instructions
- **Examples:** Added Fitness Checker powered by memory

**Improvements:**
- **Core:** Updated capture_event
- **Documentation:** Fixed curl for v2 get_all

**Bug Fixes:**
- **Vector Store:** Fixed user_id functionality
- **Client:** Various client improvements

</Update>

<Update label="2025-04-16" description="v0.1.91">

**New Features:**
- **LLM Integrations:** Added Azure OpenAI Embedding Model
- **Examples:**
  - Added movie recommendation using grok3
  - Added Voice Assistant using Elevenlabs

**Improvements:**
- **Documentation:**
  - Added keywords AI
  - Reformatted navbar page URLs
  - Updated changelog
  - Updated openai.mdx
- **FAISS:** Silenced FAISS info logs

</Update>

<Update label="2025-04-11" description="v0.1.90">

**New Features:**
- **LLM Integrations:** Added Mistral AI as LLM provider

**Improvements:**
- **Documentation:**
  - Updated changelog
  - Fixed memory exclusion example
  - Updated xAI documentation
  - Updated YouTube Chrome extension example documentation

**Bug Fixes:**
- **Core:** Fixed EmbedderFactory.create() in GraphMemory
- **Azure OpenAI:** Added patch to fix Azure OpenAI
- **Telemetry:** Fixed telemetry issue

</Update>

<Update label="2025-04-11" description="v0.1.89">

**New Features:**
- **Langchain Integration:** Added support for Langchain VectorStores
- **Examples:**
  - Added personal assistant example
  - Added personal study buddy example
  - Added YouTube assistant Chrome extension example
  - Added agno example
  - Updated OpenAI Responses API examples
- **Vector Store:** Added capability to store user_id in vector database
- **Async Memory:** Added async support for OSS

**Improvements:**
- **Documentation:** Updated formatting and examples

</Update>

<Update label="2025-04-09" description="v0.1.87">

**New Features:**
- **Upstash Vector:** Added support for Upstash Vector store

**Improvements:**
- **Code Quality:** Removed redundant code lines
- **Build:** Updated MAKEFILE
- **Documentation:** Updated memory export documentation

</Update>

<Update label="2025-04-07" description="v0.1.86">

**Improvements:**
- **FAISS:** Added embedding_dims parameter to FAISS vector store

</Update>

<Update label="2025-04-07" description="v0.1.84">

**New Features:**
- **Langchain Embedder:** Added Langchain embedder integration

**Improvements:**
- **Langchain LLM:** Updated Langchain LLM integration to directly pass the Langchain object LLM
</Update>

<Update label="2025-04-07" description="v0.1.83">

**Bug Fixes:**
- **Langchain LLM:** Fixed issues with Langchain LLM integration
</Update>

<Update label="2025-04-07" description="v0.1.82">

**New Features:**
- **LLM Integrations:** Added support for Langchain LLMs, Google as new LLM and embedder
- **Development:** Added development docker compose

**Improvements:**
- **Output Format:** Set output_format='v1.1' and updated documentation

**Documentation:**
- **Integrations:** Added LMStudio and Together.ai documentation
- **API Reference:** Updated output_format documentation
- **Integrations:** Added PipeCat integration documentation
- **Integrations:** Added Flowise integration documentation for Mem0 memory setup

**Bug Fixes:**
- **Tests:** Fixed failing unit tests
</Update>

<Update label="2025-04-02" description="v0.1.79">

**New Features:**
- **FAISS Support:** Added FAISS vector store support

</Update>

<Update label="2025-04-02" description="v0.1.78">

**New Features:**
- **Livekit Integration:** Added Mem0 livekit example
- **Evaluation:** Added evaluation framework and tools

**Documentation:**
- **Multimodal:** Updated multimodal documentation
- **Examples:** Added examples for email processing
- **API Reference:** Updated API reference section
- **Elevenlabs:** Added Elevenlabs integration example

**Bug Fixes:**
- **OpenAI Environment Variables:** Fixed issues with OpenAI environment variables
- **Deployment Errors:** Added `package.json` file to fix deployment errors
- **Tools:** Fixed tools issues and improved formatting
- **Docs:** Updated API reference section for `expiration date`
</Update>

<Update label="2025-03-26" description="v0.1.77">

**Bug Fixes:**
- **OpenAI Environment Variables:** Fixed issues with OpenAI environment variables
- **Deployment Errors:** Added `package.json` file to fix deployment errors
- **Tools:** Fixed tools issues and improved formatting
- **Docs:** Updated API reference section for `expiration date`
</Update>

<Update label="2025-03-19" description="v0.1.76">
**New Features:**
- **Supabase Vector Store:** Added support for Supabase Vector Store
- **Supabase History DB:** Added Supabase History DB to run Mem0 OSS on Serverless
- **Feedback Method:** Added feedback method to client

**Bug Fixes:**
- **Azure OpenAI:** Fixed issues with Azure OpenAI
- **Azure AI Search:** Fixed test cases for Azure AI Search
</Update>

</Tab>

<Tab title="TypeScript">

<Update label="2025-09-04" description="v2.1.38">
**New Features:**
- **Client:** Added `metadata` param to `update` method.
</Update>

<Update label="2025-08-04" description="v2.1.37">
**New Features:**
- **OSS:** Added `RedisCloud` search module check
</Update>

<Update label="2025-07-08" description="v2.1.36">
**New Features:**
- **Client:** Added `structured_data_schema` param to `add` method.
</Update>

<Update label="2025-07-08" description="v2.1.35">
**New Features:**
- **Client:** Added `createMemoryExport` and `getMemoryExport` methods.
</Update>

<Update label="2025-07-03" description="v2.1.34">
**New Features:**
- **OSS:** Added Gemini support
</Update>

<Update label="2025-06-24" description="v2.1.33">
**Improvement :**
- **Client:** Added `immutable` param to `add` method.
</Update>

<Update label="2025-06-20" description="v2.1.32">
**Improvement :**
- **Client:** Made `api_version` V2 as default.
</Update>

<Update label="2025-06-17" description="v2.1.31">
**Improvement :**
- **Client:** Added param `filter_memories`.
</Update>

<Update label="2025-06-06" description="v2.1.30">
**New Features:**
- **OSS:** Added Cloudflare support

**Improvements:**
- **OSS:** Fixed baseURL param in LLM Config.
</Update>

<Update label="2025-05-30" description="v2.1.29">
**Improvements:**
- **Client:** Added Async Mode Param for `add` method.
</Update>

<Update label="2025-05-30" description="v2.1.28">
**Improvements:**
- **SDK:** Update Google SDK Peer Dependency Version.
</Update>

<Update label="2025-05-27" description="v2.1.27">
**Improvements:**
- **OSS:** Added baseURL param in LLM Config.
</Update>
<Update label="2025-05-23" description="v2.1.26">
**Improvements:**
- **Client:** Removed type `string` from `messages` interface
</Update>

<Update label="2025-05-08" description="v2.1.25">
**Improvements:**
- **Client:** Improved error handling in client.
</Update>

<Update label="2025-05-06" description="v2.1.24">
**New Features:**
- **Client:** Added new param `output_format` to match Python SDK.
- **Client:** Added new enum `OutputFormat` for `v1.0` and `v1.1`
</Update>

<Update label="2025-05-05" description="v2.1.23">
**New Features:**
- **Client:** Updated `deleteUsers` to use `v2` API.
- **Client:** Deprecated `deleteUser` and added deprecation warning.
</Update>

<Update label="2025-05-02" description="v2.1.22">
**New Features:**
- **Client:** Updated `deleteUser` to use `entity_id` and `entity_type`
</Update>

<Update label="2025-05-01" description="v2.1.21">
**Improvements:**
- **OSS SDK:** Bumped version of `@anthropic-ai/sdk` to `0.40.1`
</Update>

<Update label="2025-04-28" description="v2.1.20">
**Improvements:**
- **Client:** Fixed `organizationId` and `projectId` being assigned to default in `ping` method
</Update>

<Update label="2025-04-22" description="v2.1.19">
**Improvements:**
- **Client:** Added support for `timestamps`
</Update>

<Update label="2025-04-17" description="v2.1.18">
**Improvements:**
- **Client:** Added support for custom instructions
</Update>

<Update label="2025-04-15" description="v2.1.17">
**New Features:**
- **OSS SDK:** Added support for Langchain LLM
- **OSS SDK:** Added support for Langchain Embedder
- **OSS SDK:** Added support for Langchain Vector Store
- **OSS SDK:** Added support for Azure OpenAI Embedder


**Improvements:**
- **OSS SDK:** Changed `model` in LLM and Embedder to use type any from `string` to use langchain llm models
- **OSS SDK:** Added client to vector store config for langchain vector store
- **OSS SDK:** - Updated Azure OpenAI to use new OpenAI SDK
</Update>

<Update label="2025-04-11" description="v2.1.16-patch.1">
**Bug Fixes:**
- **Azure OpenAI:** Fixed issues with Azure OpenAI
</Update>

<Update label="2025-04-11" description="v2.1.16">
**New Features:**
- **Azure OpenAI:** Added support for Azure OpenAI
- **Mistral LLM:** Added Mistral LLM integration in OSS

**Improvements:**
- **Zod:** Updated Zod to 3.24.1 to avoid conflicts with other packages
</Update>

<Update label="2025-04-09" description="v2.1.15">
**Improvements:**
- **Client:** Added support for Mem0 to work with Chrome Extensions
</Update>

<Update label="2025-04-01" description="v2.1.14">
**New Features:**
- **Mastra Example:** Added Mastra example
- **Integrations:** Added Flowise integration documentation for Mem0 memory setup

**Improvements:**
- **Demo:** Updated Demo Mem0AI
- **Client:** Enhanced Ping method in Mem0 Client
- **AI SDK:** Updated AI SDK implementation
</Update>

<Update label="2025-03-29" description="v2.1.13">
**Improvements:**
- **Introduced `ping` method to check if API key is valid and populate org/project id**
</Update>

<Update label="2025-03-29" description="AI SDK v1.0.0">
**New Features:**
- **Vercel AI SDK Update:** Support threshold and rerank

**Improvements:**
- **Made add calls async to avoid blocking**
- **Bump `mem0ai` to use `2.1.12`**

</Update>

<Update label="2025-03-26" description="v2.1.12">
**New Features:**
- **Mem0 OSS:** Support infer param

**Improvements:**
- **Updated Supabase TS Docs**
- **Made package size smaller**

</Update>

<Update label="2025-03-19" description="v2.1.11">
**New Features:**
- **Supabase Vector Store Integration**
- **Feedback Method**
</Update>

</Tab>

<Tab title="Platform">

<Update label="2025-07-23" description="">

**Bug Fixes:**
- **Memory:** Fixed ADD functionality

</Update>

<Update label="2025-07-19" description="">

**New Features:**
- **UI:** Added Settings UI and latency display
- **Performance:** Neo4j query optimization

**Bug Fixes:**
- **OpenMemory:** Fixed OMM raising unnecessary exceptions

</Update>

<Update label="2025-07-18" description="">

**Improvements:**
- **UI:** Updated Event UI
- **Performance:** Fixed N+1 query issue in semantic_search_v2 by optimizing MemorySerializer field selection

**Bug Fixes:**
- **Memory:** Fixed duplicate memory index sentry error

</Update>

<Update label="2025-07-17" description="">

**New Features:**
- **UI:** New Settings Page
- **Memory:** Duplicate memories entities support

**Improvements:**
- **Performance:** Optimized semantic search and get_all APIs by eliminating N+1 queries

</Update>

<Update label="2025-07-16" description="">

**New Features:**
- **Database:** Implemented read replica routing with enhanced logging and app-specific DB routing

**Improvements:**
- **Performance:** Improved query performance in search v2 and get all v2 endpoints

**Bug Fixes:**
- **API:** Fixed pagination for get all API

</Update>

<Update label="2025-07-12" description="">

**Bug Fixes:**
- **Graph:** Fixed social graph bugs and connection issues

</Update>

<Update label="2025-07-11" description="">

**Improvements:**
- **Rate Limiting:** New rate limit for V2 Search

**Bug Fixes:**
- **Slack:** Fixed Slack rate limit error with backend improvements

</Update>

<Update label="2025-07-10" description="">

**Improvements:**
- **Performance:** 
  - Changed connection pooling time to 5 minutes
  - Separated graph lambdas for better performance

</Update>

<Update label="2025-07-09" description="">

**Improvements:**
- **Graph:** Graph Optimizations V2 and memory improvements

</Update>

<Update label="2025-07-08" description="">

**New Features:**
- **Database:** Added read replica support for improved database performance
- **UI:** Implemented UI changes for Users Page
- **Feedback:** Enabled feedback functionality

**Bug Fixes:**
- **Serializer:** Fixed GET ALL Serializer

</Update>

<Update label="2025-07-05" description="">

**New Features:**
- **UI:** User Page Revamp and New Users Page

</Update>

<Update label="2025-07-04" description="">

**New Features:**
- **Users:** New Users Page implementation
- **Tools:** Added script to backfill memory categories

**Bug Fixes:**
- **Filters:** Fixed Filters Get All functionality

</Update>

<Update label="2025-07-03" description="">

**Improvements:**
- **Graph:** Graph Memory optimization
- **Memory:** Fixed exact memories and semantically similar memories retrieval

</Update>

<Update label="2025-07-02" description="">

**Improvements:**
- **Categorization:** Refactored categorization logic to utilize Gemini 2.5 Flash and improve message handling

</Update>

<Update label="2025-07-01" description="">

**Bug Fixes:**
- **Memory:** Fixed old_memory issue in Async memory addition lambda
- **Events:** Fixed missing events

</Update>

<Update label="2025-06-30" description="">

**Improvements:**
- **Graph:** Improvements to graph memory and added user to LTM-STM

</Update>

<Update label="2025-06-28" description="">

**New Features:**
- **Graph:** Added support for SQS in graph memory addition
- **Testing:** Added Locust load testing script and Grafana Dashboard

</Update>

<Update label="2025-06-27" description="">

**Improvements:**
- **Rate Limiting:** Updated rate limiting for ADD API to 1000/min
- **Performance:** Improved Neo4j performance

</Update>

<Update label="2025-06-26" description="">

**New Features:**
- **Memory:** Edit Memory From Drawer functionality
- **API:** Added Topic Suggestions API Endpoint

</Update>

<Update label="2025-06-25" description="">

**New Features:**
- **Group Chat:** Group-Chat v2 with Actor-Aware Memories
- **Memory:** Editable Metadata in Memories
- **UI:** Memory Actions Badges

</Update>

<Update label="2025-06-19" description="">

**New Features:**
- **Rate Limiting:** Implemented comprehensive rate limiting system

**Improvements:**
- **Performance:** Added performance indexes for memory stats query

**Bug Fixes:**
- **Search:** Fixed search events not respecting top-k parameter

</Update>

<Update label="2025-06-18" description="">

**New Features:**
- **Memory Management:** Implemented OpenAI Batch API for Memory Cleaning with fallback
- **Playground:** Added Claude 4 support on Playground

**Improvements:**
- **Memory:** Added ability to update memory metadata

</Update>

<Update label="2025-06-17" description="">

**New Features:**
- **UI:** New Memories Page UI design

</Update>

<Update label="2025-06-16" description="">

**Improvements:**
- **Infrastructure:** Migrated to Application Load Balancer (ALB)

</Update>

<Update label="2025-06-13" description="">

**Improvements:**
- **Memory Management:** Enhanced Memory Management with Cosine Similarity Fallback

</Update>

<Update label="2025-06-11" description="">

**New Features:**
- **OMM:** Added OMM Script and UI functionality

**Improvements:**
- **API:** Added filters validation to semantic_search_v2 endpoint

</Update>

<Update label="2025-06-09" description="">

**New Features:**
- **Intercom:** Set Intercom events for ADD and SEARCH operations
- **OpenMemory:** Added Posthog integration and feedback functionality
- **MCP:** New JavaScript MCP Server with feedback support

**Improvements:**
- **Structured Data:** Enhanced structured data handling in memory management

</Update>

<Update label="2025-06-06" description="">

**New Features:**
- **OAuth:** Added Mem0 OAuth integration
- **OMM:** Added OMM-Mem0 sync for deleted memories

</Update>

<Update label="2025-06-05" description="">

**New Features:**
- **Filters:** Implemented Wildcard Filters and refactored filter logic in V2 Views

</Update>

<Update label="2025-06-02" description="">

**New Features:**
- **OpenMemory Cloud:** Added OpenMemory Cloud support
- **Structured Data:** Added 'structured_attributes' field to Memory model

</Update>

<Update label="2025-05-30" description="">

**New Features:**
- **Projects:** Added version and enable_graph to project views
- **OpenMemory:** Added Postgres support for OpenMemory

</Update>

<Update label="2025-05-19" description="">

**Bug Fixes:**
- **Core:** Fixed unicode error in user_id, agent_id, run_id and app_id

</Update>

</Tab>

<Tab title="Vercel AI SDK">

<Update label="2025-09-03" description="v2.0.2">
**Bug Fix:**
- **Vercel AI SDK:** Fixed streaming response in the AI SDK.
</Update>

<Update label="2025-08-05" description="v2.0.1">
**New Features:**
- **Vercel AI SDK:** Added a new param `host` to the config.
</Update>

<Update label="2025-08-05" description="v2.0.0">
**New Features:**
- **Vercel AI SDK:** Migration to AI SDK V5.
</Update>

<Update label="2025-06-15" description="v1.0.6">
**New Features:**
- **Vercel AI SDK:** Added param `filter_memories`.
</Update>

<Update label="2025-05-23" description="v1.0.5">
**New Features:**
- **Vercel AI SDK:** Added support for Google provider.
</Update>

<Update label="2025-05-10" description="v1.0.4">
**New Features:**
- **Vercel AI SDK:** Added support for new param `output_format`.
</Update>

<Update label="2025-05-08" description="v1.0.3">
**Improvements:**
- **Vercel AI SDK:** Added support for graceful failure in cases services are down.
</Update>

<Update label="2025-05-01" description="v1.0.1">
**New Features:**
- **Vercel AI SDK:** Added support for graph memories
</Update>

</Tab>

</Tabs>




================================================
FILE: docs/docs.json
================================================
{
  "$schema": "https://mintlify.com/docs.json",
  "name": "Mem0",
  "description": "Mem0 is a self-improving memory layer for LLM applications, enabling personalized AI experiences that save costs and delight users.",
  "colors": {
    "primary": "#6c60f0",
    "light": "#E6FFA2",
    "dark": "#a3df02"
  },
  "favicon": "/logo/favicon.png",
  "navigation": {
    "anchors": [
      {
        "anchor": "Documentation",
        "icon": "book-open",
        "tabs": [
          {
            "tab": "Documentation",
            "groups": [
              {
                "group": "Getting Started",
                "icon": "rocket",
                "pages": [
                  "introduction",
                  "quickstart",
                  "faqs"
                ]
              },
              {
                "group": "Core Concepts",
                "icon": "brain",
                "pages": [
                  "core-concepts/memory-types",
                  {
                    "group": "Memory Operations",
                    "icon": "gear",
                    "pages": [
                      "core-concepts/memory-operations/add",
                      "core-concepts/memory-operations/search",
                      "core-concepts/memory-operations/update",
                      "core-concepts/memory-operations/delete"
                    ]
                  }
                ]
              },
              {
                "group": "Platform",
                "icon": "globe",
                "pages": [
                  "platform/overview",
                  "platform/quickstart",
                  "platform/advanced-memory-operations",
                  {
                    "group": "Features",
                    "icon": "star",
                    "pages": [
                      "platform/features/platform-overview",
                      "platform/features/contextual-add",
                      "platform/features/async-client",
                      "platform/features/graph-memory",
                      "platform/features/advanced-retrieval",
                      "platform/features/criteria-retrieval",
                      "platform/features/multimodal-support",
                      "platform/features/selective-memory",
                      "platform/features/custom-categories",
                      "platform/features/custom-instructions",
                      "platform/features/direct-import",
                      "platform/features/memory-export",
                      "platform/features/timestamp",
                      "platform/features/expiration-date",
                      "platform/features/webhooks",
                      "platform/features/feedback-mechanism",
                      "platform/features/group-chat"
                    ]
                  }
                ]
              },
              {
                "group": "Open Source",
                "icon": "code-branch",
                "pages": [
                  "open-source/overview",
                  "open-source/python-quickstart",
                  "open-source/node-quickstart",
                  {
                    "group": "Features",
                    "icon": "star",
                    "pages": [
                      "open-source/features/overview",
                      "open-source/features/async-memory",
                      "open-source/features/openai_compatibility",
                      "open-source/features/custom-fact-extraction-prompt",
                      "open-source/features/custom-update-memory-prompt",
                      "open-source/features/multimodal-support",
                      "open-source/features/rest-api"
                    ]
                  },
                  {
                    "group": "Graph Memory",
                    "icon": "spider-web",
                    "pages": [
                      "open-source/graph_memory/overview",
                      "open-source/graph_memory/features"
                    ]
                  },
                  {
                    "group": "LLMs",
                    "icon": "brain",
                    "pages": [
                      "components/llms/overview",
                      "components/llms/config",
                      {
                        "group": "Supported LLMs",
                        "icon": "list",
                        "pages": [
                          "components/llms/models/openai",
                          "components/llms/models/anthropic",
                          "components/llms/models/azure_openai",
                          "components/llms/models/ollama",
                          "components/llms/models/together",
                          "components/llms/models/groq",
                          "components/llms/models/litellm",
                          "components/llms/models/mistral_AI",
                          "components/llms/models/google_AI",
                          "components/llms/models/aws_bedrock",
                          "components/llms/models/deepseek",
                          "components/llms/models/xAI",
                          "components/llms/models/sarvam",
                          "components/llms/models/lmstudio",
                          "components/llms/models/langchain",
                          "components/llms/models/vllm"
                        ]
                      }
                    ]
                  },
                  {
                    "group": "Vector Databases",
                    "icon": "database",
                    "pages": [
                      "components/vectordbs/overview",
                      "components/vectordbs/config",
                      {
                        "group": "Supported Vector Databases",
                        "icon": "server",
                        "pages": [
                          "components/vectordbs/dbs/qdrant",
                          "components/vectordbs/dbs/chroma",
                          "components/vectordbs/dbs/pgvector",
                          "components/vectordbs/dbs/milvus",
                          "components/vectordbs/dbs/pinecone",
                          "components/vectordbs/dbs/mongodb",
                          "components/vectordbs/dbs/azure",
                          "components/vectordbs/dbs/redis",
                          "components/vectordbs/dbs/elasticsearch",
                          "components/vectordbs/dbs/opensearch",
                          "components/vectordbs/dbs/supabase",
                          "components/vectordbs/dbs/upstash-vector",
                          "components/vectordbs/dbs/vectorize",
                          "components/vectordbs/dbs/vertex_ai",
                          "components/vectordbs/dbs/weaviate",
                          "components/vectordbs/dbs/faiss",
                          "components/vectordbs/dbs/langchain",
                          "components/vectordbs/dbs/baidu",
                          "components/vectordbs/dbs/s3_vectors",
                          "components/vectordbs/dbs/databricks"
                        ]
                      }
                    ]
                  },
                  {
                    "group": "Embedding Models",
                    "icon": "layer-group",
                    "pages": [
                      "components/embedders/overview",
                      "components/embedders/config",
                      {
                        "group": "Supported Embedding Models",
                        "icon": "list",
                        "pages": [
                          "components/embedders/models/openai",
                          "components/embedders/models/azure_openai",
                          "components/embedders/models/ollama",
                          "components/embedders/models/huggingface",
                          "components/embedders/models/vertexai",
                          "components/embedders/models/google_AI",
                          "components/embedders/models/lmstudio",
                          "components/embedders/models/together",
                          "components/embedders/models/langchain",
                          "components/embedders/models/aws_bedrock"
                        ]
                      }
                    ]
                  }
                ]
              },
              {
                "group": "Contribution",
                "icon": "handshake",
                "pages": [
                  "contributing/development",
                  "contributing/documentation"
                ]
              }
            ]
          },
          {
            "tab": "OpenMemory",
            "icon": "square-terminal",
            "pages": [
              "openmemory/overview",
              "openmemory/quickstart",
              "openmemory/integrations"
            ]
          },
          {
            "tab": "Examples",
            "groups": [
              {
                "group": "💡 Examples",
                "icon": "lightbulb",
                "pages": [
                  "examples",
                  "examples/aws_example",
                  "examples/mem0-demo",
                  "examples/ai_companion_js",
                  "examples/collaborative-task-agent",
                  "examples/llamaindex-multiagent-learning-system",
                  "examples/personalized-search-tavily-mem0",
                  "examples/eliza_os",
                  "examples/mem0-mastra",
                  "examples/mem0-with-ollama",
                  "examples/personal-ai-tutor",
                  "examples/customer-support-agent",
                  "examples/personal-travel-assistant",
                  "examples/llama-index-mem0",
                  "examples/chrome-extension",
                  "examples/memory-guided-content-writing",
                  "examples/multimodal-demo",
                  "examples/personalized-deep-research",
                  "examples/mem0-agentic-tool",
                  "examples/openai-inbuilt-tools",
                  "examples/mem0-openai-voice-demo",
                  "examples/mem0-google-adk-healthcare-assistant",
                  "examples/email_processing",
                  "examples/youtube-assistant"
                ]
              }
            ]
          },
          {
            "tab": "Integrations",
            "groups": [
              {
                "group": "Integrations",
                "icon": "plug",
                "pages": [
                  "integrations",
                  "integrations/langchain",
                  "integrations/langgraph",
                  "integrations/llama-index",
                  "integrations/agno",
                  "integrations/autogen",
                  "integrations/crewai",
                  "integrations/openai-agents-sdk",
                  "integrations/google-ai-adk",
                  "integrations/mastra",
                  "integrations/vercel-ai-sdk",
                  "integrations/livekit",
                  "integrations/pipecat",
                  "integrations/elevenlabs",
                  "integrations/aws-bedrock",
                  "integrations/flowise",
                  "integrations/langchain-tools",
                  "integrations/agentops",
                  "integrations/keywords",
                  "integrations/dify",
                  "integrations/raycast"
                ]
              }
            ]
          },
          {
            "tab": "API Reference",
            "icon": "square-terminal",
            "groups": [
              {
                "group": "API Reference",
                "icon": "terminal",
                "pages": [
                  "api-reference",
                  {
                    "group": "Memory APIs",
                    "icon": "microchip",
                    "pages": [
                      "api-reference/memory/add-memories",
                      "api-reference/memory/v2-search-memories",
                      "api-reference/memory/v1-search-memories",
                      "api-reference/memory/v2-get-memories",
                      "api-reference/memory/v1-get-memories",
                      "api-reference/memory/history-memory",
                      "api-reference/memory/get-memory",
                      "api-reference/memory/update-memory",
                      "api-reference/memory/batch-update",
                      "api-reference/memory/delete-memory",
                      "api-reference/memory/batch-delete",
                      "api-reference/memory/delete-memories",
                      "api-reference/memory/create-memory-export",
                      "api-reference/memory/get-memory-export",
                      "api-reference/memory/feedback"
                    ]
                  },
                  {
                    "group": "Entities APIs",
                    "icon": "users",
                    "pages": [
                      "api-reference/entities/get-users",
                      "api-reference/entities/delete-user"
                    ]
                  },
                  {
                    "group": "Organizations APIs",
                    "icon": "building",
                    "pages": [
                      "api-reference/organization/create-org",
                      "api-reference/organization/get-orgs",
                      "api-reference/organization/get-org",
                      "api-reference/organization/get-org-members",
                      "api-reference/organization/add-org-member",
                      "api-reference/organization/delete-org"
                    ]
                  },
                  {
                    "group": "Project APIs",
                    "icon": "folder",
                    "pages": [
                      "api-reference/project/create-project",
                      "api-reference/project/get-projects",
                      "api-reference/project/get-project",
                      "api-reference/project/get-project-members",
                      "api-reference/project/add-project-member",
                      "api-reference/project/delete-project"
                    ]
                  },
                  {
                    "group": "Webhook APIs",
                    "icon": "webhook",
                    "pages": [
                      "api-reference/webhook/create-webhook",
                      "api-reference/webhook/get-webhook",
                      "api-reference/webhook/update-webhook",
                      "api-reference/webhook/delete-webhook"
                    ]
                  }
                ]
              }
            ]
          },
          {
            "tab": "Changelog",
            "icon": "clock",
            "groups": [
              {
                "group": "Product Updates",
                "icon": "rocket",
                "pages": [
                  "changelog"
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "logo": {
    "light": "/logo/light.svg",
    "dark": "/logo/dark.svg",
    "href": "https://github.com/mem0ai/mem0"
  },
  "background": {
    "color": {
      "light": "#fff",
      "dark": "#09090b"
    }
  },
  "navbar": {
    "primary": {
      "type": "button",
      "label": "Your Dashboard",
      "href": "https://app.mem0.ai"
    }
  },
  "footer": {
    "socials": {
      "discord": "https://mem0.dev/DiD",
      "x": "https://x.com/mem0ai",
      "github": "https://github.com/mem0ai",
      "linkedin": "https://www.linkedin.com/company/mem0/"
    }
  },
  "integrations": {
    "posthog": {
      "apiKey": "phc_hgJkUVJFYtmaJqrvf6CYN67TIQ8yhXAkWzUn9AMU4yX",
      "apiHost": "https://mango.mem0.ai"
    },
    "intercom": {
      "appId": "jjv2r0tt"
    }
  }
}


================================================
FILE: docs/examples.mdx
================================================
---
title: Overview
description: How to use mem0 in your existing applications?
---

With Mem0, you can create stateful LLM-based applications such as chatbots, virtual assistants, or AI agents. Mem0 enhances your applications by providing a memory layer that makes responses:

- More personalized
- More reliable
- Cost-effective by reducing the number of LLM interactions
- More engaging
- Enables long-term memory

Here are some examples of how Mem0 can be integrated into various applications:

## Examples

Explore how **Mem0** can power real-world applications and bring personalized, intelligent experiences to life:

<CardGroup cols={2}>
  <Card title="AI Companion in Node.js" icon="node" href="/examples/ai_companion_js">
    Build a personalized AI Companion in **Node.js** that remembers conversations and adapts over time using Mem0.
  </Card>

  <Card title="Mem0 with Ollama" icon="server" href="/examples/mem0-with-ollama">
    Run **Mem0 locally** with **Ollama** to create private, stateful AI experiences without relying on cloud APIs.
  </Card>

  <Card title="Personal AI Tutor" icon="graduation-cap" href="/examples/personal-ai-tutor">
    Create an **AI Tutor** that adapts to student progress, learning style, and history — for a truly customized learning experience.
  </Card>

  <Card title="Personal Travel Assistant" icon="plane" href="/examples/personal-travel-assistant">
    Develop a **Personal Travel Assistant** that remembers your preferences, past trips, and helps plan future adventures.
  </Card>

  <Card title="Customer Support Agent" icon="headset" href="/examples/customer-support-agent">
    Build a **Customer Support AI** that recalls user preferences, past chats, and provides context-aware, efficient help.
  </Card>

  <Card title="LlamaIndex + Mem0" icon="book-open" href="/examples/llama-index-mem0">
    Combine **LlamaIndex** and Mem0 to create a powerful **ReAct Agent** with persistent memory for smarter interactions.
  </Card>

  <Card title="LlamaIndex + Mem0 Learning System" icon="book-open" href="/examples/llama-index-mem0">
    Multi-agent learning system powered by memory.
  </Card>

  <Card title="Chrome Extension" icon="puzzle-piece" href="/examples/chrome-extension">
    Add **long-term memory** to ChatGPT, Claude, or Perplexity via the **Mem0 Chrome Extension** — personalize your AI chats anywhere.
  </Card>

  <Card title="YouTube Assistant" icon="puzzle-piece" href="/examples/youtube-assistant">
    Integrate **Mem0** into **YouTube's** native UI, providing personalized responses with video context. 
  </Card> 

  <Card title="Document Writing Assistant" icon="pen" href="/examples/document-writing">
    Create a **Writing Assistant** that understands and adapts to your unique style, improving consistency and productivity.
  </Card>

  <Card title="Multimodal AI Demo" icon="image" href="/examples/multimodal-demo">
    Supercharge AI with **Mem0's multimodal memory** — blend text, images, and more for richer, context-aware interactions.
  </Card>

  <Card title="Personalized Research Agent" icon="magnifying-glass" href="/examples/personalized-deep-research">
    Build a **Deep Research AI** that remembers your research goals and compiles insights from vast information sources.
  </Card>

  <Card title="Mem0 as an Agentic Tool" icon="robot" href="/examples/mem0-agentic-tool">
    Integrate Mem0's memory capabilities with OpenAI's Agents SDK to create AI agents with persistent memory.
  </Card>

  <Card title="OpenAI Inbuilt Tools" icon="robot" href="/examples/openai-inbuilt-tools">
    Use Mem0's memory capabilities with OpenAI's Inbuilt Tools to create AI agents with persistent memory.
  </Card>

  <Card title="Mem0 OpenAI Voice Demo" icon="microphone" href="/examples/mem0-openai-voice-demo">
    Use Mem0's memory capabilities with OpenAI's Inbuilt Tools to create AI agents with persistent memory.
  </Card>

  <Card title="Healthcare Assistant Google ADK" icon="microphone" href="/examples/mem0-google-adk-healthcare-assistant">
    Build a personalized healthcare assistant with persistent memory using Google's ADK and Mem0.
  </Card>

  <Card title="Email Processing" icon="envelope" href="/examples/email_processing">
    Use Mem0's memory capabilities to process emails and create AI agents with persistent memory.
  </Card>
</CardGroup>



================================================
FILE: docs/faqs.mdx
================================================
---
title: FAQs
icon: "question"
iconType: "solid"
---

<AccordionGroup>
    <Accordion title="How does Mem0 work?">
        Mem0 utilizes a sophisticated hybrid database system to efficiently manage and retrieve memories for AI agents and assistants. Each memory is linked to a unique identifier, such as a user ID or agent ID, enabling Mem0 to organize and access memories tailored to specific individuals or contexts.

        When a message is added to Mem0 via the `add` method, the system extracts pertinent facts and preferences, distributing them across various data stores: a vector database and a graph database. This hybrid strategy ensures that diverse types of information are stored optimally, facilitating swift and effective searches.

        When an AI agent or LLM needs to access memories, it employs the `search` method. Mem0 conducts a comprehensive search across these data stores, retrieving relevant information from each.

        The retrieved memories can be seamlessly integrated into the system prompt as required, enhancing the personalization and relevance of responses.
  </Accordion>

    <Accordion title="What are the key features of Mem0?">
        - **User, Session, and AI Agent Memory**: Retains information across sessions and interactions for users and AI agents, ensuring continuity and context.
        - **Adaptive Personalization**: Continuously updates memories based on user interactions and feedback.
        - **Developer-Friendly API**: Offers a straightforward API for seamless integration into various applications.
        - **Platform Consistency**: Ensures consistent behavior and data across different platforms and devices.
        - **Managed Service**: Provides a hosted solution for easy deployment and maintenance.
        - **Save Costs**: Saves costs by adding relevant memories instead of complete transcripts to context window
    </Accordion>

    <Accordion title="How Mem0 is different from traditional RAG?">
        Mem0's memory implementation for Large Language Models (LLMs) offers several advantages over Retrieval-Augmented Generation (RAG):

        - **Entity Relationships**: Mem0 can understand and relate entities across different interactions, unlike RAG which retrieves information from static documents. This leads to a deeper understanding of context and relationships.

        - **Contextual Continuity**: Mem0 retains information across sessions, maintaining continuity in conversations and interactions, which is essential for long-term engagement applications like virtual companions or personalized learning assistants.

        - **Adaptive Learning**: Mem0 improves its personalization based on user interactions and feedback, making the memory more accurate and tailored to individual users over time.

        - **Dynamic Updates**: Mem0 can dynamically update its memory with new information and interactions, unlike RAG which relies on static data. This allows for real-time adjustments and improvements, enhancing the user experience.

        These advanced memory capabilities make Mem0 a powerful tool for developers aiming to create personalized and context-aware AI applications.
    </Accordion>


    <Accordion title="What are the common use-cases of Mem0?">
        - **Personalized Learning Assistants**: Long-term memory allows learning assistants to remember user preferences, strengths and weaknesses, and progress, providing a more tailored and effective learning experience.

        - **Customer Support AI Agents**: By retaining information from previous interactions, customer support bots can offer more accurate and context-aware assistance, improving customer satisfaction and reducing resolution times.

        - **Healthcare Assistants**: Long-term memory enables healthcare assistants to keep track of patient history, medication schedules, and treatment plans, ensuring personalized and consistent care.

        - **Virtual Companions**: Virtual companions can use long-term memory to build deeper relationships with users by remembering personal details, preferences, and past conversations, making interactions more delightful.

        - **Productivity Tools**: Long-term memory helps productivity tools remember user habits, frequently used documents, and task history, streamlining workflows and enhancing efficiency.

        - **Gaming AI**: In gaming, AI with long-term memory can create more immersive experiences by remembering player choices, strategies, and progress, adapting the game environment accordingly.

    </Accordion>

    <Accordion title="Why aren't my memories being created?">
        Mem0 uses a sophisticated classification system to determine which parts of text should be extracted as memories. Not all text content will generate memories, as the system is designed to identify specific types of memorable information.
        There are several scenarios where mem0 may return an empty list of memories:

        - When users input definitional questions (e.g., "What is backpropagation?")
        - For general concept explanations that don't contain personal or experiential information
        - Technical definitions and theoretical explanations
        - General knowledge statements without personal context
        - Abstract or theoretical content

        Example Scenarios

        ```
        Input: "What is machine learning?"
        No memories extracted - Content is definitional and does not meet memory classification criteria.

        Input: "Yesterday I learned about machine learning in class"
        Memory extracted - Contains personal experience and temporal context.
        ```

        Best Practices

        To ensure successful memory extraction:
        - Include temporal markers (when events occurred)
        - Add personal context or experiences
        - Frame information in terms of real-world applications or experiences
        - Include specific examples or cases rather than general definitions
    </Accordion>

    <Accordion title="How do I configure Mem0 for AWS Lambda?">
        When deploying Mem0 on AWS Lambda, you'll need to modify the storage directory configuration due to Lambda's file system restrictions. By default, Lambda only allows writing to the `/tmp` directory.

        To configure Mem0 for AWS Lambda, set the `MEM0_DIR` environment variable to point to a writable directory in `/tmp`:

        ```bash
        MEM0_DIR=/tmp/.mem0
        ```

        If you're not using environment variables, you'll need to modify the storage path in your code:

        ```python
        # Change from
        home_dir = os.path.expanduser("~")
        mem0_dir = os.environ.get("MEM0_DIR") or os.path.join(home_dir, ".mem0")

        # To
        mem0_dir = os.environ.get("MEM0_DIR", "/tmp/.mem0")
        ```

        Note that the `/tmp` directory in Lambda has a size limit of 512MB and its contents are not persistent between function invocations.
    </Accordion>

    <Accordion title="How can I use metadata with Mem0?">
        Metadata is the recommended approach for incorporating additional information with Mem0. You can store any type of structured data as metadata during the `add` method, such as location, timestamp, weather conditions, user state, or application context. This enriches your memories with valuable contextual information that can be used for more precise retrieval and filtering.

        During retrieval, you have two main approaches for using metadata:

        1. **Pre-filtering**: Include metadata parameters in your initial search query to narrow down the memory pool
        2. **Post-processing**: Retrieve a broader set of memories based on query, then apply metadata filters to refine the results

        Examples of useful metadata you might store:

        - **Contextual information**: Location, time, device type, application state
        - **User attributes**: Preferences, skill levels, demographic information
        - **Interaction details**: Conversation topics, sentiment, urgency levels
        - **Custom tags**: Any domain-specific categorization relevant to your application

        This flexibility allows you to create highly contextually aware AI applications that can adapt to specific user needs and situations. Metadata provides an additional dimension for memory retrieval, enabling more precise and relevant responses.
    </Accordion>

    <Accordion title="How do I disable telemetry in Mem0?">
        To disable telemetry in Mem0, you can set the `MEM0_TELEMETRY` environment variable to `False`:

        ```bash
        MEM0_TELEMETRY=False
        ```

        You can also disable telemetry programmatically in your code:

        ```python
        import os
        os.environ["MEM0_TELEMETRY"] = "False"
        ```

        Setting this environment variable will prevent Mem0 from collecting and sending any usage data, ensuring complete privacy for your application.
    </Accordion>

</AccordionGroup>






================================================
FILE: docs/integrations.mdx
================================================
---
title: Overview
description: How to integrate Mem0 into other frameworks
---

Mem0 seamlessly integrates with popular AI frameworks and tools to enhance your LLM-based applications with persistent memory capabilities. By integrating Mem0, your applications benefit from:

- Enhanced context management across multiple frameworks
- Consistent memory persistence across different LLM interactions
- Optimized token usage through efficient memory retrieval
- Framework-agnostic memory layer
- Simple integration with existing AI tools and frameworks

Here are the available integrations for Mem0:

## Integrations

<CardGroup cols={2}>
  <Card
    title="AgentOps"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="25"
        height="26"
        viewBox="0 0 30 36"
        fill="none"
      >
      <path d="M10.4659 6.47277C10.45 6.37428 10.4381 6.27986 10.4303 6.18101L10.4285 6.16388C10.4212 6.09482 10.414 6.02566 10.4106 5.95626L1.18538 21.8752C0.505422 23.0493 0.323356 24.4208 0.675227 25.7289C0.849119 26.3869 1.14971 26.9859 1.55323 27.5098C1.95675 28.0338 2.46282 28.4751 3.05175 28.8143C3.83464 29.2675 4.70856 29.5 5.59028 29.5C6.03318 29.5 6.4798 29.4408 6.91899 29.3226C8.23581 28.972 9.3349 28.1326 10.0152 26.9545L15.9268 16.749V16.7449L16.5001 15.7637L17.6431 13.7936L16.5001 11.8234L15.9309 10.8381L15.9268 10.8341L13.7836 7.13406C13.6651 6.933 13.5741 6.72418 13.5109 6.51165C13.2817 5.80223 13.3292 5.04172 13.6097 4.37599L13.8115 4.02535C14.3532 3.09155 15.31 2.53987 16.3184 2.47692C16.3738 2.46915 16.4251 2.46915 16.4804 2.46915C16.5421 2.46915 16.6038 2.47257 16.6654 2.47599L16.6822 2.47692C17.6906 2.53987 18.6474 3.09155 19.1892 4.02535L21.2216 7.52838L21.8146 8.55289L21.8421 8.60399L30.1024 22.8601C30.5174 23.5814 30.6281 24.4167 30.4148 25.2205C30.1975 26.0244 29.6832 26.6942 28.9598 27.1081C28.2364 27.5258 27.3977 27.6361 26.5911 27.4195C25.7844 27.2066 25.1123 26.6905 24.6968 25.9696L18.2119 14.7788L17.069 16.7449L22.9847 26.9545C23.6646 28.1326 24.7641 28.972 26.0809 29.3226C26.5197 29.4408 26.9626 29.5 27.4096 29.5C28.2914 29.5 29.1612 29.2675 29.9482 28.8143C31.1264 28.1367 31.9728 27.0411 32.3247 25.7289C32.6766 24.4208 32.4949 23.0493 31.8145 21.8752L21.1261 3.43034C20.7029 2.51617 20.0033 1.72011 19.0621 1.18027C18.5281 0.877027 17.9708 0.675975 17.3975 0.581189C17.3027 0.565268 17.2076 0.549717 17.1129 0.537868C17.0099 0.52602 16.9074 0.518244 16.8045 0.510469C16.6027 0.498621 16.3972 0.494548 16.1914 0.510469C16.0885 0.518244 15.9859 0.52639 15.883 0.537868C15.795 0.54887 15.7067 0.563384 15.6187 0.577852L15.5984 0.581189C15.0291 0.675605 14.4673 0.876657 13.9375 1.18027C12.9885 1.72789 12.2766 2.53579 11.8537 3.46181C11.7742 3.63473 11.707 3.81282 11.6471 3.99314C11.6361 4.02668 11.6269 4.06051 11.6177 4.09435C11.612 4.11503 11.6064 4.13579 11.6003 4.15642C11.5624 4.28601 11.5275 4.41634 11.4996 4.54853C11.4885 4.60231 11.4794 4.65668 11.4703 4.71111L11.4666 4.73329C11.4443 4.86399 11.4264 4.99543 11.4145 5.12762C11.4093 5.18686 11.4045 5.24573 11.4012 5.30534C11.3934 5.44567 11.3923 5.58637 11.3963 5.72744C11.3969 5.74403 11.3962 5.76062 11.3956 5.7772C11.3949 5.79616 11.3942 5.81512 11.3952 5.83407C11.3952 5.86184 11.3952 5.88924 11.3993 5.92071C11.3998 5.9291 11.4006 5.93736 11.4014 5.94564C11.402 5.95125 11.4026 5.95687 11.403 5.96255C11.4045 5.98181 11.4064 6.00106 11.4082 6.02031C11.4097 6.03577 11.4109 6.05122 11.4122 6.06674C11.4142 6.09134 11.4163 6.11621 11.419 6.14139L11.4428 6.32282C11.4506 6.38983 11.4625 6.46092 11.4744 6.52757C11.5063 6.68863 11.5468 6.84896 11.5936 7.0078C11.5944 7.0102 11.5949 7.0127 11.5955 7.0152C11.5958 7.01662 11.5961 7.01804 11.5965 7.01944C11.5967 7.02051 11.597 7.02157 11.5974 7.02261C11.6483 7.19293 11.7081 7.36177 11.7787 7.52838C11.8619 7.72943 11.9607 7.92641 12.0715 8.11932L12.3245 8.5566V8.56067L12.4984 8.85614L12.7199 9.24232H12.7239L12.728 9.25417L14.7802 12.7927V12.7968L14.7883 12.805V12.809L15.3576 13.7943L14.7883 14.7796L8.30344 25.9703C7.88431 26.6912 7.21216 27.2077 6.40921 27.4202C6.14019 27.4913 5.86338 27.5306 5.59474 27.5306C5.053 27.5306 4.51906 27.3888 4.04085 27.1089C3.31705 26.6953 2.79909 26.0251 2.58581 25.2213C2.36845 24.4174 2.47917 23.5821 2.89829 22.8609L11.1585 8.60473L11.186 8.56141V8.55734C11.1266 8.45478 11.0753 8.35629 11.024 8.25409C11.0105 8.22496 10.9969 8.19611 10.9834 8.16739C10.9458 8.08735 10.9086 8.00836 10.8739 7.92715C10.8718 7.92504 10.8708 7.92194 10.8698 7.91887C10.8688 7.91602 10.8679 7.91319 10.8661 7.91123V7.90346C10.8423 7.8483 10.8186 7.79311 10.7989 7.73795C10.7476 7.60799 10.7041 7.47803 10.6644 7.3477C10.6012 7.15479 10.5536 6.96152 10.518 6.76861C10.4942 6.67012 10.4786 6.5757 10.4667 6.47684C10.4667 6.47684 10.47 6.47684 10.4659 6.47277Z" fill="currentColor"></path>
      </svg>
    }
    href="/integrations/agentops"
  >
    Monitor and analyze Mem0 operations with comprehensive AI agent analytics and LLM observability.
  </Card>
  <Card
    title="LangChain"
    icon={
      <svg
        role="img"
        viewBox="0 0 24 24"
        xmlns="http://www.w3.org/2000/svg"
        width="32"
        height="32"
      >
        <title>LangChain</title>
        <path
          d="M6.0988 5.9175C2.7359 5.9175 0 8.6462 0 12s2.736 6.0825 6.0988 6.0825h11.8024C21.2641 18.0825 24 15.3538 24 12s-2.736 -6.0825 -6.0988 -6.0825ZM5.9774 7.851c0.493 0.0124 1.02 0.2496 1.273 0.6228 0.3673 0.4592 0.4778 1.0668 0.8944 1.4932 0.5604 0.6118 1.199 1.1505 1.7161 1.802 0.4892 0.5954 0.8386 1.2937 1.1436 1.9975 0.1244 0.2335 0.1257 0.5202 0.31 0.7197 0.0908 0.1204 0.5346 0.4483 0.4383 0.5645 0.0555 0.1204 0.4702 0.286 0.3263 0.4027 -0.1944 0.04 -0.4129 0.0476 -0.5616 -0.1074 -0.0549 0.126 -0.183 0.0596 -0.2819 0.0432a4 4 0 0 0 -0.025 0.0736c-0.3288 0.0219 -0.5754 -0.3126 -0.732 -0.565 -0.3111 -0.168 -0.6642 -0.2702 -0.982 -0.446 -0.0182 0.2895 0.0452 0.6485 -0.231 0.8353 -0.014 0.5565 0.8436 0.0656 0.9222 0.4804 -0.061 0.0067 -0.1286 -0.0095 -0.1774 0.0373 -0.2239 0.2172 -0.4805 -0.1645 -0.7385 -0.007 -0.3464 0.174 -0.3808 0.3161 -0.8096 0.352 -0.0237 -0.0359 -0.0143 -0.0592 0.0059 -0.0811 0.1207 -0.1399 0.1295 -0.3046 0.3356 -0.3643 -0.2122 -0.0334 -0.3899 0.0833 -0.5686 0.1757 -0.2323 0.095 -0.2304 -0.2141 -0.5878 0.0164 -0.0396 -0.0322 -0.0208 -0.0615 0.0018 -0.0864 0.0908 -0.1107 0.2102 -0.127 0.345 -0.1208 -0.663 -0.3686 -0.9751 0.4507 -1.2813 0.0432 -0.092 0.0243 -0.1265 0.1068 -0.1845 0.1652 -0.05 -0.0548 -0.0123 -0.1212 -0.0099 -0.1857 -0.0598 -0.028 -0.1356 -0.041 -0.1179 -0.1366 -0.1171 -0.0395 -0.1988 0.0295 -0.286 0.0952 -0.0787 -0.0608 0.0532 -0.1492 0.0776 -0.2125 0.0702 -0.1216 0.23 -0.025 0.3111 -0.1126 0.2306 -0.1308 0.552 0.0814 0.8155 0.0455 0.203 0.0255 0.4544 -0.1825 0.3526 -0.39 -0.2171 -0.2767 -0.179 -0.6386 -0.1839 -0.9695 -0.0268 -0.1929 -0.491 -0.4382 -0.6252 -0.6462 -0.1659 -0.1873 -0.295 -0.4047 -0.4243 -0.6182 -0.4666 -0.9008 -0.3198 -2.0584 -0.9077 -2.8947 -0.266 0.1466 -0.6125 0.0774 -0.8418 -0.119 -0.1238 0.1125 -0.1292 0.2598 -0.139 0.4161 -0.297 -0.2962 -0.2593 -0.8559 -0.022 -1.1855 0.0969 -0.1302 0.2127 -0.2373 0.342 -0.3316 0.0292 -0.0213 0.0391 -0.0419 0.0385 -0.0747 0.1174 -0.5267 0.5764 -0.7391 1.0694 -0.7267m12.4071 0.46c0.5575 0 1.0806 0.2159 1.474 0.6082s0.61 0.9145 0.61 1.4704c0 0.556 -0.2167 1.078 -0.61 1.4698v0.0006l-0.902 0.8995a2.08 2.08 0 0 1 -0.8597 0.5166l-0.0164 0.0047 -0.0058 0.0164a2.05 2.05 0 0 1 -0.474 0.7308l-0.9018 0.8995c-0.3934 0.3924 -0.917 0.6083 -1.4745 0.6083s-1.0806 -0.216 -1.474 -0.6083c-0.813 -0.8107 -0.813 -2.1294 0 -2.9402l0.9019 -0.8995a2.056 2.056 0 0 1 0.858 -0.5143l0.017 -0.0053 0.0058 -0.0158a2.07 2.07 0 0 1 0.4752 -0.7337l0.9018 -0.8995c0.3934 -0.3924 0.9171 -0.6083 1.4745 -0.6083zm0 0.8965a1.18 1.18 0 0 0 -0.8388 0.3462l-0.9018 0.8995a1.181 1.181 0 0 0 -0.3427 0.9252l0.0053 0.0572c0.0323 0.2652 0.149 0.5044 0.3374 0.6917 0.13 0.1296 0.2733 0.2114 0.4471 0.2686a0.9 0.9 0 0 1 0.014 0.1582 0.884 0.884 0 0 1 -0.2609 0.6304l-0.0554 0.0554c-0.3013 -0.1028 -0.5525 -0.253 -0.7794 -0.4792a2.06 2.06 0 0 1 -0.5761 -1.0968l-0.0099 -0.0578 -0.0461 0.0368a1.1 1.1 0 0 0 -0.0876 0.0794l-0.9024 0.8995c-0.4623 0.461 -0.4623 1.212 0 1.673 0.2311 0.2305 0.535 0.346 0.8394 0.3461 0.3043 0 0.6077 -0.1156 0.8388 -0.3462l0.9019 -0.8995c0.4623 -0.461 0.4623 -1.2113 0 -1.673a1.17 1.17 0 0 0 -0.4367 -0.2749 1 1 0 0 1 -0.014 -0.1611c0 -0.2591 0.1023 -0.505 0.2901 -0.6923 0.3019 0.1028 0.57 0.2694 0.7962 0.495 0.3007 0.2999 0.4994 0.679 0.5756 1.0968l0.0105 0.0578 0.0455 -0.0373a1.1 1.1 0 0 0 0.0887 -0.0794l0.902 -0.8996c0.4622 -0.461 0.4628 -1.2124 0 -1.6735a1.18 1.18 0 0 0 -0.8395 -0.3462Zm-9.973 5.1567 -0.0006 0.0006c-0.0793 0.3078 -0.1048 0.8318 -0.506 0.847 -0.033 0.1776 0.1228 0.2445 0.2655 0.1874 0.141 -0.0645 0.2081 0.0508 0.2557 0.1657 0.2177 0.0317 0.5394 -0.0725 0.5516 -0.3298 -0.325 -0.1867 -0.4253 -0.5418 -0.5662 -0.8709"
          fill="currentColor"
        />
      </svg>
    }
    href="/integrations/langchain"
  >
    Integrate Mem0 with LangChain to build powerful agents with memory
    capabilities.
  </Card>
  <Card
    title="LlamaIndex"
    icon={
      <svg
        width="24"
        height="24"
        viewBox="0 0 80 80"
        xmlns="http://www.w3.org/2000/svg"
      >
        <path
          d="M0 16C0 7.16344 7.16925 0 16.013 0H64.0518C72.8955 0 80.0648 7.16344 80.0648 16V64C80.0648 72.8366 72.8955 80 64.0518 80H16.013C7.16924 80 0 72.8366 0 64V16Z"
          fill="currentColor"
        />
        <path
          d="M50.3091 52.6201C45.1552 54.8952 39.5718 53.963 37.4243 53.2126C37.4243 53.726 37.4009 55.3218 37.3072 57.597C37.2135 59.8721 36.4873 61.3099 36.1359 61.7444C36.1749 63.1664 36.2062 66.271 36.0188 67.3138C35.8313 68.3566 35.1598 69.2493 34.8474 69.5652H31.6848C31.9659 68.1433 33.0513 67.2348 33.5589 66.9583C33.84 64.0195 33.2856 61.4679 32.9733 60.5594C32.6609 61.6654 31.8956 64.2328 31.3334 65.6548C30.7711 67.0768 29.9278 68.3803 29.5763 68.8543H27.2337C27.1165 67.4323 27.8974 66.9583 28.405 66.9583C28.6393 66.5238 29.2015 65.1571 29.5763 63.1664C29.9512 61.1756 29.4202 57.439 29.1078 55.8195V50.7241C25.3595 48.7096 23.9539 46.6952 23.0168 44.4437C22.2672 42.6425 22.4702 39.9013 22.6654 38.7558C22.4311 38.3213 21.7481 37.217 21.4941 35.6749C21.1427 33.5419 21.3379 32.0014 21.4941 31.1719C21.2598 30.9349 20.7913 29.7263 20.7913 26.7875C20.7913 23.8488 21.6502 22.3241 22.0797 21.9291V20.6256C20.4398 20.5071 18.7999 19.7961 17.8629 18.8482C16.9258 17.9002 17.6286 16.4782 18.2143 16.0042C18.7999 15.5302 19.3856 15.8857 20.2056 15.6487C21.0255 15.4117 21.7283 15.1747 22.0797 14.4637C22.3608 13.895 21.8064 11.5408 21.494 10.4348C22.8997 10.6244 23.7977 11.8568 24.071 12.4493V10.4348C25.828 11.2643 28.9907 13.2788 30.0449 17.6632C30.8882 21.1707 31.4895 28.5255 31.6847 31.7645C36.1749 31.804 41.8755 31.1211 47.0294 32.2384C51.7148 33.2542 53.8232 35.3194 56.283 35.3194C58.7428 35.3194 60.1484 33.8974 61.9055 35.0824C63.6625 36.2674 64.5996 39.5853 64.3653 42.0738C64.1779 44.0645 62.6473 44.7202 61.9055 44.7992C60.9684 47.9276 61.9055 50.9216 62.4911 52.0276V56.5305C62.7645 56.9255 63.3111 58.1421 63.3111 59.8484C63.3111 61.5548 62.7645 62.6924 62.4911 63.0479C62.9597 65.7022 62.2959 68.4198 61.9055 69.4468H58.7428C59.1177 68.4988 59.758 68.2618 60.0313 68.2618C60.5936 65.3231 60.1875 62.6134 59.9142 61.6259C58.1337 60.5831 56.9858 58.7425 56.6344 57.9525C56.6735 58.624 56.5641 60.4883 55.8145 62.5739C55.0648 64.6595 53.9403 65.8918 53.4718 66.2473V68.7358H50.3091C50.3091 67.219 51.1681 66.9188 51.5976 66.9583C52.1443 65.9708 53.4718 64.4699 53.4718 61.5074C53.4718 59.0077 51.7148 57.834 50.4263 55.5825C49.8141 54.5128 50.1139 53.1731 50.3091 52.6201Z"
          fill="url(#paint0_linear_3021_4156)"
        />
        <defs>
          <linearGradient
            id="paint0_linear_3021_4156"
            x1="21.1546"
            y1="15.4117"
            x2="71.8865"
            y2="57.9279"
            gradientUnits="userSpaceOnUse"
          >
            <stop offset="0.0619804" stop-color="#F6DCD9" />
            <stop offset="0.325677" stop-color="#FFA5EA" />
            <stop offset="0.589257" stop-color="#45DFF8" />
            <stop offset="1" stop-color="#BC8DEB" />
          </linearGradient>
        </defs>
      </svg>
    }
    href="/integrations/llama-index"
  >
    Build RAG applications with LlamaIndex and Mem0.
  </Card>
  <Card
    title="AutoGen"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 96 85"
        fill="none"
      >
        <rect width="96" height="85" rx="6" fill="#2D2D2F" />
        <path
          d="M32.6484 28.7109L23.3672 57H15.8906L28.5703 22.875H33.3281L32.6484 28.7109ZM40.3594 57L31.0547 28.7109L30.3047 22.875H35.1094L47.8594 57H40.3594ZM39.9375 44.2969V49.8047H21.9141V44.2969H39.9375ZM77.6484 39.1641V52.6875C77.1172 53.3281 76.2969 54.0234 75.1875 54.7734C74.0781 55.5078 72.6484 56.1406 70.8984 56.6719C69.1484 57.2031 67.0312 57.4688 64.5469 57.4688C62.3438 57.4688 60.3359 57.1094 58.5234 56.3906C56.7109 55.6562 55.1484 54.5859 53.8359 53.1797C52.5391 51.7734 51.5391 50.0547 50.8359 48.0234C50.1328 45.9766 49.7812 43.6406 49.7812 41.0156V38.8828C49.7812 36.2578 50.1172 33.9219 50.7891 31.875C51.4766 29.8281 52.4531 28.1016 53.7188 26.6953C54.9844 25.2891 56.4922 24.2188 58.2422 23.4844C59.9922 22.75 61.9375 22.3828 64.0781 22.3828C67.0469 22.3828 69.4844 22.8672 71.3906 23.8359C73.2969 24.7891 74.75 26.1172 75.75 27.8203C76.7656 29.5078 77.3906 31.4453 77.625 33.6328H70.8047C70.6328 32.4766 70.3047 31.4688 69.8203 30.6094C69.3359 29.75 68.6406 29.0781 67.7344 28.5938C66.8438 28.1094 65.6875 27.8672 64.2656 27.8672C63.0938 27.8672 62.0469 28.1094 61.125 28.5938C60.2188 29.0625 59.4531 29.7578 58.8281 30.6797C58.2031 31.6016 57.7266 32.7422 57.3984 34.1016C57.0703 35.4609 56.9062 37.0391 56.9062 38.8359V41.0156C56.9062 42.7969 57.0781 44.375 57.4219 45.75C57.7656 47.1094 58.2734 48.2578 58.9453 49.1953C59.6328 50.1172 60.4766 50.8125 61.4766 51.2812C62.4766 51.75 63.6406 51.9844 64.9688 51.9844C66.0781 51.9844 67 51.8906 67.7344 51.7031C68.4844 51.5156 69.0859 51.2891 69.5391 51.0234C70.0078 50.7422 70.3672 50.4766 70.6172 50.2266V44.1797H64.1953V39.1641H77.6484Z"
          fill="white"
        />
      </svg>
    }
    href="/integrations/autogen"
  >
    Build multi-agent systems with persistent memory capabilities.
  </Card>
  <Card
    title="CrewAI"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 48 48"
        preserveAspectRatio="xMidYMid meet"
      >
        <g
          transform="translate(0.000000,48.000000) scale(0.100000,-0.100000)"
          fill="currentColor"
          stroke="none"
        >
          <path d="M252 469 c-103 -22 -213 -172 -214 -294 -1 -107 60 -168 168 -167 130 1 276 133 234 211 -13 25 -27 26 -52 4 -31 -27 -32 -6 -4 56 34 77 33 103 -6 146 -38 40 -78 55 -126 44z m103 -40 c44 -39 46 -82 9 -163 -27 -60 -42 -68 -74 -36 -24 24 -26 67 -5 117 22 51 19 60 -11 32 -72 -65 -125 -189 -105 -242 9 -23 16 -27 53 -27 54 0 122 33 154 76 34 44 54 44 54 1 0 -75 -125 -167 -225 -167 -121 0 -181 92 -145 222 17 58 86 153 137 187 63 42 110 42 158 0z" />
        </g>
      </svg>
    }
    href="/integrations/crewai"
  >
    Develop collaborative AI agents with shared memory using CrewAI and Mem0.
  </Card>
  <Card
    title="LangGraph"
    icon={
      <svg
        width="32"
        height="32"
        viewBox="0 0 63 33"
        xmlns="http://www.w3.org/2000/svg"
      >
        <path
          fill-rule="evenodd"
          clip-rule="evenodd"
          d="M16.0556 0.580566H46.6516C55.3698 0.580566 62.4621 7.69777 62.4621 16.4459C62.4621 25.194 55.3698 32.3112 46.6516 32.3112H16.0556C7.16924 32.3112 0.245117 25.194 0.245117 16.4459C0.245117 7.69777 7.16924 0.580566 16.0556 0.580566ZM30.103 25.1741C30.487 25.5781 31.0556 25.5581 31.5593 25.4534L31.5643 25.4559C31.7981 25.2657 31.4658 25.0248 31.1484 24.7948C30.9581 24.6569 30.7731 24.5228 30.7189 24.406C30.8946 24.1917 30.375 23.7053 29.9704 23.3266C29.8006 23.1676 29.6511 23.0276 29.5818 22.9347C29.2939 22.6213 29.1782 22.226 29.0618 21.8283C28.9846 21.5645 28.9071 21.2996 28.7788 21.0569C27.9883 19.2215 27.083 17.401 25.8137 15.8474C24.9979 14.8148 24.0669 13.8901 23.1356 12.965C22.5352 12.3687 21.9347 11.7722 21.3648 11.1466C20.7784 10.5413 20.4255 9.79548 20.072 9.04847C19.7761 8.42309 19.4797 7.79685 19.0456 7.25139C17.7314 5.30625 13.5818 4.77508 12.9733 7.52321C12.9758 7.60799 12.9484 7.66286 12.8735 7.71772C12.5369 7.9646 12.2376 8.2439 11.9858 8.58306C11.3698 9.44341 11.275 10.9023 12.0431 11.6753C12.0442 11.6587 12.0453 11.6422 12.0464 11.6257C12.0721 11.2354 12.0961 10.8705 12.4047 10.5905C12.9982 11.1018 13.8985 11.2838 14.5868 10.9023C15.4166 12.0926 15.681 13.5317 15.9462 14.9756C16.1671 16.1784 16.3887 17.3847 16.9384 18.4534C16.9497 18.4723 16.9611 18.4912 16.9725 18.5101C17.2955 19.048 17.6238 19.5946 18.0381 20.0643C18.1886 20.2976 18.4977 20.5495 18.8062 20.8009C19.2132 21.1326 19.6194 21.4636 19.6591 21.7501C19.6609 21.8748 19.6603 22.0012 19.6598 22.1283C19.6566 22.8808 19.6532 23.6601 20.1354 24.2788C20.4022 24.82 19.7489 25.3636 19.2227 25.2963C18.9342 25.3363 18.619 25.2602 18.306 25.1848C17.8778 25.0815 17.4537 24.9792 17.108 25.1766C17.011 25.2816 16.8716 25.2852 16.7316 25.2889C16.5657 25.2933 16.3987 25.2977 16.3 25.4708C16.2797 25.5223 16.2323 25.5804 16.183 25.6408C16.0748 25.7735 15.9575 25.9173 16.098 26.0269C16.1106 26.0174 16.1231 26.0078 16.1356 25.9983C16.3484 25.8358 16.5513 25.681 16.8386 25.7776C16.8004 25.9899 16.9375 26.0467 17.0745 26.1036C17.0984 26.1135 17.1224 26.1234 17.1454 26.1342C17.1439 26.1835 17.1342 26.2332 17.1245 26.2825C17.1015 26.4004 17.0789 26.516 17.1703 26.618C17.2137 26.5738 17.2521 26.5243 17.2905 26.4746C17.3846 26.353 17.4791 26.2308 17.6491 26.1865C18.023 26.6858 18.3996 26.4784 18.8721 26.2182C19.4051 25.9248 20.0601 25.5641 20.9708 26.0743C20.6217 26.0569 20.3099 26.0993 20.0755 26.3885C20.0182 26.4534 19.9683 26.5282 20.0705 26.613C20.6094 26.2639 20.8336 26.3893 21.0446 26.5074C21.1969 26.5927 21.3423 26.6741 21.5942 26.5706C21.6538 26.5395 21.7133 26.5074 21.7729 26.4752C22.1775 26.257 22.5877 26.0357 23.068 26.1117C22.7093 26.2152 22.5816 26.4426 22.4423 26.6908C22.3734 26.8136 22.3017 26.9414 22.1977 27.0619C22.1429 27.1167 22.1179 27.1815 22.1803 27.2738C22.9315 27.2114 23.2153 27.0209 23.5988 26.7636C23.7818 26.6408 23.9875 26.5027 24.2775 26.3561C24.5981 26.1587 24.9187 26.285 25.2293 26.4073C25.5664 26.54 25.8917 26.6681 26.1927 26.3736C26.2878 26.284 26.4071 26.2829 26.5258 26.2818C26.569 26.2814 26.6122 26.281 26.6541 26.2763C26.5604 25.7745 26.0319 25.7804 25.4955 25.7864C24.875 25.7933 24.2438 25.8004 24.2626 25.022C24.8391 24.6282 24.8444 23.9449 24.8494 23.299C24.8507 23.1431 24.8518 22.9893 24.8611 22.8424C25.2851 23.0788 25.7336 23.2636 26.1794 23.4473C26.5987 23.62 27.0156 23.7917 27.4072 24.0045C27.8162 24.6628 28.4546 25.5357 29.305 25.4783C29.3274 25.411 29.3474 25.3536 29.3723 25.2863C29.4213 25.2949 29.4731 25.308 29.5257 25.3213C29.7489 25.3778 29.9879 25.4384 30.103 25.1741ZM46.7702 17.6925C47.2625 18.1837 47.9304 18.4597 48.6267 18.4597C49.323 18.4597 49.9909 18.1837 50.4832 17.6925C50.9756 17.2013 51.2523 16.5351 51.2523 15.8404C51.2523 15.1458 50.9756 14.4795 50.4832 13.9883C49.9909 13.4971 49.323 13.2212 48.6267 13.2212C48.3006 13.2212 47.9807 13.2817 47.6822 13.3965L46.1773 11.1999L45.1285 11.9184L46.6412 14.1266C46.2297 14.6009 46.0011 15.2089 46.0011 15.8404C46.0011 16.5351 46.2778 17.2013 46.7702 17.6925ZM42.0587 10.5787C42.4271 10.7607 42.8332 10.8539 43.2443 10.8508C43.8053 10.8465 44.3501 10.663 44.7989 10.3274C45.2478 9.99169 45.577 9.52143 45.7385 8.9855C45.9 8.44957 45.8851 7.87615 45.6961 7.34925C45.5072 6.82235 45.154 6.36968 44.6884 6.05757C44.3471 5.82883 43.9568 5.68323 43.5488 5.6325C43.1409 5.58176 42.7266 5.62731 42.3396 5.76548C41.9525 5.90365 41.6033 6.13057 41.3202 6.42797C41.0371 6.72537 40.8279 7.08494 40.7096 7.47773C40.5913 7.87051 40.567 8.28552 40.6389 8.68935C40.7107 9.09317 40.8766 9.47453 41.1233 9.80269C41.3699 10.1309 41.6903 10.3967 42.0587 10.5787ZM42.0587 25.7882C42.4271 25.9702 42.8332 26.0634 43.2443 26.0602C43.8053 26.0559 44.3501 25.8725 44.7989 25.5368C45.2478 25.2011 45.577 24.7309 45.7385 24.195C45.9 23.659 45.8851 23.0856 45.6961 22.5587C45.5072 22.0318 45.154 21.5791 44.6884 21.267C44.3471 21.0383 43.9568 20.8927 43.5488 20.842C43.1409 20.7912 42.7266 20.8368 42.3396 20.9749C41.9525 21.1131 41.6033 21.34 41.3202 21.6374C41.0371 21.9348 40.8279 22.2944 40.7096 22.6872C40.5913 23.08 40.567 23.495 40.6389 23.8988C40.7107 24.3026 40.8766 24.684 41.1233 25.0122C41.3699 25.3403 41.6903 25.6061 42.0587 25.7882ZM44.4725 16.4916V15.1894H40.454C40.3529 14.7946 40.1601 14.4289 39.8911 14.1216L41.4029 11.8819L40.3034 11.1526L38.7916 13.3924C38.5145 13.2923 38.2224 13.2395 37.9277 13.2361C37.2333 13.2361 36.5675 13.5105 36.0765 13.9989C35.5856 14.4874 35.3097 15.1498 35.3097 15.8405C35.3097 16.5313 35.5856 17.1937 36.0765 17.6821C36.5675 18.1705 37.2333 18.4449 37.9277 18.4449C38.2224 18.4416 38.5145 18.3888 38.7916 18.2887L40.3034 20.5284L41.3899 19.7992L39.8911 17.5594C40.1601 17.2522 40.3529 16.8865 40.454 16.4916H44.4725Z"
          fill="currentColor"
        />
      </svg>
    }
    href="/integrations/langgraph"
  >
    Create complex agent workflows with memory persistence using LangGraph.
  </Card>
  <Card
    title="Vercel AI SDK"
    icon={
      <svg
        width="24"
        height="24"
        viewBox="0 0 128 128"
        xmlns="http://www.w3.org/2000/svg"
      >
        <path d="M64.002 8.576 128 119.424H0Z" fill="currentColor" />
      </svg>
    }
    href="/integrations/vercel-ai-sdk"
  >
    Build AI-powered applications with memory using the Vercel AI SDK.
  </Card>
  <Card
    title="LangChain Tools"
    icon={
      <svg
        role="img"
        viewBox="0 0 24 24"
        xmlns="http://www.w3.org/2000/svg"
        width="32"
        height="32"
      >
        <title>LangChain</title>
        <path
          d="M6.0988 5.9175C2.7359 5.9175 0 8.6462 0 12s2.736 6.0825 6.0988 6.0825h11.8024C21.2641 18.0825 24 15.3538 24 12s-2.736 -6.0825 -6.0988 -6.0825ZM5.9774 7.851c0.493 0.0124 1.02 0.2496 1.273 0.6228 0.3673 0.4592 0.4778 1.0668 0.8944 1.4932 0.5604 0.6118 1.199 1.1505 1.7161 1.802 0.4892 0.5954 0.8386 1.2937 1.1436 1.9975 0.1244 0.2335 0.1257 0.5202 0.31 0.7197 0.0908 0.1204 0.5346 0.4483 0.4383 0.5645 0.0555 0.1204 0.4702 0.286 0.3263 0.4027 -0.1944 0.04 -0.4129 0.0476 -0.5616 -0.1074 -0.0549 0.126 -0.183 0.0596 -0.2819 0.0432a4 4 0 0 0 -0.025 0.0736c-0.3288 0.0219 -0.5754 -0.3126 -0.732 -0.565 -0.3111 -0.168 -0.6642 -0.2702 -0.982 -0.446 -0.0182 0.2895 0.0452 0.6485 -0.231 0.8353 -0.014 0.5565 0.8436 0.0656 0.9222 0.4804 -0.061 0.0067 -0.1286 -0.0095 -0.1774 0.0373 -0.2239 0.2172 -0.4805 -0.1645 -0.7385 -0.007 -0.3464 0.174 -0.3808 0.3161 -0.8096 0.352 -0.0237 -0.0359 -0.0143 -0.0592 0.0059 -0.0811 0.1207 -0.1399 0.1295 -0.3046 0.3356 -0.3643 -0.2122 -0.0334 -0.3899 0.0833 -0.5686 0.1757 -0.2323 0.095 -0.2304 -0.2141 -0.5878 0.0164 -0.0396 -0.0322 -0.0208 -0.0615 0.0018 -0.0864 0.0908 -0.1107 0.2102 -0.127 0.345 -0.1208 -0.663 -0.3686 -0.9751 0.4507 -1.2813 0.0432 -0.092 0.0243 -0.1265 0.1068 -0.1845 0.1652 -0.05 -0.0548 -0.0123 -0.1212 -0.0099 -0.1857 -0.0598 -0.028 -0.1356 -0.041 -0.1179 -0.1366 -0.1171 -0.0395 -0.1988 0.0295 -0.286 0.0952 -0.0787 -0.0608 0.0532 -0.1492 0.0776 -0.2125 0.0702 -0.1216 0.23 -0.025 0.3111 -0.1126 0.2306 -0.1308 0.552 0.0814 0.8155 0.0455 0.203 0.0255 0.4544 -0.1825 0.3526 -0.39 -0.2171 -0.2767 -0.179 -0.6386 -0.1839 -0.9695 -0.0268 -0.1929 -0.491 -0.4382 -0.6252 -0.6462 -0.1659 -0.1873 -0.295 -0.4047 -0.4243 -0.6182 -0.4666 -0.9008 -0.3198 -2.0584 -0.9077 -2.8947 -0.266 0.1466 -0.6125 0.0774 -0.8418 -0.119 -0.1238 0.1125 -0.1292 0.2598 -0.139 0.4161 -0.297 -0.2962 -0.2593 -0.8559 -0.022 -1.1855 0.0969 -0.1302 0.2127 -0.2373 0.342 -0.3316 0.0292 -0.0213 0.0391 -0.0419 0.0385 -0.0747 0.1174 -0.5267 0.5764 -0.7391 1.0694 -0.7267m12.4071 0.46c0.5575 0 1.0806 0.2159 1.474 0.6082s0.61 0.9145 0.61 1.4704c0 0.556 -0.2167 1.078 -0.61 1.4698v0.0006l-0.902 0.8995a2.08 2.08 0 0 1 -0.8597 0.5166l-0.0164 0.0047 -0.0058 0.0164a2.05 2.05 0 0 1 -0.474 0.7308l-0.9018 0.8995c-0.3934 0.3924 -0.917 0.6083 -1.4745 0.6083s-1.0806 -0.216 -1.474 -0.6083c-0.813 -0.8107 -0.813 -2.1294 0 -2.9402l0.9019 -0.8995a2.056 2.056 0 0 1 0.858 -0.5143l0.017 -0.0053 0.0058 -0.0158a2.07 2.07 0 0 1 0.4752 -0.7337l0.9018 -0.8995c0.3934 -0.3924 0.9171 -0.6083 1.4745 -0.6083zm0 0.8965a1.18 1.18 0 0 0 -0.8388 0.3462l-0.9018 0.8995a1.181 1.181 0 0 0 -0.3427 0.9252l0.0053 0.0572c0.0323 0.2652 0.149 0.5044 0.3374 0.6917 0.13 0.1296 0.2733 0.2114 0.4471 0.2686a0.9 0.9 0 0 1 0.014 0.1582 0.884 0.884 0 0 1 -0.2609 0.6304l-0.0554 0.0554c-0.3013 -0.1028 -0.5525 -0.253 -0.7794 -0.4792a2.06 2.06 0 0 1 -0.5761 -1.0968l-0.0099 -0.0578 -0.0461 0.0368a1.1 1.1 0 0 0 -0.0876 0.0794l-0.9024 0.8995c-0.4623 0.461 -0.4623 1.212 0 1.673 0.2311 0.2305 0.535 0.346 0.8394 0.3461 0.3043 0 0.6077 -0.1156 0.8388 -0.3462l0.9019 -0.8995c0.4623 -0.461 0.4623 -1.2113 0 -1.673a1.17 1.17 0 0 0 -0.4367 -0.2749 1 1 0 0 1 -0.014 -0.1611c0 -0.2591 0.1023 -0.505 0.2901 -0.6923 0.3019 0.1028 0.57 0.2694 0.7962 0.495 0.3007 0.2999 0.4994 0.679 0.5756 1.0968l0.0105 0.0578 0.0455 -0.0373a1.1 1.1 0 0 0 0.0887 -0.0794l0.902 -0.8996c0.4622 -0.461 0.4628 -1.2124 0 -1.6735a1.18 1.18 0 0 0 -0.8395 -0.3462Zm-9.973 5.1567 -0.0006 0.0006c-0.0793 0.3078 -0.1048 0.8318 -0.506 0.847 -0.033 0.1776 0.1228 0.2445 0.2655 0.1874 0.141 -0.0645 0.2081 0.0508 0.2557 0.1657 0.2177 0.0317 0.5394 -0.0725 0.5516 -0.3298 -0.325 -0.1867 -0.4253 -0.5418 -0.5662 -0.8709"
          fill="currentColor"
        />
      </svg>
    }
    href="/integrations/langchain-tools"
  >
    Use Mem0 with LangChain Tools for enhanced agent capabilities.
  </Card>
  <Card
    title="Dify"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 200 200"
        fill="none"
      >
        <path
          d="M40 20 H120 C160 20, 160 180, 120 180 H40 V20"
          fill="currentColor"
        />
      </svg>
    }
    href="/integrations/dify"
  >
    Build AI applications with persistent memory using Dify and Mem0.
  </Card>
  <Card
    title="MCP Server"
    icon={
      <svg
        viewBox="0 0 180 180"
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
      >
        <path
          d="M45 45 L135 45 M45 90 L135 90 M45 135 L135 135"
          stroke="currentColor"
          strokeWidth="12"
          strokeLinecap="round"
          fill="none"
        />
      </svg>
    }
    href="/integrations/mcp-server"
  >
    Integrate Mem0 as an MCP Server in Cursor.
  </Card>
  <Card
    title="Livekit"
    icon={
      <svg
        viewBox="0 0 24 24"
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
      >
        <text
          x="12"
          y="16"
          fontFamily="Arial"
          fontSize="12"
          textAnchor="middle"
          fill="currentColor"
          fontWeight="bold"
        >
          LK
        </text>
      </svg>
    }
    href="/integrations/livekit"
  >
    Integrate Mem0 with Livekit for voice agents.
  </Card>
  <Card
    title="ElevenLabs"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
      >
        <rect width="24" height="24" fill="white"/>
        <rect x="8" y="4" width="2" height="16" fill="black"/>
        <rect x="14" y="4" width="2" height="16" fill="black"/>
      </svg>
    }
    href="/integrations/elevenlabs"
  >
    Build voice agents with memory using ElevenLabs Conversational AI.
  </Card>
  <Card
    title="Pipecat"
    icon={
      <svg
        viewBox="0 0 24 24"
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
      >
        <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8z" fill="currentColor"/>
        <circle cx="8.5" cy="9" r="1.5" fill="currentColor"/>
        <circle cx="15.5" cy="9" r="1.5" fill="currentColor"/>
        <path d="M12 16c1.66 0 3-1.34 3-3H9c0 1.66 1.34 3 3 3z" fill="currentColor"/>
        <path d="M17.5 12c-.83 0-1.5-.67-1.5-1.5s.67-1.5 1.5-1.5 1.5.67 1.5 1.5-.67 1.5-1.5 1.5z" fill="currentColor"/>
        <path d="M6.5 12c-.83 0-1.5-.67-1.5-1.5S5.67 9 6.5 9s1.5.67 1.5 1.5S7.33 12 6.5 12z" fill="currentColor"/>
      </svg>
    }     href="/integrations/pipecat"
  >
    Build conversational AI agents with memory using Pipecat.
  </Card>
  <Card
    title="Agno"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
      >
        <path d="M8 4h8v12h8" stroke="currentColor" strokeWidth="2" fill="none" transform="rotate(15, 12, 12)"/>
      </svg>
    }
    href="/integrations/agno"
  >
    Build autonomous agents with memory using Agno framework.
  </Card>

  <Card
    title="Keywords AI"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
      >
        <path fill-rule="evenodd" clip-rule="evenodd" d="M9.07513 1.1863C9.21663 1.07722 9.39144 1.01009 9.56624 1.01009C9.83261 1.01009 10.0823 1.12756 10.2405 1.33734L15.0101 7.4964V12.4136L16.4335 13.8401C16.7582 14.1673 16.7582 14.7043 16.4335 15.0316C16.1089 15.3588 15.5762 15.3588 15.2515 15.0316L13.3453 13.1016V8.07538L8.92529 2.36944V2.36105C8.64228 2.00024 8.70887 1.4716 9.07513 1.1863ZM18.976 14.4133C18.8344 14.3778 18.7003 14.3042 18.5894 14.1925L16.9163 12.5059C16.7249 12.3129 16.6416 12.0528 16.6749 11.8094V6.88385H16.6499L11.8553 0.691225C11.7282 0.529117 11.6716 0.333133 11.6803 0.140562C11.134 0.0481292 10.5726 0 10 0C4.47715 0 0 4.47715 0 10C0 15.5228 4.47715 20 10 20C13.9387 20 17.3456 17.7229 18.976 14.4133Z" fill="currentColor"></path>
      </svg>
    }
    href="/integrations/keywords"
  >
    Build AI applications with persistent memory and comprehensive LLM observability.
  </Card>
  <Card
    title="Raycast"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
      >
        <path
          d="M3 12L21 12M12 3L12 21M7.5 7.5L16.5 16.5M16.5 7.5L7.5 16.5"
          stroke="currentColor"
          strokeWidth="2"
          strokeLinecap="round"
        />
      </svg>
    }
    href="/integrations/raycast"
  >
    Mem0 Raycast extension for intelligent memory management and retrieval.
  </Card>
  <Card
    title="Mastra"
    icon={
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
      >
        <path
          d="M12 2L22 7L12 12L2 7L12 2Z"
          stroke="currentColor"
          strokeWidth="2"
          strokeLinejoin="round"
        />
        <path
          d="M2 17L12 22L22 17"
          stroke="currentColor"
          strokeWidth="2"
          strokeLinejoin="round"
        />
        <path
          d="M2 12L12 17L22 12"
          stroke="currentColor"
          strokeWidth="2"
          strokeLinejoin="round"
        />
      </svg>
    }
    href="/integrations/mastra"
  >
    Build AI agents with persistent memory using Mastra's framework and tools.
  </Card>
</CardGroup>



================================================
FILE: docs/introduction.mdx
================================================
---
title: Introduction
icon: "book"
iconType: "solid"
---

Mem0 is a memory layer designed for modern AI agents. It acts as a persistent memory layer that agents can use to:

- Recall relevant past interactions
- Store important user preferences and factual context
- Learn from successes and failures

It gives AI agents memory so they can remember, learn, and evolve across interactions. Mem0 integrates easily into your agent stack and scales from prototypes to production systems.


## Stateless vs. Stateful Agents

Most current agents are stateless: they process a query, generate a response, and forget everything. Even with huge context windows, everything resets the next session.

Stateful agents, powered by Mem0, are different. They retain context, recall what matters, and behave more intelligently over time.

<Frame>
  <img src="../images/stateless-vs-stateful-agent.png" />
</Frame>


## Where Memory Fits in the Agent Stack

Mem0 sits alongside your retriever, planner, and LLM. Unlike retrieval-based systems (like RAG), Mem0 tracks past interactions, stores long-term knowledge, and evolves the agent’s behavior.

<Frame>
  <img src="../images/memory-agent-stack.png" />
</Frame>

Memory is not about pushing more tokens into a prompt but about intelligently remembering context that matters. This distinction matters:

| Capability       | Context Window        | Mem0 Memory                |
|------------------|------------------------|-----------------------------|
| Retention        | Temporary              | Persistent                  |
| Cost             | Grows with input size  | Optimized (only what matters) |
| Recall           | Token proximity        | Relevance + intent-based   |
| Personalization  | None                   | Deep, evolving profile     |
| Behavior         | Reactive               | Adaptive                   |


## Memory vs. RAG: Complementary Tools

RAG (Retrieval-Augmented Generation) is great for fetching facts from documents. But it’s stateless. It doesn’t know who the user is, what they’ve asked before, or what failed last time.

Mem0 provides continuity. It stores decisions, preferences, and context—not just knowledge.

| Aspect             | RAG                          | Mem0 Memory                   |
|--------------------|-------------------------------|-------------------------------|
| Statefulness       | Stateless                     | Stateful                      |
| Recall Type        | Document lookup               | Evolving user context         |
| Use Case           | Ground answers in data        | Guide behavior across time    |

Together, they’re stronger: RAG informs the LLM; Mem0 shapes its memory.


## Types of Memory in Mem0

Mem0 supports different kinds of memory to mimic how humans store information:

- **Working Memory**: short-term session awareness
- **Factual Memory**: long-term structured knowledge (e.g., preferences, settings)
- **Episodic Memory**: records specific past conversations
- **Semantic Memory**: builds general knowledge over time


## Why Developers Choose Mem0

Mem0 isn’t a wrapper around a vector store. It’s a full memory engine with:

- **LLM-based extraction**: Intelligently decides what to remember
- **Filtering & decay**: Avoids memory bloat, forgets irrelevant info
- **Costs Reduction**: Save compute costs with smart prompt injection of only relevant memories
- **Dashboards & APIs**: Observability, fine-grained control
- **Cloud and OSS**: Use our platform version or our open-source SDK version

You plug Mem0 into your agent framework, it doesn’t replace your LLM or workflows. Instead, it adds a smart memory layer on top.


## Core Capabilities

- **Reduced token usage and faster responses**: sub-50 ms lookups
- **Semantic memory**: procedural, episodic, and factual support
- **Multimodal support**: handle both text and images
- **Graph memory**: connect insights and entities across sessions
- **Host your way**: either a managed service or a self-hosted version


## Getting Started
Mem0 offers two powerful ways to leverage our technology: our [managed platform](/platform/overview) and our [open source solution](/open-source/overview).


<CardGroup cols={3}>
  <Card title="Quickstart" icon="rocket" href="/quickstart">
    Integrate Mem0 in a few lines of code
  </Card>
  <Card title="Playground" icon="play" href="https://app.mem0.ai/playground">
    Mem0 in action
  </Card>
  <Card title="Examples" icon="lightbulb" href="/examples">
  See what you can build with Mem0
  </Card>
</CardGroup>

## Need help?
If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx"/>



================================================
FILE: docs/llms.txt
================================================
# Mem0

> Mem0 is a self-improving memory layer for LLM applications, enabling personalized AI experiences that retain context across sessions, adapt over time, and reduce costs by intelligently storing and retrieving relevant information.

Mem0 provides both a managed platform and open-source solutions for adding persistent memory to AI agents and applications. Unlike traditional RAG systems that are stateless, Mem0 creates stateful agents that remember user preferences, learn from interactions, and evolve behavior over time.

Key differentiators:
- **Stateful vs Stateless**: Retains context across sessions rather than forgetting after each interaction
- **Intelligent Memory Management**: Uses LLMs to extract, filter, and organize relevant information
- **Dual Storage Architecture**: Combines vector embeddings with graph databases for comprehensive memory
- **Sub-50ms Retrieval**: Lightning-fast memory lookups for real-time applications
- **Multimodal Support**: Handles text, images, and documents seamlessly

## Getting Started

- [Introduction](https://docs.mem0.ai/introduction): Overview of Mem0's memory layer for AI agents, including stateless vs stateful agents and how memory fits in the agent stack
- [Quickstart Guide](https://docs.mem0.ai/quickstart): Get started with either Mem0 Platform (managed) or Open Source in minutes
- [Platform Overview](https://docs.mem0.ai/platform/overview): Managed solution with 4-line integration, sub-50ms latency, and intuitive dashboard
- [Open Source Overview](https://docs.mem0.ai/open-source/overview): Self-hosted solution with full infrastructure control and customization

## Core Concepts

- [Memory Types](https://docs.mem0.ai/core-concepts/memory-types): Working memory (short-term session awareness), factual memory (structured knowledge), episodic memory (past conversations), and semantic memory (general knowledge)
- [Memory Operations - Add](https://docs.mem0.ai/core-concepts/memory-operations/add): How Mem0 processes conversations through information extraction, conflict resolution, and dual storage
- [Memory Operations - Search](https://docs.mem0.ai/core-concepts/memory-operations/search): Retrieval of relevant memories using semantic search with query processing and result ranking
- [Memory Operations - Update](https://docs.mem0.ai/core-concepts/memory-operations/update): Modifying existing memories when new information conflicts or supplements stored data
- [Memory Operations - Delete](https://docs.mem0.ai/core-concepts/memory-operations/delete): Removing outdated or irrelevant memories to maintain memory quality

## Platform (Managed Solution)

- [Platform Quickstart](https://docs.mem0.ai/platform/quickstart): Complete guide to using Mem0 Platform with Python, JavaScript, and cURL examples
- [Advanced Memory Operations](https://docs.mem0.ai/platform/advanced-memory-operations): Sophisticated memory management techniques for complex applications
- [Graph Memory](https://docs.mem0.ai/platform/features/graph-memory): Build and query relationships between entities for contextually relevant retrieval
- [Advanced Retrieval](https://docs.mem0.ai/platform/features/advanced-retrieval): Enhanced search with keyword search, reranking, and filtering capabilities
- [Multimodal Support](https://docs.mem0.ai/platform/features/multimodal-support): Integration of images and documents (JPG, PNG, MDX, TXT, PDF) via URLs or Base64
- [Memory Customization](https://docs.mem0.ai/platform/features/selective-memory): Selective memory storage through inclusion and exclusion rules
- [Custom Categories](https://docs.mem0.ai/platform/features/custom-categories): Define domain-specific categories to improve memory organization
- [Async Client](https://docs.mem0.ai/platform/features/async-client): Non-blocking operations for high-concurrency applications
- [Memory Export](https://docs.mem0.ai/platform/features/memory-export): Export memories in structured formats using customizable Pydantic schemas

## Open Source

- [Python Quickstart](https://docs.mem0.ai/open-source/python-quickstart): Installation, configuration, and usage examples for Python SDK
- [Node.js Quickstart](https://docs.mem0.ai/open-source/node-quickstart): Installation, configuration, and usage examples for Node.js SDK
- [OpenAI Compatibility](https://docs.mem0.ai/open-source/features/openai_compatibility): Seamless integration with OpenAI-compatible APIs
- [Custom Fact Extraction](https://docs.mem0.ai/open-source/features/custom-fact-extraction-prompt): Tailor information extraction for specific use cases
- [REST API Server](https://docs.mem0.ai/open-source/features/rest-api): FastAPI-based server with core operations and OpenAPI documentation
- [Graph Memory Overview](https://docs.mem0.ai/open-source/graph_memory/overview): Build and query entity relationships using graph stores like Neo4j

## Components

- [LLM Overview](https://docs.mem0.ai/components/llms/overview): Comprehensive guide to Large Language Model integration and configuration options
- [Vector Database Overview](https://docs.mem0.ai/components/vectordbs/overview): Guide to supported vector databases for semantic memory storage
- [Embeddings Overview](https://docs.mem0.ai/components/embedders/overview): Embedding model configuration for semantic understanding

### Supported LLMs

- [OpenAI](https://docs.mem0.ai/components/llms/models/openai): Integration with OpenAI models including GPT-4 and structured outputs
- [Anthropic](https://docs.mem0.ai/components/llms/models/anthropic): Claude model integration with advanced reasoning capabilities
- [Google AI](https://docs.mem0.ai/components/llms/models/google_AI): Gemini model integration for multimodal applications
- [Groq](https://docs.mem0.ai/components/llms/models/groq): High-performance LPU optimized models for fast inference
- [AWS Bedrock](https://docs.mem0.ai/components/llms/models/aws_bedrock): Enterprise-grade AWS managed model integration
- [Azure OpenAI](https://docs.mem0.ai/components/llms/models/azure_openai): Microsoft Azure hosted OpenAI models for enterprise environments
- [Ollama](https://docs.mem0.ai/components/llms/models/ollama): Local model deployment for privacy-focused applications
- [vLLM](https://docs.mem0.ai/components/llms/models/vllm): High-performance inference framework
- [LM Studio](https://docs.mem0.ai/components/llms/models/lmstudio): Local model management and deployment
- [Together](https://docs.mem0.ai/components/llms/models/together): Open-source model inference platform
- [DeepSeek](https://docs.mem0.ai/components/llms/models/deepseek): Advanced reasoning models
- [Sarvam](https://docs.mem0.ai/components/llms/models/sarvam): Indian language models
- [XAI](https://docs.mem0.ai/components/llms/models/xai): xAI models integration
- [LiteLLM](https://docs.mem0.ai/components/llms/models/litellm): Unified LLM interface and proxy
- [LangChain](https://docs.mem0.ai/components/llms/models/langchain): LangChain LLM integration
- [OpenAI Structured](https://docs.mem0.ai/components/llms/models/openai_structured): OpenAI with structured output support
- [Azure OpenAI Structured](https://docs.mem0.ai/components/llms/models/azure_openai_structured): Azure OpenAI with structured outputs

### Supported Vector Databases

- [Qdrant](https://docs.mem0.ai/components/vectordbs/dbs/qdrant): High-performance vector similarity search engine
- [Pinecone](https://docs.mem0.ai/components/vectordbs/dbs/pinecone): Managed vector database with serverless and pod deployment options
- [Chroma](https://docs.mem0.ai/components/vectordbs/dbs/chroma): AI-native open-source vector database optimized for speed
- [Weaviate](https://docs.mem0.ai/components/vectordbs/dbs/weaviate): Open-source vector search engine with built-in ML capabilities
- [PGVector](https://docs.mem0.ai/components/vectordbs/dbs/pgvector): PostgreSQL extension for vector similarity search
- [Milvus](https://docs.mem0.ai/components/vectordbs/dbs/milvus): Open-source vector database for AI applications at scale
- [Redis](https://docs.mem0.ai/components/vectordbs/dbs/redis): Real-time vector storage and search with Redis Stack
- [Supabase](https://docs.mem0.ai/components/vectordbs/dbs/supabase): Open-source Firebase alternative with vector support
- [Upstash Vector](https://docs.mem0.ai/components/vectordbs/dbs/upstash_vector): Serverless vector database
- [Elasticsearch](https://docs.mem0.ai/components/vectordbs/dbs/elasticsearch): Distributed search and analytics engine
- [OpenSearch](https://docs.mem0.ai/components/vectordbs/dbs/opensearch): Open-source search and analytics platform
- [FAISS](https://docs.mem0.ai/components/vectordbs/dbs/faiss): Facebook AI Similarity Search library
- [MongoDB](https://docs.mem0.ai/components/vectordbs/dbs/mongodb): Document database with vector search capabilities
- [Azure AI Search](https://docs.mem0.ai/components/vectordbs/dbs/azure_ai_search): Microsoft's enterprise search service
- [Vertex AI Vector Search](https://docs.mem0.ai/components/vectordbs/dbs/vertex_ai_vector_search): Google Cloud's vector search service
- [Databricks](https://docs.mem0.ai/components/vectordbs/dbs/databricks): Delta Lake integration for vector search
- [Baidu](https://docs.mem0.ai/components/vectordbs/dbs/baidu): Baidu vector database integration
- [LangChain](https://docs.mem0.ai/components/vectordbs/dbs/langchain): LangChain vector store integration
- [S3 Vectors](https://docs.mem0.ai/components/vectordbs/dbs/s3_vectors): Amazon S3 Vectors integration

### Supported Embeddings

- [OpenAI Embeddings](https://docs.mem0.ai/components/embedders/models/openai): High-quality text embeddings with customizable dimensions
- [Azure OpenAI Embeddings](https://docs.mem0.ai/components/embedders/models/azure_openai): Enterprise Azure-hosted embedding models
- [Google AI](https://docs.mem0.ai/components/embedders/models/google_ai): Gemini embedding models
- [AWS Bedrock](https://docs.mem0.ai/components/embedders/models/aws_bedrock): Amazon embedding models through Bedrock
- [Hugging Face](https://docs.mem0.ai/components/embedders/models/hugging_face): Open-source embedding models for local deployment
- [Vertex AI](https://docs.mem0.ai/components/embedders/models/vertexai): Google Cloud's enterprise embedding models
- [Ollama](https://docs.mem0.ai/components/embedders/models/ollama): Local embedding models for privacy-focused applications
- [Together](https://docs.mem0.ai/components/embedders/models/together): Open-source model embeddings
- [LM Studio](https://docs.mem0.ai/components/embedders/models/lmstudio): Local model embeddings
- [LangChain](https://docs.mem0.ai/components/embedders/models/langchain): LangChain embedder integration

## Integrations

- [LangChain](https://docs.mem0.ai/integrations/langchain): Seamless integration with LangChain framework for enhanced agent capabilities
- [LangGraph](https://docs.mem0.ai/integrations/langgraph): Build stateful, multi-actor applications with persistent memory
- [LlamaIndex](https://docs.mem0.ai/integrations/llama-index): Enhanced RAG applications with intelligent memory layer
- [CrewAI](https://docs.mem0.ai/integrations/crewai): Multi-agent systems with shared and individual memory capabilities
- [AutoGen](https://docs.mem0.ai/integrations/autogen): Microsoft's multi-agent conversation framework with memory
- [Vercel AI SDK](https://docs.mem0.ai/integrations/vercel-ai-sdk): Build AI-powered web applications with persistent memory
- [Flowise](https://docs.mem0.ai/integrations/flowise): No-code LLM workflow builder with memory capabilities
- [Dify](https://docs.mem0.ai/integrations/dify): LLMOps platform integration for production AI applications

## Examples and Use Cases

- [Personal AI Tutor](https://docs.mem0.ai/examples/personal-ai-tutor): Build an AI tutor that remembers learning progress and adapts teaching methods
- [Customer Support Agent](https://docs.mem0.ai/examples/customer-support-agent): Create support agents that remember customer history and preferences
- [Personalized Travel Assistant](https://docs.mem0.ai/examples/personal-travel-assistant): Develop travel agents that learn from past trips and preferences
- [Memory-Guided Content Writing](https://docs.mem0.ai/examples/memory-guided-content-writing): Build content generators that remember writing style and topic preferences
- [Collaborative Task Agent](https://docs.mem0.ai/examples/collaborative-task-agent): Multi-agent systems with shared memory for team coordination

## API Reference

- [Memory APIs](https://docs.mem0.ai/api-reference/memory/add-memories): Comprehensive API documentation for memory operations
- [Add Memories](https://docs.mem0.ai/api-reference/memory/add-memories): REST API for storing new memories with detailed request/response formats
- [Search Memories](https://docs.mem0.ai/api-reference/memory/v2-search-memories): Advanced search API with filtering and ranking capabilities
- [Get All Memories](https://docs.mem0.ai/api-reference/memory/v2-get-memories): Retrieve all memories with pagination and filtering options
- [Update Memory](https://docs.mem0.ai/api-reference/memory/update-memory): Modify existing memories with conflict resolution
- [Delete Memory](https://docs.mem0.ai/api-reference/memory/delete-memory): Remove memories individually or in batches

## Optional

- [FAQs](https://docs.mem0.ai/faqs): Frequently asked questions about Mem0's capabilities and implementation details
- [Changelog](https://docs.mem0.ai/changelog): Detailed product updates and version history for tracking new features and improvements
- [Contributing Guide](https://docs.mem0.ai/contributing/development): Guidelines for contributing to Mem0's open-source development
- [OpenMemory](https://docs.mem0.ai/openmemory/overview): Open-source memory infrastructure for research and experimentation



================================================
FILE: docs/quickstart.mdx
================================================
---
title: Quickstart
icon: "bolt"
iconType: "solid"
---

Mem0 offers two powerful ways to leverage our technology: [our managed platform](#mem0-platform-managed-solution) and [our open source solution](#mem0-open-source).

Check out our [Playground](https://mem0.dev/pd-pg) to see Mem0 in action.

<CardGroup cols={2}>
  <Card title="Mem0 Platform (Managed Solution)" icon="chart-simple" href="#mem0-platform-managed-solution">
    Better, faster, fully managed, and hassle free solution.
  </Card>
  <Card title="Mem0 Open Source" icon="code-branch" href="#mem0-open-source">
    Self hosted, fully customizable, and open source.
  </Card>
</CardGroup>


## Mem0 Platform (Managed Solution)

Our fully managed platform provides a hassle-free way to integrate Mem0's capabilities into your AI agents and assistants. Sign up for Mem0 platform [here](https://mem0.dev/pd).

The Mem0 SDK supports both Python and JavaScript, with full [TypeScript](/platform/quickstart/#4-11-working-with-mem0-in-typescript) support as well.

Follow the steps below to get started with Mem0 Platform:

1. [Install Mem0](#1-install-mem0)
2. [Add Memories](#2-add-memories)
3. [Retrieve Memories](#3-retrieve-memories)

### 1. Install Mem0

<AccordionGroup>
<Accordion title="Install package">
<CodeGroup>
```bash pip
pip install mem0ai
```

```bash npm
npm install mem0ai
```
</CodeGroup>
</Accordion>
<Accordion title="Get API Key">

1. Sign in to [Mem0 Platform](https://mem0.dev/pd-api)
2. Copy your API Key from the dashboard

![Get API Key from Mem0 Platform](/images/platform/api-key.png)

</Accordion>
</AccordionGroup>

### 2. Add Memories

<AccordionGroup>
<Accordion title="Instantiate client">
<CodeGroup>
```python Python
import os
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

client = MemoryClient()
```

```javascript JavaScript
import MemoryClient from 'mem0ai';
const client = new MemoryClient({ apiKey: 'your-api-key' });
```
</CodeGroup>
</Accordion>
<Accordion title="Add memories">
<CodeGroup>

```python Python
messages = [
    {"role": "user", "content": "Thinking of making a sandwich. What do you recommend?"},
    {"role": "assistant", "content": "How about adding some cheese for extra flavor?"},
    {"role": "user", "content": "Actually, I don't like cheese."},
    {"role": "assistant", "content": "I'll remember that you don't like cheese for future recommendations."}
]
client.add(messages, user_id="alex")
```

```javascript JavaScript
const messages = [
    {"role": "user", "content": "Thinking of making a sandwich. What do you recommend?"},
    {"role": "assistant", "content": "How about adding some cheese for extra flavor?"},
    {"role": "user", "content": "Actually, I don't like cheese."},
    {"role": "assistant", "content": "I'll remember that you don't like cheese for future recommendations."}
];
client.add(messages, { user_id: "alex" })
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [
             {"role": "user", "content": "I live in San Francisco. Thinking of making a sandwich. What do you recommend?"},
             {"role": "assistant", "content": "How about adding some cheese for extra flavor?"},
             {"role": "user", "content": "Actually, I don't like cheese."},
             {"role": "assistant", "content": "I'll remember that you don't like cheese for future recommendations."}
         ],
         "user_id": "alex"
     }'
```

```json Output
[
  {
    "id": "24e466b5-e1c6-4bde-8a92-f09a327ffa60",
    "memory": "Does not like cheese",
    "event": "ADD"
  },
  {
    "id": "e8d78459-fadd-4c5a-bece-abb8c3dc7ed7",
    "memory": "Lives in San Francisco",
    "event": "ADD"
  }
]
```
</CodeGroup>
</Accordion>
</AccordionGroup>

### 3. Retrieve Memories

<AccordionGroup>
<Accordion title="Search for relevant memories">
<CodeGroup>

```python Python
# Example showing location and preference-aware recommendations
query = "I'm craving some pizza. Any recommendations?"
filters = {
    "AND": [
        {
            "user_id": "alex"
        }
    ]
}
client.search(query, version="v2", filters=filters)
```

```javascript JavaScript
const query = "I'm craving some pizza. Any recommendations?";
const filters = {
    "AND": [
        {
            "user_id": "alex"
        }
    ]
};
client.search(query, { version: "v2", filters })
    .then(results => console.log(results))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/search/?version=v2" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "query": "I'm craving some pizza. Any recommendations?",
         "filters": {
             "AND": [
                 {
                    "user_id": "alex"
                 }
             ]
         }
     }'
```

```json Output
[
    {
        "id": "7f165f7e-b411-4afe-b7e5-35789b72c4a5",
        "memory": "Does not like cheese",
        "user_id": "alex",
        "metadata": null,
        "created_at": "2024-07-20T01:30:36.275141-07:00",
        "updated_at": "2024-07-20T01:30:36.275172-07:00",
        "score": 0.92
    },
    {
        "id": "8f165f7e-b411-4afe-b7e5-35789b72c4b6",
        "memory": "Lives in San Francisco",
        "user_id": "alex",
        "metadata": null,
        "created_at": "2024-07-20T01:30:36.275141-07:00",
        "updated_at": "2024-07-20T01:30:36.275172-07:00",
        "score": 0.85
    }
]
```
</CodeGroup>

</Accordion>
<Accordion title="Get all memories of a user">

<CodeGroup>

```python Python
filters = {
   "AND": [
      {
         "user_id": "alex"
      }
   ]
}

all_memories = client.get_all(version="v2", filters=filters, page=1, page_size=50)
```

```javascript JavaScript
const filters = {
   "AND": [
      {
         "user_id": "alex"
      }
   ]
};

client.getAll({ version: "v2", filters, page: 1, page_size: 50 })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));
```

```bash cURL
curl -X GET "https://api.mem0.ai/v1/memories/?version=v2&page=1&page_size=50" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "filters": {
             "AND": [
                 {
                     "user_id": "alice"
                 }
             ]
         }
     }'
```

```json Output
[
    {
        "id": "7f165f7e-b411-4afe-b7e5-35789b72c4a5",
        "memory": "Does not like cheese",
        "user_id": "alex",
        "metadata": null,
        "created_at": "2024-07-20T01:30:36.275141-07:00",
        "updated_at": "2024-07-20T01:30:36.275172-07:00",
        "score": 0.92
    },
    {
        "id": "8f165f7e-b411-4afe-b7e5-35789b72c4b6",
        "memory": "Lives in San Francisco",
        "user_id": "alex",
        "metadata": null,
        "created_at": "2024-07-20T01:30:36.275141-07:00",
        "updated_at": "2024-07-20T01:30:36.275172-07:00",
        "score": 0.85
    }
]
```
</CodeGroup>
</Accordion>
</AccordionGroup>

<Card title="Mem0 Platform" icon="chart-simple" href="/platform/overview">
  Learn more about Mem0 platform
</Card>

## Mem0 Open Source

Our open-source version is available for those who prefer full control and customization. You can self-host Mem0 on your infrastructure and integrate it with your AI agents and assistants. Checkout our [GitHub repository](https://mem0.dev/gd)

Follow the steps below to get started with Mem0 Open Source:

1. [Install Mem0 Open Source](#1-install-mem0-open-source)
2. [Add Memories](#2-add-memories-open-source)
3. [Retrieve Memories](#3-retrieve-memories-open-source)

### 1. Install Mem0 Open Source

<AccordionGroup>
<Accordion title="Install package">
<CodeGroup>
```bash pip
pip install mem0ai
```

```bash npm
npm install mem0ai
```
</CodeGroup>
</Accordion>
</AccordionGroup>

### 2. Add Memories <a name="2-add-memories-open-source"></a>

<AccordionGroup>
<Accordion title="Instantiate client">
<CodeGroup>
```python Python
from mem0 import Memory
m = Memory()
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';
const memory = new Memory();
```
</CodeGroup>
</Accordion>
<Accordion title="Add memories">
<CodeGroup>
```python Code
# For a user
messages = [
    {
        "role": "user",
        "content": "I like to drink coffee in the morning and go for a walk"
    }
]
result = m.add(messages, user_id="alice", metadata={"category": "preferences"})
```

```typescript TypeScript
const messages = [
  {
    role: "user",
    content: "I like to drink coffee in the morning and go for a walk"
  }
];
const result = memory.add(messages, { userId: "alice", metadata: { category: "preferences" } });
```

```json Output
[
    {
        "id": "3dc6f65f-fb3f-4e91-89a8-ed1a22f8898a",
        "data": {"memory": "Likes to drink coffee in the morning"},
        "event": "ADD"
    },
    {
        "id": "f1673706-e3d6-4f12-a767-0384c7697d53",
        "data": {"memory": "Likes to go for a walk"},
        "event": "ADD"
    }
]
```
</CodeGroup>
</Accordion>
</AccordionGroup>

### 3. Retrieve Memories <a name="3-retrieve-memories-open-source"></a>

<AccordionGroup>
<Accordion title="Search for relevant memories">
<CodeGroup>
```python Python
related_memories = m.search("Should I drink coffee or tea?", user_id="alice")
```

```typescript TypeScript
const relatedMemories = memory.search("Should I drink coffee or tea?", { userId: "alice" });
```

```json Output
[
    {
        "id": "3dc6f65f-fb3f-4e91-89a8-ed1a22f8898a",
        "memory": "Likes to drink coffee in the morning",
        "user_id": "alice",
        "metadata": {"category": "preferences"},
        "categories": ["user_preferences", "food"],
        "immutable": false,
        "created_at": "2025-02-24T20:11:39.010261-08:00",
        "updated_at": "2025-02-24T20:11:39.010274-08:00",
        "score": 0.5915589089130715
    },
    {
        "id": "e8d78459-fadd-4c5a-bece-abb8c3dc7ed7",
        "memory": "Likes to go for a walk",
        "user_id": "alice",
        "metadata": {"category": "preferences"},
        "categories": ["hobby", "food"],
        "immutable": false,
        "created_at": "2025-02-24T11:47:52.893038-08:00",
        "updated_at": "2025-02-24T11:47:52.893048-08:00",
        "score": 0.43263634637810866
    }
]
```
</CodeGroup>

</Accordion>
</AccordionGroup>

<CardGroup cols={2}>
<Card title="Mem0 OSS Python SDK" icon="python" href="/open-source/python-quickstart">
  Learn more about Mem0 OSS Python SDK
</Card>
<Card title="Mem0 OSS Node.js SDK" icon="node" href="/open-source/node-quickstart">
  Learn more about Mem0 OSS Node.js SDK
</Card>
</CardGroup>


================================================
FILE: docs/_snippets/async-memory-add.mdx
================================================
<Note type="info">
  📢 Heads up!
  We're moving to async memory add for a faster experience.
  If you signed up after July 1st, 2025, your add requests will work in the background and return right away.
</Note> 


================================================
FILE: docs/_snippets/blank-notif.mdx
================================================
[Empty file]


================================================
FILE: docs/_snippets/get-help.mdx
================================================
<CardGroup cols={3}>
  <Card title="Discord" icon="discord" href="https://mem0.dev/DiD" color="#7289DA">
    Join our community
  </Card>
  <Card title="GitHub" icon="github" href="https://github.com/mem0ai/mem0/discussions/new?category=q-a">
    Ask questions on GitHub
  </Card>
  <Card title="Support" icon="calendar" href="https://cal.com/taranjeetio/meet">
  Talk to founders
  </Card>
</CardGroup>



================================================
FILE: docs/_snippets/paper-release.mdx
================================================
<Note type="info">
  📢 Announcing our research paper: Mem0 achieves <strong>26%</strong> higher accuracy than OpenAI Memory, <strong>91%</strong> lower latency, and <strong>90%</strong> token savings! [Read the paper](https://mem0.ai/research) to learn how we're revolutionizing AI agent memory.
</Note>


================================================
FILE: docs/api-reference/entities/delete-user.mdx
================================================
---
title: 'Delete User'
openapi: delete /v1/entities/{entity_type}/{entity_id}/
---


================================================
FILE: docs/api-reference/entities/get-users.mdx
================================================
---
title: 'Get Users'
openapi: get /v1/entities/
---


================================================
FILE: docs/api-reference/memory/add-memories.mdx
================================================
---
title: 'Add Memories'
openapi: post /v1/memories/
---


================================================
FILE: docs/api-reference/memory/batch-delete.mdx
================================================
---
title: 'Batch Delete Memories'
openapi: delete /v1/batch/
---



================================================
FILE: docs/api-reference/memory/batch-update.mdx
================================================
---
title: 'Batch Update Memories'
openapi: put /v1/batch/
---


================================================
FILE: docs/api-reference/memory/create-memory-export.mdx
================================================
---
title: 'Create Memory Export'
openapi: post /v1/exports/
---

Submit a job to create a structured export of memories using a customizable Pydantic schema. This process may take some time to complete, especially if you’re exporting a large number of memories. You can tailor the export by applying various filters (e.g., user_id, agent_id, run_id, or session_id) and by modifying the Pydantic schema to ensure the final data matches your exact needs.



================================================
FILE: docs/api-reference/memory/delete-memories.mdx
================================================
---
title: 'Delete Memories'
openapi: delete /v1/memories/
---



================================================
FILE: docs/api-reference/memory/delete-memory.mdx
================================================
---
title: 'Delete Memory'
openapi: delete /v1/memories/{memory_id}/
---


================================================
FILE: docs/api-reference/memory/feedback.mdx
================================================
---
title: 'Feedback'
openapi: post /v1/feedback/
---



================================================
FILE: docs/api-reference/memory/get-memory-export.mdx
================================================
---
title: 'Get Memory Export'
openapi: post /v1/exports/get
---

Retrieve the latest structured memory export after submitting an export job. You can filter the export by `user_id`, `run_id`, `session_id`, or `app_id` to get the most recent export matching your filters.


================================================
FILE: docs/api-reference/memory/get-memory.mdx
================================================
---
title: 'Get Memory'
openapi: get /v1/memories/{memory_id}/
---


================================================
FILE: docs/api-reference/memory/history-memory.mdx
================================================
---
title: 'Memory History'
openapi: get /v1/memories/{memory_id}/history/
---


================================================
FILE: docs/api-reference/memory/update-memory.mdx
================================================
---
title: 'Update Memory'
openapi: put /v1/memories/{memory_id}/
---


================================================
FILE: docs/api-reference/memory/v1-get-memories.mdx
================================================
---
title: 'Get Memories (v1 - Deprecated)'
openapi: get /v1/memories/
---



================================================
FILE: docs/api-reference/memory/v1-search-memories.mdx
================================================
---
title: 'Search Memories (v1 - Deprecated)'
openapi: post /v1/memories/search/
---



================================================
FILE: docs/api-reference/memory/v2-get-memories.mdx
================================================
---
title: 'Get Memories (v2)'
openapi: post /v2/memories/
---

The v2 get memories API is powerful and flexible, allowing for more precise memory listing without the need for a search query. It supports complex logical operations (AND, OR, NOT) and comparison operators for advanced filtering capabilities. The comparison operators include:
- `in`: Matches any of the values specified
- `gte`: Greater than or equal to
- `lte`: Less than or equal to
- `gt`: Greater than
- `lt`: Less than
- `ne`: Not equal to
- `icontains`: Case-insensitive containment check
- `*`: Wildcard character that matches everything

<CodeGroup>
```python Code
memories = m.get_all(
    filters={
        "AND": [
            {
                "user_id": "alex"
            },
            {
                "created_at": {"gte": "2024-07-01", "lte": "2024-07-31"}
            }
        ]
    },
    version="v2"
)
```

```json Output
[
{
    "id":"f38b689d-6b24-45b7-bced-17fbb4d8bac7",
    "memory":"Name: Alex. Vegetarian. Allergic to nuts.",
    "user_id":"alex",
    "hash":"62bc074f56d1f909f1b4c2b639f56f6a",
    "metadata":null,
    "created_at":"2024-07-25T23:57:00.108347-07:00",
    "updated_at":"2024-07-25T23:57:00.108367-07:00"
}
]
```
</CodeGroup>

<CodeGroup>
```python Wildcard Example
# Using wildcard to get all memories for a specific user across all run_ids
memories = m.get_all(
    filters={
        "AND": [
            {
                "user_id": "alex"
            },
            {
                "run_id": "*"
            }
        ]
    },
    version="v2"
)
```
</CodeGroup>



================================================
FILE: docs/api-reference/memory/v2-search-memories.mdx
================================================
---
title: 'Search Memories (v2)'
openapi: post /v2/memories/search/
---

The v2 search API is powerful and flexible, allowing for more precise memory retrieval. It supports complex logical operations (AND, OR, NOT) and comparison operators for advanced filtering capabilities. The comparison operators include:
- `in`: Matches any of the values specified
- `gte`: Greater than or equal to
- `lte`: Less than or equal to
- `gt`: Greater than
- `lt`: Less than
- `ne`: Not equal to
- `icontains`: Case-insensitive containment check
- `*`: Wildcard character that matches everything

  <CodeGroup>
  ```python Code
  related_memories = m.search(
      query="What are Alice's hobbies?",
      version="v2",
      filters={
          "OR": [
              {
                "user_id": "alice"
              },
              {
                "agent_id": {"in": ["travel-agent", "sports-agent"]}
              }
          ]
      },
  )
  ```

  ```json Output
  {
    "memories": [
      {
        "id": "ea925981-272f-40dd-b576-be64e4871429",
        "memory": "Likes to play cricket and plays cricket on weekends.",
        "metadata": {
          "category": "hobbies"
        },
        "score": 0.32116443111457704,
        "created_at": "2024-07-26T10:29:36.630547-07:00",
        "updated_at": null,
        "user_id": "alice",
        "agent_id": "sports-agent"
      }
    ],
  }
  ```
  </CodeGroup>

  <CodeGroup>
  ```python Wildcard Example
  # Using wildcard to match all run_ids for a specific user
  all_memories = m.search(
      query="What are Alice's hobbies?",
      version="v2",
      filters={
          "AND": [
              {
                  "user_id": "alice"
              },
              {
                  "run_id": "*"
              }
          ]
      },
  )
  ```
  </CodeGroup>



================================================
FILE: docs/api-reference/organization/add-org-member.mdx
================================================
---
title: 'Add Member'
openapi: post /api/v1/orgs/organizations/{org_id}/members/
---

The API provides two roles for organization members:

- `READER`: Allows viewing of organization resources.
- `OWNER`: Grants full administrative access to manage the organization and its resources.



================================================
FILE: docs/api-reference/organization/create-org.mdx
================================================
---
title: 'Create Organization'
openapi: post /api/v1/orgs/organizations/
---


================================================
FILE: docs/api-reference/organization/delete-org.mdx
================================================
---
title: 'Delete Organization'
openapi: delete /api/v1/orgs/organizations/{org_id}/
---


================================================
FILE: docs/api-reference/organization/get-org-members.mdx
================================================
---
title: 'Get Members'
openapi: get /api/v1/orgs/organizations/{org_id}/members/
---


================================================
FILE: docs/api-reference/organization/get-org.mdx
================================================
---
title: 'Get Organization'
openapi: get /api/v1/orgs/organizations/{org_id}/
---


================================================
FILE: docs/api-reference/organization/get-orgs.mdx
================================================
---
title: 'Get Organizations'
openapi: get /api/v1/orgs/organizations/
---


================================================
FILE: docs/api-reference/project/add-project-member.mdx
================================================
---
title: 'Add Member'
openapi: post /api/v1/orgs/organizations/{org_id}/projects/{project_id}/members/
---

The API provides two roles for project members:

- `READER`: Allows viewing of project resources.
- `OWNER`: Grants full administrative access to manage the project and its resources.



================================================
FILE: docs/api-reference/project/create-project.mdx
================================================
---
title: 'Create Project'
openapi: post /api/v1/orgs/organizations/{org_id}/projects/
---


================================================
FILE: docs/api-reference/project/delete-project.mdx
================================================
---
title: 'Delete Project'
openapi: delete /api/v1/orgs/organizations/{org_id}/projects/{project_id}/
---


================================================
FILE: docs/api-reference/project/get-project-members.mdx
================================================
---
title: 'Get Members'
openapi: get /api/v1/orgs/organizations/{org_id}/projects/{project_id}/members/
---


================================================
FILE: docs/api-reference/project/get-project.mdx
================================================
---
title: 'Get Project'
openapi: get /api/v1/orgs/organizations/{org_id}/projects/{project_id}/
---


================================================
FILE: docs/api-reference/project/get-projects.mdx
================================================
---
title: 'Get Projects'
openapi: get /api/v1/orgs/organizations/{org_id}/projects/
---


================================================
FILE: docs/api-reference/webhook/create-webhook.mdx
================================================
---
title: 'Create Webhook'
openapi: post /api/v1/webhooks/projects/{project_id}/
---

## Create Webhook

Create a webhook by providing the project ID and the webhook details.




================================================
FILE: docs/api-reference/webhook/delete-webhook.mdx
================================================
---
title: 'Delete Webhook'
openapi: delete /api/v1/webhooks/{webhook_id}/
---

## Delete Webhook

Delete a webhook by providing the webhook ID.



================================================
FILE: docs/api-reference/webhook/get-webhook.mdx
================================================
---
title: 'Get Webhook'
openapi: get /api/v1/webhooks/projects/{project_id}/
---

## Get Webhook

Get a webhook by providing the project ID.




================================================
FILE: docs/api-reference/webhook/update-webhook.mdx
================================================
---
title: 'Update Webhook'
openapi: put /api/v1/webhooks/{webhook_id}/
---

## Update Webhook

Update a webhook by providing the webhook ID and the fields to update.




================================================
FILE: docs/components/embedders/config.mdx
================================================
---
title: Configurations
icon: "gear"
iconType: "solid"
---


Config in mem0 is a dictionary that specifies the settings for your embedding models. It allows you to customize the behavior and connection details of your chosen embedder.

## How to define configurations?

The config is defined as an object (or dictionary) with two main keys:
- `embedder`: Specifies the embedder provider and its configuration
  - `provider`: The name of the embedder (e.g., "openai", "ollama")
  - `config`: A nested object or dictionary containing provider-specific settings


## How to use configurations?

Here's a general example of how to use the config with mem0:

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "embedder": {
        "provider": "your_chosen_provider",
        "config": {
            # Provider-specific settings go here
        }
    }
}

m = Memory.from_config(config)
m.add("Your text here", user_id="user", metadata={"category": "example"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  embedder: {
    provider: 'openai',
    config: {
      apiKey: process.env.OPENAI_API_KEY || '',
      model: 'text-embedding-3-small',
      // Provider-specific settings go here
    },
  },
};

const memory = new Memory(config);
await memory.add("Your text here", { userId: "user", metadata: { category: "example" } });
```
</CodeGroup>

## Why is Config Needed?

Config is essential for:
1. Specifying which embedding model to use.
2. Providing necessary connection details (e.g., model, api_key, embedding_dims).
3. Ensuring proper initialization and connection to your chosen embedder.

## Master List of All Params in Config

Here's a comprehensive list of all parameters that can be used across different embedders:

<Tabs>
<Tab title="Python">
| Parameter | Description | Provider |
|-----------|-------------|----------|
| `model` | Embedding model to use | All |
| `api_key` | API key of the provider | All |
| `embedding_dims` | Dimensions of the embedding model | All |
| `http_client_proxies` | Allow proxy server settings | All |
| `ollama_base_url` | Base URL for the Ollama embedding model | Ollama |
| `model_kwargs` | Key-Value arguments for the Huggingface embedding model | Huggingface |
| `azure_kwargs` | Key-Value arguments for the AzureOpenAI embedding model | Azure OpenAI |
| `openai_base_url`    | Base URL for OpenAI API                       | OpenAI            |
| `vertex_credentials_json` | Path to the Google Cloud credentials JSON file for VertexAI                       | VertexAI            |
| `memory_add_embedding_type` | The type of embedding to use for the add memory action                       | VertexAI            |
| `memory_update_embedding_type` | The type of embedding to use for the update memory action                       | VertexAI            |
| `memory_search_embedding_type` | The type of embedding to use for the search memory action                       | VertexAI            |
| `lmstudio_base_url` | Base URL for LM Studio API                    | LM Studio         |
</Tab>
<Tab title="TypeScript">
| Parameter | Description | Provider |
|-----------|-------------|----------|
| `model` | Embedding model to use | All |
| `apiKey` | API key of the provider | All |
| `embeddingDims` | Dimensions of the embedding model | All |
</Tab>
</Tabs>

## Supported Embedding Models

For detailed information on configuring specific embedders, please visit the [Embedding Models](./models) section. There you'll find information for each supported embedder with provider-specific usage examples and configuration details.



================================================
FILE: docs/components/embedders/overview.mdx
================================================
---
title: Overview
icon: "info"
iconType: "solid"
---

Mem0 offers support for various embedding models, allowing users to choose the one that best suits their needs.

## Supported Embedders

See the list of supported embedders below.

<Note>
  The following embedders are supported in the Python implementation. The TypeScript implementation currently only supports OpenAI.
</Note>

<CardGroup cols={4}>
  <Card title="OpenAI" href="/components/embedders/models/openai"></Card>
  <Card title="Azure OpenAI" href="/components/embedders/models/azure_openai"></Card>
  <Card title="Ollama" href="/components/embedders/models/ollama"></Card>
  <Card title="Hugging Face" href="/components/embedders/models/huggingface"></Card>
  <Card title="Google AI" href="/components/embedders/models/google_AI"></Card>
  <Card title="Vertex AI" href="/components/embedders/models/vertexai"></Card>
  <Card title="Together" href="/components/embedders/models/together"></Card>
  <Card title="LM Studio" href="/components/embedders/models/lmstudio"></Card>
  <Card title="Langchain" href="/components/embedders/models/langchain"></Card>
  <Card title="AWS Bedrock" href="/components/embedders/models/aws_bedrock"></Card>
</CardGroup>

## Usage

To utilize a embedder, you must provide a configuration to customize its usage. If no configuration is supplied, a default configuration will be applied, and `OpenAI` will be used as the embedder.

For a comprehensive list of available parameters for embedder configuration, please refer to [Config](./config).



================================================
FILE: docs/components/embedders/models/aws_bedrock.mdx
================================================
---
title: AWS Bedrock
---

To use AWS Bedrock embedding models, you need to have the appropriate AWS credentials and permissions. The embeddings implementation relies on the `boto3` library.

### Setup
- Ensure you have model access from the [AWS Bedrock Console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess)
- Authenticate the boto3 client using a method described in the [AWS documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)
- Set up environment variables for authentication:
  ```bash
  export AWS_REGION=us-east-1
  export AWS_ACCESS_KEY_ID=your-access-key
  export AWS_SECRET_ACCESS_KEY=your-secret-key
  ```

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

# For LLM if needed
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# AWS credentials
os.environ["AWS_REGION"] = "us-west-2"
os.environ["AWS_ACCESS_KEY_ID"] = "your-access-key"
os.environ["AWS_SECRET_ACCESS_KEY"] = "your-secret-key"

config = {
    "embedder": {
        "provider": "aws_bedrock",
        "config": {
            "model": "amazon.titan-embed-text-v2:0"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice")
```
</CodeGroup>

### Config

Here are the parameters available for configuring AWS Bedrock embedder:

<Tabs>
<Tab title="Python">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the embedding model to use | `amazon.titan-embed-text-v1` |
</Tab>
</Tabs>



================================================
FILE: docs/components/embedders/models/azure_openai.mdx
================================================
---
title: Azure OpenAI
---

To use Azure OpenAI embedding models, set the `EMBEDDING_AZURE_OPENAI_API_KEY`, `EMBEDDING_AZURE_DEPLOYMENT`, `EMBEDDING_AZURE_ENDPOINT` and `EMBEDDING_AZURE_API_VERSION` environment variables. You can obtain the Azure OpenAI API key from the Azure.

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["EMBEDDING_AZURE_OPENAI_API_KEY"] = "your-api-key"
os.environ["EMBEDDING_AZURE_DEPLOYMENT"] = "your-deployment-name"
os.environ["EMBEDDING_AZURE_ENDPOINT"] = "your-api-base-url"
os.environ["EMBEDDING_AZURE_API_VERSION"] = "version-to-use"

os.environ["OPENAI_API_KEY"] = "your_api_key" # For LLM


config = {
    "embedder": {
        "provider": "azure_openai",
        "config": {
            "model": "text-embedding-3-large"
            "azure_kwargs": {
                  "api_version": "",
                  "azure_deployment": "",
                  "azure_endpoint": "",
                  "api_key": "",
                  "default_headers": {
                    "CustomHeader": "your-custom-header",
                  }
              }
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="john")
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
    embedder: {
        provider: "azure_openai",
        config: {
            model: "text-embedding-3-large",
            modelProperties: {
                endpoint: "your-api-base-url",
                deployment: "your-deployment-name",
                apiVersion: "version-to-use",
            }
        }
    }
}

const memory = new Memory(config);

const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]

await memory.add(messages, { userId: "john" });
```
</CodeGroup>

As an alternative to using an API key, the Azure Identity credential chain can be used to authenticate with [Azure OpenAI role-based security](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/role-based-access-control). 

<Note> If an API key is provided, it will be used for authentication over an Azure Identity </Note>

Below is a sample configuration for using Mem0 with Azure OpenAI and Azure Identity:

```python
import os
from mem0 import Memory
# You can set the values directly in the config dictionary or use environment variables

os.environ["LLM_AZURE_DEPLOYMENT"] = "your-deployment-name"
os.environ["LLM_AZURE_ENDPOINT"] = "your-api-base-url"
os.environ["LLM_AZURE_API_VERSION"] = "version-to-use"

config = {
    "llm": {
        "provider": "azure_openai_structured",
        "config": {
            "model": "your-deployment-name",
            "temperature": 0.1,
            "max_tokens": 2000,
            "azure_kwargs": {
                  "azure_deployment": "<your-deployment-name>",
                  "api_version": "<version-to-use>",
                  "azure_endpoint": "<your-api-base-url>",
                  "default_headers": {
                    "CustomHeader": "your-custom-header",
                  }
              }
        }
    }
}
```

Refer to [Azure Identity troubleshooting tips](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/TROUBLESHOOTING.md#troubleshoot-environmentcredential-authentication-issues) for setting up an Azure Identity credential.

### Config

Here are the parameters available for configuring Azure OpenAI embedder:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the embedding model to use | `text-embedding-3-small` |
| `embedding_dims` | Dimensions of the embedding model | `1536` |
| `azure_kwargs` | The Azure OpenAI configs | `config_keys` |



================================================
FILE: docs/components/embedders/models/google_AI.mdx
================================================
---
title: Google AI
---

To use Google AI embedding models, set the `GOOGLE_API_KEY` environment variables. You can obtain the Gemini API key from [here](https://aistudio.google.com/app/apikey).

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["GOOGLE_API_KEY"] = "key"
os.environ["OPENAI_API_KEY"] = "your_api_key" # For LLM

config = {
    "embedder": {
        "provider": "gemini",
        "config": {
            "model": "models/text-embedding-004",
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="john")
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  embedder: {
    provider: 'google',
    config: {
      apiKey: process.env.GOOGLE_API_KEY || '',
      model: 'text-embedding-004',
      // The output dimensionality is fixed at 768 for Google AI embeddings
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "john" });
```
</CodeGroup>

### Config

Here are the parameters available for configuring Gemini embedder:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the embedding model to use | `models/text-embedding-004` |
| `embedding_dims` | Dimensions of the embedding model (output_dimensionality will be considered as embedding_dims, so please set embedding_dims accordingly) | `768` |
| `api_key` | The Google API key | `None` |



================================================
FILE: docs/components/embedders/models/huggingface.mdx
================================================
---
title: Hugging Face
---

You can use embedding models from Huggingface to run Mem0 locally.

### Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your_api_key" # For LLM

config = {
    "embedder": {
        "provider": "huggingface",
        "config": {
            "model": "multi-qa-MiniLM-L6-cos-v1"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="john")
```

### Using Text Embeddings Inference (TEI)

You can also use Hugging Face's Text Embeddings Inference service for faster and more efficient embeddings:

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your_api_key" # For LLM

# Using HuggingFace Text Embeddings Inference API
config = {
    "embedder": {
        "provider": "huggingface",
        "config": {
            "huggingface_base_url": "http://localhost:3000/v1"
        }
    }
}

m = Memory.from_config(config)
m.add("This text will be embedded using the TEI service.", user_id="john")
```

To run the TEI service, you can use Docker:

```bash
docker run -d -p 3000:80 -v huggingfacetei:/data --platform linux/amd64 \
    ghcr.io/huggingface/text-embeddings-inference:cpu-1.6 \
    --model-id BAAI/bge-small-en-v1.5
```

### Config

Here are the parameters available for configuring Huggingface embedder:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the model to use | `multi-qa-MiniLM-L6-cos-v1` |
| `embedding_dims` | Dimensions of the embedding model | `selected_model_dimensions` |
| `model_kwargs` | Additional arguments for the model | `None` |
| `huggingface_base_url` | URL to connect to Text Embeddings Inference (TEI) API | `None` |


================================================
FILE: docs/components/embedders/models/langchain.mdx
================================================
---
title: LangChain
---

Mem0 supports LangChain as a provider to access a wide range of embedding models. LangChain is a framework for developing applications powered by language models, making it easy to integrate various embedding providers through a consistent interface.

For a complete list of available embedding models supported by LangChain, refer to the [LangChain Text Embedding documentation](https://python.langchain.com/docs/integrations/text_embedding/).

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory
from langchain_openai import OpenAIEmbeddings

# Set necessary environment variables for your chosen LangChain provider
os.environ["OPENAI_API_KEY"] = "your-api-key"

# Initialize a LangChain embeddings model directly
openai_embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    dimensions=1536
)

# Pass the initialized model to the config
config = {
    "embedder": {
        "provider": "langchain",
        "config": {
            "model": openai_embeddings
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';
import { OpenAIEmbeddings } from "@langchain/openai";

// Initialize a LangChain embeddings model directly
const openaiEmbeddings = new OpenAIEmbeddings({
    modelName: "text-embedding-3-small",
    dimensions: 1536,
    apiKey: process.env.OPENAI_API_KEY,
});

const config = {
  embedder: {
    provider: 'langchain',
    config: {
      model: openaiEmbeddings,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

## Supported LangChain Embedding Providers

LangChain supports a wide range of embedding providers, including:

- OpenAI (`OpenAIEmbeddings`)
- Cohere (`CohereEmbeddings`)
- Google (`VertexAIEmbeddings`)
- Hugging Face (`HuggingFaceEmbeddings`)
- Sentence Transformers (`HuggingFaceEmbeddings`)
- Azure OpenAI (`AzureOpenAIEmbeddings`)
- Ollama (`OllamaEmbeddings`)
- Together (`TogetherEmbeddings`)
- And many more

You can use any of these model instances directly in your configuration. For a complete and up-to-date list of available embedding providers, refer to the [LangChain Text Embedding documentation](https://python.langchain.com/docs/integrations/text_embedding/).

## Provider-Specific Configuration

When using LangChain as an embedder provider, you'll need to:

1. Set the appropriate environment variables for your chosen embedding provider
2. Import and initialize the specific model class you want to use
3. Pass the initialized model instance to the config

### Examples with Different Providers

<CodeGroup>
#### HuggingFace Embeddings

```python Python
from langchain_huggingface import HuggingFaceEmbeddings

# Initialize a HuggingFace embeddings model
hf_embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-en-v1.5",
    encode_kwargs={"normalize_embeddings": True}
)

config = {
    "embedder": {
        "provider": "langchain",
        "config": {
            "model": hf_embeddings
        }
    }
}
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';
import { HuggingFaceEmbeddings } from "@langchain/community/embeddings/hf";

// Initialize a HuggingFace embeddings model
const hfEmbeddings = new HuggingFaceEmbeddings({
    modelName: "BAAI/bge-small-en-v1.5",
    encode: {
        normalize_embeddings: true,
    },
});

const config = {
  embedder: {
    provider: 'langchain',
    config: {
      model: hfEmbeddings,
    },
  },
};
```
</CodeGroup>

<CodeGroup>
#### Ollama Embeddings

```python Python
from langchain_ollama import OllamaEmbeddings

# Initialize an Ollama embeddings model
ollama_embeddings = OllamaEmbeddings(
    model="nomic-embed-text"
)

config = {
    "embedder": {
        "provider": "langchain",
        "config": {
            "model": ollama_embeddings
        }
    }
}
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';
import { OllamaEmbeddings } from "@langchain/community/embeddings/ollama";

// Initialize an Ollama embeddings model
const ollamaEmbeddings = new OllamaEmbeddings({
    model: "nomic-embed-text",
    baseUrl: "http://localhost:11434", // Ollama server URL
});

const config = {
  embedder: {
    provider: 'langchain',
    config: {
      model: ollamaEmbeddings,
    },
  },
};
```
</CodeGroup>

<Note>
  Make sure to install the necessary LangChain packages and any provider-specific dependencies.
</Note>

## Config

All available parameters for the `langchain` embedder config are present in [Master List of All Params in Config](../config).



================================================
FILE: docs/components/embedders/models/lmstudio.mdx
================================================
You can use embedding models from LM Studio to run Mem0 locally.

### Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your_api_key" # For LLM

config = {
    "embedder": {
        "provider": "lmstudio",
        "config": {
            "model": "nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.f16.gguf"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="john")
```

### Config

Here are the parameters available for configuring Ollama embedder:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the OpenAI model to use | `nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.f16.gguf` |
| `embedding_dims` | Dimensions of the embedding model | `1536` |
| `lmstudio_base_url` | Base URL for LM Studio connection | `http://localhost:1234/v1` |


================================================
FILE: docs/components/embedders/models/ollama.mdx
================================================
You can use embedding models from Ollama to run Mem0 locally.

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your_api_key" # For LLM

config = {
    "embedder": {
        "provider": "ollama",
        "config": {
            "model": "mxbai-embed-large"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="john")
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  embedder: {
    provider: 'ollama',
    config: {
      model: 'nomic-embed-text:latest', // or any other Ollama embedding model
      url: 'http://localhost:11434', // Ollama server URL
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "john" });
```
</CodeGroup>

### Config

Here are the parameters available for configuring Ollama embedder:

<Tabs>
<Tab title="Python">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the Ollama model to use | `nomic-embed-text` |
| `embedding_dims` | Dimensions of the embedding model | `512` |
| `ollama_base_url` | Base URL for ollama connection | `None` |
</Tab>
<Tab title="TypeScript">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the Ollama model to use | `nomic-embed-text:latest` |
| `url` | Base URL for Ollama server | `http://localhost:11434` |
</Tab>
</Tabs>


================================================
FILE: docs/components/embedders/models/openai.mdx
================================================
---
title: OpenAI
---

To use OpenAI embedding models, set the `OPENAI_API_KEY` environment variable. You can obtain the OpenAI API key from the [OpenAI Platform](https://platform.openai.com/account/api-keys).

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your_api_key"

config = {
    "embedder": {
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-large"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="john")
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  embedder: {
    provider: 'openai',
    config: {
      apiKey: 'your-openai-api-key',
      model: 'text-embedding-3-large',
    },
  },
};

const memory = new Memory(config);
await memory.add("I'm visiting Paris", { userId: "john" });
```
</CodeGroup>

### Config

Here are the parameters available for configuring OpenAI embedder:

<Tabs>
<Tab title="Python">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the embedding model to use | `text-embedding-3-small` |
| `embedding_dims` | Dimensions of the embedding model | `1536` |
| `api_key` | The OpenAI API key | `None` |
</Tab>
<Tab title="TypeScript">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the embedding model to use | `text-embedding-3-small` |
| `embeddingDims` | Dimensions of the embedding model | `1536` |
| `apiKey` | The OpenAI API key | `None` |
</Tab>
</Tabs>



================================================
FILE: docs/components/embedders/models/together.mdx
================================================
---
title: Together
---

To use Together embedding models, set the `TOGETHER_API_KEY` environment variable. You can obtain the Together API key from the [Together Platform](https://api.together.xyz/settings/api-keys).

### Usage

<Note> The `embedding_model_dims` parameter for `vector_store` should be set to `768` for Together embedder. </Note>

```python
import os
from mem0 import Memory

os.environ["TOGETHER_API_KEY"] = "your_api_key"
os.environ["OPENAI_API_KEY"] = "your_api_key" # For LLM

config = {
    "embedder": {
        "provider": "together",
        "config": {
            "model": "togethercomputer/m2-bert-80M-8k-retrieval"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="john")
```

### Config

Here are the parameters available for configuring Together embedder:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the embedding model to use | `togethercomputer/m2-bert-80M-8k-retrieval` |
| `embedding_dims` | Dimensions of the embedding model | `768` |
| `api_key` | The Together API key | `None` |



================================================
FILE: docs/components/embedders/models/vertexai.mdx
================================================
### Vertex AI

To use Google Cloud's Vertex AI for text embedding models, set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to the path of your service account's credentials JSON file. These credentials can be created in the [Google Cloud Console](https://console.cloud.google.com/).

### Usage

```python
import os
from mem0 import Memory

# Set the path to your Google Cloud credentials JSON file
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/path/to/your/credentials.json"
os.environ["OPENAI_API_KEY"] = "your_api_key" # For LLM

config = {
    "embedder": {
        "provider": "vertexai",
        "config": {
            "model": "text-embedding-004",
            "memory_add_embedding_type": "RETRIEVAL_DOCUMENT",
            "memory_update_embedding_type": "RETRIEVAL_DOCUMENT",
            "memory_search_embedding_type": "RETRIEVAL_QUERY"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="john")
```
The embedding types can be one of the following:
- SEMANTIC_SIMILARITY
- CLASSIFICATION
- CLUSTERING
- RETRIEVAL_DOCUMENT, RETRIEVAL_QUERY, QUESTION_ANSWERING, FACT_VERIFICATION
- CODE_RETRIEVAL_QUERY  
Check out the [Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types#supported_task_types) for more information.  
  
### Config

Here are the parameters available for configuring the Vertex AI embedder:

| Parameter                 | Description                                      | Default Value        |
| ------------------------- | ------------------------------------------------ | -------------------- |
| `model`                   | The name of the Vertex AI embedding model to use | `text-embedding-004` |
| `vertex_credentials_json` | Path to the Google Cloud credentials JSON file   | `None`               |
| `embedding_dims`          | Dimensions of the embedding model                | `256`                |
| `memory_add_embedding_type` | The type of embedding to use for the add memory action | `RETRIEVAL_DOCUMENT` |
| `memory_update_embedding_type` | The type of embedding to use for the update memory action | `RETRIEVAL_DOCUMENT` |
| `memory_search_embedding_type` | The type of embedding to use for the search memory action | `RETRIEVAL_QUERY` |



================================================
FILE: docs/components/llms/config.mdx
================================================
---
title: Configurations
icon: "gear"
iconType: "solid"
---

## How to define configurations?

<Tabs>
  <Tab title="Python">
    The `config` is defined as a Python dictionary with two main keys:
    - `llm`: Specifies the llm provider and its configuration
      - `provider`: The name of the llm (e.g., "openai", "groq")
      - `config`: A nested dictionary containing provider-specific settings
  </Tab>
  <Tab title="TypeScript">
    The `config` is defined as a TypeScript object with these keys:
    - `llm`: Specifies the LLM provider and its configuration (required)
      - `provider`: The name of the LLM (e.g., "openai", "groq")
      - `config`: A nested object containing provider-specific settings
    - `embedder`: Specifies the embedder provider and its configuration (optional)
    - `vectorStore`: Specifies the vector store provider and its configuration (optional)
    - `historyDbPath`: Path to the history database file (optional)
  </Tab>
</Tabs>

### Config Values Precedence

Config values are applied in the following order of precedence (from highest to lowest):

1. Values explicitly set in the `config` object/dictionary
2. Environment variables (e.g., `OPENAI_API_KEY`, `OPENAI_BASE_URL`)
3. Default values defined in the LLM implementation

This means that values specified in the `config` will override corresponding environment variables, which in turn override default values.

## How to Use Config

Here's a general example of how to use the config with Mem0:

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx" # for embedder

config = {
    "llm": {
        "provider": "your_chosen_provider",
        "config": {
            # Provider-specific settings go here
        }
    }
}

m = Memory.from_config(config)
m.add("Your text here", user_id="user", metadata={"category": "example"})

```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

// Minimal configuration with just the LLM settings
const config = {
  llm: {
    provider: 'your_chosen_provider',
    config: {
      // Provider-specific settings go here
    }
  }
};

const memory = new Memory(config);
await memory.add("Your text here", { userId: "user123", metadata: { category: "example" } });
```

</CodeGroup>

## Why is Config Needed?

Config is essential for:
1. Specifying which LLM to use.
2. Providing necessary connection details (e.g., model, api_key, temperature).
3. Ensuring proper initialization and connection to your chosen LLM.

## Master List of All Params in Config

Here's a comprehensive list of all parameters that can be used across different LLMs:

<Tabs>
  <Tab title="Python">
    | Parameter            | Description                                   | Provider          |
    |----------------------|-----------------------------------------------|-------------------|
    | `model`              | Embedding model to use                        | All               |
    | `temperature`        | Temperature of the model                      | All               |
    | `api_key`            | API key to use                                | All               |
    | `max_tokens`         | Tokens to generate                            | All               |
    | `top_p`              | Probability threshold for nucleus sampling    | All               |
    | `top_k`              | Number of highest probability tokens to keep  | All               |
    | `http_client_proxies`| Allow proxy server settings                   | AzureOpenAI       |
    | `models`             | List of models                                | Openrouter        |
    | `route`              | Routing strategy                              | Openrouter        |
    | `openrouter_base_url`| Base URL for Openrouter API                   | Openrouter        |
    | `site_url`           | Site URL                                      | Openrouter        |
    | `app_name`           | Application name                              | Openrouter        |
    | `ollama_base_url`    | Base URL for Ollama API                       | Ollama            |
    | `openai_base_url`    | Base URL for OpenAI API                       | OpenAI            |
    | `azure_kwargs`       | Azure LLM args for initialization             | AzureOpenAI       |
    | `deepseek_base_url`  | Base URL for DeepSeek API                     | DeepSeek          |
    | `xai_base_url`       | Base URL for XAI API                          | XAI               |
    | `sarvam_base_url`    | Base URL for Sarvam API                       | Sarvam            |
    | `reasoning_effort`   | Reasoning level (low, medium, high)           | Sarvam            |
    | `frequency_penalty`  | Penalize frequent tokens (-2.0 to 2.0)        | Sarvam            |
    | `presence_penalty`   | Penalize existing tokens (-2.0 to 2.0)        | Sarvam            |
    | `seed`               | Seed for deterministic sampling               | Sarvam            |
    | `stop`               | Stop sequences (max 4)                        | Sarvam            |
    | `lmstudio_base_url`  | Base URL for LM Studio API                    | LM Studio         |
    | `response_callback`  | LLM response callback function                | OpenAI            |
  </Tab>
  <Tab title="TypeScript">
    | Parameter            | Description                                   | Provider          |
    |----------------------|-----------------------------------------------|-------------------|
    | `model`              | Embedding model to use                        | All               |
    | `temperature`        | Temperature of the model                      | All               |
    | `apiKey`             | API key to use                                | All               |
    | `maxTokens`          | Tokens to generate                            | All               |
    | `topP`               | Probability threshold for nucleus sampling    | All               |
    | `topK`               | Number of highest probability tokens to keep  | All               |
    | `openaiBaseUrl`      | Base URL for OpenAI API                       | OpenAI            |
  </Tab>
</Tabs>

## Supported LLMs

For detailed information on configuring specific LLMs, please visit the [LLMs](./models) section. There you'll find information for each supported LLM with provider-specific usage examples and configuration details.



================================================
FILE: docs/components/llms/overview.mdx
================================================
---
title: Overview
icon: "info"
iconType: "solid"
---

Mem0 includes built-in support for various popular large language models. Memory can utilize the LLM provided by the user, ensuring efficient use for specific needs.

## Usage

To use a llm, you must provide a configuration to customize its usage. If no configuration is supplied, a default configuration will be applied, and `OpenAI` will be used as the llm.

For a comprehensive list of available parameters for llm configuration, please refer to [Config](./config).

## Supported LLMs

See the list of supported LLMs below.

<Note>
  All LLMs are supported in Python. The following LLMs are also supported in TypeScript: **OpenAI**, **Anthropic**, and **Groq**.
</Note>

<CardGroup cols={4}>
  <Card title="OpenAI" href="/components/llms/models/openai" />
  <Card title="Ollama" href="/components/llms/models/ollama" />
  <Card title="Azure OpenAI" href="/components/llms/models/azure_openai" />
  <Card title="Anthropic" href="/components/llms/models/anthropic" />
  <Card title="Together" href="/components/llms/models/together" />
  <Card title="Groq" href="/components/llms/models/groq" />
  <Card title="Litellm" href="/components/llms/models/litellm" />
  <Card title="Mistral AI" href="/components/llms/models/mistral_ai" />
  <Card title="Google AI" href="/components/llms/models/google_ai" />
  <Card title="AWS bedrock" href="/components/llms/models/aws_bedrock" />
  <Card title="DeepSeek" href="/components/llms/models/deepseek" />
  <Card title="xAI" href="/components/llms/models/xAI" />
  <Card title="Sarvam AI" href="/components/llms/models/sarvam" />
  <Card title="LM Studio" href="/components/llms/models/lmstudio" />
  <Card title="Langchain" href="/components/llms/models/langchain" />
</CardGroup>

## Structured vs Unstructured Outputs

Mem0 supports two types of OpenAI LLM formats, each with its own strengths and use cases:

### Structured Outputs

Structured outputs are LLMs that align with OpenAI's structured outputs model:

- **Optimized for:** Returning structured responses (e.g., JSON objects)
- **Benefits:** Precise, easily parseable data
- **Ideal for:** Data extraction, form filling, API responses
- **Learn more:** [OpenAI Structured Outputs Guide](https://platform.openai.com/docs/guides/structured-outputs/introduction)

### Unstructured Outputs

Unstructured outputs correspond to OpenAI's standard, free-form text model:

- **Flexibility:** Returns open-ended, natural language responses
- **Customization:** Use the `response_format` parameter to guide output
- **Trade-off:** Less efficient than structured outputs for specific data needs
- **Best for:** Creative writing, explanations, general conversation

Choose the format that best suits your application's requirements for optimal performance and usability.



================================================
FILE: docs/components/llms/models/anthropic.mdx
================================================
---
title: Anthropic
---


To use Anthropic's models, please set the `ANTHROPIC_API_KEY` which you find on their [Account Settings Page](https://console.anthropic.com/account/keys).

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "anthropic",
        "config": {
            "model": "claude-sonnet-4-20250514",
            "temperature": 0.1,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  llm: {
    provider: 'anthropic',
    config: {
      apiKey: process.env.ANTHROPIC_API_KEY || '',
      model: 'claude-sonnet-4-20250514',
      temperature: 0.1,
      maxTokens: 2000,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

## Config

All available parameters for the `anthropic` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/aws_bedrock.mdx
================================================
---
title: AWS Bedrock
---

### Setup
- Before using the AWS Bedrock LLM, make sure you have the appropriate model access from [Bedrock Console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess).
- You will also need to authenticate the `boto3` client by using a method in the [AWS documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#configuring-credentials)
- You will have to export `AWS_REGION`, `AWS_ACCESS_KEY`, and `AWS_SECRET_ACCESS_KEY` to set environment variables.

### Usage

```python
import os
from mem0 import Memory

os.environ['AWS_REGION'] = 'us-west-2'
os.environ["AWS_ACCESS_KEY_ID"] = "xx"
os.environ["AWS_SECRET_ACCESS_KEY"] = "xx"

config = {
    "llm": {
        "provider": "aws_bedrock",
        "config": {
            "model": "anthropic.claude-3-5-haiku-20241022-v1:0",
            "temperature": 0.2,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

All available parameters for the `aws_bedrock` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/azure_openai.mdx
================================================
---
title: Azure OpenAI
---

<Note> Mem0 Now Supports Azure OpenAI Models in TypeScript SDK </Note>

To use Azure OpenAI models, you have to set the `LLM_AZURE_OPENAI_API_KEY`, `LLM_AZURE_ENDPOINT`, `LLM_AZURE_DEPLOYMENT` and `LLM_AZURE_API_VERSION` environment variables. You can obtain the Azure API key from the [Azure](https://azure.microsoft.com/).

Optionally, you can use Azure Identity to authenticate with Azure OpenAI, which allows you to use managed identities or service principals for production and Azure CLI login for development instead of an API key. If an Azure Identity is to be used, ***do not*** set the `LLM_AZURE_OPENAI_API_KEY` environment variable or the api_key in the config dictionary.

> **Note**: The following are currently unsupported with reasoning models `Parallel tool calling`,`temperature`, `top_p`, `presence_penalty`, `frequency_penalty`, `logprobs`, `top_logprobs`, `logit_bias`, `max_tokens`


## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model

os.environ["LLM_AZURE_OPENAI_API_KEY"] = "your-api-key"
os.environ["LLM_AZURE_DEPLOYMENT"] = "your-deployment-name"
os.environ["LLM_AZURE_ENDPOINT"] = "your-api-base-url"
os.environ["LLM_AZURE_API_VERSION"] = "version-to-use"

config = {
    "llm": {
        "provider": "azure_openai",
        "config": {
            "model": "your-deployment-name",
            "temperature": 0.1,
            "max_tokens": 2000,
            "azure_kwargs": {
                  "azure_deployment": "",
                  "api_version": "",
                  "azure_endpoint": "",
                  "api_key": "",
                  "default_headers": {
                    "CustomHeader": "your-custom-header",
                  }
              }
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  llm: {
    provider: 'azure_openai',
    config: {
      apiKey: process.env.AZURE_OPENAI_API_KEY || '',
      modelProperties: {
        endpoint: 'https://your-api-base-url',
        deployment: 'your-deployment-name',
        modelName: 'your-model-name',
        apiVersion: 'version-to-use',
        // Any other parameters you want to pass to the Azure OpenAI API
      },
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>


We also support the new [OpenAI structured-outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction) model. Typescript SDK does not support the `azure_openai_structured` model yet.

```python
import os
from mem0 import Memory

os.environ["LLM_AZURE_OPENAI_API_KEY"] = "your-api-key"
os.environ["LLM_AZURE_DEPLOYMENT"] = "your-deployment-name"
os.environ["LLM_AZURE_ENDPOINT"] = "your-api-base-url"
os.environ["LLM_AZURE_API_VERSION"] = "version-to-use"

config = {
    "llm": {
        "provider": "azure_openai_structured",
        "config": {
            "model": "your-deployment-name",
            "temperature": 0.1,
            "max_tokens": 2000,
            "azure_kwargs": {
                  "azure_deployment": "",
                  "api_version": "",
                  "azure_endpoint": "",
                  "api_key": "",
                  "default_headers": {
                    "CustomHeader": "your-custom-header",
                  }
              }
        }
    }
}
```

As an alternative to using an API key, the Azure Identity credential chain can be used to authenticate with [Azure OpenAI role-based security](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/role-based-access-control). 

<Note> If an API key is provided, it will be used for authentication over an Azure Identity </Note>

Below is a sample configuration for using Mem0 with Azure OpenAI and Azure Identity:

```python
import os
from mem0 import Memory
# You can set the values directly in the config dictionary or use environment variables

os.environ["LLM_AZURE_DEPLOYMENT"] = "your-deployment-name"
os.environ["LLM_AZURE_ENDPOINT"] = "your-api-base-url"
os.environ["LLM_AZURE_API_VERSION"] = "version-to-use"

config = {
    "llm": {
        "provider": "azure_openai_structured",
        "config": {
            "model": "your-deployment-name",
            "temperature": 0.1,
            "max_tokens": 2000,
            "azure_kwargs": {
                  "azure_deployment": "<your-deployment-name>",
                  "api_version": "<version-to-use>",
                  "azure_endpoint": "<your-api-base-url>",
                  "default_headers": {
                    "CustomHeader": "your-custom-header",
                  }
              }
        }
    }
}
```

Refer to [Azure Identity troubleshooting tips](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/TROUBLESHOOTING.md#troubleshoot-environmentcredential-authentication-issues) for setting up an Azure Identity credential.


## Config

All available parameters for the `azure_openai` config are present in [Master List of All Params in Config](../config).



================================================
FILE: docs/components/llms/models/deepseek.mdx
================================================
---
title: DeepSeek
---

To use DeepSeek LLM models, you have to set the `DEEPSEEK_API_KEY` environment variable. You can also optionally set `DEEPSEEK_API_BASE` if you need to use a different API endpoint (defaults to "https://api.deepseek.com").

## Usage

```python
import os
from mem0 import Memory

os.environ["DEEPSEEK_API_KEY"] = "your-api-key"
os.environ["OPENAI_API_KEY"] = "your-api-key" # for embedder model

config = {
    "llm": {
        "provider": "deepseek",
        "config": {
            "model": "deepseek-chat",  # default model
            "temperature": 0.2,
            "max_tokens": 2000,
            "top_p": 1.0
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

You can also configure the API base URL in the config:

```python
config = {
    "llm": {
        "provider": "deepseek",
        "config": {
            "model": "deepseek-chat",
            "deepseek_base_url": "https://your-custom-endpoint.com",
            "api_key": "your-api-key"  # alternatively to using environment variable
        }
    }
}
```

## Config

All available parameters for the `deepseek` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/google_AI.mdx
================================================
---
title: Google AI
---

To use the Gemini model, set the `GOOGLE_API_KEY` environment variable. You can obtain the Google/Gemini API key from [Google AI Studio](https://aistudio.google.com/app/apikey).

> **Note:** As of the latest release, Mem0 uses the new `google.genai` SDK instead of the deprecated `google.generativeai`. All message formatting and model interaction now use the updated `types` module from `google.genai`.

> **Note:** Some Gemini models are being deprecated and will retire soon. It is recommended to migrate to the latest stable models like `"gemini-2.0-flash-001"` or `"gemini-2.0-flash-lite-001"` to ensure ongoing support and improvements.

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-openai-api-key"  # Used for embedding model
os.environ["GOOGLE_API_KEY"] = "your-gemini-api-key"

config = {
    "llm": {
        "provider": "gemini",
        "config": {
            "model": "gemini-2.0-flash-001",
            "temperature": 0.2,
            "max_tokens": 2000,
            "top_p": 1.0
        }
    }
}

m = Memory.from_config(config)

messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thrillers, but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thrillers and suggest sci-fi movies instead."}
]

m.add(messages, user_id="alice", metadata={"category": "movies"})

```
```typescript TypeScript
import { Memory } from "mem0ai/oss";

const config = {
    llm: {
        // You can also use "google" as provider ( for backward compatibility )
        provider: "gemini",
        config: {
            model: "gemini-2.0-flash-001",
            temperature: 0.1
        }
    }
}

const memory = new Memory(config);

const messages = [
    { role: "user", content: "I'm planning to watch a movie tonight. Any recommendations?" },
    { role: "assistant", content: "How about thriller movies? They can be quite engaging." },
    { role: "user", content: "I’m not a big fan of thrillers, but I love sci-fi movies." },
    { role: "assistant", content: "Got it! I'll avoid thrillers and suggest sci-fi movies instead." }
]

await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

## Config

All available parameters for the `Gemini` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/groq.mdx
================================================
---
title: Groq
---

[Groq](https://groq.com/) is the creator of the world's first Language Processing Unit (LPU), providing exceptional speed performance for AI workloads running on their LPU Inference Engine.

In order to use LLMs from Groq, go to their [platform](https://console.groq.com/keys) and get the API key. Set the API key as `GROQ_API_KEY` environment variable to use the model as given below in the example.

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model
os.environ["GROQ_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "groq",
        "config": {
            "model": "mixtral-8x7b-32768",
            "temperature": 0.1,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  llm: {
    provider: 'groq',
    config: {
      apiKey: process.env.GROQ_API_KEY || '',
      model: 'mixtral-8x7b-32768',
      temperature: 0.1,
      maxTokens: 1000,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

## Config

All available parameters for the `groq` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/langchain.mdx
================================================
---
title: LangChain
---


Mem0 supports LangChain as a provider to access a wide range of LLM models. LangChain is a framework for developing applications powered by language models, making it easy to integrate various LLM providers through a consistent interface.

For a complete list of available chat models supported by LangChain, refer to the [LangChain Chat Models documentation](https://python.langchain.com/docs/integrations/chat).

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory
from langchain_openai import ChatOpenAI

# Set necessary environment variables for your chosen LangChain provider
os.environ["OPENAI_API_KEY"] = "your-api-key"

# Initialize a LangChain model directly
openai_model = ChatOpenAI(
    model="gpt-4o",
    temperature=0.2,
    max_tokens=2000
)

# Pass the initialized model to the config
config = {
    "llm": {
        "provider": "langchain",
        "config": {
            "model": openai_model
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';
import { ChatOpenAI } from "@langchain/openai";

// Initialize a LangChain model directly
const openaiModel = new ChatOpenAI({
    modelName: "gpt-4",
    temperature: 0.2,
    maxTokens: 2000,
    apiKey: process.env.OPENAI_API_KEY,
});

const config = {
  llm: {
    provider: 'langchain',
    config: {
      model: openaiModel,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

## Supported LangChain Providers

LangChain supports a wide range of LLM providers, including:

- OpenAI (`ChatOpenAI`)
- Anthropic (`ChatAnthropic`)
- Google (`ChatGoogleGenerativeAI`, `ChatGooglePalm`)
- Mistral (`ChatMistralAI`)
- Ollama (`ChatOllama`)
- Azure OpenAI (`AzureChatOpenAI`)
- HuggingFace (`HuggingFaceChatEndpoint`)
- And many more

You can use any of these model instances directly in your configuration. For a complete and up-to-date list of available providers, refer to the [LangChain Chat Models documentation](https://python.langchain.com/docs/integrations/chat).

## Provider-Specific Configuration

When using LangChain as a provider, you'll need to:

1. Set the appropriate environment variables for your chosen LLM provider
2. Import and initialize the specific model class you want to use
3. Pass the initialized model instance to the config

<Note>
  Make sure to install the necessary LangChain packages and any provider-specific dependencies.
</Note>

## Config

All available parameters for the `langchain` config are present in [Master List of All Params in Config](../config).



================================================
FILE: docs/components/llms/models/litellm.mdx
================================================
[Litellm](https://litellm.vercel.app/docs/) is compatible with over 100 large language models (LLMs), all using a standardized input/output format. You can explore the [available models](https://litellm.vercel.app/docs/providers) to use with Litellm. Ensure you set the `API_KEY` for the model you choose to use.

## Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "litellm",
        "config": {
            "model": "gpt-4o-mini",
            "temperature": 0.2,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

## Config

All available parameters for the `litellm` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/lmstudio.mdx
================================================
---
title: LM Studio
---

To use LM Studio with Mem0, you'll need to have LM Studio running locally with its server enabled. LM Studio provides a way to run local LLMs with an OpenAI-compatible API.

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model

config = {
    "llm": {
        "provider": "lmstudio",
        "config": {
            "model": "lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF/Meta-Llama-3.1-70B-Instruct-IQ2_M.gguf",
            "temperature": 0.2,
            "max_tokens": 2000,
            "lmstudio_base_url": "http://localhost:1234/v1", # default LM Studio API URL
            "lmstudio_response_format": {"type": "json_schema", "json_schema": {"type": "object", "schema": {}}},
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```
</CodeGroup>

### Running Completely Locally

You can also use LM Studio for both LLM and embedding to run Mem0 entirely locally:

```python
from mem0 import Memory

# No external API keys needed!
config = {
    "llm": {
        "provider": "lmstudio"
    },
    "embedder": {
        "provider": "lmstudio"
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice123", metadata={"category": "movies"})
```

<Note>
  When using LM Studio for both LLM and embedding, make sure you have:
  1. An LLM model loaded for generating responses
  2. An embedding model loaded for vector embeddings
  3. The server enabled with the correct endpoints accessible
</Note>

<Note>
  To use LM Studio, you need to:
  1. Download and install [LM Studio](https://lmstudio.ai/)
  2. Start a local server from the "Server" tab
  3. Set the appropriate `lmstudio_base_url` in your configuration (default is usually http://localhost:1234/v1)
</Note>

## Config

All available parameters for the `lmstudio` config are present in [Master List of All Params in Config](../config).



================================================
FILE: docs/components/llms/models/mistral_AI.mdx
================================================
---
title: Mistral AI
---

To use mistral's models, please obtain the Mistral AI api key from their [console](https://console.mistral.ai/). Set the `MISTRAL_API_KEY` environment variable to use the model as given below in the example.

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model
os.environ["MISTRAL_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "litellm",
        "config": {
            "model": "open-mixtral-8x7b",
            "temperature": 0.1,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  llm: {
    provider: 'mistral',
    config: {
      apiKey: process.env.MISTRAL_API_KEY || '',
      model: 'mistral-tiny-latest', // Or 'mistral-small-latest', 'mistral-medium-latest', etc.
      temperature: 0.1,
      maxTokens: 2000,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

## Config

All available parameters for the `litellm` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/ollama.mdx
================================================
You can use LLMs from Ollama to run Mem0 locally. These [models](https://ollama.com/search?c=tools) support tool support.

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # for embedder

config = {
    "llm": {
        "provider": "ollama",
        "config": {
            "model": "mixtral:8x7b",
            "temperature": 0.1,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  llm: {
    provider: 'ollama',
    config: {
      model: 'llama3.1:8b', // or any other Ollama model
      url: 'http://localhost:11434', // Ollama server URL
      temperature: 0.1,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

## Config

All available parameters for the `ollama` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/openai.mdx
================================================
---
title: OpenAI
---

To use OpenAI LLM models, you have to set the `OPENAI_API_KEY` environment variable. You can obtain the OpenAI API key from the [OpenAI Platform](https://platform.openai.com/account/api-keys).

> **Note**: The following are currently unsupported with reasoning models `Parallel tool calling`,`temperature`, `top_p`, `presence_penalty`, `frequency_penalty`, `logprobs`, `top_logprobs`, `logit_bias`, `max_tokens`

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o",
            "temperature": 0.2,
            "max_tokens": 2000,
        }
    }
}

# Use Openrouter by passing it's api key
# os.environ["OPENROUTER_API_KEY"] = "your-api-key"
# config = {
#    "llm": {
#        "provider": "openai",
#        "config": {
#            "model": "meta-llama/llama-3.1-70b-instruct",
#        }
#    }
# }

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  llm: {
    provider: 'openai',
    config: {
      apiKey: process.env.OPENAI_API_KEY || '',
      model: 'gpt-4-turbo-preview',
      temperature: 0.2,
      maxTokens: 1500,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

We also support the new [OpenAI structured-outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction) model.

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "openai_structured",
        "config": {
            "model": "gpt-4o-2024-08-06",
            "temperature": 0.0,
        }
    }
}

m = Memory.from_config(config)
```

## Config

All available parameters for the `openai` config are present in [Master List of All Params in Config](../config).



================================================
FILE: docs/components/llms/models/sarvam.mdx
================================================
---
title: Sarvam AI
---

**Sarvam AI** is an Indian AI company developing language models with a focus on Indian languages and cultural context. Their latest model **Sarvam-M** is designed to understand and generate content in multiple Indian languages while maintaining high performance in English.

To use Sarvam AI's models, please set the `SARVAM_API_KEY` which you can get from their [platform](https://dashboard.sarvam.ai/).

## Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model
os.environ["SARVAM_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "sarvam",
        "config": {
            "model": "sarvam-m",
            "temperature": 0.7,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alex")
```

## Advanced Usage with Sarvam-Specific Features

```python
import os
from mem0 import Memory

config = {
    "llm": {
        "provider": "sarvam",
        "config": {
            "model": {
                "name": "sarvam-m",
                "reasoning_effort": "high",  # Enable advanced reasoning
                "frequency_penalty": 0.1,    # Reduce repetition
                "seed": 42                   # For deterministic outputs
            },
            "temperature": 0.3,
            "max_tokens": 2000,
            "api_key": "your-sarvam-api-key"
        }
    }
}

m = Memory.from_config(config)

# Example with Hindi conversation
messages = [
    {"role": "user", "content": "मैं SBI में joint account खोलना चाहता हूँ।"},
    {"role": "assistant", "content": "SBI में joint account खोलने के लिए आपको कुछ documents की जरूरत होगी। क्या आप जानना चाहते हैं कि कौन से documents चाहिए?"}
]
m.add(messages, user_id="rajesh", metadata={"language": "hindi", "topic": "banking"})
```

## Config

All available parameters for the `sarvam` config are present in [Master List of All Params in Config](../config).



================================================
FILE: docs/components/llms/models/together.mdx
================================================
To use TogetherAI LLM models, you have to set the `TOGETHER_API_KEY` environment variable. You can obtain the TogetherAI API key from their [Account settings page](https://api.together.xyz/settings/api-keys).

## Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model
os.environ["TOGETHER_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "together",
        "config": {
            "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "temperature": 0.2,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

## Config

All available parameters for the `togetherai` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/llms/models/vllm.mdx
================================================
---
title: vLLM
---

[vLLM](https://docs.vllm.ai/) is a high-performance inference engine for large language models that provides significant performance improvements for local inference. It's designed to maximize throughput and memory efficiency for serving LLMs.

## Prerequisites

1. **Install vLLM**:

   ```bash
   pip install vllm
   ```

2. **Start vLLM server**:

   ```bash
   # For testing with a small model
   vllm serve microsoft/DialoGPT-medium --port 8000

   # For production with a larger model (requires GPU)
   vllm serve Qwen/Qwen2.5-32B-Instruct --port 8000
   ```

## Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key"  # used for embedding model

config = {
    "llm": {
        "provider": "vllm",
        "config": {
            "model": "Qwen/Qwen2.5-32B-Instruct",
            "vllm_base_url": "http://localhost:8000/v1",
            "temperature": 0.1,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thrillers, but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thrillers and suggest sci-fi movies instead."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

## Configuration Parameters

| Parameter       | Description                       | Default                       | Environment Variable |
| --------------- | --------------------------------- | ----------------------------- | -------------------- |
| `model`         | Model name running on vLLM server | `"Qwen/Qwen2.5-32B-Instruct"` | -                    |
| `vllm_base_url` | vLLM server URL                   | `"http://localhost:8000/v1"`  | `VLLM_BASE_URL`      |
| `api_key`       | API key (dummy for local)         | `"vllm-api-key"`              | `VLLM_API_KEY`       |
| `temperature`   | Sampling temperature              | `0.1`                         | -                    |
| `max_tokens`    | Maximum tokens to generate        | `2000`                        | -                    |

## Environment Variables

You can set these environment variables instead of specifying them in config:

```bash
export VLLM_BASE_URL="http://localhost:8000/v1"
export VLLM_API_KEY="your-vllm-api-key"
export OPENAI_API_KEY="your-openai-api-key"  # for embeddings
```

## Benefits

- **High Performance**: 2-24x faster inference than standard implementations
- **Memory Efficient**: Optimized memory usage with PagedAttention
- **Local Deployment**: Keep your data private and reduce API costs
- **Easy Integration**: Drop-in replacement for other LLM providers
- **Flexible**: Works with any model supported by vLLM

## Troubleshooting

1. **Server not responding**: Make sure vLLM server is running

   ```bash
   curl http://localhost:8000/health
   ```

2. **404 errors**: Ensure correct base URL format

   ```python
   "vllm_base_url": "http://localhost:8000/v1"  # Note the /v1
   ```

3. **Model not found**: Check model name matches server

4. **Out of memory**: Try smaller models or reduce `max_model_len`

   ```bash
   vllm serve Qwen/Qwen2.5-32B-Instruct --max-model-len 4096
   ```

## Config

All available parameters for the `vllm` config are present in [Master List of All Params in Config](../config).



================================================
FILE: docs/components/llms/models/xAI.mdx
================================================
---
title: xAI
---

[xAI](https://x.ai/) is a new AI company founded by Elon Musk that develops large language models, including Grok. Grok is trained on real-time data from X (formerly Twitter) and aims to provide accurate, up-to-date responses with a touch of wit and humor.

In order to use LLMs from xAI, go to their [platform](https://console.x.ai) and get the API key. Set the API key as `XAI_API_KEY` environment variable to use the model as given below in the example.

## Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model
os.environ["XAI_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "xai",
        "config": {
            "model": "grok-3-beta",
            "temperature": 0.1,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

## Config

All available parameters for the `xai` config are present in [Master List of All Params in Config](../config).


================================================
FILE: docs/components/vectordbs/config.mdx
================================================
---
title: Configurations
icon: "gear"
iconType: "solid"
---

## How to define configurations?

The `config` is defined as an object with two main keys:
- `vector_store`: Specifies the vector database provider and its configuration
  - `provider`: The name of the vector database (e.g., "chroma", "pgvector", "qdrant", "milvus", "upstash_vector", "azure_ai_search", "vertex_ai_vector_search")
  - `config`: A nested dictionary containing provider-specific settings


## How to Use Config

Here's a general example of how to use the config with mem0:

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "your_chosen_provider",
        "config": {
            # Provider-specific settings go here
        }
    }
}

m = Memory.from_config(config)
m.add("Your text here", user_id="user", metadata={"category": "example"})
```

```typescript TypeScript
// Example for in-memory vector database (Only supported in TypeScript)
import { Memory } from 'mem0ai/oss';

const configMemory = {
  vector_store: {
    provider: 'memory',
    config: {
      collectionName: 'memories',
      dimension: 1536,
    },
  },
};

const memory = new Memory(configMemory);
await memory.add("Your text here", { userId: "user", metadata: { category: "example" } });
```
</CodeGroup>

<Note>
  The in-memory vector database is only supported in the TypeScript implementation.
</Note>

## Why is Config Needed?

Config is essential for:
1. Specifying which vector database to use.
2. Providing necessary connection details (e.g., host, port, credentials).
3. Customizing database-specific settings (e.g., collection name, path).
4. Ensuring proper initialization and connection to your chosen vector store.

## Master List of All Params in Config

Here's a comprehensive list of all parameters that can be used across different vector databases:

<Tabs>
<Tab title="Python">
| Parameter | Description |
|-----------|-------------|
| `collection_name` | Name of the collection |
| `embedding_model_dims` | Dimensions of the embedding model |
| `client` | Custom client for the database |
| `path` | Path for the database |
| `host` | Host where the server is running |
| `port` | Port where the server is running |
| `user` | Username for database connection |
| `password` | Password for database connection |
| `dbname` | Name of the database |
| `url` | Full URL for the server |
| `api_key` | API key for the server |
| `on_disk` | Enable persistent storage |
| `endpoint_id` | Endpoint ID (vertex_ai_vector_search) |
| `index_id` | Index ID (vertex_ai_vector_search) |
| `deployment_index_id` | Deployment index ID (vertex_ai_vector_search) |
| `project_id` | Project ID (vertex_ai_vector_search) |
| `project_number` | Project number (vertex_ai_vector_search) |
| `vector_search_api_endpoint` | Vector search API endpoint (vertex_ai_vector_search) |
| `connection_string` | PostgreSQL connection string (for Supabase/PGVector) |
| `index_method` | Vector index method (for Supabase) |
| `index_measure` | Distance measure for similarity search (for Supabase) |
</Tab>
<Tab title="TypeScript">
| Parameter | Description |
|-----------|-------------|
| `collectionName` | Name of the collection |
| `embeddingModelDims` | Dimensions of the embedding model |
| `dimension` | Dimensions of the embedding model (for memory provider) |
| `host` | Host where the server is running |
| `port` | Port where the server is running |
| `url` | URL for the server |
| `apiKey` | API key for the server |
| `path` | Path for the database |
| `onDisk` | Enable persistent storage |
| `redisUrl` | URL for the Redis server |
| `username` | Username for database connection |
| `password` | Password for database connection |
</Tab>
</Tabs>

## Customizing Config

Each vector database has its own specific configuration requirements. To customize the config for your chosen vector store:

1. Identify the vector database you want to use from [supported vector databases](./dbs).
2. Refer to the `Config` section in the respective vector database's documentation.
3. Include only the relevant parameters for your chosen database in the `config` dictionary.

## Supported Vector Databases

For detailed information on configuring specific vector databases, please visit the [Supported Vector Databases](./dbs) section. There you'll find individual pages for each supported vector store with provider-specific usage examples and configuration details.



================================================
FILE: docs/components/vectordbs/overview.mdx
================================================
---
title: Overview
icon: "info"
iconType: "solid"
---

Mem0 includes built-in support for various popular databases. Memory can utilize the database provided by the user, ensuring efficient use for specific needs.

## Supported Vector Databases

See the list of supported vector databases below.

<Note>
  The following vector databases are supported in the Python implementation. The TypeScript implementation currently only supports Qdrant, Redis,Vectorize and in-memory vector database.
</Note>

<CardGroup cols={3}>
  <Card title="Qdrant" href="/components/vectordbs/dbs/qdrant"></Card>
  <Card title="Chroma" href="/components/vectordbs/dbs/chroma"></Card>
  <Card title="Pgvector" href="/components/vectordbs/dbs/pgvector"></Card>
  <Card title="Upstash Vector" href="/components/vectordbs/dbs/upstash-vector"></Card>
  <Card title="Milvus" href="/components/vectordbs/dbs/milvus"></Card>
  <Card title="Pinecone" href="/components/vectordbs/dbs/pinecone"></Card>
  <Card title="MongoDB" href="/components/vectordbs/dbs/mongodb"></Card>
  <Card title="Azure" href="/components/vectordbs/dbs/azure"></Card>
  <Card title="Redis" href="/components/vectordbs/dbs/redis"></Card>
  <Card title="Elasticsearch" href="/components/vectordbs/dbs/elasticsearch"></Card>
  <Card title="OpenSearch" href="/components/vectordbs/dbs/opensearch"></Card>
  <Card title="Supabase" href="/components/vectordbs/dbs/supabase"></Card>
  <Card title="Vertex AI" href="/components/vectordbs/dbs/vertex_ai"></Card>
  <Card title="Weaviate" href="/components/vectordbs/dbs/weaviate"></Card>
  <Card title="FAISS" href="/components/vectordbs/dbs/faiss"></Card>
  <Card title="LangChain" href="/components/vectordbs/dbs/langchain"></Card>
  <Card title="Amazon S3 Vectors" href="/components/vectordbs/dbs/s3_vectors"></Card>
  <Card title="Databricks" href="/components/vectordbs/dbs/databricks"></Card>
</CardGroup>

## Usage

To utilize a vector database, you must provide a configuration to customize its usage. If no configuration is supplied, a default configuration will be applied, and `Qdrant` will be used as the vector database.

For a comprehensive list of available parameters for vector database configuration, please refer to [Config](./config).

## Common issues

### Using model with different dimensions

If you are using customized model, which is having different dimensions other than 1536
for example 768, you may encounter below error:

`ValueError: shapes (0,1536) and (768,) not aligned: 1536 (dim 1) != 768 (dim 0)`

you could add `"embedding_model_dims": 768,` to the config of the vector_store to overcome this issue.




================================================
FILE: docs/components/vectordbs/dbs/azure.mdx
================================================
---
title: Azure AI Search
---

[Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search/) (formerly known as "Azure Cognitive Search") provides secure information retrieval at scale over user-owned content in traditional and generative AI search applications.

## Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"   # This key is used for embedding purpose

config = {
    "vector_store": {
        "provider": "azure_ai_search",
        "config": {
            "service_name": "<your-azure-ai-search-service-name>",
            "api_key": "<your-api-key>",
            "collection_name": "mem0", 
            "embedding_model_dims": 1536
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

## Using binary compression for large vector collections

```python
config = {
    "vector_store": {
        "provider": "azure_ai_search",
        "config": {
            "service_name": "<your-azure-ai-search-service-name>",
            "api_key": "<your-api-key>",
            "collection_name": "mem0", 
            "embedding_model_dims": 1536,
            "compression_type": "binary",
            "use_float16": True  # Use half precision for storage efficiency
        }
    }
}
```

## Using hybrid search

```python
config = {
    "vector_store": {
        "provider": "azure_ai_search",
        "config": {
            "service_name": "<your-azure-ai-search-service-name>",
            "api_key": "<your-api-key>",
            "collection_name": "mem0", 
            "embedding_model_dims": 1536,
            "hybrid_search": True,
            "vector_filter_mode": "postFilter"
        }
    }
}
```

## Using Azure Identity for Authentication
As an alternative to using an API key, the Azure Identity credential chain can be used to authenticate with Azure OpenAI. The list below shows the order of precedence for credential application:

1. **Environment Credential:**
Azure client ID, secret, tenant ID, or certificate in environment variables for service principal authentication.

2. **Workload Identity Credential:**
Utilizes Azure Workload Identity (relevant for Kubernetes and Azure workloads).

3. **Managed Identity Credential:**
Authenticates as a Managed Identity (for apps/services hosted in Azure with Managed Identity enabled), this is the most secure production credential.

4. **Shared Token Cache Credential / Visual Studio Credential (Windows only):**
Uses cached credentials from Visual Studio sign-ins (and sometimes VS Code if SSO is enabled).

5. **Azure CLI Credential:**
Uses the currently logged-in user from the Azure CLI (`az login`), this is the most common development credential.

6. **Azure PowerShell Credential:**
Uses the identity from Azure PowerShell (`Connect-AzAccount`).

7. **Azure Developer CLI Credential:**
Uses the session from Azure Developer CLI (`azd auth login`).

<Note> If an API is provided, it will be used for authentication over an Azure Identity </Note>
To enable Role-Based Access Control (RBAC) for Azure AI Search, follow these steps:

1. In the Azure Portal, navigate to your **Azure AI Search** service.
2. In the left menu, select **Settings** > **Keys**.
3. Change the authentication setting to **Role-based access control**, or **Both** if you need API key compatibility. The default is “Key-based authentication”—you must switch it to use Azure roles.
4. **Go to Access Control (IAM):**
    - In the Azure Portal, select your Search service.
    - Click **Access Control (IAM)** on the left.
5. **Add a Role Assignment:**
    - Click **Add** > **Add role assignment**.
6. **Choose Role:**
    - Mem0 requires the **Search Index Data Contributor** and **Search Service Contributor** role.
7. **Choose Member** 
    - To assign to a User, Group, Service Principle or Managed Identity:
        - For production it is recommended to use a service principal or managed identity.
            - For a service principal: select **User, group, or service principal** and search for the service principal.
            - For a managed identity: select **Managed identity** and choose the managed identity.
        - For development, you can assign the role to a user account.
            - For development: select ***User, group, or service principal** and pick a Azure Entra ID account (the same used with `az login`).
8. **Complete the Assignment:**
    - Click **Review + Assign**.

If you are using Azure Identity, do not set the `api_key` in the configuration.
```python
config = {
    "vector_store": {
        "provider": "azure_ai_search",
        "config": {
            "service_name": "<your-azure-ai-search-service-name>",
            "collection_name": "mem0", 
            "embedding_model_dims": 1536,
            "compression_type": "binary",
            "use_float16": True  # Use half precision for storage efficiency
        }
    }
}
```

### Environment Variables to set to use Azure Identity Credential:
* For an Environment Credential, you will need to setup a Service Principal and set the following environment variables:
  - `AZURE_TENANT_ID`: Your Azure Active Directory tenant ID.
  - `AZURE_CLIENT_ID`: The client ID of your service principal or managed identity.
  - `AZURE_CLIENT_SECRET`: The client secret of your service principal.
* For a User-Assigned Managed Identity, you will need to set the following environment variable:
  - `AZURE_CLIENT_ID`: The client ID of the user-assigned managed identity.
* For a System-Assigned Managed Identity, no additional environment variables are needed.

### Developer logins to use for a Azure Identity Credential:
* For an Azure CLI Credential, you need to have the Azure CLI installed and logged in with `az login`.
* For an Azure PowerShell Credential, you need to have the Azure PowerShell module installed and logged in with `Connect-AzAccount`.
* For an Azure Developer CLI Credential, you need to have the Azure Developer CLI installed and logged in with `azd auth login`.

Troubleshooting tips for [Azure Identity](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/TROUBLESHOOTING.md#troubleshoot-environmentcredential-authentication-issues).


## Configuration Parameters

| Parameter | Description | Default Value | Options |
| --- | --- | --- | --- |
| `service_name` | Azure AI Search service name | Required | - |
| `api_key` | API key of the Azure AI Search service | Optional | If not present, the [Azure Identity](#using-azure-identity-for-authentication) credential chain will be used |
| `collection_name` | The name of the collection/index to store vectors | `mem0` | Any valid index name |
| `embedding_model_dims` | Dimensions of the embedding model | `1536` | Any integer value |
| `compression_type` | Type of vector compression to use | `none` | `none`, `scalar`, `binary` |
| `use_float16` | Store vectors in half precision (Edm.Half) | `False` | `True`, `False` |
| `vector_filter_mode` | Vector filter mode to use | `preFilter` | `postFilter`, `preFilter` |
| `hybrid_search` | Use hybrid search | `False` | `True`, `False` |

## Notes on Configuration Options

- **compression_type**: 
  - `none`: No compression, uses full vector precision
  - `scalar`: Scalar quantization with reasonable balance of speed and accuracy
  - `binary`: Binary quantization for maximum compression with some accuracy trade-off

- **vector_filter_mode**:
  - `preFilter`: Applies filters before vector search (faster)
  - `postFilter`: Applies filters after vector search (may provide better relevance)

- **use_float16**: Using half precision (float16) reduces storage requirements but may slightly impact accuracy. Useful for very large vector collections.

- **Filterable Fields**: The implementation automatically extracts `user_id`, `run_id`, and `agent_id` fields from payloads for filtering.


================================================
FILE: docs/components/vectordbs/dbs/baidu.mdx
================================================
---
title: Baidu VectorDB (Mochow)
---

[Baidu VectorDB](https://cloud.baidu.com/doc/VDB/index.html) is an enterprise-level distributed vector database service developed by Baidu Intelligent Cloud. It is powered by Baidu's proprietary "Mochow" vector database kernel, providing high performance, availability, and security for vector search.

### Usage

```python
import os
from mem0 import Memory

config = {
    "vector_store": {
        "provider": "baidu",
        "config": {
            "endpoint": "http://your-mochow-endpoint:8287",
            "account": "root",
            "api_key": "your-api-key",
            "database_name": "mem0",
            "table_name": "mem0_table",
            "embedding_model_dims": 1536,
            "metric_type": "COSINE"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movie? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

Here are the available parameters for the `mochow` config:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `endpoint` | Endpoint URL for your Baidu VectorDB instance | Required |
| `account` | Baidu VectorDB account name | `root` |
| `api_key` | API key for accessing Baidu VectorDB | Required |
| `database_name` | Name of the database | `mem0` |
| `table_name` | Name of the table | `mem0_table` |
| `embedding_model_dims` | Dimensions of the embedding model | `1536` |
| `metric_type` | Distance metric for similarity search | `L2` |

### Distance Metrics

The following distance metrics are supported:

- `L2`: Euclidean distance (default)
- `IP`: Inner product
- `COSINE`: Cosine similarity

### Index Configuration

The vector index is automatically configured with the following HNSW parameters:

- `m`: 16 (number of connections per element)
- `efconstruction`: 200 (size of the dynamic candidate list)
- `auto_build`: true (automatically build index)
- `auto_build_index_policy`: Incremental build with 10000 rows increment



================================================
FILE: docs/components/vectordbs/dbs/chroma.mdx
================================================
[Chroma](https://www.trychroma.com/) is an AI-native open-source vector database that simplifies building LLM apps by providing tools for storing, embedding, and searching embeddings with a focus on simplicity and speed.

### Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "chroma",
        "config": {
            "collection_name": "test",
            "path": "db",
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

Here are the parameters available for configuring Chroma:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `collection_name` | The name of the collection | `mem0` |
| `client` | Custom client for Chroma | `None` |
| `path` | Path for the Chroma database | `db` |
| `host` | The host where the Chroma server is running | `None` |
| `port` | The port where the Chroma server is running | `None` |


================================================
FILE: docs/components/vectordbs/dbs/databricks.mdx
================================================
[Databricks Vector Search](https://docs.databricks.com/en/generative-ai/vector-search.html) is a serverless similarity search engine that allows you to store a vector representation of your data, including metadata, in a vector database. With Vector Search, you can create auto-updating vector search indexes from Delta tables managed by Unity Catalog and query them with a simple API to return the most similar vectors.

### Usage

```python
import os
from mem0 import Memory

config = {
    "vector_store": {
        "provider": "databricks",
        "config": {
            "workspace_url": "https://your-workspace.databricks.com",
            "access_token": "your-access-token",
            "endpoint_name": "your-vector-search-endpoint",
            "index_name": "catalog.schema.index_name",
            "source_table_name": "catalog.schema.source_table",
            "embedding_dimension": 1536
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

Here are the parameters available for configuring Databricks Vector Search:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `workspace_url` | The URL of your Databricks workspace | **Required** |
| `access_token` | Personal Access Token for authentication | `None` |
| `service_principal_client_id` | Service principal client ID (alternative to access_token) | `None` |
| `service_principal_client_secret` | Service principal client secret (required with client_id) | `None` |
| `endpoint_name` | Name of the Vector Search endpoint | **Required** |
| `index_name` | Name of the vector index (Unity Catalog format: catalog.schema.index) | **Required** |
| `source_table_name` | Name of the source Delta table (Unity Catalog format: catalog.schema.table) | **Required** |
| `embedding_dimension` | Dimension of self-managed embeddings | `1536` |
| `embedding_source_column` | Column name for text when using Databricks-computed embeddings | `None` |
| `embedding_model_endpoint_name` | Databricks serving endpoint for embeddings | `None` |
| `embedding_vector_column` | Column name for self-managed embedding vectors | `embedding` |
| `endpoint_type` | Type of endpoint (`STANDARD` or `STORAGE_OPTIMIZED`) | `STANDARD` |
| `sync_computed_embeddings` | Whether to sync computed embeddings automatically | `True` |

### Authentication

Databricks Vector Search supports two authentication methods:

#### Service Principal (Recommended for Production)
```python
config = {
    "vector_store": {
        "provider": "databricks",
        "config": {
            "workspace_url": "https://your-workspace.databricks.com",
            "service_principal_client_id": "your-service-principal-id",
            "service_principal_client_secret": "your-service-principal-secret",
            "endpoint_name": "your-endpoint",
            "index_name": "catalog.schema.index_name",
            "source_table_name": "catalog.schema.source_table"
        }
    }
}
```

#### Personal Access Token (for Development)
```python
config = {
    "vector_store": {
        "provider": "databricks",
        "config": {
            "workspace_url": "https://your-workspace.databricks.com",
            "access_token": "your-personal-access-token",
            "endpoint_name": "your-endpoint",
            "index_name": "catalog.schema.index_name",
            "source_table_name": "catalog.schema.source_table"
        }
    }
}
```

### Embedding Options

#### Self-Managed Embeddings (Default)
Use your own embedding model and provide vectors directly:

```python
config = {
    "vector_store": {
        "provider": "databricks",
        "config": {
            # ... authentication config ...
            "embedding_dimension": 768,  # Match your embedding model
            "embedding_vector_column": "embedding"
        }
    }
}
```

#### Databricks-Computed Embeddings
Let Databricks compute embeddings from text using a serving endpoint:

```python
config = {
    "vector_store": {
        "provider": "databricks",
        "config": {
            # ... authentication config ...
            "embedding_source_column": "text",
            "embedding_model_endpoint_name": "e5-small-v2"
        }
    }
}
```

### Important Notes

- **Delta Sync Index**: This implementation uses Delta Sync Index, which automatically syncs with your source Delta table. Direct vector insertion/deletion/update operations will log warnings as they're not supported with Delta Sync.
- **Unity Catalog**: Both the source table and index must be in Unity Catalog format (`catalog.schema.table_name`).
- **Endpoint Auto-Creation**: If the specified endpoint doesn't exist, it will be created automatically.
- **Index Auto-Creation**: If the specified index doesn't exist, it will be created automatically with the provided configuration.
- **Filter Support**: Supports filtering by metadata fields, with different syntax for STANDARD vs STORAGE_OPTIMIZED endpoints.



================================================
FILE: docs/components/vectordbs/dbs/elasticsearch.mdx
================================================
[Elasticsearch](https://www.elastic.co/) is a distributed, RESTful search and analytics engine that can efficiently store and search vector data using dense vectors and k-NN search.

### Installation

Elasticsearch support requires additional dependencies. Install them with:

```bash
pip install elasticsearch>=8.0.0
```

### Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "elasticsearch",
        "config": {
            "collection_name": "mem0",
            "host": "localhost",
            "port": 9200,
            "embedding_model_dims": 1536
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

Let's see the available parameters for the `elasticsearch` config:

| Parameter              | Description                                        | Default Value |
| ---------------------- | -------------------------------------------------- | ------------- |
| `collection_name`      | The name of the index to store the vectors         | `mem0`        |
| `embedding_model_dims` | Dimensions of the embedding model                  | `1536`        |
| `host`                 | The host where the Elasticsearch server is running | `localhost`   |
| `port`                 | The port where the Elasticsearch server is running | `9200`        |
| `cloud_id`             | Cloud ID for Elastic Cloud deployment              | `None`        |
| `api_key`              | API key for authentication                         | `None`        |
| `user`                 | Username for basic authentication                  | `None`        |
| `password`             | Password for basic authentication                  | `None`        |
| `verify_certs`         | Whether to verify SSL certificates                 | `True`        |
| `auto_create_index`    | Whether to automatically create the index          | `True`        |
| `custom_search_query`  | Function returning a custom search query           | `None`        |
| `headers`              | Custom headers to include in requests              | `None`        |

### Features

- Efficient vector search using Elasticsearch's native k-NN search
- Support for both local and cloud deployments (Elastic Cloud)
- Multiple authentication methods (Basic Auth, API Key)
- Automatic index creation with optimized mappings for vector search
- Memory isolation through payload filtering
- Custom search query function to customize the search query

### Custom Search Query

The `custom_search_query` parameter allows you to customize the search query when `Memory.search` is called.  
  
__Example__  
```python
import os
from typing import List, Optional, Dict
from mem0 import Memory

def custom_search_query(query: List[float], limit: int, filters: Optional[Dict]) -> Dict:
    return {
        "knn": {
            "field": "vector", 
            "query_vector": query, 
            "k": limit, 
            "num_candidates": limit * 2
        }
    }

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "elasticsearch",
        "config": {
            "collection_name": "mem0",
            "host": "localhost",
            "port": 9200,
            "embedding_model_dims": 1536,
            "custom_search_query": custom_search_query
        }
    }
}
```
It should be a function that takes the following parameters:
- `query`: a query vector used in `Memory.search`
- `limit`: a number of results used in `Memory.search`
- `filters`: a dictionary of key-value pairs used in `Memory.search`. You can add custom pairs for the custom search query.  
  
The function should return a query body for the Elasticsearch search API.


================================================
FILE: docs/components/vectordbs/dbs/faiss.mdx
================================================
[FAISS](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors. It is designed to work with large-scale datasets and provides a high-performance search engine for vector data. FAISS is optimized for memory usage and search speed, making it an excellent choice for production environments.

### Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "faiss",
        "config": {
            "collection_name": "test",
            "path": "/tmp/faiss_memories",
            "distance_strategy": "euclidean"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Installation

To use FAISS in your mem0 project, you need to install the appropriate FAISS package for your environment:

```bash
# For CPU version
pip install faiss-cpu

# For GPU version (requires CUDA)
pip install faiss-gpu
```

### Config

Here are the parameters available for configuring FAISS:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `collection_name` | The name of the collection | `mem0` |
| `path` | Path to store FAISS index and metadata | `/tmp/faiss/<collection_name>` |
| `distance_strategy` | Distance metric strategy to use (options: 'euclidean', 'inner_product', 'cosine') | `euclidean` |
| `normalize_L2` | Whether to normalize L2 vectors (only applicable for euclidean distance) | `False` |

### Performance Considerations

FAISS offers several advantages for vector search:

1. **Efficiency**: FAISS is optimized for memory usage and speed, making it suitable for large-scale applications.
2. **Offline Support**: FAISS works entirely locally, with no need for external servers or API calls.
3. **Storage Options**: Vectors can be stored in-memory for maximum speed or persisted to disk.
4. **Multiple Index Types**: FAISS supports different index types optimized for various use cases (though mem0 currently uses the basic flat index).

### Distance Strategies

FAISS in mem0 supports three distance strategies:

- **euclidean**: L2 distance, suitable for most embedding models
- **inner_product**: Dot product similarity, useful for some specialized embeddings
- **cosine**: Cosine similarity, best for comparing semantic similarity regardless of vector magnitude

When using `cosine` or `inner_product` with normalized vectors, you may want to set `normalize_L2=True` for better results.



================================================
FILE: docs/components/vectordbs/dbs/langchain.mdx
================================================
---
title: LangChain
---

Mem0 supports LangChain as a provider for vector store integration. LangChain provides a unified interface to various vector databases, making it easy to integrate different vector store providers through a consistent API.

<Note>
  When using LangChain as your vector store provider, you must set the collection name to "mem0". This is a required configuration for proper integration with Mem0.
</Note>

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# Initialize a LangChain vector store
embeddings = OpenAIEmbeddings()
vector_store = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
    collection_name="mem0"  # Required collection name
)

# Pass the initialized vector store to the config
config = {
    "vector_store": {
        "provider": "langchain",
        "config": {
            "client": vector_store
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from "mem0ai";
import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore as LangchainMemoryStore } from "langchain/vectorstores/memory";

const embeddings = new OpenAIEmbeddings();
const vectorStore = new LangchainVectorStore(embeddings);

const config = {
    "vector_store": {
        "provider": "langchain",
        "config": { "client": vectorStore }
    }
}

const memory = new Memory(config);

const messages = [
    { role: "user", content: "I'm planning to watch a movie tonight. Any recommendations?" },
    { role: "assistant", content: "How about a thriller movies? They can be quite engaging." },
    { role: "user", content: "I'm not a big fan of thriller movies but I love sci-fi movies." },
    { role: "assistant", content: "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future." }
]

memory.add(messages, user_id="alice", metadata={"category": "movies"})
```
</CodeGroup>

## Supported LangChain Vector Stores

LangChain supports a wide range of vector store providers, including:

- Chroma
- FAISS
- Pinecone
- Weaviate
- Milvus
- Qdrant
- And many more

You can use any of these vector store instances directly in your configuration. For a complete and up-to-date list of available providers, refer to the [LangChain Vector Stores documentation](https://python.langchain.com/docs/integrations/vectorstores).

## Limitations

When using LangChain as a vector store provider, there are some limitations to be aware of:

1. **Bulk Operations**: The `get_all` and `delete_all` operations are not supported when using LangChain as the vector store provider. This is because LangChain's vector store interface doesn't provide standardized methods for these bulk operations across all providers.

2. **Provider-Specific Features**: Some advanced features may not be available depending on the specific vector store implementation you're using through LangChain.

## Provider-Specific Configuration

When using LangChain as a vector store provider, you'll need to:

1. Set the appropriate environment variables for your chosen vector store provider
2. Import and initialize the specific vector store class you want to use
3. Pass the initialized vector store instance to the config

<Note>
  Make sure to install the necessary LangChain packages and any provider-specific dependencies.
</Note>

## Config

All available parameters for the `langchain` vector store config are present in [Master List of All Params in Config](../config).



================================================
FILE: docs/components/vectordbs/dbs/milvus.mdx
================================================
[Milvus](https://milvus.io/) Milvus is an open-source vector database that suits AI applications of every size from running a demo chatbot in Jupyter notebook to building web-scale search that serves billions of users.

### Usage

```python
import os
from mem0 import Memory

config = {
    "vector_store": {
        "provider": "milvus",
        "config": {
            "collection_name": "test",
            "embedding_model_dims": "123",
            "url": "127.0.0.1",
            "token": "8e4b8ca8cf2c67",
            "db_name": "my_database",
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

Here's the parameters available for configuring Milvus Database:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `url` | Full URL/Uri for Milvus/Zilliz server | `http://localhost:19530` |
| `token` | Token for Zilliz server / for local setup defaults to None. | `None` |
| `collection_name` | The name of the collection | `mem0` |
| `embedding_model_dims` | Dimensions of the embedding model | `1536` |
| `metric_type` | Metric type for similarity search | `L2` |
| `db_name` | Name of the database | `""` |



================================================
FILE: docs/components/vectordbs/dbs/mongodb.mdx
================================================
# MongoDB

[MongoDB](https://www.mongodb.com/) is a versatile document database that supports vector search capabilities, allowing for efficient high-dimensional similarity searches over large datasets with robust scalability and performance.

## Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "mongodb",
        "config": {
            "db_name": "mem0-db",
            "collection_name": "mem0-collection",
            "mongo_uri":"mongodb://username:password@localhost:27017"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

## Config

Here are the parameters available for configuring MongoDB:

| Parameter | Description | Default Value |
| --- | --- | --- |
| db_name | Name of the MongoDB database | `"mem0_db"` |
| collection_name | Name of the MongoDB collection | `"mem0_collection"` |
| embedding_model_dims | Dimensions of the embedding vectors | `1536` |
| mongo_uri | The mongo URI connection string | mongodb://username:password@localhost:27017 |

> **Note**: If Mongo_uri is not provided it will default to mongodb://username:password@localhost:27017.



================================================
FILE: docs/components/vectordbs/dbs/opensearch.mdx
================================================
[OpenSearch](https://opensearch.org/) is an enterprise-grade search and observability suite that brings order to unstructured data at scale. OpenSearch supports k-NN (k-Nearest Neighbors) and allows you to store and retrieve high-dimensional vector embeddings efficiently.

### Installation

OpenSearch support requires additional dependencies. Install them with:

```bash
pip install opensearch-py
```

### Prerequisites

Before using OpenSearch with Mem0, you need to set up a collection in AWS OpenSearch Service.

#### AWS OpenSearch Service
You can create a collection through the AWS Console:
- Navigate to [OpenSearch Service Console](https://console.aws.amazon.com/aos/home)
- Click "Create collection"
- Select "Serverless collection" and then enable "Vector search" capabilities
- Once created, note the endpoint URL (host) for your configuration


### Usage

```python
import os
from mem0 import Memory
import boto3
from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth

# For AWS OpenSearch Service with IAM authentication
region = 'us-west-2'
service = 'aoss'
credentials = boto3.Session().get_credentials()
auth = AWSV4SignerAuth(credentials, region, service)

config = {
    "vector_store": {
        "provider": "opensearch",
        "config": {
            "collection_name": "mem0",
            "host": "your-domain.us-west-2.aoss.amazonaws.com",
            "port": 443,
            "http_auth": auth,
            "embedding_model_dims": 1024,
            "connection_class": RequestsHttpConnection,
            "pool_maxsize": 20,
            "use_ssl": True,
            "verify_certs": True
        }
    }
}
```

### Add Memories

```python
m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Search Memories

```python
results = m.search("What kind of movies does Alice like?", user_id="alice")
```

### Features

- Fast and Efficient Vector Search
- Can be deployed on-premises, in containers, or on cloud platforms like AWS OpenSearch Service.
- Multiple Authentication and Security Methods (Basic Authentication, API Keys, LDAP, SAML, and OpenID Connect)
- Automatic index creation with optimized mappings for vector search
- Memory Optimization through Disk-Based Vector Search and Quantization
- Real-Time Analytics and Observability



================================================
FILE: docs/components/vectordbs/dbs/pgvector.mdx
================================================
[pgvector](https://github.com/pgvector/pgvector) is open-source vector similarity search for Postgres. After connecting with postgres run `CREATE EXTENSION IF NOT EXISTS vector;` to create the vector extension.

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "pgvector",
        "config": {
            "user": "test",
            "password": "123",
            "host": "127.0.0.1",
            "port": "5432",
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  vectorStore: {
    provider: 'pgvector',
    config: {
      collectionName: 'memories',
      embeddingModelDims: 1536,
      user: 'test',
      password: '123',
      host: '127.0.0.1',
      port: 5432,
      dbname: 'vector_store', // Optional, defaults to 'postgres'
      diskann: false, // Optional, requires pgvectorscale extension
      hnsw: false, // Optional, for HNSW indexing
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

### Config

Here's the parameters available for configuring pgvector:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `dbname` | The name of the database | `postgres` |
| `collection_name` | The name of the collection | `mem0` |
| `embedding_model_dims` | Dimensions of the embedding model | `1536` |
| `user` | User name to connect to the database | `None` |
| `password` | Password to connect to the database | `None` |
| `host` | The host where the Postgres server is running | `None` |
| `port` | The port where the Postgres server is running | `None` |
| `diskann` | Whether to use diskann for vector similarity search (requires pgvectorscale) | `True` |
| `hnsw` | Whether to use hnsw for vector similarity search | `False` |
| `sslmode` | SSL mode for PostgreSQL connection (e.g., 'require', 'prefer', 'disable') | `None` |
| `connection_string` | PostgreSQL connection string (overrides individual connection parameters) | `None` |
| `connection_pool` | psycopg2 connection pool object (overrides connection string and individual parameters) | `None` |

**Note**: The connection parameters have the following priority:
1. `connection_pool` (highest priority)
2. `connection_string`
3. Individual connection parameters (`user`, `password`, `host`, `port`, `sslmode`)


================================================
FILE: docs/components/vectordbs/dbs/pinecone.mdx
================================================
[Pinecone](https://www.pinecone.io/) is a fully managed vector database designed for machine learning applications, offering high performance vector search with low latency at scale. It's particularly well-suited for semantic search, recommendation systems, and other AI-powered applications.

> **New**: Pinecone integration now supports custom namespaces! Use the `namespace` parameter to logically separate data within the same index. This is especially useful for multi-tenant or multi-user applications.

> **Note**: Before configuring Pinecone, you need to select an embedding model (e.g., OpenAI, Cohere, or custom models) and ensure the `embedding_model_dims` in your config matches your chosen model's dimensions. For example, OpenAI's text-embedding-3-small uses 1536 dimensions.

### Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"
os.environ["PINECONE_API_KEY"] = "your-api-key"

# Example using serverless configuration
config = {
    "vector_store": {
        "provider": "pinecone",
        "config": {
            "collection_name": "testing",
            "embedding_model_dims": 1536,  # Matches OpenAI's text-embedding-3-small
            "namespace": "my-namespace", # Optional: specify a namespace for multi-tenancy
            "serverless_config": {
                "cloud": "aws",  # Choose between 'aws' or 'gcp' or 'azure'
                "region": "us-east-1"
            },
            "metric": "cosine"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

Here are the parameters available for configuring Pinecone:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `collection_name` | Name of the index/collection | Required |
| `embedding_model_dims` | Dimensions of the embedding model (must match your chosen embedding model) | Required |
| `client` | Existing Pinecone client instance | `None` |
| `api_key` | API key for Pinecone | Environment variable: `PINECONE_API_KEY` |
| `environment` | Pinecone environment | `None` |
| `serverless_config` | Configuration for serverless deployment (AWS or GCP or Azure) | `None` |
| `pod_config` | Configuration for pod-based deployment | `None` |
| `hybrid_search` | Whether to enable hybrid search | `False` |
| `metric` | Distance metric for vector similarity | `"cosine"` |
| `batch_size` | Batch size for operations | `100` |
| `namespace` | Namespace for the collection, useful for multi-tenancy. | `None` |

> **Important**: You must choose either `serverless_config` or `pod_config` for your deployment, but not both.

#### Serverless Config Example
```python
config = {
    "vector_store": {
        "provider": "pinecone",
        "config": {
            "collection_name": "memory_index",
            "embedding_model_dims": 1536,  # For OpenAI's text-embedding-3-small
            "namespace": "my-namespace",  # Optional: custom namespace
            "serverless_config": {
                "cloud": "aws",  # or "gcp" or "azure"
                "region": "us-east-1"  # Choose appropriate region
            }
        }
    }
}
```

#### Pod Config Example
```python
config = {
    "vector_store": {
        "provider": "pinecone",
        "config": {
            "collection_name": "memory_index",
            "embedding_model_dims": 1536,  # For OpenAI's text-embedding-ada-002
            "namespace": "my-namespace",  # Optional: custom namespace
            "pod_config": {
                "environment": "gcp-starter",
                "replicas": 1,
                "pod_type": "starter"
            }
        }
    }
}
```


================================================
FILE: docs/components/vectordbs/dbs/qdrant.mdx
================================================
[Qdrant](https://qdrant.tech/) is an open-source vector search engine. It is designed to work with large-scale datasets and provides a high-performance search engine for vector data.

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name": "test",
            "host": "localhost",
            "port": 6333,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  vectorStore: {
    provider: 'qdrant',
    config: {
      collectionName: 'memories',
      embeddingModelDims: 1536,
      host: 'localhost',
      port: 6333,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

### Config

Let's see the available parameters for the `qdrant` config:

<Tabs>
<Tab title="Python">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `collection_name` | The name of the collection to store the vectors | `mem0` |
| `embedding_model_dims` | Dimensions of the embedding model | `1536` |
| `client` | Custom client for qdrant | `None` |
| `host` | The host where the qdrant server is running | `None` |
| `port` | The port where the qdrant server is running | `None` |
| `path` | Path for the qdrant database | `/tmp/qdrant` |
| `url` | Full URL for the qdrant server | `None` |
| `api_key` | API key for the qdrant server | `None` |
| `on_disk` | For enabling persistent storage | `False` |
</Tab>
<Tab title="TypeScript">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `collectionName` | The name of the collection to store the vectors | `mem0` |
| `embeddingModelDims` | Dimensions of the embedding model | `1536` |
| `host` | The host where the Qdrant server is running | `None` |
| `port` | The port where the Qdrant server is running | `None` |
| `path` | Path for the Qdrant database | `/tmp/qdrant` |
| `url` | Full URL for the Qdrant server | `None` |
| `apiKey` | API key for the Qdrant server | `None` |
| `onDisk` | For enabling persistent storage | `False` |
</Tab>
</Tabs>


================================================
FILE: docs/components/vectordbs/dbs/redis.mdx
================================================
[Redis](https://redis.io/) is a scalable, real-time database that can store, search, and analyze vector data.

### Installation
```bash
pip install redis redisvl
```

Redis Stack using Docker:
```bash
docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest
```

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "redis",
        "config": {
            "collection_name": "mem0",
            "embedding_model_dims": 1536,
            "redis_url": "redis://localhost:6379"
        }
    },
    "version": "v1.1"
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  vectorStore: {
    provider: 'redis',
    config: {
      collectionName: 'memories',
      embeddingModelDims: 1536,
      redisUrl: 'redis://localhost:6379',
      username: 'your-redis-username',
      password: 'your-redis-password',
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

### Config

Let's see the available parameters for the `redis` config:

<Tabs>
<Tab title="Python">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `collection_name` | The name of the collection to store the vectors | `mem0` |
| `embedding_model_dims` | Dimensions of the embedding model | `1536` |
| `redis_url` | The URL of the Redis server | `None` |
</Tab>
<Tab title="TypeScript">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `collectionName` | The name of the collection to store the vectors | `mem0` |
| `embeddingModelDims` | Dimensions of the embedding model | `1536` |
| `redisUrl` | The URL of the Redis server | `None` |
| `username` | Username for Redis connection | `None` |
| `password` | Password for Redis connection | `None` |
</Tab>
</Tabs>


================================================
FILE: docs/components/vectordbs/dbs/s3_vectors.mdx
================================================
---
title: Amazon S3 Vectors
---

[Amazon S3 Vectors](https://aws.amazon.com/s3/features/vectors/) is a purpose-built, cost-optimized vector storage and query service for semantic search and AI applications. It provides S3-level elasticity and durability with sub-second query performance.

### Installation

S3 Vectors support requires additional dependencies. Install them with:

```bash
pip install boto3
```

### Usage

To use Amazon S3 Vectors with Mem0, you need to have an AWS account and the necessary IAM permissions (`s3vectors:*`). Ensure your environment is configured with AWS credentials (e.g., via `~/.aws/credentials` or environment variables).

```python
import os
from mem0 import Memory

# Ensure your AWS credentials are configured in your environment
# e.g., by setting AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_DEFAULT_REGION

config = {
    "vector_store": {
        "provider": "s3_vectors",
        "config": {
            "vector_bucket_name": "my-mem0-vector-bucket",
            "index_name": "my-memories-index",
            "embedding_model_dims": 1536,
            "distance_metric": "cosine",
            "region_name": "us-east-1"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movie? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

Here are the available parameters for the `s3_vectors` config:

| Parameter              | Description                                                          | Default Value |
| ---------------------- | -------------------------------------------------------------------- | ------------- |
| `vector_bucket_name`   | The name of the S3 Vector bucket to use. It will be created if it doesn't exist. | Required      |
| `index_name`           | The name of the vector index within the bucket.                        | `mem0`        |
| `embedding_model_dims` | Dimensions of the embedding model. Must match your embedder.         | `1536`        |
| `distance_metric`      | Distance metric for similarity search. Options: `cosine`, `euclidean`. | `cosine`      |
| `region_name`          | The AWS region where the bucket and index reside.                    | `None` (uses default from AWS config) |

### IAM Permissions

Your AWS identity (user or role) needs permissions to perform actions on S3 Vectors. A minimal policy would look like this:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3vectors:*",
            "Resource": "*"
        }
    ]
}
```

For production, it is recommended to scope down the resource ARN to your specific buckets and indexes.


================================================
FILE: docs/components/vectordbs/dbs/supabase.mdx
================================================
[Supabase](https://supabase.com/) is an open-source Firebase alternative that provides a PostgreSQL database with pgvector extension for vector similarity search. It offers a powerful and scalable solution for storing and querying vector embeddings.

Create a [Supabase](https://supabase.com/dashboard/projects) account and project, then get your connection string from Project Settings > Database. See the [docs](https://supabase.github.io/vecs/hosting/) for details.

### Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "supabase",
        "config": {
            "connection_string": "postgresql://user:password@host:port/database",
            "collection_name": "memories",
            "index_method": "hnsw",  # Optional: defaults to "auto"
            "index_measure": "cosine_distance"  # Optional: defaults to "cosine_distance"
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript Typescript
import { Memory } from "mem0ai/oss";

const config = {
    vectorStore: {
      provider: "supabase",
      config: {
        collectionName: "memories",
        embeddingModelDims: 1536,
        supabaseUrl: process.env.SUPABASE_URL || "",
        supabaseKey: process.env.SUPABASE_KEY || "",
        tableName: "memories",
      },
    },
}

const memory = new Memory(config);

const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]

await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

### SQL Migrations for TypeScript Implementation

The following SQL migrations are required to enable the vector extension and create the memories table:

```sql
-- Enable the vector extension
create extension if not exists vector;

-- Create the memories table
create table if not exists memories (
  id text primary key,
  embedding vector(1536),
  metadata jsonb,
  created_at timestamp with time zone default timezone('utc', now()),
  updated_at timestamp with time zone default timezone('utc', now())
);

-- Create the vector similarity search function
create or replace function match_vectors(
  query_embedding vector(1536),
  match_count int,
  filter jsonb default '{}'::jsonb
)
returns table (
  id text,
  similarity float,
  metadata jsonb
)
language plpgsql
as $$
begin
  return query
  select
    t.id::text,
    1 - (t.embedding <=> query_embedding) as similarity,
    t.metadata
  from memories t
  where case
    when filter::text = '{}'::text then true
    else t.metadata @> filter
  end
  order by t.embedding <=> query_embedding
  limit match_count;
end;
$$;
```

Goto [Supabase](https://supabase.com/dashboard/projects) and run the above SQL migrations inside the SQL Editor.

### Config

Here are the parameters available for configuring Supabase:

<Tabs>
<Tab title="Python">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `connection_string` | PostgreSQL connection string (required) | None |
| `collection_name` | Name for the vector collection | `mem0` |
| `embedding_model_dims` | Dimensions of the embedding model | `1536` |
| `index_method` | Vector index method to use | `auto` |
| `index_measure` | Distance measure for similarity search | `cosine_distance` |
</Tab>
<Tab title="TypeScript">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `collectionName` | Name for the vector collection | `mem0` |
| `embeddingModelDims` | Dimensions of the embedding model | `1536` |
| `supabaseUrl` | Supabase URL | None |
| `supabaseKey` | Supabase key | None |
| `tableName` | Name for the vector table | `memories` |
</Tab>
</Tabs>

### Index Methods

The following index methods are supported:

- `auto`: Automatically selects the best available index method
- `hnsw`: Hierarchical Navigable Small World graph index (faster search, more memory usage)
- `ivfflat`: Inverted File Flat index (good balance of speed and memory)

### Distance Measures

Available distance measures for similarity search:

- `cosine_distance`: Cosine similarity (recommended for most embedding models)
- `l2_distance`: Euclidean distance
- `l1_distance`: Manhattan distance
- `max_inner_product`: Maximum inner product similarity

### Best Practices

1. **Index Method Selection**:
   - Use `hnsw` for fastest search performance when memory is not a constraint
   - Use `ivfflat` for a good balance of search speed and memory usage
   - Use `auto` if unsure, it will select the best method based on your data

2. **Distance Measure Selection**:
   - Use `cosine_distance` for most embedding models (OpenAI, Hugging Face, etc.)
   - Use `max_inner_product` if your vectors are normalized
   - Use `l2_distance` or `l1_distance` if working with raw feature vectors

3. **Connection String**:
   - Always use environment variables for sensitive information in the connection string
   - Format: `postgresql://user:password@host:port/database`



================================================
FILE: docs/components/vectordbs/dbs/upstash-vector.mdx
================================================
[Upstash Vector](https://upstash.com/docs/vector) is a serverless vector database with built-in embedding models.

### Usage with Upstash embeddings

You can enable the built-in embedding models by setting `enable_embeddings` to `True`. This allows you to use Upstash's embedding models for vectorization.

```python
import os
from mem0 import Memory

os.environ["UPSTASH_VECTOR_REST_URL"] = "..."
os.environ["UPSTASH_VECTOR_REST_TOKEN"] = "..."

config = {
    "vector_store": {
        "provider": "upstash_vector",
        "enable_embeddings": True,
    }
}

m = Memory.from_config(config)
m.add("Likes to play cricket on weekends", user_id="alice", metadata={"category": "hobbies"})
```

<Note>
    Setting `enable_embeddings` to `True` will bypass any external embedding provider you have configured.
</Note>

### Usage with external embedding providers

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "..."
os.environ["UPSTASH_VECTOR_REST_URL"] = "..."
os.environ["UPSTASH_VECTOR_REST_TOKEN"] = "..."

config = {
    "vector_store": {
        "provider": "upstash_vector",
    },
    "embedder": {
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-large"
        },
    }
}

m = Memory.from_config(config)
m.add("Likes to play cricket on weekends", user_id="alice", metadata={"category": "hobbies"})
```

### Config

Here are the parameters available for configuring Upstash Vector:

| Parameter           | Description                        | Default Value |
| ------------------- | ---------------------------------- | ------------- |
| `url`               | URL for the Upstash Vector index   | `None`        |
| `token`             | Token for the Upstash Vector index | `None`        |
| `client`            | An `upstash_vector.Index` instance | `None`        |
| `collection_name`   | The default namespace used         | `""`          |
| `enable_embeddings` | Whether to use Upstash embeddings  | `False`       |

<Note>
  When `url` and `token` are not provided, the `UPSTASH_VECTOR_REST_URL` and
  `UPSTASH_VECTOR_REST_TOKEN` environment variables are used.
</Note>



================================================
FILE: docs/components/vectordbs/dbs/vectorize.mdx
================================================
[Cloudflare Vectorize](https://developers.cloudflare.com/vectorize/) is a vector database offering from Cloudflare, allowing you to build AI-powered applications with vector embeddings.

### Usage

<CodeGroup>
```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  vectorStore: {
    provider: 'vectorize',
    config: {
      indexName: 'my-memory-index',
      accountId: 'your-cloudflare-account-id',
      apiKey: 'your-cloudflare-api-key',
      dimension: 1536, // Optional: defaults to 1536
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm looking for a good book to read."},
    {"role": "assistant", "content": "Sure, what genre are you interested in?"},
    {"role": "user", "content": "I enjoy fantasy novels with strong world-building."},
    {"role": "assistant", "content": "Great! I'll keep that in mind for future recommendations."}
]
await memory.add(messages, { userId: "bob", metadata: { interest: "books" } });
```
</CodeGroup>

### Config

Let's see the available parameters for the `vectorize` config:

<Tabs>
<Tab title="TypeScript">
| Parameter | Description | Default Value |
| --- | --- | --- |
| `indexName` | The name of the Vectorize index | `None` (Required) |
| `accountId` | Your Cloudflare account ID | `None` (Required) |
| `apiKey` | Your Cloudflare API token | `None` (Required) |
| `dimension` | Dimensions of the embedding model | `1536` |
</Tab>
</Tabs>



================================================
FILE: docs/components/vectordbs/dbs/vertex_ai.mdx
================================================
---
title: Vertex AI Vector Search
---


### Usage

To use Google Cloud Vertex AI Vector Search with `mem0`, you need to configure the `vector_store` in your `mem0` config:


```python
import os
from mem0 import Memory

os.environ["GOOGLE_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "vertex_ai_vector_search",
        "config": {
            "endpoint_id": "YOUR_ENDPOINT_ID",            # Required: Vector Search endpoint ID
            "index_id": "YOUR_INDEX_ID",                  # Required: Vector Search index ID 
            "deployment_index_id": "YOUR_DEPLOYMENT_INDEX_ID",  # Required: Deployment-specific ID
            "project_id": "YOUR_PROJECT_ID",              # Required: Google Cloud project ID
            "project_number": "YOUR_PROJECT_NUMBER",      # Required: Google Cloud project number
            "region": "YOUR_REGION",                      # Optional: Defaults to GOOGLE_CLOUD_REGION
            "credentials_path": "path/to/credentials.json", # Optional: Defaults to GOOGLE_APPLICATION_CREDENTIALS
            "vector_search_api_endpoint": "YOUR_API_ENDPOINT" # Required for get operations
        }
    }
}
m = Memory.from_config(config)
m.add("Your text here", user_id="user", metadata={"category": "example"})
```


### Required Parameters

| Parameter | Description | Required |
|-----------|-------------|----------|
| `endpoint_id` | Vector Search endpoint ID | Yes |
| `index_id` | Vector Search index ID | Yes |
| `deployment_index_id` | Deployment-specific index ID | Yes |
| `project_id` | Google Cloud project ID | Yes |
| `project_number` | Google Cloud project number | Yes |
| `vector_search_api_endpoint` | Vector search API endpoint | Yes (for get operations) |
| `region` | Google Cloud region | No (defaults to GOOGLE_CLOUD_REGION) |
| `credentials_path` | Path to service account credentials | No (defaults to GOOGLE_APPLICATION_CREDENTIALS) |



================================================
FILE: docs/components/vectordbs/dbs/weaviate.mdx
================================================
[Weaviate](https://weaviate.io/) is an open-source vector search engine. It allows efficient storage and retrieval of high-dimensional vector embeddings, enabling powerful search and retrieval capabilities.


### Installation
```bash
pip install weaviate weaviate-client
```

### Usage

```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "sk-xx"

config = {
    "vector_store": {
        "provider": "weaviate",
        "config": {
            "collection_name": "test",
            "cluster_url": "http://localhost:8080",
            "auth_client_secret": None,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movie? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

### Config

Let's see the available parameters for the `weaviate` config:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `collection_name` | The name of the collection to store the vectors | `mem0` |
| `embedding_model_dims` | Dimensions of the embedding model | `1536` |
| `cluster_url` | URL for the Weaviate server | `None` |
| `auth_client_secret` | API key for Weaviate authentication | `None` |


================================================
FILE: docs/contributing/development.mdx
================================================
---
title: Development
icon: "code"
---

# Development Contributions

We strive to make contributions **easy, collaborative, and enjoyable**. Follow the steps below to ensure a smooth contribution process.

## Submitting Your Contribution through PR

To contribute, follow these steps:

1. **Fork & Clone** the repository: [Mem0 on GitHub](https://github.com/mem0ai/mem0)
2. **Create a Feature Branch**: Use a dedicated branch for your changes, e.g., `feature/my-new-feature`
3. **Implement Changes**: If adding a feature or fixing a bug, ensure to:
   - Write necessary **tests**
   - Add **documentation, docstrings, and runnable examples**
4. **Code Quality Checks**:
   - Run **linting** to catch style issues
   - Ensure **all tests pass**
5. **Submit a Pull Request** 🚀

For detailed guidance on pull requests, refer to [GitHub's documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request).

---

## 📦 Dependency Management

We use `hatch` as our package manager. Install it by following the [official instructions](https://hatch.pypa.io/latest/install/).

⚠️ **Do NOT use `pip` or `conda` for dependency management.** Instead, follow these steps in order:

```bash
# 1. Install base dependencies
make install

# 2. Activate virtual environment (this will install deps.)
hatch shell (for default env)
hatch -e dev_py_3_11 shell (for dev_py_3_11) (differences are mentioned in pyproject.toml)

# 3. Install all optional dependencies
make install_all
```

---

## 🛠️ Development Standards

### ✅ Pre-commit Hooks

Ensure `pre-commit` is installed before contributing:

```bash
pre-commit install
```

### 🔍 Linting with `ruff`

Run the linter and fix any reported issues before submitting your PR:

```bash
make lint
```

### 🎨 Code Formatting

To maintain a consistent code style, format your code:

```bash
make format
```

### 🧪 Testing with `pytest`

Run tests to verify functionality before submitting your PR:

```bash
make test
```

💡 **Note:** Some dependencies have been removed from the main dependencies to reduce package size. Run `make install_all` to install necessary dependencies before running tests.

---

## 🚀 Release Process

Currently, releases are handled manually. We aim for frequent releases, typically when new features or bug fixes are introduced.

---

Thank you for contributing to Mem0! 🎉


================================================
FILE: docs/contributing/documentation.mdx
================================================
---
title: Documentation
icon: "book"
---

# Documentation Contributions

## 📌 Prerequisites

Before getting started, ensure you have **Node.js (version 23.6.0 or higher)** installed on your system.

---

## 🚀 Setting Up Mintlify

### Step 1: Install Mintlify

Install Mintlify globally using your preferred package manager:

<CodeGroup>

```bash npm
npm i -g mintlify
```

```bash yarn
yarn global add mintlify
```

</CodeGroup>

### Step 2: Run the Documentation Server

Navigate to the `docs/` directory (where `docs.json` is located) and start the development server:

```bash
mintlify dev
```

The documentation website will be available at: [http://localhost:3000](http://localhost:3000).

---

## 🔧 Custom Ports

By default, Mintlify runs on **port 3000**. To use a different port, add the `--port` flag:

```bash
mintlify dev --port 3333
```

---

By following these steps, you can efficiently contribute to **Mem0's documentation**. Happy documenting! ✍️




================================================
FILE: docs/core-concepts/memory-types.mdx
================================================
---
title: Memory Types
description: Understanding different types of memory in AI Applications
icon: "memory"
iconType: "solid"
---

To build useful AI applications, we need to understand how different memory systems work together. This guide explores the fundamental types of memory in AI systems and shows how Mem0 implements these concepts.

## Why Memory Matters

AI systems need memory for three key purposes:
1. Maintaining context during conversations
2. Learning from past interactions
3. Building personalized experiences over time

Without proper memory systems, AI applications would treat each interaction as completely new, losing valuable context and personalization opportunities.

## Short-Term Memory

The most basic form of memory in AI systems holds immediate context - like a person remembering what was just said in a conversation. This includes:

- **Conversation History**: Recent messages and their order
- **Working Memory**: Temporary variables and state
- **Attention Context**: Current focus of the conversation

## Long-Term Memory

More sophisticated AI applications implement long-term memory to retain information across conversations. This includes:

- **Factual Memory**: Stored knowledge about users, preferences, and domain-specific information
- **Episodic Memory**: Past interactions and experiences
- **Semantic Memory**: Understanding of concepts and their relationships

## Memory Characteristics

Each memory type has distinct characteristics:

| Type | Persistence | Access Speed | Use Case |
|------|-------------|--------------|-----------|
| Short-Term | Temporary | Instant | Active conversations |
| Long-Term | Persistent | Fast | User preferences and history |

## How Mem0 Implements Long-Term Memory
Mem0's long-term memory system builds on these foundations by:

1. Using vector embeddings to store and retrieve semantic information
2. Maintaining user-specific context across sessions
3. Implementing efficient retrieval mechanisms for relevant past interactions


================================================
FILE: docs/core-concepts/memory-operations/add.mdx
================================================
---
title: Add Memory
description: Add memory into the Mem0 platform by storing user-assistant interactions and facts for later retrieval.
icon: "plus"
iconType: "solid"
---


## Overview

The `add` operation is how you store memory into Mem0. Whether you're working with a chatbot, a voice assistant, or a multi-agent system, this is the entry point to create long-term memory.

Memories typically come from a **user-assistant interaction** and Mem0 handles the extraction, transformation, and storage for you.

Mem0 offers two implementation flows:

- **Mem0 Platform** (Managed, scalable, with dashboard + API)
- **Mem0 Open Source** (Lightweight, fully local, flexible SDKs)

Each supports the same core memory operations, but with slightly different setup. Below, we walk through examples for both.


## Architecture

<Frame caption="Architecture diagram illustrating the process of adding memories.">
  <img src="../../images/add_architecture.png" />
</Frame>

When you call `add`, Mem0 performs the following steps under the hood:

1. **Information Extraction**
   The input messages are passed through an LLM that extracts key facts, decisions, preferences, or events worth remembering.

2. **Conflict Resolution**
   Mem0 compares the new memory against existing ones to detect duplication or contradiction and handles updates accordingly.

3. **Memory Storage**
   The result is stored in a vector database (for semantic search) and optionally in a graph structure (for relationship mapping).

You don’t need to handle any of this manually, Mem0 takes care of it with a single API call or SDK method.

---

## Example: Mem0 Platform

<CodeGroup>
```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

messages = [
    {"role": "user", "content": "I'm planning a trip to Tokyo next month."},
    {"role": "assistant", "content": "Great! I’ll remember that for future suggestions."}
]

client.add(
    messages=messages,
    user_id="alice",
    version="v2"
)
```

```javascript JavaScript
import { MemoryClient } from "mem0ai";

const client = new MemoryClient({apiKey: "your-api-key"});

const messages = [
  { role: "user", content: "I'm planning a trip to Tokyo next month." },
  { role: "assistant", content: "Great! I’ll remember that for future suggestions." }
];

await client.add({
  messages,
  user_id: "alice",
  version: "v2"
});
```
</CodeGroup>

---

## Example: Mem0 Open Source

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key"

m = Memory()

messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]

# Store inferred memories (default behavior)
result = m.add(messages, user_id="alice", metadata={"category": "movie_recommendations"})

# Optionally store raw messages without inference
result = m.add(messages, user_id="alice", metadata={"category": "movie_recommendations"}, infer=False)
```

```javascript JavaScript
import { Memory } from 'mem0ai/oss';

const memory = new Memory();

const messages = [
  {
    role: "user",
    content: "I like to drink coffee in the morning and go for a walk"
  }
];

const result = memory.add(messages, {
  userId: "alice",
  metadata: { category: "preferences" }
});
```
</CodeGroup>

---

## When Should You Add Memory?

Add memory whenever your agent learns something useful:

- A new user preference is shared
- A decision or suggestion is made
- A goal or task is completed
- A new entity is introduced
- A user gives feedback or clarification

Storing this context allows the agent to reason better in future interactions.


### More Details

For full list of supported fields, required formats, and advanced options, see the
[Add Memory API Reference](/api-reference/memory/add-memories).

---

## Need help?
If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx"/>


================================================
FILE: docs/core-concepts/memory-operations/delete.mdx
================================================
---
title: Delete Memory
description: Remove memories from Mem0 either individually, in bulk, or via filters.
icon: "trash"
iconType: "solid"
---

## Overview

Memories can become outdated, irrelevant, or need to be removed for privacy or compliance reasons. Mem0 offers flexible ways to delete memory:

1. **Delete a Single Memory**: Using a specific memory ID
2. **Batch Delete**: Delete multiple known memory IDs (up to 1000)
3. **Filtered Delete**: Delete memories matching a filter (e.g., `user_id`, `metadata`, `run_id`)

This page walks through code example for each method.


## Use Cases

- Forget a user’s past preferences by request
- Remove outdated or incorrect memory entries
- Clean up memory after session expiration
- Comply with data deletion requests (e.g., GDPR)

---

## 1. Delete a Single Memory by ID

<CodeGroup>
```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

memory_id = "your_memory_id"
client.delete(memory_id=memory_id)
```

```javascript JavaScript
import MemoryClient from 'mem0ai';

const client = new MemoryClient({ apiKey: "your-api-key" });

client.delete("your_memory_id")
  .then(result => console.log(result))
  .catch(error => console.error(error));
```
</CodeGroup>

---

## 2. Batch Delete Multiple Memories

<CodeGroup>
```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

delete_memories = [
    {"memory_id": "id1"},
    {"memory_id": "id2"}
]

response = client.batch_delete(delete_memories)
print(response)
```

```javascript JavaScript
import MemoryClient from 'mem0ai';

const client = new MemoryClient({ apiKey: "your-api-key" });

const deleteMemories = [
  { memory_id: "id1" },
  { memory_id: "id2" }
];

client.batchDelete(deleteMemories)
  .then(response => console.log('Batch delete response:', response))
  .catch(error => console.error(error));
```
</CodeGroup>

---

## 3. Delete Memories by Filter (e.g., user_id)

<CodeGroup>
```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

# Delete all memories for a specific user
client.delete_all(user_id="alice")
```

```javascript JavaScript
import MemoryClient from 'mem0ai';

const client = new MemoryClient({ apiKey: "your-api-key" });

client.deleteAll({ user_id: "alice" })
  .then(result => console.log(result))
  .catch(error => console.error(error));
```
</CodeGroup>

You can also filter by other parameters such as:
- `agent_id`
- `run_id`
- `metadata` (as JSON string)

---

## Key Differences

| Method                | Use When                                | IDs Needed | Filters |
|----------------------|-------------------------------------------|------------|----------|
| `delete(memory_id)`  | You know exactly which memory to remove   | ✔          | ✘        |
| `batch_delete([...])`| You have a known list of memory IDs       | ✔          | ✘        |
| `delete_all(...)`    | You want to delete by user/agent/run/etc | ✘          | ✔        |


### More Details

For request/response schema and additional filtering options, see:
- [Delete Memory API Reference](/api-reference/memory/delete-memory)
- [Batch Delete API Reference](/api-reference/memory/batch-delete)
- [Delete Memories by Filter Reference](/api-reference/memory/delete-memories)

You’ve now seen how to add, search, update, and delete memories in Mem0.

---

## Need help?
If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx"/>



================================================
FILE: docs/core-concepts/memory-operations/search.mdx
================================================
---
title: Search Memory
description: Retrieve relevant memories from Mem0 using powerful semantic and filtered search capabilities.
icon: "magnifying-glass"
iconType: "solid"
---

## Overview

The `search` operation allows you to retrieve relevant memories based on a natural language query and optional filters like user ID, agent ID, categories, and more. This is the foundation of giving your agents memory-aware behavior.

Mem0 supports:
- Semantic similarity search
- Metadata filtering (with advanced logic)
- Reranking and thresholds
- Cross-agent, multi-session context resolution

This applies to both:
- **Mem0 Platform** (hosted API with full-scale features)
- **Mem0 Open Source** (local-first with LLM inference and local vector DB)


## Architecture

<Frame caption="Architecture diagram illustrating the memory search process.">
  <img src="../../images/search_architecture.png" />
</Frame>

The search flow follows these steps:

1. **Query Processing**
   An LLM refines and optimizes your natural language query.

2. **Vector Search**
   Semantic embeddings are used to find the most relevant memories using cosine similarity.

3. **Filtering & Ranking**
   Logical and comparison-based filters are applied. Memories are scored, filtered, and optionally reranked.

4. **Results Delivery**
   Relevant memories are returned with associated metadata and timestamps.

---

## Example: Mem0 Platform

<CodeGroup>
```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

query = "What do you know about me?"
filters = {
   "OR": [
      {"user_id": "alice"},
      {"agent_id": {"in": ["travel-assistant", "customer-support"]}}
   ]
}

results = client.search(query, version="v2", filters=filters)
```

```javascript JavaScript
import { MemoryClient } from "mem0ai";

const client = new MemoryClient({apiKey: "your-api-key"});

const query = "I'm craving some pizza. Any recommendations?";
const filters = {
  AND: [
    { user_id: "alice" }
  ]
};

const results = await client.search(query, {
  version: "v2",
  filters
});
```
</CodeGroup>

---

## Example: Mem0 Open Source

<CodeGroup>
```python Python
from mem0 import Memory

m = Memory()
related_memories = m.search("Should I drink coffee or tea?", user_id="alice")
```

```javascript JavaScript
import { Memory } from 'mem0ai/oss';

const memory = new Memory();
const relatedMemories = memory.search("Should I drink coffee or tea?", { userId: "alice" });
```
</CodeGroup>

---

## Tips for Better Search

- Use descriptive natural queries (Mem0 can interpret intent)
- Apply filters for scoped, faster lookup
- Use `version: "v2"` for enhanced results
- Consider wildcard filters (e.g., `run_id: "*"`) for broader matches
- Tune with `top_k`, `threshold`, or `rerank` if needed


### More Details

For the full list of filter logic, comparison operators, and optional search parameters, see the
[Search Memory API Reference](/api-reference/memory/v2-search-memories).

---

## Need help?
If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx"/>



================================================
FILE: docs/core-concepts/memory-operations/update.mdx
================================================
---
title: Update Memory
description: Modify an existing memory by updating its content or metadata.
icon: "pencil"
iconType: "solid"
---

## Overview

User preferences, interests, and behaviors often evolve over time. The `update` operation lets you revise a stored memory, whether it's updating facts and memories, rephrasing a message, or enriching metadata.

Mem0 supports both:
- **Single Memory Update** for one specific memory using its ID
- **Batch Update** for updating many memories at once (up to 1000)

This guide includes usage for both single update and batch update of memories through **Mem0 Platform**


## Use Cases

- Refine a vague or incorrect memory after a correction
- Add or edit memory with new metadata (e.g., categories, tags)
- Evolve factual knowledge as the user’s profile changes
- A user profile evolves: “I love spicy food” → later says “Actually, I can’t handle spicy food.”

Updating memory ensures your agents remain accurate, adaptive, and personalized.

---

## Update Memory

<CodeGroup>
```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

memory_id = "your_memory_id"
client.update(
    memory_id=memory_id,
    text="Updated memory content about the user",
    metadata={"category": "profile-update"}
)
```

```javascript JavaScript
import MemoryClient from 'mem0ai';

const client = new MemoryClient({ apiKey: "your-api-key" });
const memory_id = "your_memory_id";

client.update(memory_id, {
  text: "Updated memory content about the user",
  metadata: { category: "profile-update" }
})
  .then(result => console.log(result))
  .catch(error => console.error(error));
```
</CodeGroup>

---

## Batch Update

Update up to 1000 memories in one call.

<CodeGroup>
```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

update_memories = [
    {"memory_id": "id1", "text": "Watches football"},
    {"memory_id": "id2", "text": "Likes to travel"}
]

response = client.batch_update(update_memories)
print(response)
```

```javascript JavaScript
import MemoryClient from 'mem0ai';

const client = new MemoryClient({ apiKey: "your-api-key" });

const updateMemories = [
  { memoryId: "id1", text: "Watches football" },
  { memoryId: "id2", text: "Likes to travel" }
];

client.batchUpdate(updateMemories)
  .then(response => console.log('Batch update response:', response))
  .catch(error => console.error(error));
```
</CodeGroup>

---

## Tips

- You can update both `text` and `metadata` in the same call.
- Use `batchUpdate` when you're applying similar corrections at scale.
- If memory is marked `immutable`, it must first be deleted and re-added.
- Combine this with feedback mechanisms (e.g., user thumbs-up/down) to self-improve memory.


### More Details

Refer to the full [Update Memory API Reference](/api-reference/memory/update-memory) and [Batch Update Reference](/api-reference/memory/batch-update) for schema and advanced fields.

---

## Need help?
If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx"/>



================================================
FILE: docs/examples/ai_companion_js.mdx
================================================
---
title: AI Companion in Node.js
---

You can create a personalised AI Companion using Mem0. This guide will walk you through the necessary steps and provide the complete code to get you started.

## Overview

The Personalized AI Companion leverages Mem0 to retain information across interactions, enabling a tailored learning experience. It creates memories for each user interaction and integrates with OpenAI's GPT models to provide detailed and context-aware responses to user queries.

## Setup

Before you begin, ensure you have Node.js installed and create a new project. Install the required dependencies using npm:

```bash
npm install openai mem0ai
```

## Full Code Example

Below is the complete code to create and interact with an AI Companion using Mem0:

```javascript
import { OpenAI } from 'openai';
import { Memory } from 'mem0ai/oss';
import * as readline from 'readline';

const openaiClient = new OpenAI();
const memory = new Memory();

async function chatWithMemories(message, userId = "default_user") {
  const relevantMemories = await memory.search(message, { userId: userId });
  
  const memoriesStr = relevantMemories.results
    .map(entry => `- ${entry.memory}`)
    .join('\n');
  
  const systemPrompt = `You are a helpful AI. Answer the question based on query and memories.
User Memories:
${memoriesStr}`;
  
  const messages = [
    { role: "system", content: systemPrompt },
    { role: "user", content: message }
  ];
  
  const response = await openaiClient.chat.completions.create({
    model: "gpt-4o-mini",
    messages: messages
  });
  
  const assistantResponse = response.choices[0].message.content || "";
  
  messages.push({ role: "assistant", content: assistantResponse });
  await memory.add(messages, { userId: userId });
  
  return assistantResponse;
}

async function main() {
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout
  });
  
  console.log("Chat with AI (type 'exit' to quit)");
  
  const askQuestion = () => {
    return new Promise((resolve) => {
      rl.question("You: ", (input) => {
        resolve(input.trim());
      });
    });
  };
  
  try {
    while (true) {
      const userInput = await askQuestion();
      
      if (userInput.toLowerCase() === 'exit') {
        console.log("Goodbye!");
        rl.close();
        break;
      }
      
      const response = await chatWithMemories(userInput, "sample_user");
      console.log(`AI: ${response}`);
    }
  } catch (error) {
    console.error("An error occurred:", error);
    rl.close();
  }
}

main().catch(console.error);
```

### Key Components

1. **Initialization**
   - The code initializes both OpenAI and Mem0 Memory clients
   - Uses Node.js's built-in readline module for command-line interaction

2. **Memory Management (chatWithMemories function)**
   - Retrieves relevant memories using Mem0's search functionality
   - Constructs a system prompt that includes past memories
   - Makes API calls to OpenAI for generating responses
   - Stores new interactions in memory

3. **Interactive Chat Interface (main function)**
   - Creates a command-line interface for user interaction
   - Handles user input and displays AI responses
   - Includes graceful exit functionality

### Environment Setup

Make sure to set up your environment variables:
```bash
export OPENAI_API_KEY=your_api_key
```

### Conclusion

This implementation demonstrates how to create an AI Companion that maintains context across conversations using Mem0's memory capabilities. The system automatically stores and retrieves relevant information, creating a more personalized and context-aware interaction experience.

As users interact with the system, Mem0's memory system continuously learns and adapts, making future responses more relevant and personalized. This setup is ideal for creating long-term learning AI assistants that can maintain context and provide increasingly personalized responses over time.



================================================
FILE: docs/examples/aws_example.mdx
================================================
---
title: "Amazon Stack: AWS Bedrock, AOSS, and Neptune Analytics"
---

This example demonstrates how to configure and use the `mem0ai` SDK with **AWS Bedrock**, **OpenSearch Service (AOSS)**, and **AWS Neptune Analytics** for persistent memory capabilities in Python.

## Installation

Install the required dependencies to include the Amazon data stack, including **boto3**, **opensearch-py**, and **langchain-aws**:

```bash
pip install "mem0ai[graph,extras]"
```

## Environment Setup

Set your AWS environment variables:

```python
import os

# Set these in your environment or notebook
os.environ['AWS_REGION'] = 'us-west-2'
os.environ['AWS_ACCESS_KEY_ID'] = 'AK00000000000000000'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'AS00000000000000000'

# Confirm they are set
print(os.environ['AWS_REGION'])
print(os.environ['AWS_ACCESS_KEY_ID'])
print(os.environ['AWS_SECRET_ACCESS_KEY'])
```

## Configuration and Usage

This sets up Mem0 with:
- [AWS Bedrock for LLM](https://docs.mem0.ai/components/llms/models/aws_bedrock)
- [AWS Bedrock for embeddings](https://docs.mem0.ai/components/embedders/models/aws_bedrock#aws-bedrock)
- [OpenSearch as the vector store](https://docs.mem0.ai/components/vectordbs/dbs/opensearch)
- [Neptune Analytics as your graph store](https://docs.mem0.ai/open-source/graph_memory/overview#initialize-neptune-analytics).

```python
import boto3
from opensearchpy import RequestsHttpConnection, AWSV4SignerAuth
from mem0.memory.main import Memory

region = 'us-west-2'
service = 'aoss'
credentials = boto3.Session().get_credentials()
auth = AWSV4SignerAuth(credentials, region, service)

config = {
    "embedder": {
        "provider": "aws_bedrock",
        "config": {
            "model": "amazon.titan-embed-text-v2:0"
        }
    },
    "llm": {
        "provider": "aws_bedrock",
        "config": {
            "model": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
            "temperature": 0.1,
            "max_tokens": 2000
        }
    },
    "vector_store": {
        "provider": "opensearch",
        "config": {
            "collection_name": "mem0",
            "host": "your-opensearch-domain.us-west-2.es.amazonaws.com",
            "port": 443,
            "http_auth": auth,
            "connection_class": RequestsHttpConnection,
            "pool_maxsize": 20,
            "use_ssl": True,
            "verify_certs": True,
            "embedding_model_dims": 1024,
        }
    },
    "graph_store": {
        "provider": "neptune",
        "config": {
            "endpoint": f"neptune-graph://my-graph-identifier",
        },
    },
}

# Initialize the memory system
m = Memory.from_config(config)
```

## Usage

Reference [Notebook example](https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/neptune-example.ipynb)

#### Add a memory:

```python
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]

# Store inferred memories (default behavior)
result = m.add(messages, user_id="alice", metadata={"category": "movie_recommendations"})
```

#### Search a memory:
```python
relevant_memories = m.search(query, user_id="alice")
```

#### Get all memories:
```python
all_memories = m.get_all(user_id="alice")
```

#### Get a specific memory:
```python
memory = m.get(memory_id)
```


---

## Conclusion

With Mem0 and AWS services like Bedrock, OpenSearch, and Neptune Analytics, you can build intelligent AI companions that remember, adapt, and personalize their responses over time. This makes them ideal for long-term assistants, tutors, or support bots with persistent memory and natural conversation abilities.



================================================
FILE: docs/examples/chrome-extension.mdx
================================================
# Mem0 Chrome Extension

Enhance your AI interactions with **Mem0**, a Chrome extension that introduces a universal memory layer across platforms like `ChatGPT`, `Claude`, and `Perplexity`. Mem0 ensures seamless context sharing, making your AI experiences more personalized and efficient.

<Note>
  🎉 We now support Grok! The Mem0 Chrome Extension has been updated to work with Grok, bringing the same powerful memory capabilities to your Grok conversations.
</Note>


## Features

- **Universal Memory Layer**: Share context seamlessly across ChatGPT, Claude, Perplexity, and Grok.
- **Smart Context Detection**: Automatically captures relevant information from your conversations.
- **Intelligent Memory Retrieval**: Surfaces pertinent memories at the right time.
- **One-Click Sync**: Easily synchronize with existing ChatGPT memories.
- **Memory Dashboard**: Manage all your memories in one centralized location.

## Installation

You can install the Mem0 Chrome Extension using one of the following methods:

### Method 1: Chrome Web Store Installation

1. **Download the Extension**: Open Google Chrome and navigate to the [Mem0 Chrome Extension page](https://chromewebstore.google.com/detail/mem0/onihkkbipkfeijkadecaafbgagkhglop?hl=en).
2. **Add to Chrome**: Click on the "Add to Chrome" button.
3. **Confirm Installation**: In the pop-up dialog, click "Add extension" to confirm. The Mem0 icon should now appear in your Chrome toolbar.

### Method 2: Manual Installation

1. **Download the Extension**: Clone or download the extension files from the [Mem0 Chrome Extension GitHub repository](https://github.com/mem0ai/mem0-chrome-extension).
2. **Access Chrome Extensions**: Open Google Chrome and navigate to `chrome://extensions`.
3. **Enable Developer Mode**: Toggle the "Developer mode" switch in the top right corner.
4. **Load Unpacked Extension**: Click "Load unpacked" and select the directory containing the extension files.
5. **Confirm Installation**: The Mem0 Chrome Extension should now appear in your Chrome toolbar.

## Usage

1. **Locate the Mem0 Icon**: After installation, find the Mem0 icon in your Chrome toolbar.
2. **Sign In**: Click the icon and sign in with your Google account.
3. **Interact with AI Assistants**:
   - **ChatGPT and Perplexity**: Continue your conversations as usual; Mem0 operates seamlessly in the background.
   - **Claude**: Click the Mem0 button or use the shortcut `Ctrl + M` to activate memory functions.

## Configuration

- **API Key**: Obtain your API key from the Mem0 Dashboard to connect the extension to the Mem0 API.
- **User ID**: This is your unique identifier in the Mem0 system. If not provided, it defaults to 'chrome-extension-user'.

## Demo Video

<iframe width="700" height="400" src="https://www.youtube.com/embed/dqenCMMlfwQ?si=zhGVrkq6IS_0Jwyj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Privacy and Data Security

Your messages are sent to the Mem0 API for extracting and retrieving memories. Mem0 is committed to ensuring your data's privacy and security.



================================================
FILE: docs/examples/collaborative-task-agent.mdx
================================================
---
title: Multi-User Collaboration with Mem0
---

## Overview

Build a multi-user collaborative chat or task management system with Mem0. Each message is attributed to its author, and all messages are stored in a shared project space. Mem0 makes it easy to track contributions, sort and group messages, and collaborate in real time.

## Setup

Install the required packages:

```bash
pip install openai mem0ai
```

## Full Code Example

```python
from openai import OpenAI
from mem0 import Memory
import os
from datetime import datetime
from collections import defaultdict

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "sk-your-key"

# Shared project context
RUN_ID = "project-demo"

# Initialize Mem0
mem = Memory()

class CollaborativeAgent:
    def __init__(self, run_id):
        self.run_id = run_id
        self.mem = mem

    def add_message(self, role, name, content):
        msg = {"role": role, "name": name, "content": content}
        self.mem.add([msg], run_id=self.run_id, infer=False)

    def brainstorm(self, prompt):
        # Get recent messages for context
        memories = self.mem.search(prompt, run_id=self.run_id, limit=5)["results"]
        context = "\n".join(f"- {m['memory']} (by {m.get('actor_id', 'Unknown')})" for m in memories)
        client = OpenAI()
        messages = [
            {"role": "system", "content": "You are a helpful project assistant."},
            {"role": "user", "content": f"Prompt: {prompt}\nContext:\n{context}"}
        ]
        reply = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages
        ).choices[0].message.content.strip()
        self.add_message("assistant", "assistant", reply)
        return reply

    def get_all_messages(self):
        return self.mem.get_all(run_id=self.run_id)["results"]

    def print_sorted_by_time(self):
        messages = self.get_all_messages()
        messages.sort(key=lambda m: m.get('created_at', ''))
        print("\n--- Messages (sorted by time) ---")
        for m in messages:
            who = m.get("actor_id") or "Unknown"
            ts = m.get('created_at', 'Timestamp N/A')
            try:
                dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                ts_fmt = dt.strftime('%Y-%m-%d %H:%M:%S')
            except Exception:
                ts_fmt = ts
            print(f"[{ts_fmt}] [{who}] {m['memory']}")

    def print_grouped_by_actor(self):
        messages = self.get_all_messages()
        grouped = defaultdict(list)
        for m in messages:
            grouped[m.get("actor_id") or "Unknown"].append(m)
        print("\n--- Messages (grouped by actor) ---")
        for actor, mems in grouped.items():
            print(f"\n=== {actor} ===")
            for m in mems:
                ts = m.get('created_at', 'Timestamp N/A')
                try:
                    dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                    ts_fmt = dt.strftime('%Y-%m-%d %H:%M:%S')
                except Exception:
                    ts_fmt = ts
                print(f"[{ts_fmt}] {m['memory']}")
```

## Usage

```python
# Example usage
agent = CollaborativeAgent(RUN_ID)
agent.add_message("user", "alice", "Let's list tasks for the new landing page.")
agent.add_message("user", "bob", "I'll own the hero section copy.")
agent.add_message("user", "carol", "I'll choose product screenshots.")

# Brainstorm with context
print("\nAssistant reply:\n", agent.brainstorm("What are the current open tasks?"))

# Print all messages sorted by time
agent.print_sorted_by_time()

# Print all messages grouped by actor
agent.print_grouped_by_actor()
```

## Key Points

- Each message is attributed to a user or agent (actor)
- All messages are stored in a shared project space (`run_id`)
- You can sort messages by time, group by actor, and format timestamps for clarity
- Mem0 makes it easy to build collaborative, attributed chat/task systems

## Conclusion

Mem0 enables fast, transparent collaboration for teams and agents, with full attribution, flexible memory search, and easy message organization.



================================================
FILE: docs/examples/customer-support-agent.mdx
================================================
---
title: Customer Support AI Agent
---


You can create a personalized Customer Support AI Agent using Mem0. This guide will walk you through the necessary steps and provide the complete code to get you started.

## Overview

The Customer Support AI Agent leverages Mem0 to retain information across interactions, enabling a personalized and efficient support experience.

## Setup

Install the necessary packages using pip:

```bash
pip install openai mem0ai
```

## Full Code Example

Below is the simplified code to create and interact with a Customer Support AI Agent using Mem0:

```python
import os
from openai import OpenAI
from mem0 import Memory

# Set the OpenAI API key
os.environ['OPENAI_API_KEY'] = 'sk-xxx'

class CustomerSupportAIAgent:
    def __init__(self):
        """
        Initialize the CustomerSupportAIAgent with memory configuration and OpenAI client.
        """
        config = {
            "vector_store": {
                "provider": "qdrant",
                "config": {
                    "host": "localhost",
                    "port": 6333,
                }
            },
        }
        self.memory = Memory.from_config(config)
        self.client = OpenAI()
        self.app_id = "customer-support"

    def handle_query(self, query, user_id=None):
        """
        Handle a customer query and store the relevant information in memory.

        :param query: The customer query to handle.
        :param user_id: Optional user ID to associate with the memory.
        """
        # Start a streaming chat completion request to the AI
        stream = self.client.chat.completions.create(
            model="gpt-4",
            stream=True,
            messages=[
                {"role": "system", "content": "You are a customer support AI agent."},
                {"role": "user", "content": query}
            ]
        )
        # Store the query in memory
        self.memory.add(query, user_id=user_id, metadata={"app_id": self.app_id})

        # Print the response from the AI in real-time
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                print(chunk.choices[0].delta.content, end="")

    def get_memories(self, user_id=None):
        """
        Retrieve all memories associated with the given customer ID.

        :param user_id: Optional user ID to filter memories.
        :return: List of memories.
        """
        return self.memory.get_all(user_id=user_id)

# Instantiate the CustomerSupportAIAgent
support_agent = CustomerSupportAIAgent()

# Define a customer ID
customer_id = "jane_doe"

# Handle a customer query
support_agent.handle_query("I need help with my recent order. It hasn't arrived yet.", user_id=customer_id)
```

### Fetching Memories

You can fetch all the memories at any point in time using the following code:

```python
memories = support_agent.get_memories(user_id=customer_id)
for m in memories['results']:
    print(m['memory'])
```

### Key Points

- **Initialization**: The CustomerSupportAIAgent class is initialized with the necessary memory configuration and OpenAI client setup.
- **Handling Queries**: The handle_query method sends a query to the AI and stores the relevant information in memory.
- **Retrieving Memories**: The get_memories method fetches all stored memories associated with a customer.

### Conclusion

As the conversation progresses, Mem0's memory automatically updates based on the interactions, providing a continuously improving personalized support experience.


================================================
FILE: docs/examples/eliza_os.mdx
================================================
---
title: Eliza OS Character
---

You can create a personalised Eliza OS Character using Mem0. This guide will walk you through the necessary steps and provide the complete code to get you started.

## Overview

ElizaOS is a powerful AI agent framework for autonomy & personality. It is a collection of tools that help you create a personalised AI agent.

## Setup
You can start by cloning the eliza-os repository:

```bash
git clone https://github.com/elizaOS/eliza.git
```

Change the directory to the eliza-os repository:

```bash
cd eliza
```

Install the dependencies:

```bash
pnpm install
```

Build the project:

```bash
pnpm build
```

## Setup ENVs

Create a `.env` file in the root of the project and add the following ( You can use the `.env.example` file as a reference):

```bash
# Mem0 Configuration
MEM0_API_KEY= # Mem0 API Key ( Get from https://app.mem0.ai/dashboard/api-keys )
MEM0_USER_ID= # Default: eliza-os-user
MEM0_PROVIDER= # Default: openai
MEM0_PROVIDER_API_KEY= # API Key for the provider (openai, anthropic, etc.)
SMALL_MEM0_MODEL= # Default: gpt-4o-mini
MEDIUM_MEM0_MODEL= # Default: gpt-4o
LARGE_MEM0_MODEL= # Default: gpt-4o
```

## Make the default character use Mem0

By default, there is a character called `eliza` that uses the `ollama` model. You can make this character use Mem0 by changing the config in the `agent/src/defaultCharacter.ts` file.

```ts
modelProvider: ModelProviderName.MEM0,
```

This will make the character use Mem0 to generate responses.

## Run the project

```bash
pnpm start
```

## Conclusion

You have now created a personalised Eliza OS Character using Mem0. You can now start interacting with the character by running the project and talking to the character.

This is a simple example of how to use Mem0 to create a personalised AI agent. You can use this as a starting point to create your own AI agent.





================================================
FILE: docs/examples/email_processing.mdx
================================================
---
title: Email Processing with Mem0
---

This guide demonstrates how to build an intelligent email processing system using Mem0's memory capabilities. You'll learn how to store, categorize, retrieve, and analyze emails to create a smart email management solution.

## Overview

Email overload is a common challenge for many professionals. By leveraging Mem0's memory capabilities, you can build an intelligent system that:

- Stores emails as searchable memories
- Categorizes emails automatically
- Retrieves relevant past conversations
- Prioritizes messages based on importance
- Generates summaries and action items

## Setup

Before you begin, ensure you have the required dependencies installed:

```bash
pip install mem0ai openai
```

## Implementation

### Basic Email Memory System

The following example shows how to create a basic email processing system with Mem0:

```python
import os
from mem0 import MemoryClient
from email.parser import Parser

# Configure API keys
os.environ["MEM0_API_KEY"] = "your-mem0-api-key"

# Initialize Mem0 client
client = MemoryClient()

class EmailProcessor:
    def __init__(self):
        """Initialize the Email Processor with Mem0 memory client"""
        self.client = client
        
    def process_email(self, email_content, user_id):
        """
        Process an email and store it in Mem0 memory
        
        Args:
            email_content (str): Raw email content
            user_id (str): User identifier for memory association
        """
        # Parse email
        parser = Parser()
        email = parser.parsestr(email_content)
        
        # Extract email details
        sender = email['from']
        recipient = email['to']
        subject = email['subject']
        date = email['date']
        body = self._get_email_body(email)
        
        # Create message object for Mem0
        message = {
            "role": "user",
            "content": f"Email from {sender}: {subject}\n\n{body}"
        }
        
        # Create metadata for better retrieval
        metadata = {
            "email_type": "incoming",
            "sender": sender,
            "recipient": recipient,
            "subject": subject,
            "date": date
        }
        
        # Store in Mem0 with appropriate categories
        response = self.client.add(
            messages=[message],
            user_id=user_id,
            metadata=metadata,
            categories=["email", "correspondence"],
            version="v2"
        )
        
        return response
    
    def _get_email_body(self, email):
        """Extract the body content from an email"""
        # Simplified extraction - in real-world, handle multipart emails
        if email.is_multipart():
            for part in email.walk():
                if part.get_content_type() == "text/plain":
                    return part.get_payload(decode=True).decode()
        else:
            return email.get_payload(decode=True).decode()
    
    def search_emails(self, query, user_id):
        """
        Search through stored emails
        
        Args:
            query (str): Search query
            user_id (str): User identifier
        """
        # Search Mem0 for relevant emails
        results = self.client.search(
            query=query,
            user_id=user_id,
            categories=["email"],
            output_format="v1.1",
            version="v2"
        )
        
        return results
        
    def get_email_thread(self, subject, user_id):
        """
        Retrieve all emails in a thread based on subject
        
        Args:
            subject (str): Email subject to match
            user_id (str): User identifier
        """
        filters = {
            "AND": [
                {"user_id": user_id},
                {"categories": {"contains": "email"}},
                {"metadata": {"subject": {"contains": subject}}}
            ]
        }
        
        thread = self.client.get_all(
            version="v2",
            filters=filters,
            output_format="v1.1"
        )
        
        return thread

# Initialize the processor
processor = EmailProcessor()

# Example raw email
sample_email = """From: alice@example.com
To: bob@example.com
Subject: Meeting Schedule Update
Date: Mon, 15 Jul 2024 14:22:05 -0700

Hi Bob,

I wanted to update you on the schedule for our upcoming project meeting.
We'll be meeting this Thursday at 2pm instead of Friday.

Could you please prepare your section of the presentation?

Thanks,
Alice
"""

# Process and store the email
user_id = "bob@example.com"
processor.process_email(sample_email, user_id)

# Later, search for emails about meetings
meeting_emails = processor.search_emails("meeting schedule", user_id)
print(f"Found {len(meeting_emails['results'])} relevant emails")
```

## Key Features and Benefits

- **Long-term Email Memory**: Store and retrieve email conversations across long periods
- **Semantic Search**: Find relevant emails even if they don't contain exact keywords
- **Intelligent Categorization**: Automatically sort emails into meaningful categories
- **Action Item Extraction**: Identify and track tasks mentioned in emails
- **Priority Management**: Focus on important emails based on AI-determined priority
- **Context Awareness**: Maintain thread context for more relevant interactions

## Conclusion

By combining Mem0's memory capabilities with email processing, you can create intelligent email management systems that help users organize, prioritize, and act on their inbox effectively. The advanced capabilities like automatic categorization, action item extraction, and priority management can significantly reduce the time spent on email management, allowing users to focus on more important tasks.




================================================
FILE: docs/examples/llama-index-mem0.mdx
================================================
---
title: LlamaIndex ReAct Agent
---

Create a ReAct Agent with LlamaIndex which uses Mem0 as the memory store.

### Overview
A ReAct agent combines reasoning and action capabilities, making it versatile for tasks requiring both thought processes (reasoning) and interaction with tools or APIs (acting). Mem0 as memory enhances these capabilities by allowing the agent to store and retrieve contextual information from past interactions.

### Setup
```bash
pip install llama-index-core llama-index-memory-mem0
```

Initialize the LLM.
```python
import os
from llama_index.llms.openai import OpenAI

os.environ["OPENAI_API_KEY"] = "<your-openai-api-key>"
llm = OpenAI(model="gpt-4o")
```

Initialize the Mem0 client. You can find your API key [here](https://app.mem0.ai/dashboard/api-keys). Read about Mem0 [Open Source](https://docs.mem0.ai/open-source/overview).
```python
os.environ["MEM0_API_KEY"] = "<your-mem0-api-key>"

from llama_index.memory.mem0 import Mem0Memory

context = {"user_id": "david"}
memory_from_client = Mem0Memory.from_client(
    context=context,
    api_key=os.environ["MEM0_API_KEY"],
    search_msg_limit=4,  # optional, default is 5
)
```

Create the tools. These tools will be used by the agent to perform actions.
```python
from llama_index.core.tools import FunctionTool

def call_fn(name: str):
    """Call the provided name.
    Args:
        name: str (Name of the person)
    """
    return f"Calling... {name}"

def email_fn(name: str):
    """Email the provided name.
    Args:
        name: str (Name of the person)
    """
    return f"Emailing... {name}"

def order_food(name: str, dish: str):
    """Order food for the provided name.
    Args:
        name: str (Name of the person)
        dish: str (Name of the dish)
    """
    return f"Ordering {dish} for {name}"

call_tool = FunctionTool.from_defaults(fn=call_fn)
email_tool = FunctionTool.from_defaults(fn=email_fn)
order_food_tool = FunctionTool.from_defaults(fn=order_food)
```

Initialize the agent with tools and memory.
```python
from llama_index.core.agent import FunctionCallingAgent

agent = FunctionCallingAgent.from_tools(
    [call_tool, email_tool, order_food_tool],
    llm=llm,
    memory=memory_from_client,  # or memory_from_config
    verbose=True,
)
```

Start the chat.
<Note> The agent will use the Mem0 to store the relevant memories from the chat. </Note>

Input
```python
response = agent.chat("Hi, My name is David")
print(response)
```
Output
```text
> Running step bf44a75a-a920-4cf3-944e-b6e6b5695043. Step input: Hi, My name is David
Added user message to memory: Hi, My name is David
=== LLM Response ===
Hello, David! How can I assist you today?
```

Input
```python
response = agent.chat("I love to eat pizza on weekends")
print(response)
```
Output
```text
> Running step 845783b0-b85b-487c-baee-8460ebe8b38d. Step input: I love to eat pizza on weekends
Added user message to memory: I love to eat pizza on weekends
=== LLM Response ===
Pizza is a great choice for the weekend! If you'd like, I can help you order some. Just let me know what kind of pizza you prefer!
```
Input
```python
response = agent.chat("My preferred way of communication is email")
print(response)
```
Output
```text
> Running step 345842f0-f8a0-42ea-a1b7-612265d72a92. Step input: My preferred way of communication is email
Added user message to memory: My preferred way of communication is email
=== LLM Response ===
Got it! If you need any assistance or have any requests, feel free to let me know, and I can communicate with you via email.
```

### Using the agent WITHOUT memory
Input
```python
agent = FunctionCallingAgent.from_tools(
    [call_tool, email_tool, order_food_tool],
    # memory is not provided
    llm=llm,
    verbose=True,
)
response = agent.chat("I am feeling hungry, order me something and send me the bill")
print(response)
```
Output
```text
> Running step e89eb75d-75e1-4dea-a8c8-5c3d4b77882d. Step input: I am feeling hungry, order me something and send me the bill
Added user message to memory: I am feeling hungry, order me something and send me the bill
=== LLM Response ===
Please let me know your name and the dish you'd like to order, and I'll take care of it for you!
```
<Note> The agent is not able to remember the past preferences that user shared in previous chats. </Note>

### Using the agent WITH memory
Input
```python
agent = FunctionCallingAgent.from_tools(
    [call_tool, email_tool, order_food_tool],
    llm=llm,
    # memory is provided
    memory=memory_from_client,  # or memory_from_config
    verbose=True,
)
response = agent.chat("I am feeling hungry, order me something and send me the bill")
print(response)
```

Output
```text
> Running step 5e473db9-3973-4cb1-a5fd-860be0ab0006. Step input: I am feeling hungry, order me something and send me the bill
Added user message to memory: I am feeling hungry, order me something and send me the bill
=== Calling Function ===
Calling function: order_food with args: {"name": "David", "dish": "pizza"}
=== Function Output ===
Ordering pizza for David
=== Calling Function ===
Calling function: email_fn with args: {"name": "David"}
=== Function Output ===
Emailing... David
> Running step 38080544-6b37-4bb2-aab2-7670100d926e. Step input: None
=== LLM Response ===
I've ordered a pizza for you, and the bill has been sent to your email. Enjoy your meal! If there's anything else you need, feel free to let me know.
```
<Note> The agent is able to remember the past preferences that user shared and use them to perform actions. </Note>



================================================
FILE: docs/examples/llamaindex-multiagent-learning-system.mdx
================================================
---
title: LlamaIndex Multi-Agent Learning System
---

<Snippet file="blank-notif.mdx" />

Build an intelligent multi-agent learning system that uses Mem0 to maintain persistent memory across multiple specialized agents. This example demonstrates how to create a tutoring system where different agents collaborate while sharing a unified memory layer.

## Overview

This example showcases a **Multi-Agent Personal Learning System** that combines:
- **LlamaIndex AgentWorkflow** for multi-agent orchestration
- **Mem0** for persistent, shared memory across agents
- **Multi-agents** that collaborate on teaching tasks

The system consists of two agents:
- **TutorAgent**: Primary instructor for explanations and concept teaching
- **PracticeAgent**: Generates exercises and tracks learning progress

Both agents share the same memory context, enabling seamless collaboration and continuous learning from student interactions.

## Key Features

- **Persistent Memory**: Agents remember previous interactions across sessions
- **Multi-Agent Collaboration**: Agents can hand off tasks to each other
- **Personalized Learning**: Adapts to individual student needs and learning styles
- **Progress Tracking**: Monitors learning patterns and skill development
- **Memory-Driven Teaching**: References past struggles and successes

## Prerequisites

Install the required packages:

```bash
pip install llama-index-core llama-index-memory-mem0 openai python-dotenv
```

Set up your environment variables:
- `MEM0_API_KEY`: Your Mem0 Platform API key
- `OPENAI_API_KEY`: Your OpenAI API key

You can obtain your Mem0 Platform API key from the [Mem0 Platform](https://app.mem0.ai).

## Complete Implementation

```python
"""
Multi-Agent Personal Learning System: Mem0 + LlamaIndex AgentWorkflow Example

INSTALLATIONS:
!pip install llama-index-core llama-index-memory-mem0 openai

You need MEM0_API_KEY and OPENAI_API_KEY to run the example.
"""

import asyncio
from datetime import datetime
from dotenv import load_dotenv

# LlamaIndex imports
from llama_index.core.agent.workflow import AgentWorkflow, FunctionAgent
from llama_index.llms.openai import OpenAI
from llama_index.core.tools import FunctionTool

# Memory integration
from llama_index.memory.mem0 import Mem0Memory

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

load_dotenv()


class MultiAgentLearningSystem:
    """
    Multi-Agent Architecture:
    - TutorAgent: Main teaching and explanations
    - PracticeAgent: Exercises and skill reinforcement
    - Shared Memory: Both agents learn from student interactions
    """

    def __init__(self, student_id: str):
        self.student_id = student_id
        self.llm = OpenAI(model="gpt-4o", temperature=0.2)

        # Memory context for this student
        self.memory_context = {"user_id": student_id, "app": "learning_assistant"}
        self.memory = Mem0Memory.from_client(
            context=self.memory_context
        )

        self._setup_agents()

    def _setup_agents(self):
        """Setup two agents that work together and share memory"""

        # TOOLS
        async def assess_understanding(topic: str, student_response: str) -> str:
            """Assess student's understanding of a topic and save insights"""
            # Simulate assessment logic
            if "confused" in student_response.lower() or "don't understand" in student_response.lower():
                assessment = f"STRUGGLING with {topic}: {student_response}"
                insight = f"Student needs more help with {topic}. Prefers step-by-step explanations."
            elif "makes sense" in student_response.lower() or "got it" in student_response.lower():
                assessment = f"UNDERSTANDS {topic}: {student_response}"
                insight = f"Student grasped {topic} quickly. Can move to advanced concepts."
            else:
                assessment = f"PARTIAL understanding of {topic}: {student_response}"
                insight = f"Student has basic understanding of {topic}. Needs reinforcement."

            return f"Assessment: {assessment}\nInsight saved: {insight}"

        async def track_progress(topic: str, success_rate: str) -> str:
            """Track learning progress and identify patterns"""
            progress_note = f"Progress on {topic}: {success_rate} - {datetime.now().strftime('%Y-%m-%d')}"
            return f"Progress tracked: {progress_note}"

        # Convert to FunctionTools
        tools = [
            FunctionTool.from_defaults(async_fn=assess_understanding),
            FunctionTool.from_defaults(async_fn=track_progress)
        ]

        # AGENTS
        # Tutor Agent - Main teaching and explanation
        self.tutor_agent = FunctionAgent(
            name="TutorAgent",
            description="Primary instructor that explains concepts and adapts to student needs",
            system_prompt="""
            You are a patient, adaptive programming tutor. Your key strength is REMEMBERING and BUILDING on previous interactions.

            Key Behaviors:
            1. Always check what the student has learned before (use memory context)
            2. Adapt explanations based on their preferred learning style
            3. Reference previous struggles or successes
            4. Build progressively on past lessons
            5. Use assess_understanding to evaluate responses and save insights

            MEMORY-DRIVEN TEACHING:
            - "Last time you struggled with X, so let's approach Y differently..."
            - "Since you prefer visual examples, here's a diagram..."
            - "Building on the functions we covered yesterday..."

            When student shows understanding, hand off to PracticeAgent for exercises.
            """,
            tools=tools,
            llm=self.llm,
            can_handoff_to=["PracticeAgent"]
        )

        # Practice Agent - Exercises and reinforcement
        self.practice_agent = FunctionAgent(
            name="PracticeAgent",
            description="Creates practice exercises and tracks progress based on student's learning history",
            system_prompt="""
            You create personalized practice exercises based on the student's learning history and current level.

            Key Behaviors:
            1. Generate problems that match their skill level (from memory)
            2. Focus on areas they've struggled with previously
            3. Gradually increase difficulty based on their progress
            4. Use track_progress to record their performance
            5. Provide encouraging feedback that references their growth

            MEMORY-DRIVEN PRACTICE:
            - "Let's practice loops again since you wanted more examples..."
            - "Here's a harder version of the problem you solved yesterday..."
            - "You've improved a lot in functions, ready for the next level?"

            After practice, can hand back to TutorAgent for concept review if needed.
            """,
            tools=tools,
            llm=self.llm,
            can_handoff_to=["TutorAgent"]
        )

        # Create the multi-agent workflow
        self.workflow = AgentWorkflow(
            agents=[self.tutor_agent, self.practice_agent],
            root_agent=self.tutor_agent.name,
            initial_state={
                "current_topic": "",
                "student_level": "beginner",
                "learning_style": "unknown",
                "session_goals": []
            }
        )

    async def start_learning_session(self, topic: str, student_message: str = "") -> str:
        """
        Start a learning session with multi-agent memory-aware teaching
        """

        if student_message:
            request = f"I want to learn about {topic}. {student_message}"
        else:
            request = f"I want to learn about {topic}."

        # The magic happens here - multi-agent memory is automatically shared!
        response = await self.workflow.run(
            user_msg=request,
            memory=self.memory
        )

        return str(response)

    async def get_learning_history(self) -> str:
        """Show what the system remembers about this student"""
        try:
            # Search memory for learning patterns
            memories = self.memory.search(
                user_id=self.student_id,
                query="learning machine learning"
            )

            if memories and memories.get('results'):
                history = "\n".join(f"- {m['memory']}" for m in memories['results'])
                return history
            else:
                return "No learning history found yet. Let's start building your profile!"

        except Exception as e:
            return f"Memory retrieval error: {str(e)}"


async def run_learning_agent():

    learning_system = MultiAgentLearningSystem(student_id="Alexander")

    # First session
    print("Session 1:")
    response = await learning_system.start_learning_session(
        "Vision Language Models",
        "I'm new to machine learning but I have good hold on Python and have 4 years of work experience.")
    print(response)

    # Second session - multi-agent memory will remember the first
    print("\nSession 2:")
    response2 = await learning_system.start_learning_session(
        "Machine Learning", "what all did I cover so far?")
    print(response2)

    # Show what the multi-agent system remembers
    print("\nLearning History:")
    history = await learning_system.get_learning_history()
    print(history)


if __name__ == "__main__":
    """Run the example"""
    print("Multi-agent Learning System powered by LlamaIndex and Mem0")

    async def main():
        await run_learning_agent()

    asyncio.run(main())
```

## How It Works

### 1. Memory Context Setup

```python
# Memory context for this student
self.memory_context = {"user_id": student_id, "app": "learning_assistant"}
self.memory = Mem0Memory.from_client(context=self.memory_context)
```

The memory context identifies the specific student and application, ensuring memory isolation and proper retrieval.

### 2. Agent Collaboration

```python
# Agents can hand off to each other
can_handoff_to=["PracticeAgent"]  # TutorAgent can hand off to PracticeAgent
can_handoff_to=["TutorAgent"]     # PracticeAgent can hand off back
```

Agents collaborate seamlessly, with the TutorAgent handling explanations and the PracticeAgent managing exercises.

### 3. Shared Memory

```python
# Both agents share the same memory instance
response = await self.workflow.run(
    user_msg=request,
    memory=self.memory  # Shared across all agents
)
```

All agents in the workflow share the same memory context, enabling true collaborative learning.

### 4. Memory-Driven Interactions

The system prompts guide agents to:
- Reference previous learning sessions
- Adapt to discovered learning styles
- Build progressively on past lessons
- Track and respond to learning patterns

## Running the Example

```python
# Initialize the learning system
learning_system = MultiAgentLearningSystem(student_id="Alexander")

# Start a learning session
response = await learning_system.start_learning_session(
    "Vision Language Models",
    "I'm new to machine learning but I have good hold on Python and have 4 years of work experience."
)

# Continue learning in a new session (memory persists)
response2 = await learning_system.start_learning_session(
    "Machine Learning",
    "what all did I cover so far?"
)

# Check learning history
history = await learning_system.get_learning_history()
```

## Expected Output

The system will demonstrate memory-aware interactions:

```
Session 1:
I understand you want to learn about Vision Language Models and you mentioned you're new to machine learning but have a strong Python background with 4 years of experience. That's a great foundation to build on!

Let me start with an explanation tailored to your programming background...
[Agent provides explanation and may hand off to PracticeAgent for exercises]

Session 2:
Based on our previous session, I remember we covered Vision Language Models and I noted that you have a strong Python background with 4 years of experience. You mentioned being new to machine learning, so we started with foundational concepts...
[Agent references previous session and builds upon it]
```

## Key Benefits

1. **Persistent Learning**: Agents remember across sessions, creating continuity
2. **Collaborative Teaching**: Multiple specialized agents work together seamlessly
3. **Personalized Adaptation**: System learns and adapts to individual learning styles
4. **Scalable Architecture**: Easy to add more specialized agents
5. **Memory Efficiency**: Shared memory prevents duplication and ensures consistency


## Best Practices

1. **Clear Agent Roles**: Define specific responsibilities for each agent
2. **Memory Context**: Use descriptive context for memory isolation
3. **Handoff Strategy**: Design clear handoff criteria between agents
5. **Memory Hygiene**: Regularly review and clean memory for optimal performance

## Help & Resources

- [LlamaIndex Agent Workflows](https://docs.llamaindex.ai/en/stable/use_cases/agents/)
- [Mem0 Platform](https://app.mem0.ai/)

<Snippet file="get-help.mdx" />


================================================
FILE: docs/examples/mem0-agentic-tool.mdx
================================================
---
title: Mem0 as an Agentic Tool
---


Integrate Mem0's memory capabilities with OpenAI's Agents SDK to create AI agents with persistent memory.
You can create agents that remember past conversations and use that context to provide better responses.

## Installation

First, install the required packages:
```bash
pip install mem0ai pydantic openai-agents
```

You'll also need a custom agents framework for this implementation.

## Setting Up Environment Variables

Store your Mem0 API key as an environment variable:

```bash
export MEM0_API_KEY="your_mem0_api_key"
```

Or in your Python script:

```python
import os
os.environ["MEM0_API_KEY"] = "your_mem0_api_key"
```

## Code Structure

The integration consists of three main components:

1. **Context Manager**: Defines user context for memory operations
2. **Memory Tools**: Functions to add, search, and retrieve memories
3. **Memory Agent**: An agent configured to use these memory tools

## Step-by-Step Implementation

### 1. Import Dependencies

```python
from __future__ import annotations
import os
import asyncio
from pydantic import BaseModel
try:
    from mem0 import AsyncMemoryClient
except ImportError:
    raise ImportError("mem0 is not installed. Please install it using 'pip install mem0ai'.")
from agents import (
    Agent,
    ItemHelpers,
    MessageOutputItem,
    RunContextWrapper,
    Runner,
    ToolCallItem,
    ToolCallOutputItem,
    TResponseInputItem,
    function_tool,
)
```

### 2. Define Memory Context

```python
class Mem0Context(BaseModel):
    user_id: str | None = None
```

### 3. Initialize the Mem0 Client

```python
client = AsyncMemoryClient(api_key=os.getenv("MEM0_API_KEY"))
```

### 4. Create Memory Tools

#### Add to Memory

```python
@function_tool
async def add_to_memory(
    context: RunContextWrapper[Mem0Context],
    content: str,
) -> str:
    """
    Add a message to Mem0
    Args:
        content: The content to store in memory.
    """
    messages = [{"role": "user", "content": content}]
    user_id = context.context.user_id or "default_user"
    await client.add(messages, user_id=user_id)
    return f"Stored message: {content}"
```

#### Search Memory

```python
@function_tool
async def search_memory(
    context: RunContextWrapper[Mem0Context],
    query: str,
) -> str:
    """
    Search for memories in Mem0
    Args:
        query: The search query.
    """
    user_id = context.context.user_id or "default_user"
    memories = await client.search(query, user_id=user_id, output_format="v1.1")
    results = '\n'.join([result["memory"] for result in memories["results"]])
    return str(results)
```

#### Get All Memories

```python
@function_tool
async def get_all_memory(
    context: RunContextWrapper[Mem0Context],
) -> str:
    """Retrieve all memories from Mem0"""
    user_id = context.context.user_id or "default_user"
    memories = await client.get_all(user_id=user_id, output_format="v1.1")
    results = '\n'.join([result["memory"] for result in memories["results"]])
    return str(results)
```

### 5. Configure the Memory Agent

```python
memory_agent = Agent[Mem0Context](
    name="Memory Assistant",
    instructions="""You are a helpful assistant with memory capabilities. You can:
    1. Store new information using add_to_memory
    2. Search existing information using search_memory
    3. Retrieve all stored information using get_all_memory
    When users ask questions:
    - If they want to store information, use add_to_memory
    - If they're searching for specific information, use search_memory
    - If they want to see everything stored, use get_all_memory""",
    tools=[add_to_memory, search_memory, get_all_memory],
)
```

### 6. Implement the Main Runtime Loop

```python
async def main():
    current_agent: Agent[Mem0Context] = memory_agent
    input_items: list[TResponseInputItem] = []
    context = Mem0Context()
    while True:
        user_input = input("Enter your message (or 'quit' to exit): ")
        if user_input.lower() == 'quit':
            break
        input_items.append({"content": user_input, "role": "user"})
        result = await Runner.run(current_agent, input_items, context=context)
        for new_item in result.new_items:
            agent_name = new_item.agent.name
            if isinstance(new_item, MessageOutputItem):
                print(f"{agent_name}: {ItemHelpers.text_message_output(new_item)}")
            elif isinstance(new_item, ToolCallItem):
                print(f"{agent_name}: Calling a tool")
            elif isinstance(new_item, ToolCallOutputItem):
                print(f"{agent_name}: Tool call output: {new_item.output}")
            else:
                print(f"{agent_name}: Skipping item: {new_item.__class__.__name__}")
        input_items = result.to_input_list()

if __name__ == "__main__":
    asyncio.run(main())
```

## Usage Examples

### Storing Information

```
User: Remember that my favorite color is blue
Agent: Calling a tool
Agent: Tool call output: Stored message: my favorite color is blue
Agent: I've stored that your favorite color is blue in my memory. I'll remember that for future conversations.
```

### Searching Memory

```
User: What's my favorite color?
Agent: Calling a tool
Agent: Tool call output: my favorite color is blue
Agent: Your favorite color is blue, based on what you've told me earlier.
```

### Retrieving All Memories

```
User: What do you know about me?
Agent: Calling a tool
Agent: Tool call output: favorite color is blue
my birthday is on March 15
Agent: Based on our previous conversations, I know that:
1. Your favorite color is blue
2. Your birthday is on March 15
```

## Advanced Configuration

### Custom User IDs

You can specify different user IDs to maintain separate memory stores for multiple users:

```python
context = Mem0Context(user_id="user123")
```


## Resources

- [Mem0 Documentation](https://docs.mem0.ai)
- [Mem0 Dashboard](https://app.mem0.ai/dashboard)
- [API Reference](https://docs.mem0.ai/api-reference)



================================================
FILE: docs/examples/mem0-demo.mdx
================================================
---
title: Mem0 Demo
---

You can create a personalized AI Companion using Mem0. This guide will walk you through the necessary steps and provide the complete setup instructions to get you started.

<video
  autoPlay
  muted
  loop
  playsInline
  className="w-full aspect-video rounded-lg"
  src="https://github.com/user-attachments/assets/cebc4f8e-bdb9-4837-868d-13c5ab7bb433"
></video>

You can try the [Mem0 Demo](https://mem0-4vmi.vercel.app) live here.

## Overview

The Personalized AI Companion leverages Mem0 to retain information across interactions, enabling a tailored learning experience. It creates memories for each user interaction and integrates with OpenAI's GPT models to provide detailed and context-aware responses to user queries.

## Setup

Before you begin, follow these steps to set up the demo application:

1. Clone the Mem0 repository:
   ```bash
   git clone https://github.com/mem0ai/mem0.git
   ```

2. Navigate to the demo application folder:
   ```bash
   cd mem0/examples/mem0-demo
   ```

3. Install dependencies:
   ```bash
   pnpm install
   ```

4. Set up environment variables by creating a `.env` file in the project root with the following content:
   ```bash
   OPENAI_API_KEY=your_openai_api_key
   MEM0_API_KEY=your_mem0_api_key
   ```
   You can obtain your `MEM0_API_KEY` by signing up at [Mem0 API Dashboard](https://app.mem0.ai/dashboard/api-keys).

5. Start the development server:
   ```bash
   pnpm run dev
   ```

## Enhancing the Next.js Application

Once the demo is running, you can customize and enhance the Next.js application by modifying the components in the `mem0-demo` folder. Consider:
- Adding new memory features to improve contextual retention.
- Customizing the UI to better suit your application needs.
- Integrating additional APIs or third-party services to extend functionality.

## Full Code

You can find the complete source code for this demo on GitHub:
[Mem0 Demo GitHub](https://github.com/mem0ai/mem0/tree/main/examples/mem0-demo)

## Conclusion

This setup demonstrates how to build an AI Companion that maintains memory across interactions using Mem0. The system continuously adapts to user interactions, making future responses more relevant and personalized. Experiment with the application and enhance it further to suit your use case!




================================================
FILE: docs/examples/mem0-google-adk-healthcare-assistant.mdx
================================================
---
title: 'Healthcare Assistant with Mem0 and Google ADK'
description: 'Build a personalized healthcare agent that remembers patient information across conversations using Mem0 and Google ADK'
---


# Healthcare Assistant with Memory

This example demonstrates how to build a healthcare assistant that remembers patient information across conversations using Google ADK and Mem0.

## Overview

The Healthcare Assistant helps patients by:
- Remembering their medical history and symptoms
- Providing general health information
- Scheduling appointment reminders
- Maintaining a personalized experience across conversations

By integrating Mem0's memory layer with Google ADK, the assistant maintains context about the patient without requiring them to repeat information.

## Setup

Before you begin, make sure you have:

Installed Google ADK and Mem0 SDK:
```bash
pip install google-adk mem0ai python-dotenv
```

## Code Breakdown

Let's get started and understand the different components required in building a healthcare assistant powered by memory

```python
# Import dependencies
import os
import asyncio
from google.adk.agents import Agent
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from mem0 import MemoryClient
from dotenv import load_dotenv

load_dotenv()

# Set up environment variables
# os.environ["GOOGLE_API_KEY"] = "your-google-api-key"
# os.environ["MEM0_API_KEY"] = "your-mem0-api-key"

# Define a global user ID for simplicity
USER_ID = "Alex"

# Initialize Mem0 client
mem0 = MemoryClient()
```

## Define Memory Tools

First, we'll create tools that allow our agent to store and retrieve information using Mem0:

```python
def save_patient_info(information: str) -> dict:
    """Saves important patient information to memory."""

    # Store in Mem0
    response = mem0_client.add(
        [{"role": "user", "content": information}],
        user_id=USER_ID,
        run_id="healthcare_session",
        metadata={"type": "patient_information"}
    )


def retrieve_patient_info(query: str) -> dict:
    """Retrieves relevant patient information from memory."""

    # Search Mem0
    results = mem0_client.search(
        query,
        user_id=USER_ID,
        limit=5,
        threshold=0.7,  # Higher threshold for more relevant results
        output_format="v1.1"
    )

    # Format and return the results
    if results and len(results) > 0:
        memories = [memory["memory"] for memory in results.get('results', [])]
        return {
            "status": "success",
            "memories": memories,
            "count": len(memories)
        }
    else:
        return {
            "status": "no_results",
            "memories": [],
            "count": 0
        }
```

## Define Healthcare Tools

Next, we'll add tools specific to healthcare assistance:

```python
def schedule_appointment(date: str, time: str, reason: str) -> dict:
    """Schedules a doctor's appointment."""
    # In a real app, this would connect to a scheduling system
    appointment_id = f"APT-{hash(date + time) % 10000}"

    return {
        "status": "success",
        "appointment_id": appointment_id,
        "confirmation": f"Appointment scheduled for {date} at {time} for {reason}",
        "message": "Please arrive 15 minutes early to complete paperwork."
    }
```

## Create the Healthcare Assistant Agent

Now we'll create our main agent with all the tools:

```python
# Create the agent
healthcare_agent = Agent(
    name="healthcare_assistant",
    model="gemini-1.5-flash",  # Using Gemini for healthcare assistant
    description="Healthcare assistant that helps patients with health information and appointment scheduling.",
    instruction="""You are a helpful Healthcare Assistant with memory capabilities.

Your primary responsibilities are to:
1. Remember patient information using the 'save_patient_info' tool when they share symptoms, conditions, or preferences.
2. Retrieve past patient information using the 'retrieve_patient_info' tool when relevant to the current conversation.
3. Help schedule appointments using the 'schedule_appointment' tool.

IMPORTANT GUIDELINES:
- Always be empathetic, professional, and helpful.
- Save important patient information like symptoms, conditions, allergies, and preferences.
- Check if you have relevant patient information before asking for details they may have shared previously.
- Make it clear you are not a doctor and cannot provide medical diagnosis or treatment.
- For serious symptoms, always recommend consulting a healthcare professional.
- Keep all patient information confidential.
""",
    tools=[save_patient_info, retrieve_patient_info, schedule_appointment]
)
```

## Set Up Session and Runner

```python
# Set up Session Service and Runner
session_service = InMemorySessionService()

# Define constants for the conversation
APP_NAME = "healthcare_assistant_app"
USER_ID = "Alex"
SESSION_ID = "session_001"

# Create a session
session = session_service.create_session(
    app_name=APP_NAME,
    user_id=USER_ID,
    session_id=SESSION_ID
)

# Create the runner
runner = Runner(
    agent=healthcare_agent,
    app_name=APP_NAME,
    session_service=session_service
)
```

## Interact with the Healthcare Assistant

```python
# Function to interact with the agent
async def call_agent_async(query, runner, user_id, session_id):
    """Sends a query to the agent and returns the final response."""
    print(f"\n>>> Patient: {query}")

    # Format the user's message
    content = types.Content(
        role='user',
        parts=[types.Part(text=query)]
    )

    # Set user_id for tools to access
    save_patient_info.user_id = user_id
    retrieve_patient_info.user_id = user_id

    # Run the agent
    async for event in runner.run_async(
        user_id=user_id,
        session_id=session_id,
        new_message=content
    ):
        if event.is_final_response():
            if event.content and event.content.parts:
                response = event.content.parts[0].text
                print(f"<<< Assistant: {response}")
                return response

    return "No response received."

# Example conversation flow
async def run_conversation():
    # First interaction - patient introduces themselves with key information
    await call_agent_async(
        "Hi, I'm Alex. I've been having headaches for the past week, and I have a penicillin allergy.",
        runner=runner,
        user_id=USER_ID,
        session_id=SESSION_ID
    )

    # Request for health information
    await call_agent_async(
        "Can you tell me more about what might be causing my headaches?",
        runner=runner,
        user_id=USER_ID,
        session_id=SESSION_ID
    )

    # Schedule an appointment
    await call_agent_async(
        "I think I should see a doctor. Can you help me schedule an appointment for next Monday at 2pm?",
        runner=runner,
        user_id=USER_ID,
        session_id=SESSION_ID
    )

    # Test memory - should remember patient name, symptoms, and allergy
    await call_agent_async(
        "What medications should I avoid for my headaches?",
        runner=runner,
        user_id=USER_ID,
        session_id=SESSION_ID
    )

# Run the conversation example
if __name__ == "__main__":
    asyncio.run(run_conversation())
```

## How It Works

This healthcare assistant demonstrates several key capabilities:

1. **Memory Storage**: When Alex mentions her headaches and penicillin allergy, the agent stores this information in Mem0 using the `save_patient_info` tool.

2. **Contextual Retrieval**: When Alex asks about headache causes, the agent uses the `retrieve_patient_info` tool to recall her specific situation.

3. **Memory Application**: When discussing medications, the agent remembers Alex's penicillin allergy without her needing to repeat it, providing safer and more personalized advice.

4. **Conversation Continuity**: The agent maintains context across the entire conversation session, creating a more natural and efficient interaction.

## Key Implementation Details

### User ID Management

Instead of passing the user ID as a parameter to the memory tools (which would require modifying the ADK's tool calling system), we attach it directly to the function object:

```python
# Set user_id for tools to access
save_patient_info.user_id = user_id
retrieve_patient_info.user_id = user_id
```

Inside the tool functions, we retrieve this attribute:

```python
# Get user_id from session state or use default
user_id = getattr(save_patient_info, 'user_id', 'default_user')
```

This approach allows our tools to maintain user context without complicating their parameter signatures.

### Mem0 Integration

The integration with Mem0 happens through two primary functions:

1. `mem0_client.add()` - Stores new information with appropriate metadata
2. `mem0_client.search()` - Retrieves relevant memories using semantic search

The `threshold` parameter in the search function ensures that only highly relevant memories are returned.

## Conclusion

This example demonstrates how to build a healthcare assistant with persistent memory using Google ADK and Mem0. The integration allows for a more personalized patient experience by maintaining context across conversation turns, which is particularly valuable in healthcare scenarios where continuity of information is crucial.

By storing and retrieving patient information intelligently, the assistant provides more relevant responses without requiring the patient to repeat their medical history, symptoms, or preferences.



================================================
FILE: docs/examples/mem0-mastra.mdx
================================================
---
title: Mem0 with Mastra
---

In this example you'll learn how to use the Mem0 to add long-term memory capabilities to [Mastra's agent](https://mastra.ai/) via tool-use.
This memory integration can work alongside Mastra's [agent memory features](https://mastra.ai/docs/agents/01-agent-memory).

You can find the complete example code in the [Mastra repository](https://github.com/mastra-ai/mastra/tree/main/examples/memory-with-mem0).

## Overview

This guide will show you how to integrate Mem0 with Mastra to add long-term memory capabilities to your agents. We'll create tools that allow agents to save and retrieve memories using Mem0's API.

### Installation

1. **Install the Integration Package**

To install the Mem0 integration, run:

```bash
npm install @mastra/mem0
```

2. **Add the Integration to Your Project**

Create a new file for your integrations and import the integration:

```typescript integrations/index.ts
import { Mem0Integration } from "@mastra/mem0";

export const mem0 = new Mem0Integration({
  config: {
    apiKey: process.env.MEM0_API_KEY!,
    userId: "alice",
  },
});
```

3. **Use the Integration in Tools or Workflows**

You can now use the integration when defining tools for your agents or in workflows.

```typescript tools/index.ts
import { createTool } from "@mastra/core";
import { z } from "zod";
import { mem0 } from "../integrations";

export const mem0RememberTool = createTool({
  id: "Mem0-remember",
  description:
    "Remember your agent memories that you've previously saved using the Mem0-memorize tool.",
  inputSchema: z.object({
    question: z
      .string()
      .describe("Question used to look up the answer in saved memories."),
  }),
  outputSchema: z.object({
    answer: z.string().describe("Remembered answer"),
  }),
  execute: async ({ context }) => {
    console.log(`Searching memory "${context.question}"`);
    const memory = await mem0.searchMemory(context.question);
    console.log(`\nFound memory "${memory}"\n`);

    return {
      answer: memory,
    };
  },
});

export const mem0MemorizeTool = createTool({
  id: "Mem0-memorize",
  description:
    "Save information to mem0 so you can remember it later using the Mem0-remember tool.",
  inputSchema: z.object({
    statement: z.string().describe("A statement to save into memory"),
  }),
  execute: async ({ context }) => {
    console.log(`\nCreating memory "${context.statement}"\n`);
    // to reduce latency memories can be saved async without blocking tool execution
    void mem0.createMemory(context.statement).then(() => {
      console.log(`\nMemory "${context.statement}" saved.\n`);
    });
    return { success: true };
  },
});
```

4. **Create a new agent**

```typescript agents/index.ts
import { openai } from '@ai-sdk/openai';
import { Agent } from '@mastra/core/agent';
import { mem0MemorizeTool, mem0RememberTool } from '../tools';

export const mem0Agent = new Agent({
  name: 'Mem0 Agent',
  instructions: `
    You are a helpful assistant that has the ability to memorize and remember facts using Mem0.
  `,
  model: openai('gpt-4o'),
  tools: { mem0RememberTool, mem0MemorizeTool },
});
```

5. **Run the agent**

```typescript index.ts
import { Mastra } from '@mastra/core/mastra';
import { createLogger } from '@mastra/core/logger';

import { mem0Agent } from './agents';

export const mastra = new Mastra({
  agents: { mem0Agent },
  logger: createLogger({
    name: 'Mastra',
    level: 'error',
  }),
});
```

In the example above:
- We import the `@mastra/mem0` integration.
- We define two tools that uses the Mem0 API client to create new memories and recall previously saved memories.
- The tool accepts `question` as an input and returns the memory as a string.


================================================
FILE: docs/examples/mem0-openai-voice-demo.mdx
================================================
---
title: 'Mem0 with OpenAI Agents SDK for Voice'
description: 'Integrate memory capabilities into your voice agents using Mem0 and OpenAI Agents SDK'
---

# Building Voice Agents with Memory using Mem0 and OpenAI Agents SDK

This guide demonstrates how to combine OpenAI's Agents SDK for voice applications with Mem0's memory capabilities to create a voice assistant that remembers user preferences and past interactions.

## Prerequisites

Before you begin, make sure you have:

1. Installed OpenAI Agents SDK with voice dependencies:
```bash
pip install 'openai-agents[voice]'
```

2. Installed Mem0 SDK:
```bash
pip install mem0ai
```

3. Installed other required dependencies:
```bash
pip install numpy sounddevice pydantic
```

4. Set up your API keys:
   - OpenAI API key for the Agents SDK
   - Mem0 API key from the Mem0 Platform

## Code Breakdown

Let's break down the key components of this implementation:

### 1. Setting Up Dependencies and Environment

```python
# OpenAI Agents SDK imports
from agents import (
    Agent,
    function_tool
)
from agents.voice import (
    AudioInput,
    SingleAgentVoiceWorkflow,
    VoicePipeline
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions

# Mem0 imports
from mem0 import AsyncMemoryClient

# Set up API keys (replace with your actual keys)
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["MEM0_API_KEY"] = "your-mem0-api-key"

# Define a global user ID for simplicity
USER_ID = "voice_user"

# Initialize Mem0 client
mem0_client = AsyncMemoryClient()
```

This section handles:
- Importing required modules from OpenAI Agents SDK and Mem0
- Setting up environment variables for API keys
- Defining a simple user identification system (using a global variable)
- Initializing the Mem0 client that will handle memory operations

### 2. Memory Tools with Function Decorators

The `@function_tool` decorator transforms Python functions into callable tools for the OpenAI agent. Here are the key memory tools:

#### Storing User Memories

```python
import logging

# Set up logging at the top of your file
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    force=True
)
logger = logging.getLogger("memory_voice_agent")

# Then use logger in your function tools
@function_tool
async def save_memories(
    memory: str
) -> str:
    """Store a user memory in memory."""
    # This will be visible in your console
    logger.debug(f"Saving memory: {memory} for user {USER_ID}")
    
    # Store the preference in Mem0
    memory_content = f"User memory - {memory}"
    await mem0_client.add(
        memory_content,
        user_id=USER_ID,
    )

    return f"I've saved your memory: {memory}"
```

This function:
- Takes a memory string
- Creates a formatted memory string
- Stores it in Mem0 using the `add()` method
- Includes metadata to categorize the memory for easier retrieval
- Returns a confirmation message that the agent will speak

#### Finding Relevant Memories

```python
@function_tool
async def search_memories(
    query: str
) -> str:
    """
    Find memories relevant to the current conversation.
    Args:
        query: The search query to find relevant memories
    """
    print(f"Finding memories related to: {query}")
    results = await mem0_client.search(
        query,
        user_id=USER_ID,
        limit=5,
        threshold=0.7,  # Higher threshold for more relevant results
        output_format="v1.1"
    )
    
    # Format and return the results
    if not results.get('results', []):
        return "I don't have any relevant memories about this topic."
    
    memories = [f"• {result['memory']}" for result in results.get('results', [])]
    return "Here's what I remember that might be relevant:\n" + "\n".join(memories)
```

This tool:
- Takes a search query string
- Passes it to Mem0's semantic search to find related memories
- Sets a threshold for relevance to ensure quality results
- Returns a formatted list of relevant memories or a default message

### 3. Creating the Voice Agent

```python
def create_memory_voice_agent():
    # Create the agent with memory-enabled tools
    agent = Agent(
        name="Memory Assistant",
        instructions=prompt_with_handoff_instructions(
            """You're speaking to a human, so be polite and concise.
            Always respond in clear, natural English.
            You have the ability to remember information about the user.
            Use the save_memories tool when the user shares an important information worth remembering.
            Use the search_memories tool when you need context from past conversations or user asks you to recall something.
            """,
        ),
        model="gpt-4o",
        tools=[save_memories, search_memories],
    )
    
    return agent
```

This function:
- Creates an OpenAI Agent with specific instructions
- Configures it to use gpt-4o (you can use other models)
- Registers the memory-related tools with the agent
- Uses `prompt_with_handoff_instructions` to include standard voice agent behaviors

### 4. Microphone Recording Functionality

```python
async def record_from_microphone(duration=5, samplerate=24000):
    """Record audio from the microphone for a specified duration."""
    print(f"Recording for {duration} seconds...")
    
    # Create a buffer to store the recorded audio
    frames = []
    
    # Callback function to store audio data
    def callback(indata, frames_count, time_info, status):
        frames.append(indata.copy())
    
    # Start recording
    with sd.InputStream(samplerate=samplerate, channels=1, callback=callback, dtype=np.int16):
        await asyncio.sleep(duration)
    
    # Combine all frames into a single numpy array
    audio_data = np.concatenate(frames)
    return audio_data
```

This function:
- Creates a simple asynchronous microphone recording function
- Uses the sounddevice library to capture audio input
- Stores frames in a buffer during recording
- Combines frames into a single numpy array when complete
- Returns the audio data for processing

### 5. Main Loop and Voice Processing

```python
async def main():
    # Create the agent
    agent = create_memory_voice_agent()
    
    # Set up the voice pipeline
    pipeline = VoicePipeline(
        workflow=SingleAgentVoiceWorkflow(agent)
    )
    
    # Configure TTS settings
    pipeline.config.tts_settings.voice = "alloy"
    pipeline.config.tts_settings.speed = 1.0
    
    try:
        while True:
            # Get user input
            print("\nPress Enter to start recording (or 'q' to quit)...")
            user_input = input()
            if user_input.lower() == 'q':
                break
            
            # Record and process audio
            audio_data = await record_from_microphone(duration=5)
            audio_input = AudioInput(buffer=audio_data)
            result = await pipeline.run(audio_input)
            
            # Play response and handle events
            player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
            player.start()
            
            agent_response = ""
            print("\nAgent response:")
            
            async for event in result.stream():
                if event.type == "voice_stream_event_audio":
                    player.write(event.data)
                elif event.type == "voice_stream_event_content":
                    content = event.data
                    agent_response += content
                    print(content, end="", flush=True)
            
            # Save the agent's response to memory
            if agent_response:
                try:
                    await mem0_client.add(
                        f"Agent response: {agent_response}", 
                        user_id=USER_ID,
                        metadata={"type": "agent_response"}
                    )
                except Exception as e:
                    print(f"Failed to store memory: {e}")
    
    except KeyboardInterrupt:
        print("\nExiting...")
```

This main function orchestrates the entire process:
1. Creates the memory-enabled voice agent
2. Sets up the voice pipeline with TTS settings
3. Implements an interactive loop for recording and processing voice input
4. Handles streaming of response events (both audio and text)
5. Automatically saves the agent's responses to memory
6. Includes proper error handling and exit mechanisms

## Create a Memory-Enabled Voice Agent

Now that we've explained each component, here's the complete implementation that combines OpenAI Agents SDK for voice with Mem0's memory capabilities:

```python
import asyncio
import os
import logging
from typing import Optional, List, Dict, Any
import numpy as np
import sounddevice as sd
from pydantic import BaseModel

# OpenAI Agents SDK imports
from agents import (
    Agent,
    function_tool
)
from agents.voice import (
    AudioInput,
    SingleAgentVoiceWorkflow,
    VoicePipeline
)
from agents.extensions.handoff_prompt import prompt_with_handoff_instructions

# Mem0 imports
from mem0 import AsyncMemoryClient

# Set up API keys (replace with your actual keys)
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["MEM0_API_KEY"] = "your-mem0-api-key"

# Define a global user ID for simplicity
USER_ID = "voice_user"

# Initialize Mem0 client
mem0_client = AsyncMemoryClient()

# Create tools that utilize Mem0's memory
@function_tool
async def save_memories(
    memory: str
) -> str:
    """
    Store a user memory in memory.
    Args:
        memory: The memory to save
    """
    print(f"Saving memory: {memory} for user {USER_ID}")

    # Store the preference in Mem0
    memory_content = f"User memory - {memory}"
    await mem0_client.add(
        memory_content,
        user_id=USER_ID,
    )

    return f"I've saved your memory: {memory}"

@function_tool
async def search_memories(
    query: str
) -> str:
    """
    Find memories relevant to the current conversation.
    Args:
        query: The search query to find relevant memories
    """
    print(f"Finding memories related to: {query}")
    results = await mem0_client.search(
        query,
        user_id=USER_ID,
        limit=5,
        threshold=0.7,  # Higher threshold for more relevant results
        output_format="v1.1"
    )
    
    # Format and return the results
    if not results.get('results', []):
        return "I don't have any relevant memories about this topic."
    
    memories = [f"• {result['memory']}" for result in results.get('results', [])]
    return "Here's what I remember that might be relevant:\n" + "\n".join(memories)

# Create the agent with memory-enabled tools
def create_memory_voice_agent():
    # Create the agent with memory-enabled tools
    agent = Agent(
        name="Memory Assistant",
        instructions=prompt_with_handoff_instructions(
            """You're speaking to a human, so be polite and concise.
            Always respond in clear, natural English.
            You have the ability to remember information about the user.
            Use the save_memories tool when the user shares an important information worth remembering.
            Use the search_memories tool when you need context from past conversations or user asks you to recall something.
            """,
        ),
        model="gpt-4o",
        tools=[save_memories, search_memories],
    )
    
    return agent

async def record_from_microphone(duration=5, samplerate=24000):
    """Record audio from the microphone for a specified duration."""
    print(f"Recording for {duration} seconds...")
    
    # Create a buffer to store the recorded audio
    frames = []
    
    # Callback function to store audio data
    def callback(indata, frames_count, time_info, status):
        frames.append(indata.copy())
    
    # Start recording
    with sd.InputStream(samplerate=samplerate, channels=1, callback=callback, dtype=np.int16):
        await asyncio.sleep(duration)
    
    # Combine all frames into a single numpy array
    audio_data = np.concatenate(frames)
    return audio_data

async def main():
    print("Starting Memory Voice Agent")
    
    # Create the agent and context
    agent = create_memory_voice_agent()
    
    # Set up the voice pipeline
    pipeline = VoicePipeline(
        workflow=SingleAgentVoiceWorkflow(agent)
    )
    
    # Configure TTS settings
    pipeline.config.tts_settings.voice = "alloy"
    pipeline.config.tts_settings.speed = 1.0
    
    try:
        while True:
            # Get user input
            print("\nPress Enter to start recording (or 'q' to quit)...")
            user_input = input()
            if user_input.lower() == 'q':
                break
            
            # Record and process audio
            audio_data = await record_from_microphone(duration=5)
            audio_input = AudioInput(buffer=audio_data)
            
            print("Processing your request...")
            
            # Process the audio input
            result = await pipeline.run(audio_input)
            
            # Create an audio player
            player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)
            player.start()
            
            # Store the agent's response for adding to memory
            agent_response = ""
            
            print("\nAgent response:")
            # Play the audio stream as it comes in
            async for event in result.stream():
                if event.type == "voice_stream_event_audio":
                    player.write(event.data)
                elif event.type == "voice_stream_event_content":
                    # Accumulate and print the text response
                    content = event.data
                    agent_response += content
                    print(content, end="", flush=True)
            
            print("\n")
            
            # Example of saving the conversation to Mem0 after completion
            if agent_response:
                try:
                    await mem0_client.add(
                        f"Agent response: {agent_response}", 
                        user_id=USER_ID,
                        metadata={"type": "agent_response"}
                    )
                except Exception as e:
                    print(f"Failed to store memory: {e}")
    
    except KeyboardInterrupt:
        print("\nExiting...")

if __name__ == "__main__":
    asyncio.run(main())
```

## Key Features of This Implementation

This implementation offers several key features:

1. **Simplified User Management**: Uses a global `USER_ID` variable for simplicity, but can be extended to manage multiple users.

2. **Real Microphone Input**: Includes a `record_from_microphone()` function that captures actual voice input from your microphone.

3. **Interactive Voice Loop**: Implements a continuous interaction loop, allowing for multiple back-and-forth exchanges.

4. **Memory Management Tools**:
   - `save_memories`: Stores user memories in Mem0
   - `search_memories`: Searches for relevant past information

5. **Voice Configuration**: Demonstrates how to configure TTS settings for the voice response.

## Running the Example

To run this example:

1. Replace the placeholder API keys with your actual keys
2. Make sure your microphone is properly connected
3. Run the script with Python 3.8 or newer
4. Press Enter to start recording, then speak your request
5. Press 'q' to quit the application

The agent will listen to your request, process it through the OpenAI model, utilize Mem0 for memory operations as needed, and respond both through text output and voice speech.

## Best Practices for Voice Agents with Memory

1. **Optimizing Memory for Voice**: Keep memories concise and relevant for voice responses.

2. **Forgetting Mechanism**: Implement a way to delete or expire memories that are no longer relevant.

3. **Context Preservation**: Store enough context with each memory to make retrieval effective.

4. **Error Handling**: Implement robust error handling for memory operations, as voice interactions should continue smoothly even if memory operations fail.

## Conclusion

By combining OpenAI's Agents SDK with Mem0's memory capabilities, you can create voice agents that maintain persistent memory of user preferences and past interactions. This significantly enhances the user experience by making conversations more natural and personalized.

As you build your voice application, experiment with different memory strategies and filtering approaches to find the optimal balance between comprehensive memory and efficient retrieval for your specific use case.

## Debugging Function Tools

When working with the OpenAI Agents SDK, you might notice that regular `print()` statements inside `@function_tool` decorated functions don't appear in your console output. This is because the Agents SDK captures and redirects standard output when executing these functions.

To effectively debug your function tools, use Python's `logging` module instead:

```python
import logging

# Set up logging at the top of your file
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    force=True
)
logger = logging.getLogger("memory_voice_agent")

# Then use logger in your function tools
@function_tool
async def save_memories(
    memory: str
) -> str:
    """Store a user memory in memory."""
    # This will be visible in your console
    logger.debug(f"Saving memory: {memory} for user {USER_ID}")
    
    # Rest of your function...
```


================================================
FILE: docs/examples/mem0-with-ollama.mdx
================================================
---
title: Mem0 with Ollama
---

## Running Mem0 Locally with Ollama

Mem0 can be utilized entirely locally by leveraging Ollama for both the embedding model and the language model (LLM). This guide will walk you through the necessary steps and provide the complete code to get you started.

### Overview

By using Ollama, you can run Mem0 locally, which allows for greater control over your data and models. This setup uses Ollama for both the embedding model and the language model, providing a fully local solution.

### Setup

Before you begin, ensure you have Mem0 and Ollama installed and properly configured on your local machine.

### Full Code Example

Below is the complete code to set up and use Mem0 locally with Ollama:

```python
from mem0 import Memory

config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name": "test",
            "host": "localhost",
            "port": 6333,
            "embedding_model_dims": 768,  # Change this according to your local model's dimensions
        },
    },
    "llm": {
        "provider": "ollama",
        "config": {
            "model": "llama3.1:latest",
            "temperature": 0,
            "max_tokens": 2000,
            "ollama_base_url": "http://localhost:11434",  # Ensure this URL is correct
        },
    },
    "embedder": {
        "provider": "ollama",
        "config": {
            "model": "nomic-embed-text:latest",
            # Alternatively, you can use "snowflake-arctic-embed:latest"
            "ollama_base_url": "http://localhost:11434",
        },
    },
}

# Initialize Memory with the configuration
m = Memory.from_config(config)

# Add a memory
m.add("I'm visiting Paris", user_id="john")

# Retrieve memories
memories = m.get_all(user_id="john")
```

### Key Points

- **Configuration**: The setup involves configuring the vector store, language model, and embedding model to use local resources.
- **Vector Store**: Qdrant is used as the vector store, running on localhost.
- **Language Model**: Ollama is used as the LLM provider, with the "llama3.1:latest" model.
- **Embedding Model**: Ollama is also used for embeddings, with the "nomic-embed-text:latest" model.

### Conclusion

This local setup of Mem0 using Ollama provides a fully self-contained solution for memory management and AI interactions. It allows for greater control over your data and models while still leveraging the powerful capabilities of Mem0.


================================================
FILE: docs/examples/memory-guided-content-writing.mdx
================================================
---
title: Memory-Guided Content Writing
---

This guide demonstrates how to leverage **Mem0** to streamline content writing by applying your unique writing style and preferences using persistent memory.

## Why Use Mem0?

Integrating Mem0 into your writing workflow helps you:

1. **Store persistent writing preferences** ensuring consistent tone, formatting, and structure.
2. **Automate content refinement** by retrieving preferences when rewriting or reviewing content.
3. **Scale your writing style** so it applies consistently across multiple documents or sessions.

## Setup

```python
import os
from openai import OpenAI
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-mem0-api-key"
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"


# Set up Mem0 and OpenAI client
client = MemoryClient()
openai = OpenAI()

USER_ID = "content_writer"
RUN_ID = "smart_editing_session"
```

## **Storing Your Writing Preferences in Mem0**

```python
def store_writing_preferences():
    """Store your writing preferences in Mem0."""
    
    preferences = """My writing preferences:
1. Use headings and sub-headings for structure.
2. Keep paragraphs concise (8–10 sentences max).
3. Incorporate specific numbers and statistics.
4. Provide concrete examples.
5. Use bullet points for clarity.
6. Avoid jargon and buzzwords."""

    messages = [
        {"role": "user", "content": "Here are my writing style preferences."},
        {"role": "assistant", "content": preferences}
    ]

    response = client.add(
        messages,
        user_id=USER_ID,
        run_id=RUN_ID,
        metadata={"type": "preferences", "category": "writing_style"}
    )

    return response
```

## **Editing Content Using Stored Preferences**

```python
def apply_writing_style(original_content):
    """Use preferences stored in Mem0 to guide content rewriting."""

    results = client.search(
        query="What are my writing style preferences?",
        version="v2",
        filters={
            "AND": [
                {
                    "user_id": USER_ID
                },
                {
                    "run_id": RUN_ID
                }
            ]
        },
    )

    if not results:
        print("No preferences found.")
        return None

    preferences = "\n".join(r["memory"] for r in results.get('results', []))

    system_prompt = f"""
You are a writing assistant.

Apply the following writing style preferences to improve the user's content:

Preferences:
{preferences}
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"""Original Content:
    {original_content}"""}
    ]

    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages
    )
    clean_response = response.choices[0].message.content.strip()

    return clean_response
```

## **Complete Workflow: Content Editing**

```python
def content_writing_workflow(content):
    """Automated workflow for editing a document based on writing preferences."""
    
    # Store writing preferences (if not already stored)
    store_writing_preferences()  # Ideally done once, or with a conditional check
    
    # Edit the document with Mem0 preferences
    edited_content = apply_writing_style(content)
    
    if not edited_content:
        return "Failed to edit document."
    
    # Display results
    print("\n=== ORIGINAL DOCUMENT ===\n")
    print(content)
    
    print("\n=== EDITED DOCUMENT ===\n")
    print(edited_content)
    
    return edited_content
```

## **Example Usage**

```python
# Define your document
original_content = """Project Proposal
    
The following proposal outlines our strategy for the Q3 marketing campaign. 
We believe this approach will significantly increase our market share.

Increase brand awareness
Boost sales by 15%
Expand our social media following

We plan to launch the campaign in July and continue through September.
"""

# Run the workflow
result = content_writing_workflow(original_content)
```

## **Expected Output**

Your document will be transformed into a structured, well-formatted version based on your preferences.

### **Original Document**
```
Project Proposal
    
The following proposal outlines our strategy for the Q3 marketing campaign. 
We believe this approach will significantly increase our market share.

Increase brand awareness
Boost sales by 15%
Expand our social media following

We plan to launch the campaign in July and continue through September.
```

### **Edited Document**
```
# **Project Proposal**

## **Q3 Marketing Campaign Strategy**

This proposal outlines our strategy for the Q3 marketing campaign. We aim to significantly increase our market share with this approach.

### **Objectives**

- **Increase Brand Awareness**: Implement targeted advertising and community engagement to enhance visibility.
- **Boost Sales by 15%**: Increase sales by 15% compared to Q2 figures.
- **Expand Social Media Following**: Grow our social media audience by 20%.

### **Timeline**

- **Launch Date**: July
- **Duration**: July – September

### **Key Actions**

- **Targeted Advertising**: Utilize platforms like Google Ads and Facebook to reach specific demographics.
- **Community Engagement**: Host webinars and live Q&A sessions.
- **Content Creation**: Produce engaging videos and infographics.

### **Supporting Data**

- **Previous Campaign Success**: Our Q2 campaign increased sales by 12%. We will refine similar strategies for Q3.
- **Social Media Growth**: Last year, our Instagram followers grew by 25% during a similar campaign.

### **Conclusion**

We believe this strategy will effectively increase our market share. To achieve these goals, we need your support and collaboration. Let’s work together to make this campaign a success. Please review the proposal and provide your feedback by the end of the week.
```

Mem0 enables a seamless, intelligent content-writing workflow, perfect for content creators, marketers, and technical writers looking to scale their personal tone and structure across work.

## Help & Resources

- [Mem0 Platform](https://app.mem0.ai/)

<Snippet file="get-help.mdx" />


================================================
FILE: docs/examples/multimodal-demo.mdx
================================================
---
title: Multimodal Demo with Mem0
---

Enhance your AI interactions with **Mem0**'s multimodal capabilities. Mem0 now supports image understanding, allowing for richer context and more natural interactions across supported AI platforms.

> Experience the power of multimodal AI! Test out Mem0's image understanding capabilities at [multimodal-demo.mem0.ai](https://multimodal-demo.mem0.ai)

## Features

- **Image Understanding**: Share and discuss images with AI assistants while maintaining context.
- **Smart Visual Context**: Automatically capture and reference visual elements in conversations.
- **Cross-Modal Memory**: Link visual and textual information seamlessly in your memory layer.
- **Cross-Session Recall**: Reference previously discussed visual content across different conversations.
- **Seamless Integration**: Works naturally with existing chat interfaces for a smooth experience.

## How It Works

1. **Upload Visual Content**: Simply drag and drop or paste images into your conversations.
2. **Natural Interaction**: Discuss the visual content naturally with AI assistants.
3. **Memory Integration**: Visual context is automatically stored and linked with your conversation history.
4. **Persistent Recall**: Retrieve and reference past visual content effortlessly.

## Demo Video

<iframe width="700" height="400" src="https://www.youtube.com/embed/2Md5AEFVpmg?si=rXXupn6CiDUPJsi3" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Try It Out

Visit [multimodal-demo.mem0.ai](https://multimodal-demo.mem0.ai) to experience Mem0's multimodal capabilities firsthand. Upload images and see how Mem0 understands and remembers visual context across your conversations.




================================================
FILE: docs/examples/openai-inbuilt-tools.mdx
================================================
---
title: OpenAI Inbuilt Tools
---

Integrate Mem0’s memory capabilities with OpenAI’s Inbuilt Tools to create AI agents with persistent memory.

## Getting Started

### Installation

```bash
npm install mem0ai openai zod
```

## Environment Setup

Save your Mem0 and OpenAI API keys in a `.env` file:

```
MEM0_API_KEY=your_mem0_api_key
OPENAI_API_KEY=your_openai_api_key
```

Get your Mem0 API key from the [Mem0 Dashboard](https://app.mem0.ai/dashboard/api-keys).

### Configuration

```javascript
const mem0Config = {
    apiKey: process.env.MEM0_API_KEY,
    user_id: "sample-user",
};

const openAIClient = new OpenAI();
const mem0Client = new MemoryClient(mem0Config);
```

### Adding Memories

Store user preferences, past interactions, or any relevant information:
<CodeGroup>
```javascript JavaScript
async function addUserPreferences() {
    const mem0Client = new MemoryClient(mem0Config);
    
    const userPreferences = "I Love BMW, Audi and Porsche. I Hate Mercedes. I love Red cars and Maroon cars. I have a budget of 120K to 150K USD. I like Audi the most.";
    
    await mem0Client.add([{
        role: "user",
        content: userPreferences,
    }], mem0Config);
}

await addUserPreferences();
```

```json Output (Memories)
 [
  {
    "id": "ff9f3367-9e83-415d-b9c5-dc8befd9a4b4",
    "data": { "memory": "Loves BMW, Audi, and Porsche" },
    "event": "ADD"
  },
  {
    "id": "04172ce6-3d7b-45a3-b4a1-ee9798593cb4",
    "data": { "memory": "Hates Mercedes" },
    "event": "ADD"
  },
  {
    "id": "db363a5d-d258-4953-9e4c-777c120de34d",
    "data": { "memory": "Loves red cars and maroon cars" },
    "event": "ADD"
  },
  {
    "id": "5519aaad-a2ac-4c0d-81d7-0d55c6ecdba8",
    "data": { "memory": "Has a budget of 120K to 150K USD" },
    "event": "ADD"
  },
  {
    "id": "523b7693-7344-4563-922f-5db08edc8634",
    "data": { "memory": "Likes Audi the most" },
    "event": "ADD"
  }
]
```
</CodeGroup>
### Retrieving Memories

Search for relevant memories based on the current user input:

```javascript
const relevantMemories = await mem0Client.search(userInput, mem0Config);
```

### Structured Responses with Zod

Define structured response schemas to get consistent output formats:

```javascript
// Define the schema for a car recommendation
const CarSchema = z.object({
  car_name: z.string(),
  car_price: z.string(),
  car_url: z.string(),
  car_image: z.string(),
  car_description: z.string(),
});

// Schema for a list of car recommendations
const Cars = z.object({
  cars: z.array(CarSchema),
});

// Create a function tool based on the schema
const carRecommendationTool = zodResponsesFunction({ 
    name: "carRecommendations", 
    parameters: Cars 
});

// Use the tool in your OpenAI request
const response = await openAIClient.responses.create({
    model: "gpt-4o",
    tools: [{ type: "web_search_preview" }, carRecommendationTool],
    input: `${getMemoryString(relevantMemories)}\n${userInput}`,
});
```

### Using Web Search

Combine memory with web search for up-to-date recommendations:

```javascript
const response = await openAIClient.responses.create({
    model: "gpt-4o",
    tools: [{ type: "web_search_preview" }, carRecommendationTool],
    input: `${getMemoryString(relevantMemories)}\n${userInput}`,
});
```

## Examples

### Complete Car Recommendation System

```javascript
import MemoryClient from "mem0ai";
import { OpenAI } from "openai";
import { zodResponsesFunction } from "openai/helpers/zod";
import { z } from "zod";
import dotenv from 'dotenv';

dotenv.config();

const mem0Config = {
    apiKey: process.env.MEM0_API_KEY,
    user_id: "sample-user",
};

async function run() {
    // Responses without memories
    console.log("\n\nRESPONSES WITHOUT MEMORIES\n\n");
    await main();

    // Adding sample memories
    await addSampleMemories();

    // Responses with memories
    console.log("\n\nRESPONSES WITH MEMORIES\n\n");
    await main(true);
}

// OpenAI Response Schema
const CarSchema = z.object({
  car_name: z.string(),
  car_price: z.string(),
  car_url: z.string(),
  car_image: z.string(),
  car_description: z.string(),
});

const Cars = z.object({
  cars: z.array(CarSchema),
});

async function main(memory = false) {
  const openAIClient = new OpenAI();
  const mem0Client = new MemoryClient(mem0Config);

  const input = "Suggest me some cars that I can buy today.";

  const tool = zodResponsesFunction({ name: "carRecommendations", parameters: Cars });

  // Store the user input as a memory
  await mem0Client.add([{
    role: "user",
    content: input,
  }], mem0Config);

  // Search for relevant memories
  let relevantMemories = []
  if (memory) {
    relevantMemories = await mem0Client.search(input, mem0Config);
  }

  const response = await openAIClient.responses.create({
    model: "gpt-4o",
    tools: [{ type: "web_search_preview" }, tool],
    input: `${getMemoryString(relevantMemories)}\n${input}`,
  });

  console.log(response.output);
}

async function addSampleMemories() {
  const mem0Client = new MemoryClient(mem0Config);

  const myInterests = "I Love BMW, Audi and Porsche. I Hate Mercedes. I love Red cars and Maroon cars. I have a budget of 120K to 150K USD. I like Audi the most.";
  
  await mem0Client.add([{
    role: "user",
    content: myInterests,
  }], mem0Config);
}

const getMemoryString = (memories) => {
    const MEMORY_STRING_PREFIX = "These are the memories I have stored. Give more weightage to the question by users and try to answer that first. You have to modify your answer based on the memories I have provided. If the memories are irrelevant you can ignore them. Also don't reply to this section of the prompt, or the memories, they are only for your reference. The MEMORIES of the USER are: \n\n";
    const memoryString = (memories?.results || memories).map((mem) => `${mem.memory}`).join("\n") ?? "";
    return memoryString.length > 0 ? `${MEMORY_STRING_PREFIX}${memoryString}` : "";
};

run().catch(console.error);
```

### Responses

<CodeGroup>
    ```json Without Memories
    {
      "cars": [
        {
          "car_name": "Toyota Camry",
          "car_price": "$25,000",
          "car_url": "https://www.toyota.com/camry/",
          "car_image": "https://link-to-toyota-camry-image.com",
          "car_description": "Reliable mid-size sedan with great fuel efficiency."
        },
        {
          "car_name": "Honda Accord",
          "car_price": "$26,000",
          "car_url": "https://www.honda.com/accord/",
          "car_image": "https://link-to-honda-accord-image.com",
          "car_description": "Comfortable and spacious with advanced safety features."
        },
        {
          "car_name": "Ford Mustang",
          "car_price": "$28,000",
          "car_url": "https://www.ford.com/mustang/",
          "car_image": "https://link-to-ford-mustang-image.com",
          "car_description": "Iconic sports car with powerful engine options."
        },
        {
          "car_name": "Tesla Model 3",
          "car_price": "$38,000",
          "car_url": "https://www.tesla.com/model3",
          "car_image": "https://link-to-tesla-model3-image.com",
          "car_description": "Electric vehicle with advanced technology and long range."
        },
        {
          "car_name": "Chevrolet Equinox",
          "car_price": "$24,000",
          "car_url": "https://www.chevrolet.com/equinox/",
          "car_image": "https://link-to-chevron-equinox-image.com",
          "car_description": "Compact SUV with a spacious interior and user-friendly technology."
        }
      ]
    }
    ```
  
    ```json With Memories
    {
      "cars": [
        {
          "car_name": "Audi RS7",
          "car_price": "$118,500",
          "car_url": "https://www.audiusa.com/us/web/en/models/rs7/2023/overview.html",
          "car_image": "https://www.audiusa.com/content/dam/nemo/us/models/rs7/my23/gallery/1920x1080_AOZ_A717_191004.jpg",
          "car_description": "The Audi RS7 is a high-performance hatchback with a sleek design, powerful 591-hp twin-turbo V8, and luxurious interior. It's available in various colors including red."
        },
        {
          "car_name": "Porsche Panamera GTS",
          "car_price": "$129,300",
          "car_url": "https://www.porsche.com/usa/models/panamera/panamera-models/panamera-gts/",
          "car_image": "https://files.porsche.com/filestore/image/multimedia/noneporsche-panamera-gts-sample-m02-high/normal/8a6327c3-6c7f-4c6f-a9a8-fb9f58b21795;sP;twebp/porsche-normal.webp",
          "car_description": "The Porsche Panamera GTS is a luxury sports sedan with a 473-hp V8 engine, exquisite handling, and available in stunning red. Balances sportiness and comfort."
        },
        {
          "car_name": "BMW M5",
          "car_price": "$105,500",
          "car_url": "https://www.bmwusa.com/vehicles/m-models/m5/sedan/overview.html",
          "car_image": "https://www.bmwusa.com/content/dam/bmwusa/M/m5/2023/bmw-my23-m5-sapphire-black-twilight-purple-exterior-02.jpg",
          "car_description": "The BMW M5 is a powerhouse sedan with a 600-hp V8 engine, known for its great handling and luxury. It comes in several distinctive colors including maroon."
        }
      ]
    }
    ```
</CodeGroup>

## Resources

- [Mem0 Documentation](https://docs.mem0.ai)
- [Mem0 Dashboard](https://app.mem0.ai/dashboard)
- [API Reference](https://docs.mem0.ai/api-reference)
- [OpenAI Documentation](https://platform.openai.com/docs)


================================================
FILE: docs/examples/personal-ai-tutor.mdx
================================================
---
title: Personalized AI Tutor
---

You can create a personalized AI Tutor using Mem0. This guide will walk you through the necessary steps and provide the complete code to get you started.

## Overview

The Personalized AI Tutor leverages Mem0 to retain information across interactions, enabling a tailored learning experience. By integrating with OpenAI's GPT-4 model, the tutor can provide detailed and context-aware responses to user queries.

## Setup
Before you begin, ensure you have the required dependencies installed. You can install the necessary packages using pip:

```bash
pip install openai mem0ai
```

## Full Code Example

Below is the complete code to create and interact with a Personalized AI Tutor using Mem0:

```python
import os 
from openai import OpenAI
from mem0 import Memory

# Set the OpenAI API key
os.environ['OPENAI_API_KEY'] = 'sk-xxx'

# Initialize the OpenAI client
client = OpenAI()

class PersonalAITutor:
    def __init__(self):
        """
        Initialize the PersonalAITutor with memory configuration and OpenAI client.
        """
        config = {
            "vector_store": {
                "provider": "qdrant",
                "config": {
                    "host": "localhost",
                    "port": 6333,
                }
            },
        }
        self.memory = Memory.from_config(config)
        self.client = client
        self.app_id = "app-1"

    def ask(self, question, user_id=None):
        """
        Ask a question to the AI and store the relevant facts in memory

        :param question: The question to ask the AI.
        :param user_id: Optional user ID to associate with the memory.
        """
        # Start a streaming response request to the AI
        response = self.client.responses.create(
            model="gpt-4o",
            instructions="You are a personal AI Tutor.",
            input=question,
            stream=True
        )

        # Store the question in memory
        self.memory.add(question, user_id=user_id, metadata={"app_id": self.app_id})

        # Print the response from the AI in real-time
        for event in response:
            if event.type == "response.output_text.delta":
                print(event.delta, end="")

    def get_memories(self, user_id=None):
        """
        Retrieve all memories associated with the given user ID.

        :param user_id: Optional user ID to filter memories.
        :return: List of memories.
        """
        return self.memory.get_all(user_id=user_id)

# Instantiate the PersonalAITutor
ai_tutor = PersonalAITutor()

# Define a user ID
user_id = "john_doe"

# Ask a question
ai_tutor.ask("I am learning introduction to CS. What is queue? Briefly explain.", user_id=user_id)
```

### Fetching Memories

You can fetch all the memories at any point in time using the following code:

```python
memories = ai_tutor.get_memories(user_id=user_id)
for m in memories['results']:
    print(m['memory'])
```

### Key Points

- **Initialization**: The PersonalAITutor class is initialized with the necessary memory configuration and OpenAI client setup.
- **Asking Questions**: The ask method sends a question to the AI and stores the relevant information in memory.
- **Retrieving Memories**: The get_memories method fetches all stored memories associated with a user.

### Conclusion

As the conversation progresses, Mem0's memory automatically updates based on the interactions, providing a continuously improving personalized learning experience. This setup ensures that the AI Tutor can offer contextually relevant and accurate responses, enhancing the overall educational process.



================================================
FILE: docs/examples/personal-travel-assistant.mdx
================================================
---
title: Personal AI Travel Assistant
---


Create a personalized AI Travel Assistant using Mem0. This guide provides step-by-step instructions and the complete code to get you started.

## Overview

The Personalized AI Travel Assistant uses Mem0 to store and retrieve information across interactions, enabling a tailored travel planning experience. It integrates with OpenAI's GPT-4 model to provide detailed and context-aware responses to user queries.

## Setup

Install the required dependencies using pip:

```bash
pip install openai mem0ai
```

## Full Code Example

Here's the complete code to create and interact with a Personalized AI Travel Assistant using Mem0:

<CodeGroup>

```python After v1.1
import os
from openai import OpenAI
from mem0 import Memory

# Set the OpenAI API key
os.environ['OPENAI_API_KEY'] = "sk-xxx"

config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o",
            "temperature": 0.1,
            "max_tokens": 2000,
        }
    },
    "embedder": {
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-large"
        }
    },
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name": "test",
            "embedding_model_dims": 3072,
        }
    },
    "version": "v1.1",
}

class PersonalTravelAssistant:
    def __init__(self):
        self.client = OpenAI()
        self.memory = Memory.from_config(config)
        self.messages = [{"role": "system", "content": "You are a personal AI Assistant."}]

    def ask_question(self, question, user_id):
        # Fetch previous related memories
        previous_memories = self.search_memories(question, user_id=user_id)

        # Build the prompt
        system_message = "You are a personal AI Assistant."

        if previous_memories:
            prompt = f"{system_message}\n\nUser input: {question}\nPrevious memories: {', '.join(previous_memories)}"
        else:
            prompt = f"{system_message}\n\nUser input: {question}"

        # Generate response using Responses API
        response = self.client.responses.create(
            model="gpt-4o",
            input=prompt
        )

        # Extract answer from the response
        answer = response.output[0].content[0].text

        # Store the question in memory
        self.memory.add(question, user_id=user_id)
        return answer

    def get_memories(self, user_id):
        memories = self.memory.get_all(user_id=user_id)
        return [m['memory'] for m in memories['results']]

    def search_memories(self, query, user_id):
        memories = self.memory.search(query, user_id=user_id)
        return [m['memory'] for m in memories['results']]

# Usage example
user_id = "traveler_123"
ai_assistant = PersonalTravelAssistant()

def main():
    while True:
        question = input("Question: ")
        if question.lower() in ['q', 'exit']:
            print("Exiting...")
            break

        answer = ai_assistant.ask_question(question, user_id=user_id)
        print(f"Answer: {answer}")
        memories = ai_assistant.get_memories(user_id=user_id)
        print("Memories:")
        for memory in memories:
            print(f"- {memory}")
        print("-----")

if __name__ == "__main__":
    main()
```

```python Before v1.1
import os
from openai import OpenAI
from mem0 import Memory

# Set the OpenAI API key
os.environ['OPENAI_API_KEY'] = 'sk-xxx'

class PersonalTravelAssistant:
    def __init__(self):
        self.client = OpenAI()
        self.memory = Memory()
        self.messages = [{"role": "system", "content": "You are a personal AI Assistant."}]

    def ask_question(self, question, user_id):
        # Fetch previous related memories
        previous_memories = self.search_memories(question, user_id=user_id)
        prompt = question
        if previous_memories:
            prompt = f"User input: {question}\n Previous memories: {previous_memories}"
        self.messages.append({"role": "user", "content": prompt})

        # Generate response using GPT-4o
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=self.messages
        )
        answer = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": answer})

        # Store the question in memory
        self.memory.add(question, user_id=user_id)
        return answer

    def get_memories(self, user_id):
        memories = self.memory.get_all(user_id=user_id)
        return [m['memory'] for m in memories.get('results', [])]

    def search_memories(self, query, user_id):
        memories = self.memory.search(query, user_id=user_id)
        return [m['memory'] for m in memories.get('results', [])]

# Usage example
user_id = "traveler_123"
ai_assistant = PersonalTravelAssistant()

def main():
    while True:
        question = input("Question: ")
        if question.lower() in ['q', 'exit']:
            print("Exiting...")
            break

        answer = ai_assistant.ask_question(question, user_id=user_id)
        print(f"Answer: {answer}")
        memories = ai_assistant.get_memories(user_id=user_id)
        print("Memories:")
        for memory in memories:
            print(f"- {memory}")
        print("-----")

if __name__ == "__main__":
    main()
```
</CodeGroup>


## Key Components

- **Initialization**: The `PersonalTravelAssistant` class is initialized with the OpenAI client and Mem0 memory setup.
- **Asking Questions**: The `ask_question` method sends a question to the AI, incorporates previous memories, and stores new information.
- **Memory Management**: The `get_memories` and search_memories methods handle retrieval and searching of stored memories.

## Usage

1. Set your OpenAI API key in the environment variable.
2. Instantiate the `PersonalTravelAssistant`.
3. Use the `main()` function to interact with the assistant in a loop.

## Conclusion

This Personalized AI Travel Assistant leverages Mem0's memory capabilities to provide context-aware responses. As you interact with it, the assistant learns and improves, offering increasingly personalized travel advice and information.


================================================
FILE: docs/examples/personalized-deep-research.mdx
================================================
---
title: Personalized Deep Research
---

Deep Research is an intelligent agent that synthesizes large amounts of online data and completes complex research tasks, customized to your unique preferences and insights. Built on Mem0's technology, it enhances AI-driven online exploration with personalized memories.

You can checkout GitHub repositry here: [Personalized Deep Research](https://github.com/mem0ai/personalized-deep-research/tree/mem0)

## Overview

Deep Research leverages Mem0's memory capabilities to:
- Synthesize large amounts of online data
- Complete complex research tasks
- Customize results to your preferences
- Store and utilize personal insights
- Maintain context across research sessions

## Demo

Watch Deep Research in action:

<iframe 
  width="700" 
  height="400" 
  src="https://www.youtube.com/embed/8vQlCtXzF60?si=b8iTOgummAVzR7ia" 
  title="YouTube video player" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
  referrerpolicy="strict-origin-when-cross-origin" 
  allowfullscreen
></iframe>

## Features

### 1. Personalized Research
- Analyzes your background and expertise
- Tailors research depth and complexity to your level
- Incorporates your previous research context

### 2. Comprehensive Data Synthesis
- Processes multiple online sources
- Extracts relevant information
- Provides coherent summaries

### 3. Memory Integration
- Stores research findings for future reference
- Maintains context across sessions
- Links related research topics

### 4. Interactive Exploration
- Allows real-time query refinement
- Supports follow-up questions
- Enables deep-diving into specific areas

## Use Cases

- **Academic Research**: Literature reviews, thesis research, paper writing
- **Market Research**: Industry analysis, competitor research, trend identification
- **Technical Research**: Technology evaluation, solution comparison
- **Business Research**: Strategic planning, opportunity analysis


## Try It Out

> To try it yourself, clone the repository and follow the instructions in the README to run it locally or deploy it.

- [Personalized Deep Research GitHub](https://github.com/mem0ai/personalized-deep-research/tree/mem0)



================================================
FILE: docs/examples/personalized-search-tavily-mem0.mdx
================================================
---
title: 'Personalized Search with Mem0 and Tavily'
---

<Snippet file="security-compliance.mdx" />

Imagine asking a search assistant for "coffee shops nearby" and instead of generic results, it shows remote-work-friendly cafes with great wifi in your city because it remembers you mentioned working remotely before. Or when you search for "lunchbox ideas for kids" it knows you have a **7-year-old daughter** and recommends **peanut-free options** that align with her allergy.

That's what we are going to build today, a **Personalized Search Assistant** powered by **Mem0** for memory and [Tavily](https://tavily.com) for real-time search.


## Why Personalized Search

Most assistants treat every query like they’ve never seen you before. That means repeating yourself about your location, diet, or preferences, and getting results that feel generic.

- With **Mem0**, your assistant builds a memory of the user’s world.
- With **Tavily**, it fetches fresh and accurate results in real time.

Together, they make every interaction **smarter, faster, and more personal**.

## Prerequisites

Before you begin, make sure you have:

1. Installed the dependencies:
```bash
pip install langchain mem0ai langchain-tavily langchain-openai
```

2. Set up your API keys in a .env file:
```bash
OPENAI_API_KEY=your-openai-key
TAVILY_API_KEY=your-tavily-key
MEM0_API_KEY=your-mem0-key
```

## Code Walkthrough
Let’s break down the main components.

### 1: Initialize Mem0 with Custom Instructions

We configure Mem0 with custom instructions that guide it to infer user memories tailored specifically for our usecase.

```python
from mem0 import MemoryClient

mem0_client = MemoryClient()

mem0_client.project.update(
    custom_instructions='''
INFER THE MEMORIES FROM USER QUERIES EVEN IF IT'S A QUESTION.

We are building personalized search for which we need to understand about user's preferences and life
and extract facts and memories accordingly.
'''
)
```
Now, if a user casually mentions "I need to pick up my daughter", or "What's the weather at Los Angeles", Mem0 remembers they have a daughter or user is somewhat interested/connected with Los Angeles in terms of location, those will be referred for future searches.

### 2. Simulating User History
To test personalization, we preload some sample conversation history for a user:

```python
def setup_user_history(user_id):
    conversations = [
        [{"role": "user", "content": "What will be the weather today at Los Angeles? I need to pick up my daughter from office."},
         {"role": "assistant", "content": "I'll check the weather in LA for you."}],
        [{"role": "user", "content": "I'm looking for vegan restaurants in Santa Monica"},
         {"role": "assistant", "content": "I'll find great vegan options in Santa Monica."}],
        [{"role": "user", "content": "My 7-year-old daughter is allergic to peanuts"},
         {"role": "assistant", "content": "I'll remember to check for peanut-free options."}],
        [{"role": "user", "content": "I work remotely and need coffee shops with good wifi"},
         {"role": "assistant", "content": "I'll find remote-work-friendly coffee shops."}],
        [{"role": "user", "content": "We love hiking and outdoor activities on weekends"},
         {"role": "assistant", "content": "Great! I'll keep your outdoor activity preferences in mind."}],
    ]

    for conversation in conversations:
        mem0_client.add(conversation, user_id=user_id, output_format="v1.1")
```
This gives the agent a baseline understanding of the user’s lifestyle and needs.

### 3. Retrieving User Context from Memory
When a user makes a new search query, we retrieve relevant memories to enhance the search query:

```python
def get_user_context(user_id, query):
    filters = {"AND": [{"user_id": user_id}]}
    user_memories = mem0_client.search(query=query, version="v2", filters=filters)

    if user_memories:
        context = "\n".join([f"- {memory['memory']}" for memory in user_memories])
        return context
    else:
        return "No previous user context available."
```
This context is injected into the search agent so results are personalized.

### 4. Creating the Personalized Search Agent
The agent uses Tavily search, but always augments search queries with user context:

```python
def create_personalized_search_agent(user_context):
    tavily_search = TavilySearch(
        max_results=10,
        search_depth="advanced",
        include_answer=True,
        topic="general"
    )

    tools = [tavily_search]

    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""You are a personalized search assistant.

USER CONTEXT AND PREFERENCES:
{user_context}

YOUR ROLE:
1. Analyze the user's query and context.
2. Enhance the query with relevant personal memories.
3. Always use tavily_search for results.
4. Explain which memories influenced personalization.
"""),
        MessagesPlaceholder(variable_name="messages"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    agent = create_openai_tools_agent(llm=llm, tools=tools, prompt=prompt)
    return AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)
```

### 5. Run a Personalized Search
The workflow ties everything together:

```python
def conduct_personalized_search(user_id, query):
    user_context = get_user_context(user_id, query)
    agent_executor = create_personalized_search_agent(user_context)

    response = agent_executor.invoke({"messages": [HumanMessage(content=query)]})
    return {"agent_response": response['output']}
```

### 6. Store New Interactions
Every new query/response pair is stored for future personalization:

```python
def store_search_interaction(user_id, original_query, agent_response):
    interaction = [
        {"role": "user", "content": f"Searched for: {original_query}"},
        {"role": "assistant", "content": f"Results based on preferences: {agent_response}"}
    ]
    mem0_client.add(messages=interaction, user_id=user_id, output_format="v1.1")
```

### Full Example Run

```python
if __name__ == "__main__":
    user_id = "john"
    setup_user_history(user_id)

    queries = [
        "good coffee shops nearby for working",
        "what can I make for my kid in lunch?"
    ]

    for q in queries:
        results = conduct_personalized_search(user_id, q)
        print(f"\nQuery: {q}")
        print(f"Personalized Response: {results['agent_response']}")
```

## How It Works in Practice
Here’s how personalization plays out:

- Context Gathering: User previously mentioned living in Los Angeles, being vegan, and having a 7-year-old daughter allergic to peanuts.
- Enhanced Search Query:
Query -> "good coffee shops nearby for working"
Enhanced Query -> "good coffee shops in Los Angeles with strong wifi, remote-work-friendly"
- Personalized Results: The assistant only returns wifi-friendly, work-friendly cafes near Los Angeles.
- Memory Update: Interaction is saved for better future recommendations.

## Conclusion
With Mem0 + Tavily, you can build a search assistant that doesn’t just fetch results but it understands the person behind the query.

Whether for shopping, travel, or daily life, this approach turns a generic search into a truly personalized experience.

Full Code: [Personalized Search GitHub](https://github.com/mem0ai/mem0/blob/main/examples/misc/personalized_search.py)


================================================
FILE: docs/examples/youtube-assistant.mdx
================================================
---
title: YouTube Assistant Extension
---

Enhance your YouTube experience with Mem0's **YouTube Assistant**, a Chrome extension that brings AI-powered chat directly to your YouTube videos. Get instant, personalized answers about video content while leveraging your own knowledge and memories - all without leaving the page.

## Features

- **Contextual AI Chat**: Ask questions about videos you're watching
- **Seamless Integration**: Chat interface sits alongside YouTube's native UI
- **Memory Integration**: Personalized responses based on your knowledge through Mem0
- **Real-Time Memory**: Memories are updated in real-time based on your interactions

## Demo Video

<video
  autoPlay
  muted
  loop
  playsInline
  width="700"
  height="400"
  src="https://github.com/user-attachments/assets/c0334ccd-311b-4dd7-8034-ef88204fc751"
></video>

## Installation

This extension is not available on the Chrome Web Store yet. You can install it manually using below method:

### Manual Installation (Developer Mode)

1. **Download the Extension**: Clone or download the extension files from the [Mem0 GitHub repository](https://github.com/mem0ai/mem0/tree/main/examples).
2. **Build**: Run `npm install` followed by `npm run build` to install the dependencies and build the extension.
3. **Access Chrome Extensions**: Open Google Chrome and navigate to `chrome://extensions`.
4. **Enable Developer Mode**: Toggle the "Developer mode" switch in the top right corner.
5. **Load Unpacked Extension**: Click "Load unpacked" and select the directory containing the extension files.
6. **Confirm Installation**: The Mem0 YouTube Assistant Extension should now appear in your Chrome toolbar.

## Setup

1. **Configure API Settings**: Click the extension icon and enter your OpenAI API key (required to use the extension)
2. **Customize Settings**: Configure additional settings such as model, temperature, and memory settings
3. **Navigate to YouTube**: Start using the assistant on any YouTube video
4. **Memories**: Enter your Mem0 API key to enable personalized responses, and feed initial memories from settings

## Example Prompts

- "Can you summarize the main points of this video?"
- "Explain the concept they just mentioned"
- "How does this relate to what I already know?"
- "What are some practical applications of this topic related to my work?"


## Privacy and Data Security

Your API keys are stored locally in your browser. Your messages are sent to the Mem0 API for extracting and retrieving memories. Mem0 is committed to ensuring your data's privacy and security.



================================================
FILE: docs/integrations/agentops.mdx
================================================
---
title: AgentOps
---

Integrate [**Mem0**](https://github.com/mem0ai/mem0) with [AgentOps](https://agentops.ai), a comprehensive monitoring and analytics platform for AI agents. This integration enables automatic tracking and analysis of memory operations, providing insights into agent performance and memory usage patterns.

## Overview

1. Automatic monitoring of Mem0 operations and performance metrics
2. Real-time tracking of memory add, search, and retrieval operations
3. Analytics dashboard with memory usage patterns and insights
4. Error tracking and debugging capabilities for memory operations

## Prerequisites

Before setting up Mem0 with AgentOps, ensure you have:

1. Installed the required packages:
```bash
pip install mem0ai agentops python-dotenv
```

2. Valid API keys:
   - [AgentOps API Key](https://app.agentops.ai/dashboard/api-keys)
   - OpenAI API Key (for LLM operations)
   - [Mem0 API Key](https://app.mem0.ai/dashboard/api-keys) (optional, for cloud operations)

## Basic Integration Example

The following example demonstrates how to integrate Mem0 with AgentOps monitoring for comprehensive memory operation tracking:

```python
#Import the required libraries for local memory management with Mem0
from mem0 import Memory, AsyncMemory
import os
import asyncio
import logging
from dotenv import load_dotenv
import agentops
import openai

load_dotenv()
#Set up environment variables for API keys
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

#Set up the configuration for local memory storage and define sample user data. 
local_config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o-mini",
            "temperature": 0.1,
            "max_tokens": 2000,
        },
    }
}
user_id = "alice_demo"
agent_id = "assistant_demo"
run_id = "session_001"

sample_messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {
        "role": "assistant",
        "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.",
    },
]

sample_preferences = [
    "I prefer dark roast coffee over light roast",
    "I exercise every morning at 6 AM",
    "I'm vegetarian and avoid all meat products",
    "I love reading science fiction novels",
    "I work in software engineering",
]

#This function demonstrates sequential memory operations using the synchronous Memory class
def demonstrate_sync_memory(local_config, sample_messages, sample_preferences, user_id):
    """
    Demonstrate synchronous Memory class operations.
    """

    agentops.start_trace("mem0_memory_example", tags=["mem0_memory_example"])
    try:
        
        memory = Memory.from_config(local_config)

        result = memory.add(
            sample_messages, user_id=user_id, metadata={"category": "movie_preferences", "session": "demo"}
        )

        for i, preference in enumerate(sample_preferences):
            result = memory.add(preference, user_id=user_id, metadata={"type": "preference", "index": i})
       
        search_queries = [
            "What movies does the user like?",
            "What are the user's food preferences?",
            "When does the user exercise?",
        ]

        for query in search_queries:
            results = memory.search(query, user_id=user_id)
        
            if results and "results" in results:
                for j, result in enumerate(results['results']): 
                    print(f"Result {j+1}: {result.get('memory', 'N/A')}")
            else:
                print("No results found")

        all_memories = memory.get_all(user_id=user_id)
        if all_memories and "results" in all_memories:
            print(f"Total memories: {len(all_memories['results'])}")

        delete_all_result = memory.delete_all(user_id=user_id)
        print(f"Delete all result: {delete_all_result}")

        agentops.end_trace(end_state="success")
    except Exception as e:
        agentops.end_trace(end_state="error")

# Execute sync demonstrations
demonstrate_sync_memory(local_config, sample_messages, sample_preferences, user_id)

```

For detailed information on this integration, refer to the official [Agentops Mem0 integration documentation](https://docs.agentops.ai/v2/integrations/mem0).


## Key Features

### 1. Automatic Operation Tracking

AgentOps automatically monitors all Mem0 operations:

- **Memory Operations**: Track add, search, get_all, delete operations and much more
- **Performance Metrics**: Monitor response times and success rates
- **Error Tracking**: Capture and analyze operation failures

### 2. Real-time Analytics Dashboard

Access comprehensive analytics through the AgentOps dashboard:

- **Usage Patterns**: Visualize memory usage trends over time
- **User Behavior**: Analyze how different users interact with memory
- **Performance Insights**: Identify bottlenecks and optimization opportunities

### 3. Session Management

Organize your monitoring with structured sessions:

- **Session Tracking**: Group related operations into logical sessions
- **Success/Failure Rates**: Track session outcomes for reliability monitoring
- **Custom Metadata**: Add context to sessions for better analysis

## Best Practices

1. **Initialize Early**: Always initialize AgentOps before importing Mem0 classes
2. **Session Management**: Use meaningful session names and end sessions appropriately
3. **Error Handling**: Wrap operations in try-catch blocks and report failures
4. **Tagging**: Use tags to organize different types of memory operations
5. **Environment Separation**: Use different projects or tags for dev/staging/prod

## Help & Resources

- [AgentOps Documentation](https://docs.agentops.ai/)
- [AgentOps Dashboard](https://app.agentops.ai/)
- [Mem0 Platform](https://app.mem0.ai/)

<Snippet file="get-help.mdx" /> 
 


================================================
FILE: docs/integrations/agno.mdx
================================================
---
title: Agno
---

This integration of [**Mem0**](https://github.com/mem0ai/mem0) with [Agno](https://github.com/agno-agi/agno, enables persistent, multimodal memory for Agno-based agents - improving personalization, context awareness, and continuity across conversations.

## Overview

1. Store and retrieve memories from Mem0 within Agno agents
2. Support for multimodal interactions (text and images)
3. Semantic search for relevant past conversations
4. Personalized responses based on user history
5. One-line memory integration via `Mem0Tools`

## Prerequisites

Before setting up Mem0 with Agno, ensure you have:

1. Installed the required packages:
```bash
pip install agno mem0ai python-dotenv
```

2. Valid API keys:
   - [Mem0 API Key](https://app.mem0.ai/dashboard/api-keys)
   - OpenAI API Key (for the agent model)

## Quick Integration (Using `Mem0Tools`)

The simplest way to integrate Mem0 with Agno Agents is to use Mem0 as a tool using built-in `Mem0Tools`:

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mem0 import Mem0Tools

agent = Agent(
    name="Memory Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[Mem0Tools()],
    description="An assistant that remembers and personalizes using Mem0 memory."
)
```

This enables memory functionality out of the box:

- **Persistent memory writing**: `Mem0Tools` uses `MemoryClient.add(...)` to store messages from user-agent interactions, including optional metadata such as user ID or session.
- **Contextual memory search**: Compatible queries use `MemoryClient.search(...)` to retrieve relevant past messages, improving contextual understanding.
- **Multimodal support**: Both text and image inputs are supported, allowing richer memory records.

> `Mem0Tools` uses the `MemoryClient` under the hood and requires no additional setup. You can customize its behavior by modifying your tools list or extending it in code.

## Full Manual Example

> Note: Mem0 can also be used with Agno Agents as a separate memory layer.

The following example demonstrates how to create an Agno agent with Mem0 memory integration, including support for image processing:

```python
import base64
from pathlib import Path
from typing import Optional

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from mem0 import MemoryClient

# Initialize the Mem0 client
client = MemoryClient()

# Define the agent
agent = Agent(
    name="Personal Agent",
    model=OpenAIChat(id="gpt-4"),
    description="You are a helpful personal agent that helps me with day to day activities."
                "You can process both text and images.",
    markdown=True
)


def chat_user(
    user_input: Optional[str] = None,
    user_id: str = "alex",
    image_path: Optional[str] = None
) -> str:
    """
    Handle user input with memory integration, supporting both text and images.

    Args:
        user_input: The user's text input
        user_id: Unique identifier for the user
        image_path: Path to an image file if provided

    Returns:
        The agent's response as a string
    """
    if image_path:
        # Convert image to base64
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode("utf-8")

        # Create message objects for text and image
        messages = []

        if user_input:
            messages.append({
                "role": "user",
                "content": user_input
            })

        messages.append({
            "role": "user",
            "content": {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            }
        })

        # Store messages in memory
        client.add(messages, user_id=user_id, output_format='v1.1')
        print("✅ Image and text stored in memory.")

    if user_input:
        # Search for relevant memories
        memories = client.search(user_input, user_id=user_id, output_format='v1.1')
        memory_context = "\n".join(f"- {m['memory']}" for m in memories['results'])

        # Construct the prompt
        prompt = f"""
You are a helpful personal assistant who helps users with their day-to-day activities and keeps track of everything.

Your task is to:
1. Analyze the given image (if present) and extract meaningful details to answer the user's question.
2. Use your past memory of the user to personalize your answer.
3. Combine the image content and memory to generate a helpful, context-aware response.

Here is what I remember about the user:
{memory_context}

User question:
{user_input}
"""
        # Get response from agent
        if image_path:
            response = agent.run(prompt, images=[Image(filepath=Path(image_path))])
        else:
            response = agent.run(prompt)

        # Store the interaction in memory
        interaction_message = [{"role": "user", "content": f"User: {user_input}\nAssistant: {response.content}"}]
        client.add(interaction_message, user_id=user_id, output_format='v1.1')
        return response.content

    return "No user input or image provided."


# Example Usage
if __name__ == "__main__":
    response = chat_user(
        "I like to travel and my favorite destination is London",
        image_path="travel_items.jpeg",
        user_id="alex"
    )
    print(response)
```

## Key Features

### 1. Multimodal Memory Storage

The integration supports storing both text and image data:

- **Text Storage**: Conversation history is saved in a structured format
- **Image Analysis**: Agents can analyze images and store visual information
- **Combined Context**: Memory retrieval combines both text and visual data

### 2. Personalized Agent Responses

Improve your agent's context awareness:

- **Memory Retrieval**: Semantic search finds relevant past interactions
- **User Preferences**: Personalize responses based on stored user information
- **Continuity**: Maintain conversation threads across multiple sessions

### 3. Flexible Configuration

Customize the integration to your needs:

- **Use `Mem0Tools()`** for drop-in memory support
- **Use `MemoryClient` directly** for advanced control
- **User Identification**: Organize memories by user ID
- **Memory Search**: Configure search relevance and result count
- **Memory Formatting**: Support for various OpenAI message formats

## Help & Resources

- [Agno Documentation](https://docs.agno.com/introduction)
- [Mem0 Platform](https://app.mem0.ai/)

<Snippet file="get-help.mdx" />



================================================
FILE: docs/integrations/autogen.mdx
================================================
---
title: AutoGen
---

Build conversational AI agents with memory capabilities. This integration combines AutoGen for creating AI agents with Mem0 for memory management, enabling context-aware and personalized interactions.

## Overview

In this guide, we'll explore an example of creating a conversational AI system with memory:
- A customer service bot that can recall previous interactions and provide personalized responses.

## Setup and Configuration

Install necessary libraries:

```bash
pip install autogen mem0ai openai python-dotenv
```

First, we'll import the necessary libraries and set up our configurations.

<Note>Remember to get the Mem0 API key from [Mem0 Platform](https://app.mem0.ai).</Note>

```python
import os
from autogen import ConversableAgent
from mem0 import MemoryClient
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Configuration
# OPENAI_API_KEY = 'sk-xxx'  # Replace with your actual OpenAI API key
# MEM0_API_KEY = 'your-mem0-key'  # Replace with your actual Mem0 API key from https://app.mem0.ai
USER_ID = "alice"

# Set up OpenAI API key
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
# os.environ['MEM0_API_KEY'] = MEM0_API_KEY

# Initialize Mem0 and AutoGen agents
memory_client = MemoryClient()
agent = ConversableAgent(
    "chatbot",
    llm_config={"config_list": [{"model": "gpt-4", "api_key": OPENAI_API_KEY}]},
    code_execution_config=False,
    human_input_mode="NEVER",
)
```

## Storing Conversations in Memory

Add conversation history to Mem0 for future reference:

```python
conversation = [
    {"role": "assistant", "content": "Hi, I'm Best Buy's chatbot! How can I help you?"},
    {"role": "user", "content": "I'm seeing horizontal lines on my TV."},
    {"role": "assistant", "content": "I'm sorry to hear that. Can you provide your TV model?"},
    {"role": "user", "content": "It's a Sony - 77\" Class BRAVIA XR A80K OLED 4K UHD Smart Google TV"},
    {"role": "assistant", "content": "Thank you for the information. Let's troubleshoot this issue..."}
]

memory_client.add(messages=conversation, user_id=USER_ID, output_format="v1.1")
print("Conversation added to memory.")
```

## Retrieving and Using Memory

Create a function to get context-aware responses based on user's question and previous interactions:

```python
def get_context_aware_response(question):
    relevant_memories = memory_client.search(question, user_id=USER_ID, output_format='v1.1')
    context = "\n".join([m["memory"] for m in relevant_memories.get('results', [])])

    prompt = f"""Answer the user question considering the previous interactions:
    Previous interactions:
    {context}

    Question: {question}
    """

    reply = agent.generate_reply(messages=[{"content": prompt, "role": "user"}])
    return reply

# Example usage
question = "What was the issue with my TV?"
answer = get_context_aware_response(question)
print("Context-aware answer:", answer)
```

## Multi-Agent Conversation

For more complex scenarios, you can create multiple agents:

```python
manager = ConversableAgent(
    "manager",
    system_message="You are a manager who helps in resolving complex customer issues.",
    llm_config={"config_list": [{"model": "gpt-4", "api_key": OPENAI_API_KEY}]},
    human_input_mode="NEVER"
)

def escalate_to_manager(question):
    relevant_memories = memory_client.search(question, user_id=USER_ID, output_format='v1.1')
    context = "\n".join([m["memory"] for m in relevant_memories.get('results', [])])

    prompt = f"""
    Context from previous interactions:
    {context}

    Customer question: {question}

    As a manager, how would you address this issue?
    """

    manager_response = manager.generate_reply(messages=[{"content": prompt, "role": "user"}])
    return manager_response

# Example usage
complex_question = "I'm not satisfied with the troubleshooting steps. What else can be done?"
manager_answer = escalate_to_manager(complex_question)
print("Manager's response:", manager_answer)
```

## Conclusion

By integrating AutoGen with Mem0, you've created a conversational AI system with memory capabilities. This example demonstrates a customer service bot that can recall previous interactions and provide context-aware responses, with the ability to escalate complex issues to a manager agent.

This integration enables the creation of more intelligent and personalized AI agents for various applications, such as customer support, virtual assistants, and interactive chatbots.

## Help

In case of any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/integrations/aws-bedrock.mdx
================================================
---
title: AWS Bedrock
---

This integration demonstrates how to use **Mem0** with **AWS Bedrock** and **Amazon OpenSearch Service (AOSS)** to enable persistent, semantic memory in intelligent agents.

## Overview

In this guide, you'll:

1. Configure AWS credentials to enable Bedrock and OpenSearch access
2. Set up the Mem0 SDK to use Bedrock for embeddings and LLM
3. Store and retrieve memories using OpenSearch as a vector store
4. Build memory-aware applications with scalable cloud infrastructure

## Prerequisites

- AWS account with access to:
  - Bedrock foundation models (e.g., Titan, Claude)
  - OpenSearch Service with a configured domain
- Python 3.8+
- Valid AWS credentials (via environment or IAM role)

## Setup and Installation

Install required packages:

```bash
pip install mem0ai boto3 opensearch-py
```

Set environment variables:

Be sure to configure your AWS credentials using environment variables, IAM roles, or the AWS CLI.

```python
import os

os.environ['AWS_REGION'] = 'us-west-2'
os.environ['AWS_ACCESS_KEY_ID'] = 'AKIA...'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'AS...'
```

## Initialize Mem0 Integration

Import necessary modules and configure Mem0:

```python
import boto3
from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth
from mem0.memory.main import Memory

region = 'us-west-2'
service = 'aoss'
credentials = boto3.Session().get_credentials()
auth = AWSV4SignerAuth(credentials, region, service)

config = {
    "embedder": {
        "provider": "aws_bedrock",
        "config": {
            "model": "amazon.titan-embed-text-v2:0"
        }
    },
    "llm": {
        "provider": "aws_bedrock",
        "config": {
            "model": "anthropic.claude-3-5-haiku-20241022-v1:0",
            "temperature": 0.1,
            "max_tokens": 2000
        }
    },
    "vector_store": {
        "provider": "opensearch",
        "config": {
            "collection_name": "mem0",
            "host": "your-opensearch-domain.us-west-2.es.amazonaws.com",
            "port": 443,
            "http_auth": auth,
            "embedding_model_dims": 1024,
            "connection_class": RequestsHttpConnection,
            "pool_maxsize": 20,
            "use_ssl": True,
            "verify_certs": True
        }
    }
}

# Initialize memory system
m = Memory.from_config(config)
```

## Memory Operations

Use Mem0 with your Bedrock-powered LLM and OpenSearch storage backend:

```python
# Store conversational context
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller?"},
    {"role": "user", "content": "I prefer sci-fi."},
    {"role": "assistant", "content": "Noted! I'll suggest sci-fi movies next time."}
]

m.add(messages, user_id="alice", metadata={"category": "movie_recommendations"})

# Search for memory
relevant = m.search("What kind of movies does Alice like?", user_id="alice")

# Retrieve all user memories
all_memories = m.get_all(user_id="alice")
```

## Key Features

1. **Serverless Memory Embeddings**: Use Titan or other Bedrock models for fast, cloud-native embeddings
2. **Scalable Vector Search**: Store and retrieve vectorized memories via OpenSearch
3. **Seamless AWS Auth**: Uses AWS IAM or environment variables to securely authenticate
4. **User-specific Memory Spaces**: Memories are isolated per user ID
5. **Persistent Memory Context**: Maintain and recall history across sessions

## Help

- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
- [Amazon OpenSearch Service Docs](https://docs.aws.amazon.com/opensearch-service/)
- [Mem0 Platform](https://app.mem0.ai)

<Snippet file="get-help.mdx" />




================================================
FILE: docs/integrations/crewai.mdx
================================================
---
title: CrewAI
---

Build an AI system that combines CrewAI's agent-based architecture with Mem0's memory capabilities. This integration enables persistent memory across agent interactions and personalized task execution based on user history.

## Overview

In this guide, we'll create a CrewAI agent that:
1. Uses CrewAI to manage AI agents and tasks
2. Leverages Mem0 to store and retrieve conversation history
3. Creates personalized experiences based on stored user preferences

## Setup and Configuration

Install necessary libraries:

```bash
pip install crewai crewai-tools mem0ai
```

Import required modules and set up configurations:

<Note>Remember to get your API keys from [Mem0 Platform](https://app.mem0.ai), [OpenAI](https://platform.openai.com) and [Serper Dev](https://serper.dev) for search capabilities.</Note>

```python
import os
from mem0 import MemoryClient
from crewai import Agent, Task, Crew, Process
from crewai_tools import SerperDevTool

# Configuration
os.environ["MEM0_API_KEY"] = "your-mem0-api-key"
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["SERPER_API_KEY"] = "your-serper-api-key"

# Initialize Mem0 client
client = MemoryClient()
```

## Store User Preferences

Set up initial conversation and preferences storage:

```python
def store_user_preferences(user_id: str, conversation: list):
    """Store user preferences from conversation history"""
    client.add(conversation, user_id=user_id)

# Example conversation storage
messages = [
    {
        "role": "user",
        "content": "Hi there! I'm planning a vacation and could use some advice.",
    },
    {
        "role": "assistant",
        "content": "Hello! I'd be happy to help with your vacation planning. What kind of destination do you prefer?",
    },
    {"role": "user", "content": "I am more of a beach person than a mountain person."},
    {
        "role": "assistant",
        "content": "That's interesting. Do you like hotels or airbnb?",
    },
    {"role": "user", "content": "I like airbnb more."},
]

store_user_preferences("crew_user_1", messages)
```

## Create CrewAI Agent

Define an agent with memory capabilities:

```python
def create_travel_agent():
    """Create a travel planning agent with search capabilities"""
    search_tool = SerperDevTool()

    return Agent(
        role="Personalized Travel Planner Agent",
        goal="Plan personalized travel itineraries",
        backstory="""You are a seasoned travel planner, known for your meticulous attention to detail.""",
        allow_delegation=False,
        memory=True,
        tools=[search_tool],
    )
```

## Define Tasks

Create tasks for your agent:

```python
def create_planning_task(agent, destination: str):
    """Create a travel planning task"""
    return Task(
        description=f"""Find places to live, eat, and visit in {destination}.""",
        expected_output=f"A detailed list of places to live, eat, and visit in {destination}.",
        agent=agent,
    )
```

## Set Up Crew

Configure the crew with memory integration:

```python
def setup_crew(agents: list, tasks: list):
    """Set up a crew with Mem0 memory integration"""
    return Crew(
        agents=agents,
        tasks=tasks,
        process=Process.sequential,
        memory=True,
        memory_config={
            "provider": "mem0",
            "config": {"user_id": "crew_user_1"},
        }
    )
```

## Main Execution Function

Implement the main function to run the travel planning system:

```python
def plan_trip(destination: str, user_id: str):
    # Create agent
    travel_agent = create_travel_agent()

    # Create task
    planning_task = create_planning_task(travel_agent, destination)

    # Setup crew
    crew = setup_crew([travel_agent], [planning_task])

    # Execute and return results
    return crew.kickoff()

# Example usage
if __name__ == "__main__":
    result = plan_trip("San Francisco", "crew_user_1")
    print(result)
```

## Key Features

1. **Persistent Memory**: Uses Mem0 to maintain user preferences and conversation history
2. **Agent-Based Architecture**: Leverages CrewAI's agent system for task execution
3. **Search Integration**: Includes SerperDev tool for real-world information retrieval
4. **Personalization**: Utilizes stored preferences for tailored recommendations

## Benefits

1. **Persistent Context & Memory**: Maintains user preferences and interaction history across sessions
2. **Flexible & Scalable Design**: Easily extendable with new agents, tasks and capabilities

## Conclusion

By combining CrewAI with Mem0, you can create sophisticated AI systems that maintain context and provide personalized experiences while leveraging the power of autonomous agents.

## Help

- [CrewAI Documentation](https://docs.crewai.com/)
- [Mem0 Platform](https://app.mem0.ai/)

<Snippet file="get-help.mdx" />



================================================
FILE: docs/integrations/dify.mdx
================================================
---
title: Dify
---

# Integrating Mem0 with Dify AI

Mem0 brings a robust memory layer to Dify AI, empowering your AI agents with persistent conversation storage and retrieval capabilities. With Mem0, your Dify applications gain the ability to recall past interactions and maintain context, ensuring more natural and insightful conversations.

---

## How to Integrate Mem0 in Your Dify Workflow

1. **Install the Mem0 Plugin:**  
   Head to the [Dify Marketplace](https://marketplace.dify.ai/plugins/yevanchen/mem0) and install the Mem0 plugin. This is your first step toward adding intelligent memory to your AI applications.

2. **Create or Open Your Dify Project:**  
   Whether you're starting fresh or updating an existing project, simply create or open your Dify workspace.

3. **Add the Mem0 Plugin to Your Project:**  
   Within your project, add the Mem0 plugin. This integration connects Mem0’s memory management capabilities directly to your Dify application.

4. **Configure Your Mem0 Settings:**  
   Customize Mem0 to suit your needs—set preferences for how conversation history is stored, the search parameters, and any other context-aware features.

5. **Leverage Mem0 in Your Workflow:**  
   Use Mem0 to store every conversation turn and retrieve past interactions seamlessly. This integration ensures that your AI agents can refer back to important context, making multi-turn dialogues more effective and user-centric.

---

![Mem0 Dify Integration](/images/dify-mem0-integration.png)

Enhance your Dify-powered AI with Mem0 and transform your conversational experiences. Start integrating intelligent memory management today and give your agents the context they need to excel!

[Explore Mem0 on Dify Marketplace](https://marketplace.dify.ai/plugins/yevanchen/mem0)


================================================
FILE: docs/integrations/elevenlabs.mdx
================================================
---
title: ElevenLabs
---

Create voice-based conversational AI agents with memory capabilities by integrating ElevenLabs and Mem0. This integration enables persistent, context-aware voice interactions that remember past conversations.

## Overview

In this guide, we'll build a voice agent that:
1. Uses ElevenLabs Conversational AI for voice interaction
2. Leverages Mem0 to store and retrieve memories from past conversations
3. Provides personalized responses based on user history

## Setup and Configuration

Install necessary libraries:

```bash
pip install elevenlabs mem0ai python-dotenv
```

Configure your environment variables:

<Note>You'll need both an ElevenLabs API key and a Mem0 API key to use this integration.</Note>

```bash
# Create a .env file with these variables
AGENT_ID=your-agent-id
USER_ID=unique-user-identifier
ELEVENLABS_API_KEY=your-elevenlabs-api-key
MEM0_API_KEY=your-mem0-api-key
```

## Integration Code Breakdown

Let's break down the implementation into manageable parts:

### 1. Imports and Environment Setup

First, we import required libraries and set up the environment:

```python
import os
import signal
import sys
from mem0 import AsyncMemoryClient

from elevenlabs.client import ElevenLabs
from elevenlabs.conversational_ai.conversation import Conversation
from elevenlabs.conversational_ai.default_audio_interface import DefaultAudioInterface
from elevenlabs.conversational_ai.conversation import ClientTools
```

These imports provide:
- Standard Python libraries for system operations and signal handling
- `AsyncMemoryClient` from Mem0 for memory operations
- ElevenLabs components for voice interaction

### 2. Environment Variables and Validation

Next, we validate the required environment variables:

```python
def main():
    # Required environment variables
    AGENT_ID = os.environ.get('AGENT_ID')
    USER_ID = os.environ.get('USER_ID')
    API_KEY = os.environ.get('ELEVENLABS_API_KEY')
    MEM0_API_KEY = os.environ.get('MEM0_API_KEY')

    # Validate required environment variables
    if not AGENT_ID:
        sys.stderr.write("AGENT_ID environment variable must be set\n")
        sys.exit(1)

    if not USER_ID:
        sys.stderr.write("USER_ID environment variable must be set\n")
        sys.exit(1)

    if not API_KEY:
        sys.stderr.write("ELEVENLABS_API_KEY not set, assuming the agent is public\n")

    if not MEM0_API_KEY:
        sys.stderr.write("MEM0_API_KEY environment variable must be set\n")
        sys.exit(1)

    # Set up Mem0 API key in the environment
    os.environ['MEM0_API_KEY'] = MEM0_API_KEY
```

This section:
- Retrieves required environment variables
- Performs validation to ensure required variables are present
- Exits the application with an error message if required variables are missing
- Sets the Mem0 API key in the environment for the Mem0 client to use

### 3. Client Initialization

Initialize both the ElevenLabs and Mem0 clients:

```python
    # Initialize ElevenLabs client
    client = ElevenLabs(api_key=API_KEY)

    # Initialize memory client and tools
    client_tools = ClientTools()
    mem0_client = AsyncMemoryClient()
```

Here we:
- Create an ElevenLabs client with the API key
- Initialize a ClientTools object for registering function tools
- Create an AsyncMemoryClient instance for Mem0 interactions

### 4. Memory Function Definitions

Define the two key memory functions that will be registered as tools:

```python
    # Define memory-related functions for the agent
    async def add_memories(parameters):
        """Add a message to the memory store"""
        message = parameters.get("message")
        await mem0_client.add(
            messages=message, 
            user_id=USER_ID, 
            output_format="v1.1", 
            version="v2"
        )
        return "Memory added successfully"

    async def retrieve_memories(parameters):
        """Retrieve relevant memories based on the input message"""
        message = parameters.get("message")

        # Set up filters to retrieve memories for this specific user
        filters = {
            "AND": [
                {
                    "user_id": USER_ID
                }
            ]
        }

        # Search for relevant memories using the message as a query
        results = await mem0_client.search(
            query=message,
            version="v2", 
            filters=filters
        )

        # Extract and join the memory texts
        memories = ' '.join([result["memory"] for result in results.get('results', [])])
        print("[ Memories ]", memories)

        if memories:
            return memories
        return "No memories found"
```

These functions:

#### `add_memories`:
- Takes a message parameter containing information to remember
- Stores the message in Mem0 using the `add` method
- Associates the memory with the specific USER_ID
- Returns a success message to the agent

#### `retrieve_memories`:
- Takes a message parameter as the search query
- Sets up filters to only retrieve memories for the current user
- Uses semantic search to find relevant memories
- Joins all retrieved memories into a single text
- Prints retrieved memories to the console for debugging
- Returns the memories or a "No memories found" message if none are found

### 5. Registering Memory Functions as Tools

Register the memory functions with the ElevenLabs ClientTools system:

```python
    # Register the memory functions as tools for the agent
    client_tools.register("addMemories", add_memories, is_async=True)
    client_tools.register("retrieveMemories", retrieve_memories, is_async=True)
```

This allows the ElevenLabs agent to:
- Access these functions through function calling
- Wait for asynchronous results (is_async=True)
- Call these functions by name ("addMemories" and "retrieveMemories")

### 6. Conversation Setup

Configure the conversation with ElevenLabs:

```python
    # Initialize the conversation
    conversation = Conversation(
        client,
        AGENT_ID,
        # Assume auth is required when API_KEY is set
        requires_auth=bool(API_KEY),
        audio_interface=DefaultAudioInterface(),
        client_tools=client_tools,
        callback_agent_response=lambda response: print(f"Agent: {response}"),
        callback_agent_response_correction=lambda original, corrected: print(f"Agent: {original} -> {corrected}"),
        callback_user_transcript=lambda transcript: print(f"User: {transcript}"),
        # callback_latency_measurement=lambda latency: print(f"Latency: {latency}ms"),
    )
```

This sets up the conversation with:
- The ElevenLabs client and Agent ID
- Authentication requirements based on API key presence
- DefaultAudioInterface for handling audio I/O
- The client_tools with our memory functions
- Callback functions for:
  - Displaying agent responses
  - Showing corrected responses (when the agent self-corrects)
  - Displaying user transcripts for debugging
  - (Commented out) Latency measurements

### 7. Conversation Management

Start and manage the conversation:

```python
    # Start the conversation
    print(f"Starting conversation with user_id: {USER_ID}")
    conversation.start_session()

    # Handle Ctrl+C to gracefully end the session
    signal.signal(signal.SIGINT, lambda sig, frame: conversation.end_session())

    # Wait for the conversation to end and get the conversation ID
    conversation_id = conversation.wait_for_session_end()
    print(f"Conversation ID: {conversation_id}")


if __name__ == '__main__':
    main()
```

This final section:
- Prints a message indicating the conversation has started
- Starts the conversation session
- Sets up a signal handler to gracefully end the session on Ctrl+C
- Waits for the session to end and gets the conversation ID
- Prints the conversation ID for reference

## Memory Tools Overview

This integration provides two key memory functions to your conversational AI agent:

### 1. Adding Memories (`addMemories`)

The `addMemories` tool allows your agent to store important information during a conversation, including:
- User preferences
- Important facts shared by the user
- Decisions or commitments made during the conversation
- Action items to follow up on

When the agent identifies information worth remembering, it calls this function to store it in the Mem0 database with the appropriate user ID.

#### How it works:
1. The agent identifies information that should be remembered
2. It formats the information as a message string
3. It calls the `addMemories` function with this message
4. The function stores the memory in Mem0 linked to the user's ID
5. Later conversations can retrieve this memory

#### Example usage in agent prompt:
```
When the user shares important information like preferences or personal details, 
use the addMemories function to store this information for future reference.
```

### 2. Retrieving Memories (`retrieveMemories`)

The `retrieveMemories` tool allows your agent to search for and retrieve relevant memories from previous conversations. The agent can:
- Search for context related to the current topic
- Recall user preferences
- Remember previous interactions on similar topics
- Create continuity across multiple sessions

#### How it works:
1. The agent needs context for the current conversation
2. It calls `retrieveMemories` with the current conversation topic or question
3. The function performs a semantic search in Mem0
4. Relevant memories are returned to the agent
5. The agent incorporates these memories into its response

#### Example usage in agent prompt:
```
At the beginning of each conversation turn, use retrieveMemories to check if we've 
discussed this topic before or if the user has shared relevant preferences.
```

## Configuring Your ElevenLabs Agent

To enable your agent to effectively use memory:

1. Add function calling capabilities to your agent in the ElevenLabs platform:
   - Go to your agent settings in the ElevenLabs platform
   - Navigate to the "Tools" section
   - Enable function calling for your agent
   - Add the memory tools as described below

2. Add the `addMemories` and `retrieveMemories` tools to your agent with these specifications:

For `addMemories`:
```json
{
  "name": "addMemories",
  "description": "Stores important information from the conversation to remember for future interactions",
  "parameters": {
    "type": "object",
    "properties": {
      "message": {
        "type": "string",
        "description": "The important information to remember"
      }
    },
    "required": ["message"]
  }
}
```

For `retrieveMemories`:
```json
{
  "name": "retrieveMemories",
  "description": "Retrieves relevant information from past conversations",
  "parameters": {
    "type": "object",
    "properties": {
      "message": {
        "type": "string",
        "description": "The query to search for in past memories"
      }
    },
    "required": ["message"]
  }
}
```

3. Update your agent's prompt to instruct it to use these memory functions. For example:

```
You are a helpful voice assistant that remembers past conversations with the user.

You have access to memory tools that allow you to remember important information:
- Use retrieveMemories at the beginning of the conversation to recall relevant context from prior conversations
- Use addMemories to store new important information such as:
  * User preferences
  * Personal details the user shares
  * Important decisions made
  * Tasks or follow-ups promised to the user

Before responding to complex questions, always check for relevant memories first.
When the user shares important information, make sure to store it for future reference.
```

## Example Conversation Flow

Here's how a typical conversation with memory might flow:

1. **User speaks**: "Hi, do you remember my favorite color?"

2. **Agent retrieves memories**:
   ```python
   # Agent calls retrieve_memories
   memories = retrieve_memories({"message": "user's favorite color"})
   # If found: "The user's favorite color is blue"
   ```

3. **Agent processes with context**:
   - If memories found: Prepares a personalized response
   - If no memories: Prepares to ask and store the information

4. **Agent responds**:
   - With memory: "Yes, your favorite color is blue!"
   - Without memory: "I don't think you've told me your favorite color before. What is it?"

5. **User responds**: "It's actually green."

6. **Agent stores new information**:
   ```python
   # Agent calls add_memories
   add_memories({"message": "The user's favorite color is green"})
   ```

7. **Agent confirms**: "Thanks, I'll remember that your favorite color is green."

## Example Use Cases

- **Personal Assistant** - Remember user preferences, past requests, and important dates
  ```
  User: "What restaurants did I say I liked last time?"
  Agent: *retrieves memories* "You mentioned enjoying Bella Italia and The Golden Dragon."
  ```

- **Customer Support** - Recall previous issues a customer has had
  ```
  User: "I'm having that same problem again!"
  Agent: *retrieves memories* "Is this related to the login issue you reported last week?"
  ```

- **Educational AI** - Track student progress and tailor teaching accordingly
  ```
  User: "Let's continue our math lesson."
  Agent: *retrieves memories* "Last time we were working on quadratic equations. Would you like to continue with that?"
  ```

- **Healthcare Assistant** - Remember symptoms, medications, and health concerns
  ```
  User: "Have I told you about my allergy medication?"
  Agent: *retrieves memories* "Yes, you mentioned you're taking Claritin for your pollen allergies."
  ```

## Troubleshooting

- **Missing API Keys**: 
  - Error: "API_KEY environment variable must be set"
  - Solution: Ensure all environment variables are set correctly in your .env file or system environment
  
- **Connection Issues**:
  - Error: "Failed to connect to API"
  - Solution: Check your network connection and API key permissions. Verify the API keys are valid and have the necessary permissions.
  
- **Empty Memory Results**:
  - Symptom: Agent always responds with "No memories found"
  - Solution: This is normal for new users. The memory database builds up over time as conversations occur. It's also possible your query isn't semantically similar to stored memories - try different phrasing.
  
- **Agent Not Using Memories**:
  - Symptom: The agent retrieves memories but doesn't incorporate them in responses
  - Solution: Update the agent's prompt to explicitly instruct it to use the retrieved memories in its responses

## Conclusion

By integrating ElevenLabs Conversational AI with Mem0, you can create voice agents that maintain context across conversations and provide personalized responses based on user history. This powerful combination enables:

- More natural, context-aware conversations
- Personalized user experiences that improve over time
- Reduced need for users to repeat information
- Long-term relationship building between users and AI agents

## Help

- For more details on ElevenLabs, visit the [ElevenLabs Conversational AI Documentation](https://elevenlabs.io/docs/api-reference/conversational-ai)
- For Mem0 documentation, refer to the [Mem0 Platform](https://app.mem0.ai/)
- If you need further assistance, please feel free to reach out to us through the following methods:

<Snippet file="get-help.mdx" /> 


================================================
FILE: docs/integrations/flowise.mdx
================================================
---
title: Flowise
---

The [**Mem0 Memory**](https://github.com/mem0ai/mem0) integration with [Flowise](https://github.com/FlowiseAI/Flowise) enables persistent memory capabilities for your AI chatflows. [Flowise](https://flowiseai.com/) is an open-source low-code tool for developers to build customized LLM orchestration flows & AI agents using a drag & drop interface.

## Overview

1. 🧠 Provides persistent memory storage for Flowise chatflows
2. 🔄 Seamless integration with existing Flowise templates
3. 🚀 Compatible with various LLM nodes in Flowise
4. 📝 Supports custom memory configurations
5. ⚡ Easy to set up and manage

## Prerequisites

Before setting up Mem0 with Flowise, ensure you have:

1. [Flowise installed](https://github.com/FlowiseAI/Flowise#⚡quick-start) (NodeJS >= 18.15.0 required):
```bash
npm install -g flowise
npx flowise start
```

2. Access to the Flowise UI at http://localhost:3000
3. Basic familiarity with [Flowise's LLM orchestration](https://flowiseai.com/#features) concepts

## Setup and Configuration

### 1. Set Up Flowise

1. Open the Flowise application and create a new canvas, or select a template from the Flowise marketplace.
2. In this example, we use the **Conversation Chain** template.
3. Replace the default **Buffer Memory** with **Mem0 Memory**.

![Flowise Memory Integration](https://raw.githubusercontent.com/FlowiseAI/FlowiseDocs/main/en/.gitbook/assets/mem0/flowise-flow.png)

### 2. Obtain Your Mem0 API Key

1. Navigate to the [Mem0 API Key dashboard](https://app.mem0.ai/dashboard/api-keys).
2. Generate or copy your existing Mem0 API Key.

![Mem0 API Key](https://raw.githubusercontent.com/FlowiseAI/FlowiseDocs/main/en/.gitbook/assets/mem0/api-key.png)

### 3. Configure Mem0 Credentials

1. Enter the **Mem0 API Key** in the Mem0 Credentials section.
2. Configure additional settings as needed:

```typescript
{
  "apiKey": "m0-xxx",
  "userId": "user-123",  // Optional: Specify user ID
  "projectId": "proj-xxx",  // Optional: Specify project ID
  "orgId": "org-xxx"  // Optional: Specify organization ID
}
```

<figure>
  <img src="https://raw.githubusercontent.com/FlowiseAI/FlowiseDocs/main/en/.gitbook/assets/mem0/creds.png" alt="Mem0 Credentials" />
  <figcaption>Configure API Credentials</figcaption>
</figure>

## Memory Features

### 1. Basic Memory Storage

Test your memory configuration:

1. Save your Flowise configuration
2. Run a test chat and store some information
3. Verify the stored memories in the [Mem0 Dashboard](https://app.mem0.ai/dashboard/requests)

![Flowise Test Chat](https://raw.githubusercontent.com/FlowiseAI/FlowiseDocs/main/en/.gitbook/assets/mem0/flowise-chat-1.png)

### 2. Memory Retention

Validate memory persistence:

1. Clear the chat history in Flowise
2. Ask a question about previously stored information
3. Confirm that the AI remembers the context

![Testing Memory Retention](https://raw.githubusercontent.com/FlowiseAI/FlowiseDocs/main/en/.gitbook/assets/mem0/flowise-chat-2.png)

## Advanced Configuration

### Memory Settings

![Mem0 Settings](https://raw.githubusercontent.com/FlowiseAI/FlowiseDocs/main/en/.gitbook/assets/mem0/settings.png)

Available settings include:

1. **Search Only Mode**: Enable memory retrieval without creating new memories
2. **Mem0 Entities**: Configure identifiers:
   - `user_id`: Unique identifier for each user
   - `run_id`: Specific conversation session ID
   - `app_id`: Application identifier
   - `agent_id`: AI agent identifier
3. **Project ID**: Assign memories to specific projects
4. **Organization ID**: Organize memories by organization

### Platform Configuration

Additional settings available in [Mem0 Project Settings](https://app.mem0.ai/dashboard/project-settings):

1. **Custom Instructions**: Define memory extraction rules
2. **Expiration Date**: Set automatic memory cleanup periods

![Mem0 Project Settings](https://raw.githubusercontent.com/FlowiseAI/FlowiseDocs/main/en/.gitbook/assets/mem0/mem0-settings.png)

## Best Practices

1. **User Identification**: Use consistent `user_id` values for reliable memory retrieval
2. **Memory Organization**: Utilize projects and organizations for better memory management
3. **Regular Maintenance**: Monitor and clean up unused memories periodically

## Help & Resources

- [Flowise Documentation](https://flowiseai.com/docs)
- [Flowise GitHub Repository](https://github.com/FlowiseAI/Flowise)
- [Flowise Website](https://flowiseai.com/)
- [Mem0 Platform](https://app.mem0.ai/)
- Need assistance? Reach out through:

<Snippet file="get-help.mdx" /> 


================================================
FILE: docs/integrations/google-ai-adk.mdx
================================================
---
title: Google Agent Development Kit
---

Integrate [**Mem0**](https://github.com/mem0ai/mem0) with [Google Agent Development Kit (ADK)](https://github.com/google/adk-python), an open-source framework for building multi-agent workflows. This integration enables agents to access persistent memory across conversations, enhancing context retention and personalization.

## Overview

1. Store and retrieve memories from Mem0 within Google ADK agents
2. Multi-agent workflows with shared memory across hierarchies
3. Retrieve relevant memories from past conversations
4. Personalized responses

## Prerequisites

Before setting up Mem0 with Google ADK, ensure you have:

1. Installed the required packages:
```bash
pip install google-adk mem0ai python-dotenv
```

2. Valid API keys:
   - [Mem0 API Key](https://app.mem0.ai/dashboard/api-keys)
   - Google AI Studio API Key

## Basic Integration Example

The following example demonstrates how to create a Google ADK agent with Mem0 memory integration:

```python
import os
import asyncio
from google.adk.agents import Agent
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from mem0 import MemoryClient
from dotenv import load_dotenv

load_dotenv()

# Set up environment variables
# os.environ["GOOGLE_API_KEY"] = "your-google-api-key"
# os.environ["MEM0_API_KEY"] = "your-mem0-api-key"

# Initialize Mem0 client
mem0 = MemoryClient()

# Define memory function tools
def search_memory(query: str, user_id: str) -> dict:
    """Search through past conversations and memories"""
    memories = mem0.search(query, user_id=user_id, output_format='v1.1')
    if memories.get('results', []):
        memory_list = memories['results']
        memory_context = "\n".join([f"- {mem['memory']}" for mem in memory_list])
        return {"status": "success", "memories": memory_context}
    return {"status": "no_memories", "message": "No relevant memories found"}

def save_memory(content: str, user_id: str) -> dict:
    """Save important information to memory"""
    try:
        result = mem0.add([{"role": "user", "content": content}], user_id=user_id, output_format='v1.1')
        return {"status": "success", "message": "Information saved to memory", "result": result}
    except Exception as e:
        return {"status": "error", "message": f"Failed to save memory: {str(e)}"}

# Create agent with memory capabilities
personal_assistant = Agent(
    name="personal_assistant",
    model="gemini-2.0-flash",
    instruction="""You are a helpful personal assistant with memory capabilities.
    Use the search_memory function to recall past conversations and user preferences.
    Use the save_memory function to store important information about the user.
    Always personalize your responses based on available memory.""",
    description="A personal assistant that remembers user preferences and past interactions",
    tools=[search_memory, save_memory]
)

async def chat_with_agent(user_input: str, user_id: str) -> str:
    """
    Handle user input with automatic memory integration.

    Args:
        user_input: The user's message
        user_id: Unique identifier for the user

    Returns:
        The agent's response
    """
    # Set up session and runner
    session_service = InMemorySessionService()
    session = await session_service.create_session(
        app_name="memory_assistant",
        user_id=user_id,
        session_id=f"session_{user_id}"
    )
    runner = Runner(agent=personal_assistant, app_name="memory_assistant", session_service=session_service)

    # Create content and run agent
    content = types.Content(role='user', parts=[types.Part(text=user_input)])
    events = runner.run(user_id=user_id, session_id=session.id, new_message=content)

    # Extract final response
    for event in events:
        if event.is_final_response():
            response = event.content.parts[0].text

            return response

    return "No response generated"

# Example usage
if __name__ == "__main__":
    response = asyncio.run(chat_with_agent(
        "I love Italian food and I'm planning a trip to Rome next month",
        user_id="alice"
    ))
    print(response)
```

## Multi-Agent Hierarchy with Shared Memory

Create specialized agents in a hierarchy that share memory:

```python
from google.adk.tools.agent_tool import AgentTool

# Travel specialist agent
travel_agent = Agent(
    name="travel_specialist",
    model="gemini-2.0-flash",
    instruction="""You are a travel planning specialist. Use get_user_context to
    understand the user's travel preferences and history before making recommendations.
    After providing advice, use store_interaction to save travel-related information.""",
    description="Specialist in travel planning and recommendations",
    tools=[search_memory, save_memory]
)

# Health advisor agent
health_agent = Agent(
    name="health_advisor",
    model="gemini-2.0-flash",
    instruction="""You are a health and wellness advisor. Use get_user_context to
    understand the user's health goals and dietary preferences.
    After providing advice, use store_interaction to save health-related information.""",
    description="Specialist in health and wellness advice",
    tools=[search_memory, save_memory]
)

# Coordinator agent that delegates to specialists
coordinator_agent = Agent(
    name="coordinator",
    model="gemini-2.0-flash",
    instruction="""You are a coordinator that delegates requests to specialist agents.
    For travel-related questions (trips, hotels, flights, destinations), delegate to the travel specialist.
    For health-related questions (fitness, diet, wellness, exercise), delegate to the health advisor.
    Use get_user_context to understand the user before delegation.""",
    description="Coordinates requests between specialist agents",
    tools=[
        AgentTool(agent=travel_agent, skip_summarization=False),
        AgentTool(agent=health_agent, skip_summarization=False)
    ]
)

def chat_with_specialists(user_input: str, user_id: str) -> str:
    """
    Handle user input with specialist agent delegation and memory.

    Args:
        user_input: The user's message
        user_id: Unique identifier for the user

    Returns:
        The specialist agent's response
    """
    session_service = InMemorySessionService()
    session = session_service.create_session(
        app_name="specialist_system",
        user_id=user_id,
        session_id=f"session_{user_id}"
    )
    runner = Runner(agent=coordinator_agent, app_name="specialist_system", session_service=session_service)

    content = types.Content(role='user', parts=[types.Part(text=user_input)])
    events = runner.run(user_id=user_id, session_id=session.id, new_message=content)

    for event in events:
        if event.is_final_response():
            response = event.content.parts[0].text

            # Store the conversation in shared memory
            conversation = [
                {"role": "user", "content": user_input},
                {"role": "assistant", "content": response}
            ]
            mem0.add(conversation, user_id=user_id)

            return response

    return "No response generated"

# Example usage
response = chat_with_specialists("Plan a healthy meal for my Italy trip", user_id="alice")
print(response)
```



## Quick Start Chat Interface

Simple interactive chat with memory and Google ADK:

```python
def interactive_chat():
    """Interactive chat interface with memory and ADK"""
    user_id = input("Enter your user ID: ") or "demo_user"
    print(f"Chat started for user: {user_id}")
    print("Type 'quit' to exit")
    print("=" * 50)

    while True:
        user_input = input("\nYou: ")

        if user_input.lower() == 'quit':
            print("Goodbye! Your conversation has been saved to memory.")
            break
        else:
            response = chat_with_specialists(user_input, user_id)
            print(f"Assistant: {response}")

if __name__ == "__main__":
    interactive_chat()
```

## Key Features

### 1. Memory-Enhanced Function Tools
- **Function Tools**: Standard Python functions that can search and save memories
- **Tool Context**: Access to session state and memory through function parameters
- **Structured Returns**: Dictionary-based returns with status indicators for better LLM understanding

### 2. Multi-Agent Memory Sharing
- **Agent-as-a-Tool**: Specialists can be called as tools while maintaining shared memory
- **Hierarchical Delegation**: Coordinator agents route to specialists based on context
- **Memory Categories**: Store interactions with metadata for better organization

### 3. Flexible Memory Operations
- **Search Capabilities**: Retrieve relevant memories through conversation history
- **User Segmentation**: Organize memories by user ID
- **Memory Management**: Built-in tools for saving and retrieving information

## Configuration Options

Customize memory behavior and agent setup:

```python
# Configure memory search with metadata
memories = mem0.search(
    query="travel preferences",
    user_id="alice",
    limit=5,
    filters={"category": "travel"}  # Filter by category if supported
)

# Configure agent with custom model settings
agent = Agent(
    name="custom_agent",
    model="gemini-2.0-flash",  # or use LiteLLM for other models
    instruction="Custom agent behavior",
    tools=[memory_tools],
    # Additional ADK configurations
)

# Use Google Cloud Vertex AI instead of AI Studio
os.environ["GOOGLE_GENAI_USE_VERTEXAI"] = "True"
os.environ["GOOGLE_CLOUD_PROJECT"] = "your-project-id"
os.environ["GOOGLE_CLOUD_LOCATION"] = "us-central1"
```

## Help

- [Google ADK Documentation](https://google.github.io/adk-docs/)
- [Mem0 Platform](https://app.mem0.ai/)
- If you need further assistance, please feel free to reach out to us through the following methods:

<Snippet file="get-help.mdx" />


================================================
FILE: docs/integrations/keywords.mdx
================================================
---
title: Keywords AI
---

Build AI applications with persistent memory and comprehensive LLM observability by integrating Mem0 with Keywords AI.

## Overview

Mem0 is a self-improving memory layer for LLM applications, enabling personalized AI experiences that save costs and delight users. Keywords AI provides complete LLM observability.

Combining Mem0 with Keywords AI allows you to:
1. Add persistent memory to your AI applications
2. Track interactions across sessions
3. Monitor memory usage and retrieval with Keywords AI observability
4. Optimize token usage and reduce costs

<Note>
You can get your Mem0 API key, user_id, and org_id from the [Mem0 dashboard](https://app.mem0.ai/). These are required for proper integration.
</Note>

## Setup and Configuration

Install the necessary libraries:

```bash
pip install mem0 keywordsai-sdk
```

Set up your environment variables:

```python
import os

# Set your API keys
os.environ["MEM0_API_KEY"] = "your-mem0-api-key"
os.environ["KEYWORDSAI_API_KEY"] = "your-keywords-api-key"
os.environ["KEYWORDSAI_BASE_URL"] = "https://api.keywordsai.co/api/"
```

## Basic Integration Example

Here's a simple example of using Mem0 with Keywords AI:

```python
from mem0 import Memory
import os

# Configuration
api_key = os.getenv("MEM0_API_KEY")
keywordsai_api_key = os.getenv("KEYWORDSAI_API_KEY")
base_url = os.getenv("KEYWORDSAI_BASE_URL") # "https://api.keywordsai.co/api/"

# Set up Mem0 with Keywords AI as the LLM provider
config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o-mini",
            "temperature": 0.0,
            "api_key": keywordsai_api_key,
            "openai_base_url": base_url,
        },
    }
}

# Initialize Memory
memory = Memory.from_config(config_dict=config)

# Add a memory
result = memory.add(
    "I like to take long walks on weekends.",
    user_id="alice",
    metadata={"category": "hobbies"},
)

print(result)
```

## Advanced Integration with OpenAI SDK

For more advanced use cases, you can integrate Keywords AI with Mem0 through the OpenAI SDK:

```python
from openai import OpenAI
import os
import json

# Initialize client
client = OpenAI(
    api_key=os.environ.get("KEYWORDSAI_API_KEY"),
    base_url=os.environ.get("KEYWORDSAI_BASE_URL"),
)

# Sample conversation messages
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]

# Add memory and generate a response
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=messages,
    extra_body={
        "mem0_params": {
            "user_id": "test_user",
            "org_id": "org_1",
            "api_key": os.environ.get("MEM0_API_KEY"),
            "add_memories": {
                "messages": messages,
            },
        }
    },
)

print(json.dumps(response.model_dump(), indent=4))
```

For detailed information on this integration, refer to the official [Keywords AI Mem0 integration documentation](https://docs.keywordsai.co/integration/development-frameworks/mem0).

## Key Features

1. **Memory Integration**: Store and retrieve relevant information from past interactions
2. **LLM Observability**: Track memory usage and retrieval patterns with Keywords AI
3. **Session Persistence**: Maintain context across multiple user sessions
4. **Cost Optimization**: Reduce token usage through efficient memory retrieval

## Conclusion

Integrating Mem0 with Keywords AI provides a powerful combination for building AI applications with persistent memory and comprehensive observability. This integration enables more personalized user experiences while providing insights into your application's memory usage.

## Help

For more information, refer to:
- [Keywords AI Documentation](https://docs.keywordsai.co)
- [Mem0 Platform]((https://app.mem0.ai/))

<Snippet file="get-help.mdx" />



================================================
FILE: docs/integrations/langchain-tools.mdx
================================================
---
title: Langchain Tools
description: 'Integrate Mem0 with LangChain tools to enable AI agents to store, search, and manage memories through structured interfaces'
---

## Overview

Mem0 provides a suite of tools for storing, searching, and retrieving memories, enabling agents to maintain context and learn from past interactions. The tools are built as Langchain tools, making them easily integrable with any AI agent implementation.

## Installation

Install the required dependencies:

```bash
pip install langchain_core
pip install mem0ai
```

## Authentication

Import the necessary dependencies and initialize the client:

```python
from langchain_core.tools import StructuredTool
from mem0 import MemoryClient
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import os

os.environ["MEM0_API_KEY"] = "your-api-key"

client = MemoryClient(
    org_id=your_org_id,
    project_id=your_project_id
)
```

## Available Tools

Mem0 provides three main tools for memory management:

### 1. ADD Memory Tool

The ADD tool allows you to store new memories with associated metadata. It's particularly useful for saving conversation history and user preferences.

#### Schema

```python
class Message(BaseModel):
    role: str = Field(description="Role of the message sender (user or assistant)")
    content: str = Field(description="Content of the message")

class AddMemoryInput(BaseModel):
    messages: List[Message] = Field(description="List of messages to add to memory")
    user_id: str = Field(description="ID of the user associated with these messages")
    output_format: str = Field(description="Version format for the output")
    metadata: Optional[Dict[str, Any]] = Field(description="Additional metadata for the messages", default=None)

    class Config:
        json_schema_extra = {
            "examples": [{
                "messages": [
                    {"role": "user", "content": "Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts."},
                    {"role": "assistant", "content": "Hello Alex! I've noted that you're a vegetarian and have a nut allergy."}
                ],
                "user_id": "alex",
                "output_format": "v1.1",
                "metadata": {"food": "vegan"}
            }]
        }
```

#### Implementation

```python
def add_memory(messages: List[Message], user_id: str, output_format: str, metadata: Optional[Dict[str, Any]] = None) -> Any:
    """Add messages to memory with associated user ID and metadata."""
    message_dicts = [msg.dict() for msg in messages]
    return client.add(message_dicts, user_id=user_id, output_format=output_format, metadata=metadata)

add_tool = StructuredTool(
    name="add_memory",
    description="Add new messages to memory with associated metadata",
    func=add_memory,
    args_schema=AddMemoryInput
)
```

#### Example Usage

<CodeGroup>
```python Code
add_input = {
    "messages": [
        {"role": "user", "content": "Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts."},
        {"role": "assistant", "content": "Hello Alex! I've noted that you're a vegetarian and have a nut allergy."}
    ],
    "user_id": "alex",
    "output_format": "v1.1",
    "metadata": {"food": "vegan"}
}
add_result = add_tool.invoke(add_input)
```

```json Output
{
  "results": [
    {
      "memory": "Name is Alex",
      "event": "ADD"
    },
    {
      "memory": "Is a vegetarian", 
      "event": "ADD"
    },
    {
      "memory": "Is allergic to nuts",
      "event": "ADD"
    }
  ]
}
```
</CodeGroup>

### 2. SEARCH Memory Tool

The SEARCH tool enables querying stored memories using natural language queries and advanced filtering options.

#### Schema

```python
class SearchMemoryInput(BaseModel):
    query: str = Field(description="The search query string")
    filters: Dict[str, Any] = Field(description="Filters to apply to the search")
    version: str = Field(description="Version of the memory to search")

    class Config:
        json_schema_extra = {
            "examples": [{
                "query": "tell me about my allergies?",
                "filters": {
                    "AND": [
                        {"user_id": "alex"},
                        {"created_at": {"gte": "2024-01-01", "lte": "2024-12-31"}}
                    ]
                },
                "version": "v2"
            }]
        }
```

#### Implementation

```python
def search_memory(query: str, filters: Dict[str, Any], version: str) -> Any:
    """Search memory with the given query and filters."""
    return client.search(query=query, version=version, filters=filters)

search_tool = StructuredTool(
    name="search_memory",
    description="Search through memories with a query and filters",
    func=search_memory,
    args_schema=SearchMemoryInput
)
```

#### Example Usage

<CodeGroup>
```python Code
search_input = {
    "query": "what is my name?",
    "filters": {
        "AND": [
            {"created_at": {"gte": "2024-07-20", "lte": "2024-12-10"}},
            {"user_id": "alex"}
        ]
    },
    "version": "v2"
}
result = search_tool.invoke(search_input)
```

```json Output
[
  {
    "id": "1a75e827-7eca-45ea-8c5c-cfd43299f061",
    "memory": "Name is Alex",
    "user_id": "alex", 
    "hash": "d0fccc8fa47f7a149ee95750c37bb0ca",
    "metadata": {
      "food": "vegan"
    },
    "categories": [
      "personal_details"
    ],
    "created_at": "2024-11-27T16:53:43.276872-08:00",
    "updated_at": "2024-11-27T16:53:43.276885-08:00",
    "score": 0.3810526501504994
  }
]
```
</CodeGroup>

### 3. GET_ALL Memory Tool

The GET_ALL tool retrieves all memories matching specified criteria, with support for pagination.

#### Schema

```python
class GetAllMemoryInput(BaseModel):
    version: str = Field(description="Version of the memory to retrieve")
    filters: Dict[str, Any] = Field(description="Filters to apply to the retrieval")
    page: Optional[int] = Field(description="Page number for pagination", default=1)
    page_size: Optional[int] = Field(description="Number of items per page", default=50)

    class Config:
        json_schema_extra = {
            "examples": [{
                "version": "v2",
                "filters": {
                    "AND": [
                        {"user_id": "alex"},
                        {"created_at": {"gte": "2024-07-01", "lte": "2024-07-31"}},
                        {"categories": {"contains": "food_preferences"}}
                    ]
                },
                "page": 1,
                "page_size": 50
            }]
        }
```

#### Implementation

```python
def get_all_memory(version: str, filters: Dict[str, Any], page: int = 1, page_size: int = 50) -> Any:
    """Retrieve all memories matching the specified criteria."""
    return client.get_all(version=version, filters=filters, page=page, page_size=page_size)

get_all_tool = StructuredTool(
    name="get_all_memory",
    description="Retrieve all memories matching specified filters",
    func=get_all_memory,
    args_schema=GetAllMemoryInput
)
```

#### Example Usage

<CodeGroup>
```python Code
get_all_input = {
    "version": "v2",
    "filters": {
        "AND": [
            {"user_id": "alex"},
            {"created_at": {"gte": "2024-07-01", "lte": "2024-12-31"}}
        ]
    },
    "page": 1,
    "page_size": 50
}
get_all_result = get_all_tool.invoke(get_all_input)
```

```json Output
{
  "count": 3,
  "next": null,
  "previous": null,
  "results": [
    {
      "id": "1a75e827-7eca-45ea-8c5c-cfd43299f061",
      "memory": "Name is Alex",
      "user_id": "alex", 
      "hash": "d0fccc8fa47f7a149ee95750c37bb0ca",
      "metadata": {
        "food": "vegan"
      },
      "categories": [
        "personal_details"
      ],
      "created_at": "2024-11-27T16:53:43.276872-08:00",
      "updated_at": "2024-11-27T16:53:43.276885-08:00"
    },
    {
      "id": "91509588-0b39-408a-8df3-84b3bce8c521",
      "memory": "Is a vegetarian",
      "user_id": "alex",
      "hash": "ce6b1c84586772ab9995a9477032df99", 
      "metadata": {
        "food": "vegan"
      },
      "categories": [
        "user_preferences",
        "food"
      ],
      "created_at": "2024-11-27T16:53:43.308027-08:00",
      "updated_at": "2024-11-27T16:53:43.308037-08:00"
    },
    {
      "id": "8d74f7a0-6107-4589-bd6f-210f6bf4fbbb",
      "memory": "Is allergic to nuts",
      "user_id": "alex",
      "hash": "7873cd0e5a29c513253d9fad038e758b",
      "metadata": {
        "food": "vegan"
      },
      "categories": [
        "health"
      ],
      "created_at": "2024-11-27T16:53:43.337253-08:00",
      "updated_at": "2024-11-27T16:53:43.337262-08:00"
    }
  ]
}
```
</CodeGroup>

## Integration with AI Agents

All tools are implemented as Langchain `StructuredTool` instances, making them compatible with any AI agent that supports the Langchain tools interface. To use these tools with your agent:

1. Initialize the tools as shown above
2. Add the tools to your agent's toolset
3. The agent can now use these tools to manage memories through natural language interactions

Each tool provides structured input validation through Pydantic models and returns consistent responses that can be processed by your agent.

## Help

In case of any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/integrations/langchain.mdx
================================================
---
title: Langchain
---

Build a personalized Travel Agent AI using LangChain for conversation flow and Mem0 for memory retention. This integration enables context-aware and efficient travel planning experiences.

## Overview

In this guide, we'll create a Travel Agent AI that:
1. Uses LangChain to manage conversation flow
2. Leverages Mem0 to store and retrieve relevant information from past interactions
3. Provides personalized travel recommendations based on user history

## Setup and Configuration

Install necessary libraries:

```bash
pip install langchain langchain_openai mem0ai python-dotenv
```

Import required modules and set up configurations:

<Note>Remember to get the Mem0 API key from [Mem0 Platform](https://app.mem0.ai).</Note>

```python
import os
from typing import List, Dict
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from mem0 import MemoryClient
from dotenv import load_dotenv

load_dotenv()

# Configuration
# os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
# os.environ["MEM0_API_KEY"] = "your-mem0-api-key"

# Initialize LangChain and Mem0
llm = ChatOpenAI(model="gpt-4o-mini")
mem0 = MemoryClient()
```

## Create Prompt Template

Set up the conversation prompt template:

```python
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="""You are a helpful travel agent AI. Use the provided context to personalize your responses and remember user preferences and past interactions. 
    Provide travel recommendations, itinerary suggestions, and answer questions about destinations. 
    If you don't have specific information, you can make general suggestions based on common travel knowledge."""),
    MessagesPlaceholder(variable_name="context"),
    HumanMessage(content="{input}")
])
```

## Define Helper Functions

Create functions to handle context retrieval, response generation, and addition to Mem0:

```python
def retrieve_context(query: str, user_id: str) -> List[Dict]:
    """Retrieve relevant context from Mem0"""
    try:
        memories = mem0.search(query, user_id=user_id, output_format='v1.1')
        memory_list = memories['results']
        
        serialized_memories = ' '.join([mem["memory"] for mem in memory_list])
        context = [
            {
                "role": "system", 
                "content": f"Relevant information: {serialized_memories}"
            },
            {
                "role": "user",
                "content": query
            }
        ]
        return context
    except Exception as e:
        print(f"Error retrieving memories: {e}")
        # Return empty context if there's an error
        return [{"role": "user", "content": query}]

def generate_response(input: str, context: List[Dict]) -> str:
    """Generate a response using the language model"""
    chain = prompt | llm
    response = chain.invoke({
        "context": context,
        "input": input
    })
    return response.content

def save_interaction(user_id: str, user_input: str, assistant_response: str):
    """Save the interaction to Mem0"""
    try:
        interaction = [
            {
              "role": "user",
              "content": user_input
            },
            {
                "role": "assistant",
                "content": assistant_response
            }
        ]
        result = mem0.add(interaction, user_id=user_id, output_format='v1.1')
        print(f"Memory saved successfully: {len(result.get('results', []))} memories added")
    except Exception as e:
        print(f"Error saving interaction: {e}")
```

## Create Chat Turn Function

Implement the main function to manage a single turn of conversation:

```python
def chat_turn(user_input: str, user_id: str) -> str:
    # Retrieve context
    context = retrieve_context(user_input, user_id)
    
    # Generate response
    response = generate_response(user_input, context)
    
    # Save interaction
    save_interaction(user_id, user_input, response)
    
    return response
```

## Main Interaction Loop

Set up the main program loop for user interaction:

```python
if __name__ == "__main__":
    print("Welcome to your personal Travel Agent Planner! How can I assist you with your travel plans today?")
    user_id = "alice"
    
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit', 'bye']:
            print("Travel Agent: Thank you for using our travel planning service. Have a great trip!")
            break
        
        response = chat_turn(user_input, user_id)
        print(f"Travel Agent: {response}")
```

## Key Features

1. **Memory Integration**: Uses Mem0 to store and retrieve relevant information from past interactions.
2. **Personalization**: Provides context-aware responses based on user history and preferences.
3. **Flexible Architecture**: LangChain structure allows for easy expansion of the conversation flow.
4. **Continuous Learning**: Each interaction is stored, improving future responses.

## Conclusion

By integrating LangChain with Mem0, you can build a personalized Travel Agent AI that can maintain context across interactions and provide tailored travel recommendations and assistance.

## Help

- For more details on LangChain, visit the [LangChain documentation](https://python.langchain.com/).
- [Mem0 Platform](https://app.mem0.ai/).
- If you need further assistance, please feel free to reach out to us through the following methods:

<Snippet file="get-help.mdx" />




================================================
FILE: docs/integrations/langgraph.mdx
================================================
---
title: LangGraph
---

Build a personalized Customer Support AI Agent using LangGraph for conversation flow and Mem0 for memory retention. This integration enables context-aware and efficient support experiences.

## Overview

In this guide, we'll create a Customer Support AI Agent that:
1. Uses LangGraph to manage conversation flow
2. Leverages Mem0 to store and retrieve relevant information from past interactions
3. Provides personalized responses based on user history

## Setup and Configuration

Install necessary libraries:

```bash
pip install langgraph langchain-openai mem0ai python-dotenv
```


Import required modules and set up configurations:

<Note>Remember to get the Mem0 API key from [Mem0 Platform](https://app.mem0.ai).</Note>

```python
from typing import Annotated, TypedDict, List
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from mem0 import MemoryClient
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from dotenv import load_dotenv

load_dotenv()

# Configuration
# OPENAI_API_KEY = 'sk-xxx'  # Replace with your actual OpenAI API key
# MEM0_API_KEY = 'your-mem0-key'  # Replace with your actual Mem0 API key

# Initialize LangChain and Mem0
llm = ChatOpenAI(model="gpt-4")
mem0 = MemoryClient()
```

## Define State and Graph

Set up the conversation state and LangGraph structure:

```python
class State(TypedDict):
    messages: Annotated[List[HumanMessage | AIMessage], add_messages]
    mem0_user_id: str

graph = StateGraph(State)
```

## Create Chatbot Function

Define the core logic for the Customer Support AI Agent:

```python
def chatbot(state: State):
    messages = state["messages"]
    user_id = state["mem0_user_id"]

    try:
        # Retrieve relevant memories
        memories = mem0.search(messages[-1].content, user_id=user_id, output_format='v1.1')
        
        # Handle dict response format
        memory_list = memories['results']

        context = "Relevant information from previous conversations:\n"
        for memory in memory_list:
            context += f"- {memory['memory']}\n"

        system_message = SystemMessage(content=f"""You are a helpful customer support assistant. Use the provided context to personalize your responses and remember user preferences and past interactions.
{context}""")

        full_messages = [system_message] + messages
        response = llm.invoke(full_messages)

        # Store the interaction in Mem0
        try:
            interaction = [
                {
                    "role": "user",
                    "content": messages[-1].content
                },
                {
                    "role": "assistant", 
                    "content": response.content
                }
            ]
            result = mem0.add(interaction, user_id=user_id, output_format='v1.1')
            print(f"Memory saved: {len(result.get('results', []))} memories added")
        except Exception as e:
            print(f"Error saving memory: {e}")
            
        return {"messages": [response]}
        
    except Exception as e:
        print(f"Error in chatbot: {e}")
        # Fallback response without memory context
        response = llm.invoke(messages)
        return {"messages": [response]}
```

## Set Up Graph Structure

Configure the LangGraph with appropriate nodes and edges:

```python
graph.add_node("chatbot", chatbot)
graph.add_edge(START, "chatbot")
graph.add_edge("chatbot", "chatbot")

compiled_graph = graph.compile()
```

## Create Conversation Runner

Implement a function to manage the conversation flow:

```python
def run_conversation(user_input: str, mem0_user_id: str):
    config = {"configurable": {"thread_id": mem0_user_id}}
    state = {"messages": [HumanMessage(content=user_input)], "mem0_user_id": mem0_user_id}

    for event in compiled_graph.stream(state, config):
        for value in event.values():
            if value.get("messages"):
                print("Customer Support:", value["messages"][-1].content)
                return
```

## Main Interaction Loop

Set up the main program loop for user interaction:

```python
if __name__ == "__main__":
    print("Welcome to Customer Support! How can I assist you today?")
    mem0_user_id = "alice"  # You can generate or retrieve this based on your user management system
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit', 'bye']:
            print("Customer Support: Thank you for contacting us. Have a great day!")
            break
        run_conversation(user_input, mem0_user_id)
```

## Key Features

1. **Memory Integration**: Uses Mem0 to store and retrieve relevant information from past interactions.
2. **Personalization**: Provides context-aware responses based on user history.
3. **Flexible Architecture**: LangGraph structure allows for easy expansion of the conversation flow.
4. **Continuous Learning**: Each interaction is stored, improving future responses.

## Conclusion

By integrating LangGraph with Mem0, you can build a personalized Customer Support AI Agent that can maintain context across interactions and provide personalized assistance.

## Help

- For more details on LangGraph, visit the [LangChain documentation](https://python.langchain.com/docs/langgraph).
- [Mem0 Platform](https://app.mem0.ai/).
- If you need further assistance, please feel free to reach out to us through following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/integrations/livekit.mdx
================================================
---
title: Livekit
---

This guide demonstrates how to create a memory-enabled voice assistant using LiveKit, Deepgram, OpenAI, and Mem0, focusing on creating an intelligent, context-aware travel planning agent.

## Prerequisites

Before you begin, make sure you have:

1. Installed Livekit Agents SDK with voice dependencies of silero and deepgram:
```bash
pip install livekit livekit-agents \
livekit-plugins-silero \
livekit-plugins-deepgram \
livekit-plugins-openai \
livekit-plugins-turn-detector \
livekit-plugins-noise-cancellation
```

2. Installed Mem0 SDK:
```bash
pip install mem0ai
```

3. Set up your API keys in a `.env` file:
```sh
LIVEKIT_URL=your_livekit_url
LIVEKIT_API_KEY=your_livekit_api_key
LIVEKIT_API_SECRET=your_livekit_api_secret
DEEPGRAM_API_KEY=your_deepgram_api_key
MEM0_API_KEY=your_mem0_api_key
OPENAI_API_KEY=your_openai_api_key
```

> **Note**: Make sure to have a Livekit and Deepgram account. You can find these variables `LIVEKIT_URL` , `LIVEKIT_API_KEY` and `LIVEKIT_API_SECRET` from [LiveKit Cloud Console](https://cloud.livekit.io/) and for more information you can refer this website [LiveKit Documentation](https://docs.livekit.io/home/cloud/keys-and-tokens/). For `DEEPGRAM_API_KEY` you can get from [Deepgram Console](https://console.deepgram.com/) refer this website [Deepgram Documentation](https://developers.deepgram.com/docs/create-additional-api-keys) for more details.

## Code Breakdown

Let's break down the key components of this implementation using LiveKit Agents:

### 1. Setting Up Dependencies and Environment

```python
import os
import logging
from pathlib import Path
from dotenv import load_dotenv

from mem0 import AsyncMemoryClient

from livekit.agents import (
    JobContext,
    WorkerOptions,
    cli,
    ChatContext,
    ChatMessage,
    RoomInputOptions,
    Agent,
    AgentSession,
)
from livekit.plugins import openai, silero, deepgram, noise_cancellation
from livekit.plugins.turn_detector.english import EnglishModel

# Load environment variables
load_dotenv()

```

### 2. Mem0 Client and Agent Definition

```python
# User ID for RAG data in Mem0
RAG_USER_ID = "livekit-mem0"
mem0_client = AsyncMemoryClient()

class MemoryEnabledAgent(Agent):
    """
    An agent that can answer questions using RAG (Retrieval Augmented Generation) with Mem0.
    """
    def __init__(self) -> None:
        super().__init__(
            instructions="""
                You are a helpful voice assistant.
                You are a travel guide named George and will help the user to plan a travel trip of their dreams.
                You should help the user plan for various adventures like work retreats, family vacations or solo backpacking trips.
                You should be careful to not suggest anything that would be dangerous, illegal or inappropriate.
                You can remember past interactions and use them to inform your answers.
                Use semantic memory retrieval to provide contextually relevant responses.
            """,
        )
        self._seen_results = set()  # Track previously seen result IDs
        logger.info(f"Mem0 Agent initialized. Using user_id: {RAG_USER_ID}")

    async def on_enter(self):
        self.session.generate_reply(
            instructions="Briefly greet the user and offer your assistance."
        )

    async def on_user_turn_completed(self, turn_ctx: ChatContext, new_message: ChatMessage) -> None:
        # Persist the user message in Mem0
        try:
            logger.info(f"Adding user message to Mem0: {new_message.text_content}")
            add_result = await mem0_client.add(
                [{"role": "user", "content": new_message.text_content}],
                user_id=RAG_USER_ID
            )
            logger.info(f"Mem0 add result (user): {add_result}")
        except Exception as e:
            logger.warning(f"Failed to store user message in Mem0: {e}")

        # RAG: Retrieve relevant context from Mem0 and inject as assistant message
        try:
            logger.info("About to await mem0_client.search for RAG context")
            search_results = await mem0_client.search(
                new_message.text_content,
                user_id=RAG_USER_ID,
            )
            logger.info(f"mem0_client.search returned: {search_results}")
            if search_results and search_results.get('results', []):
                context_parts = []
                for result in search_results.get('results', []):
                    paragraph = result.get("memory") or result.get("text")
                    if paragraph:
                        source = "mem0 Memories"
                        if "from [" in paragraph:
                            source = paragraph.split("from [")[1].split("]")[0]
                            paragraph = paragraph.split("]")[1].strip()
                        context_parts.append(f"Source: {source}\nContent: {paragraph}\n")
                if context_parts:
                    full_context = "\n\n".join(context_parts)
                    logger.info(f"Injecting RAG context: {full_context}")
                    turn_ctx.add_message(role="assistant", content=full_context)
                    await self.update_chat_ctx(turn_ctx)
        except Exception as e:
            logger.warning(f"Failed to inject RAG context from Mem0: {e}")

        await super().on_user_turn_completed(turn_ctx, new_message)
```

### 3. Entrypoint and Session Setup

```python
async def entrypoint(ctx: JobContext):
    """Main entrypoint for the agent."""
    await ctx.connect()

    session = AgentSession(
        stt=deepgram.STT(),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=openai.TTS(voice="ash",),
        turn_detection=EnglishModel(),
        vad=silero.VAD.load(),
    )

    await session.start(
        agent=MemoryEnabledAgent(),
        room=ctx.room,
        room_input_options=RoomInputOptions(
            noise_cancellation=noise_cancellation.BVC(),
        ),
    )

    # Initial greeting
    await session.generate_reply(
        instructions="Greet the user warmly as George the travel guide and ask how you can help them plan their next adventure.",
        allow_interruptions=True
    )

# Run the application
if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

## Key Features of This Implementation

1. **Semantic Memory Retrieval**: Uses Mem0 to store and retrieve contextually relevant memories
2. **Voice Interaction**: Leverages LiveKit for voice communication with proper turn detection
3. **Intelligent Context Management**: Augments conversations with past interactions
4. **Travel Planning Specialization**: Focused on creating a helpful travel guide assistant
5. **Function Tools**: Modern tool definition for enhanced capabilities

## Running the Example

To run this example:

1. Install all required dependencies
2. Set up your `.env` file with the necessary API keys
3. Ensure your microphone and audio setup are configured
4. Run the script with Python 3.11 or newer and with the following command:
```sh
python mem0-livekit-voice-agent.py start
```
or to start your agent in console mode to run inside your terminal:

```sh
python mem0-livekit-voice-agent.py console
```
5. After the script starts, you can interact with the voice agent using [Livekit's Agent Platform](https://agents-playground.livekit.io/) and connect to the agent inorder to start conversations.

## Best Practices for Voice Agents with Memory

1. **Context Preservation**: Store enough context with each memory for effective retrieval
2. **Privacy Considerations**: Implement secure memory management
3. **Relevant Memory Filtering**: Use semantic search to retrieve only the most relevant memories
4. **Error Handling**: Implement robust error handling for memory operations

## Debugging Function Tools

- To run the script in debug mode simply start the assistant with `dev` mode:
```sh
python mem0-livekit-voice-agent.py dev
```

- When working with memory-enabled voice agents, use Python's `logging` module for effective debugging:

```python
import logging

# Set up logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("memory_voice_agent")
```

- Check the logs for any issues with API keys, connectivity, or memory operations.
- Ensure your `.env` file is correctly configured and loaded.


## Help & Resources

- [LiveKit Documentation](https://docs.livekit.io/)
- [Mem0 Platform](https://app.mem0.ai/)
- Need assistance? Reach out through:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/integrations/llama-index.mdx
================================================
---
title: LlamaIndex
---

LlamaIndex supports Mem0 as a [memory store](https://llamahub.ai/l/memory/llama-index-memory-mem0). In this guide, we'll show you how to use it.

<Note type="info">
  🎉 Exciting news! [**Mem0Memory**](https://docs.llamaindex.ai/en/stable/examples/memory/Mem0Memory/) now supports **ReAct** and **FunctionCalling** agents.
</Note>

### Installation

To install the required package, run:

```bash
pip install llama-index-core llama-index-memory-mem0 python-dotenv
```

### Setup with Mem0 Platform

Set your Mem0 Platform API key as an environment variable. You can replace `<your-mem0-api-key>` with your actual API key:

<Note type="info">
  You can obtain your Mem0 Platform API key from the [Mem0 Platform](https://app.mem0.ai/login).
</Note>

```python
from dotenv import load_dotenv
import os

load_dotenv()

# os.environ["MEM0_API_KEY"] = "<your-mem0-api-key>"
```

Import the necessary modules and create a Mem0Memory instance:
```python
from llama_index.memory.mem0 import Mem0Memory

context = {"user_id": "alice"}
memory_from_client = Mem0Memory.from_client(
    context=context,
    search_msg_limit=4,  # optional, default is 5
    output_format='v1.1',  # Remove deprecation warnings
)
```

Context is used to identify the user, agent or the conversation in the Mem0. It is required to be passed in the at least one of the fields in the `Mem0Memory` constructor. It can be any of the following:

```python
context = {
    "user_id": "alice", 
    "agent_id": "llama_agent_1",
    "run_id": "run_1",
}
```

`search_msg_limit` is optional, default is 5. It is the number of messages from the chat history to be used for memory retrieval from Mem0. More number of messages will result in more context being used for retrieval but will also increase the retrieval time and might result in some unwanted results.

<Note type="info">
  `search_msg_limit` is different from `limit`. `limit` is the number of messages to be retrieved from Mem0 and is used in search.
</Note>

### Setup with Mem0 OSS

Set your Mem0 OSS by providing configuration details:

<Note type="info">
  To know more about Mem0 OSS, read [Mem0 OSS Quickstart](https://docs.mem0.ai/open-source/overview).
</Note>

```python
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name": "test_9",
            "host": "localhost",
            "port": 6333,
            "embedding_model_dims": 1536,  # Change this according to your local model's dimensions
        },
    },
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o",
            "temperature": 0.2,
            "max_tokens": 2000,
        },
    },
    "embedder": {
        "provider": "openai",
        "config": {"model": "text-embedding-3-small"},
    },
    "version": "v1.1",
}
```

Create a Mem0Memory instance:

```python
memory_from_config = Mem0Memory.from_config(
    context=context,
    config=config,
    search_msg_limit=4,  # optional, default is 5
    output_format='v1.1',  # Remove deprecation warnings
)
```

Initialize the LLM

```python
from llama_index.llms.openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# os.environ["OPENAI_API_KEY"] = "<your-openai-api-key>"
llm = OpenAI(model="gpt-4o-mini")
```

### SimpleChatEngine
Use the `SimpleChatEngine` to start a chat with the agent with the memory.

```python
from llama_index.core.chat_engine import SimpleChatEngine

agent = SimpleChatEngine.from_defaults(
    llm=llm, memory=memory_from_client  # or memory_from_config
)

# Start the chat
response = agent.chat("Hi, My name is Alice")
print(response)
```
Now we will learn how to use Mem0 with FunctionCalling and ReAct agents.

Initialize the tools:

```python
from llama_index.core.tools import FunctionTool


def call_fn(name: str):
    """Call the provided name.
    Args:
        name: str (Name of the person)
    """
    print(f"Calling... {name}")


def email_fn(name: str):
    """Email the provided name.
    Args:
        name: str (Name of the person)
    """
    print(f"Emailing... {name}")


call_tool = FunctionTool.from_defaults(fn=call_fn)
email_tool = FunctionTool.from_defaults(fn=email_fn)
```
### FunctionCallingAgent

```python
from llama_index.core.agent import FunctionCallingAgent

agent = FunctionCallingAgent.from_tools(
    [call_tool, email_tool],
    llm=llm,
    memory=memory_from_client,  # or memory_from_config
    verbose=True,
)

# Start the chat
response = agent.chat("Hi, My name is Alice")
print(response)
```

### ReActAgent

```python
from llama_index.core.agent import ReActAgent

agent = ReActAgent.from_tools(
    [call_tool, email_tool],
    llm=llm,
    memory=memory_from_client,  # or memory_from_config
    verbose=True,
)

# Start the chat
response = agent.chat("Hi, My name is Alice")
print(response)
```

## Key Features

1. **Memory Integration**: Uses Mem0 to store and retrieve relevant information from past interactions.
2. **Personalization**: Provides context-aware agent responses based on user history and preferences.
3. **Flexible Architecture**: LlamaIndex allows for easy integration of the memory with the agent.
4. **Continuous Learning**: Each interaction is stored, improving future responses.

## Conclusion

By integrating LlamaIndex with Mem0, you can build a personalized agent that can maintain context across interactions with the agent and provide tailored recommendations and assistance.

## Help

- For more details on LlamaIndex, visit the [LlamaIndex documentation](https://llamahub.ai/l/memory/llama-index-memory-mem0).
- [Mem0 Platform](https://app.mem0.ai/).
- If you need further assistance, please feel free to reach out to us through following methods:

<Snippet file="get-help.mdx" />







================================================
FILE: docs/integrations/mastra.mdx
================================================
---
title: Mastra
---

The [**Mastra**](https://mastra.ai/) integration demonstrates how to use Mastra's agent system with Mem0 as the memory backend through custom tools. This enables agents to remember and recall information across conversations.

## Overview

In this guide, we'll create a Mastra agent that:
1. Uses Mem0 to store information using a memory tool
2. Retrieves relevant memories using a search tool
3. Provides personalized responses based on past interactions
4. Maintains context across conversations and sessions

## Setup and Configuration

Install the required libraries:

```bash
npm install @mastra/core @mastra/mem0 @ai-sdk/openai zod
```

Set up your environment variables:

<Note>Remember to get the Mem0 API key from [Mem0 Platform](https://app.mem0.ai).</Note>

```bash
MEM0_API_KEY=your-mem0-api-key
OPENAI_API_KEY=your-openai-api-key
```

## Initialize Mem0 Integration

Import required modules and set up the Mem0 integration:

```typescript
import { Mem0Integration } from '@mastra/mem0';
import { createTool } from '@mastra/core/tools';
import { Agent } from '@mastra/core/agent';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

// Initialize Mem0 integration
const mem0 = new Mem0Integration({
  config: {
    apiKey: process.env.MEM0_API_KEY || '',
    user_id: 'alice', // Unique user identifier
  },
});
```

## Create Memory Tools

Set up tools for memorizing and remembering information:

```typescript
// Tool for remembering saved memories
const mem0RememberTool = createTool({
  id: 'Mem0-remember',
  description: "Remember your agent memories that you've previously saved using the Mem0-memorize tool.",
  inputSchema: z.object({
    question: z.string().describe('Question used to look up the answer in saved memories.'),
  }),
  outputSchema: z.object({
    answer: z.string().describe('Remembered answer'),
  }),
  execute: async ({ context }) => {
    console.log(`Searching memory "${context.question}"`);
    const memory = await mem0.searchMemory(context.question);
    console.log(`\nFound memory "${memory}"\n`);

    return {
      answer: memory,
    };
  },
});

// Tool for saving new memories
const mem0MemorizeTool = createTool({
  id: 'Mem0-memorize',
  description: 'Save information to mem0 so you can remember it later using the Mem0-remember tool.',
  inputSchema: z.object({
    statement: z.string().describe('A statement to save into memory'),
  }),
  execute: async ({ context }) => {
    console.log(`\nCreating memory "${context.statement}"\n`);
    // To reduce latency, memories can be saved async without blocking tool execution
    void mem0.createMemory(context.statement).then(() => {
      console.log(`\nMemory "${context.statement}" saved.\n`);
    });
    return { success: true };
  },
});
```

## Create Mastra Agent

Initialize an agent with memory tools and clear instructions:

```typescript
// Create an agent with memory tools
const mem0Agent = new Agent({
  name: 'Mem0 Agent',
  instructions: `
    You are a helpful assistant that has the ability to memorize and remember facts using Mem0.
    Use the Mem0-memorize tool to save important information that might be useful later.
    Use the Mem0-remember tool to recall previously saved information when answering questions.
  `,
  model: openai('gpt-4o'),
  tools: { mem0RememberTool, mem0MemorizeTool },
});
```


## Key Features

1. **Tool-based Memory Control**: The agent decides when to save and retrieve information using specific tools
2. **Semantic Search**: Mem0 finds relevant memories based on semantic similarity, not just exact matches
3. **User-specific Memory Spaces**: Each user_id maintains separate memory contexts
4. **Asynchronous Saving**: Memories are saved in the background to reduce response latency
5. **Cross-conversation Persistence**: Memories persist across different conversation threads
6. **Transparent Operations**: Memory operations are visible through tool usage

## Conclusion

By integrating Mastra with Mem0, you can build intelligent agents that learn and remember information across conversations. The tool-based approach provides transparency and control over memory operations, making it easy to create personalized and context-aware AI experiences.

## Help

- For more details on Mastra, visit the [Mastra documentation](https://docs.mastra.ai/).
- [Mem0 Platform](https://app.mem0.ai/).
- If you need further assistance, please feel free to reach out to us through the following methods:

<Snippet file="get-help.mdx" /> 


================================================
FILE: docs/integrations/openai-agents-sdk.mdx
================================================
---
title: OpenAI Agents SDK
---

Integrate [**Mem0**](https://github.com/mem0ai/mem0) with [OpenAI Agents SDK](https://github.com/openai/openai-agents-python), a lightweight framework for building multi-agent workflows. This integration enables agents to access persistent memory across conversations, enhancing context retention and personalization.

## Overview

1. Store and retrieve memories from Mem0 within OpenAI agents
2. Multi-agent workflows with shared memory
3. Retrieve relevant memories for past conversations
4. Personalized responses based on user history

## Prerequisites

Before setting up Mem0 with OpenAI Agents SDK, ensure you have:

1. Installed the required packages:
```bash
pip install openai-agents mem0ai
```

2. Valid API keys:
   - [Mem0 API Key](https://app.mem0.ai/dashboard/api-keys)
   - [OpenAI API Key](https://platform.openai.com/api-keys)

## Basic Integration Example

The following example demonstrates how to create an OpenAI agent with Mem0 memory integration:

```python
import os
from agents import Agent, Runner, function_tool
from mem0 import MemoryClient

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["MEM0_API_KEY"] = "your-mem0-api-key"

# Initialize Mem0 client
mem0 = MemoryClient()

# Define memory tools for the agent
@function_tool
def search_memory(query: str, user_id: str) -> str:
    """Search through past conversations and memories"""
    memories = mem0.search(query, user_id=user_id, limit=3)
    if memories and memories.get('results'):
        return "\n".join([f"- {mem['memory']}" for mem in memories['results']])
    return "No relevant memories found."

@function_tool
def save_memory(content: str, user_id: str) -> str:
    """Save important information to memory"""
    mem0.add([{"role": "user", "content": content}], user_id=user_id)
    return "Information saved to memory."

# Create agent with memory capabilities
agent = Agent(
    name="Personal Assistant",
    instructions="""You are a helpful personal assistant with memory capabilities.
    Use the search_memory tool to recall past conversations and user preferences.
    Use the save_memory tool to store important information about the user.
    Always personalize your responses based on available memory.""",
    tools=[search_memory, save_memory],
    model="gpt-4o"
)

def chat_with_agent(user_input: str, user_id: str) -> str:
    """
    Handle user input with automatic memory integration.

    Args:
        user_input: The user's message
        user_id: Unique identifier for the user

    Returns:
        The agent's response
    """
    # Run the agent (it will automatically use memory tools when needed)
    result = Runner.run_sync(agent, user_input)

    return result.final_output

# Example usage
if __name__ == "__main__":

    # preferences will be saved in memory (using save_memory tool)
    response_1 = chat_with_agent(
        "I love Italian food and I'm planning a trip to Rome next month",
        user_id="alice"
    )
    print(response_1)

    # memory will be retrieved using search_memory tool to answer the user query
    response_2 = chat_with_agent(
        "Give me some recommendations for food",
        user_id="alice"
    )
    print(response_2)
```

## Multi-Agent Workflow with Handoffs

Create multiple specialized agents with proper handoffs and shared memory:

```python
from agents import Agent, Runner, handoffs, function_tool

# Specialized agents
travel_agent = Agent(
    name="Travel Planner",
    instructions="""You are a travel planning specialist. Use get_user_context to
    understand the user's travel preferences and history before making recommendations.
    After providing your response, use store_conversation to save important details.""",
    tools=[search_memory, save_memory],
    model="gpt-4o"
)

health_agent = Agent(
    name="Health Advisor",
    instructions="""You are a health and wellness advisor. Use get_user_context to
    understand the user's health goals and dietary preferences.
    After providing advice, use store_conversation to save relevant information.""",
    tools=[search_memory, save_memory],
    model="gpt-4o"
)

# Triage agent with handoffs
triage_agent = Agent(
    name="Personal Assistant",
    instructions="""You are a helpful personal assistant that routes requests to specialists.
    For travel-related questions (trips, hotels, flights, destinations), hand off to Travel Planner.
    For health-related questions (fitness, diet, wellness, exercise), hand off to Health Advisor.
    For general questions, you can handle them directly using available tools.""",
    handoffs=[travel_agent, health_agent],
    model="gpt-4o"
)

def chat_with_handoffs(user_input: str, user_id: str) -> str:
    """
    Handle user input with automatic agent handoffs and memory integration.

    Args:
        user_input: The user's message
        user_id: Unique identifier for the user

    Returns:
        The agent's response
    """
    # Run the triage agent (it will automatically handoff when needed)
    result = Runner.run_sync(triage_agent, user_input)

    # Store the original conversation in memory
    conversation = [
        {"role": "user", "content": user_input},
        {"role": "assistant", "content": result.final_output}
    ]
    mem0.add(conversation, user_id=user_id)

    return result.final_output

# Example usage
response = chat_with_handoffs("Plan a healthy meal for my Italy trip", user_id="alex")
print(response)
```

## Quick Start Chat Interface

Simple interactive chat with memory:

```python
def interactive_chat():
    """Interactive chat interface with memory and handoffs"""
    user_id = input("Enter your user ID: ") or "demo_user"
    print(f"Chat started for user: {user_id}")
    print("Type 'quit' to exit\n")

    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break

        response = chat_with_handoffs(user_input, user_id)
        print(f"Assistant: {response}\n")

if __name__ == "__main__":
    interactive_chat()
```

## Key Features

### 1. Automatic Memory Integration
- **Tool-Based Memory**: Agents use function tools to search and save memories
- **Conversation Storage**: All interactions are automatically stored
- **Context Retrieval**: Agents can access relevant past conversations

### 2. Multi-Agent Memory Sharing
- **Shared Context**: Multiple agents access the same memory store
- **Specialized Agents**: Create domain-specific agents with shared memory
- **Seamless Handoffs**: Agents maintain context across handoffs

### 3. Flexible Memory Operations
- **Retrieve Capabilities**: Retrieve relevant memories from previous conversation
- **User Segmentation**: Organize memories by user ID
- **Memory Management**: Built-in tools for saving and retrieving information

## Configuration Options

Customize memory behavior:

```python
# Configure memory search
memories = mem0.search(
    query="travel preferences",
    user_id="alex",
    limit=5  # Number of memories to retrieve
)

# Add metadata to memories
mem0.add(
    messages=[{"role": "user", "content": "I prefer luxury hotels"}],
    user_id="alex",
    metadata={"category": "travel", "importance": "high"}
)
```

## Help

- [OpenAI Agents SDK Documentation](https://openai.github.io/openai-agents-python/)
- [Mem0 Platform](https://app.mem0.ai/)
- If you need further assistance, please feel free to reach out to us through the following methods:

<Snippet file="get-help.mdx" />


================================================
FILE: docs/integrations/pipecat.mdx
================================================
---
title: 'Pipecat'
description: 'Integrate Mem0 with Pipecat for conversational memory in AI agents'
---

# Pipecat Integration

Mem0 seamlessly integrates with [Pipecat](https://pipecat.ai), providing long-term memory capabilities for conversational AI agents. This integration allows your Pipecat-powered applications to remember past conversations and provide personalized responses based on user history.

## Installation

To use Mem0 with Pipecat, install the required dependencies:

```bash
pip install "pipecat-ai[mem0]"
```

You'll also need to set up your Mem0 API key as an environment variable:

```bash
export MEM0_API_KEY=your_mem0_api_key
```

You can obtain a Mem0 API key by signing up at [mem0.ai](https://mem0.ai).

## Configuration

Mem0 integration is provided through the `Mem0MemoryService` class in Pipecat. Here's how to configure it:

```python
from pipecat.services.mem0 import Mem0MemoryService

memory = Mem0MemoryService(
    api_key=os.getenv("MEM0_API_KEY"),  # Your Mem0 API key
    user_id="unique_user_id",           # Unique identifier for the end user
    agent_id="my_agent",                # Identifier for the agent using the memory
    run_id="session_123",               # Optional: specific conversation session ID
    params={                            # Optional: configuration parameters
        "search_limit": 10,             # Maximum memories to retrieve per query
        "search_threshold": 0.1,        # Relevance threshold (0.0 to 1.0)
        "system_prompt": "Here are your past memories:", # Custom prefix for memories
        "add_as_system_message": True,  # Add memories as system (True) or user (False) message
        "position": 1,                  # Position in context to insert memories
    }
)
```

## Pipeline Integration

The `Mem0MemoryService` should be positioned between your context aggregator and LLM service in the Pipecat pipeline:

```python
pipeline = Pipeline([
    transport.input(),
    stt,                # Speech-to-text for audio input
    user_context,       # User context aggregator
    memory,             # Mem0 Memory service enhances context here
    llm,                # LLM for response generation
    tts,                # Optional: Text-to-speech
    transport.output(),
    assistant_context   # Assistant context aggregator
])
```

## Example: Voice Agent with Memory

Here's a complete example of a Pipecat voice agent with Mem0 memory integration:

```python
import asyncio
import os
from fastapi import FastAPI, WebSocket

from pipecat.frames.frames import TextFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineTask
from pipecat.pipeline.runner import PipelineRunner
from pipecat.services.mem0 import Mem0MemoryService
from pipecat.services.openai import OpenAILLMService, OpenAIUserContextAggregator, OpenAIAssistantContextAggregator
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketTransport,
    FastAPIWebsocketParams
)
from pipecat.serializers.protobuf import ProtobufFrameSerializer
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.services.whisper import WhisperSTTService

app = FastAPI()

@app.websocket("/chat")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    
    # Basic setup with minimal configuration
    user_id = "alice"
    
    # WebSocket transport
    transport = FastAPIWebsocketTransport(
        websocket=websocket,
        params=FastAPIWebsocketParams(
            audio_out_enabled=True,
            vad_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            vad_audio_passthrough=True,
            serializer=ProtobufFrameSerializer(),
        )
    )
    
    # Core services
    user_context = OpenAIUserContextAggregator()
    assistant_context = OpenAIAssistantContextAggregator()
    stt = WhisperSTTService(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Memory service - the key component
    memory = Mem0MemoryService(
        api_key=os.getenv("MEM0_API_KEY"),
        user_id=user_id,
        agent_id="fastapi_memory_bot"
    )
    
    # LLM for response generation
    llm = OpenAILLMService(
        api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-3.5-turbo",
        system_prompt="You are a helpful assistant that remembers past conversations."
    )
    
    # Simple pipeline
    pipeline = Pipeline([
        transport.input(),
        stt,                # Speech-to-text for audio input
        user_context,
        memory,             # Memory service enhances context here
        llm,
        transport.output(),
        assistant_context
    ])
    
    # Run the pipeline
    runner = PipelineRunner()
    task = PipelineTask(pipeline)
    
    # Event handlers for WebSocket connections
    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Send welcome message when client connects
        await task.queue_frame(TextFrame("Hello! I'm a memory bot. I'll remember our conversation."))
    
    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        # Clean up when client disconnects
        await task.cancel()
    
    await runner.run(task)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## How It Works

When integrated with Pipecat, Mem0 provides two key functionalities:

### 1. Message Storage

All conversation messages are automatically stored in Mem0 for future reference:
- Captures the full message history from context frames
- Associates messages with the specified user, agent, and run IDs
- Stores metadata to enable efficient retrieval

### 2. Memory Retrieval

When a new user message is detected:
1. The message is used as a search query to find relevant past memories
2. Relevant memories are retrieved from Mem0's database
3. Memories are formatted and added to the conversation context
4. The enhanced context is passed to the LLM for response generation

## Additional Configuration Options

### Memory Search Parameters

You can customize how memories are retrieved and used:

```python
memory = Mem0MemoryService(
    api_key=os.getenv("MEM0_API_KEY"),
    user_id="user123",
    params={
        "search_limit": 5,            # Retrieve up to 5 memories
        "search_threshold": 0.2,      # Higher threshold for more relevant matches
        "api_version": "v2",          # Mem0 API version
    }
)
```

### Memory Presentation Options

Control how memories are presented to the LLM:

```python
memory = Mem0MemoryService(
    api_key=os.getenv("MEM0_API_KEY"),
    user_id="user123",
    params={
        "system_prompt": "Previous conversations with this user:",
        "add_as_system_message": True,  # Add as system message instead of user message
        "position": 0,                  # Insert at the beginning of the context
    }
)
```

## Resources

- [Mem0 Pipecat Integration](https://docs.pipecat.ai/server/services/memory/mem0)
- [Pipecat Documentation](https://docs.pipecat.ai)




================================================
FILE: docs/integrations/raycast.mdx
================================================
---
title: "Raycast Extension"
description: "Mem0 Raycast extension for intelligent memory management"
---

Mem0 is a self-improving memory layer for LLM applications, enabling personalized AI experiences that save costs and delight users. This extension lets you store and retrieve text snippets using Mem0's intelligent memory system. Find Mem0 in [Raycast Store](https://www.raycast.com/dev_khant/mem0) for using it.

## Getting Started

**Get your API Key**: You'll need a Mem0 API key to use this extension:

a. Sign up at [app.mem0.ai](https://app.mem0.ai)

b. Navigate to your API Keys page

c. Copy your API key

d. Enter this key in the extension preferences

**Basic Usage**:

- Store memories and text snippets
- Retrieve context-aware information
- Manage persistent user preferences
- Search through stored memories

## ✨ Features

**Remember Everything**: Never lose important information - store notes, preferences, and conversations that your AI can recall later

**Smart Connections**: Automatically links related topics, just like your brain does - helping you discover useful connections

**Cost Saver**: Spend less on AI usage by efficiently retrieving relevant information instead of regenerating responses

## 🔑 How This Helps You

**More Personal Experience**: Your AI remembers your preferences and past conversations, making interactions feel more natural

**Learn Your Style**: Adapts to how you work and what you like, becoming more helpful over time

**No More Repetition**: Stop explaining the same things over and over - your AI remembers your context and preferences

---

<Snippet file="get-help.mdx" /> 



================================================
FILE: docs/integrations/vercel-ai-sdk.mdx
================================================
---
title: Vercel AI SDK
---

The [**Mem0 AI SDK Provider**](https://www.npmjs.com/package/@mem0/vercel-ai-provider) is a library developed by **Mem0** to integrate with the Vercel AI SDK. This library brings enhanced AI interaction capabilities to your applications by introducing persistent memory functionality.

<Note type="info">
  🎉 Exciting news! Mem0 AI SDK now supports <strong>Vercel AI SDK V5</strong>.
</Note>

## Overview

1. 🧠 Offers persistent memory storage for conversational AI
2. 🔄 Enables smooth integration with the Vercel AI SDK
3. 🚀 Ensures compatibility with multiple LLM providers
4. 📝 Supports structured message formats for clarity
5. ⚡ Facilitates streaming response capabilities

## Setup and Configuration

Install the SDK provider using npm:

```bash
npm install @mem0/vercel-ai-provider
```

## Getting Started

### Setting Up Mem0

1. Get your **Mem0 API Key** from the [Mem0 Dashboard](https://app.mem0.ai/dashboard/api-keys).

2. Initialize the Mem0 Client in your application:

    ```typescript
    import { createMem0 } from "@mem0/vercel-ai-provider";

    const mem0 = createMem0({
      provider: "openai",
      mem0ApiKey: "m0-xxx",
      apiKey: "provider-api-key",
      config: {
        // Options for LLM Provider
      },
      // Optional Mem0 Global Config
      mem0Config: {
        user_id: "mem0-user-id",
      },
    });
    ```

    > **Note**: The `openai` provider is set as default. Consider using `MEM0_API_KEY` and `OPENAI_API_KEY` as environment variables for security.

    > **Note**: The `mem0Config` is optional. It is used to set the global config for the Mem0 Client (eg. `user_id`, `agent_id`, `app_id`, `run_id`, `org_id`, `project_id` etc).

3. Add Memories to Enhance Context:

    ```typescript
    import { LanguageModelV2Prompt } from "@ai-sdk/provider";
    import { addMemories } from "@mem0/vercel-ai-provider";

    const messages: LanguageModelV2Prompt = [
      { role: "user", content: [{ type: "text", text: "I love red cars." }] },
    ];

    await addMemories(messages, { user_id: "borat" });
    ```

### Standalone Features:

    ```typescript
    await addMemories(messages, { user_id: "borat", mem0ApiKey: "m0-xxx" });
    await retrieveMemories(prompt, { user_id: "borat", mem0ApiKey: "m0-xxx" });
    await getMemories(prompt, { user_id: "borat", mem0ApiKey: "m0-xxx" });
    ```
     > For standalone features, such as `addMemories`, `retrieveMemories`, and `getMemories`, you must either set `MEM0_API_KEY` as an environment variable or pass it directly in the function call.

     > `getMemories` will return raw memories in the form of an array of objects, while `retrieveMemories` will return a response in string format with a system prompt ingested with the retrieved memories.

     > `getMemories` is an object with two keys: `results` and `relations` if `enable_graph` is enabled. Otherwise, it will return an array of objects.

### 1. Basic Text Generation with Memory Context

    ```typescript
    import { generateText } from "ai";
    import { createMem0 } from "@mem0/vercel-ai-provider";

    const mem0 = createMem0();

    const { text } = await generateText({
      model: mem0("gpt-4-turbo", { user_id: "borat" }),
      prompt: "Suggest me a good car to buy!",
    });
    ```

### 2. Combining OpenAI Provider with Memory Utils

    ```typescript
    import { generateText } from "ai";
    import { openai } from "@ai-sdk/openai";
    import { retrieveMemories } from "@mem0/vercel-ai-provider";

    const prompt = "Suggest me a good car to buy.";
    const memories = await retrieveMemories(prompt, { user_id: "borat" });

    const { text } = await generateText({
      model: openai("gpt-4-turbo"),
      prompt: prompt,
      system: memories,
    });
    ```

### 3. Structured Message Format with Memory

    ```typescript
    import { generateText } from "ai";
    import { createMem0 } from "@mem0/vercel-ai-provider";

    const mem0 = createMem0();

    const { text } = await generateText({
      model: mem0("gpt-4-turbo", { user_id: "borat" }),
      messages: [
        {
          role: "user",
          content: [
            { type: "text", text: "Suggest me a good car to buy." },
            { type: "text", text: "Why is it better than the other cars for me?" },
          ],
        },
      ],
    });
    ```

### 3. Streaming Responses with Memory Context

    ```typescript
    import { streamText } from "ai";
    import { createMem0 } from "@mem0/vercel-ai-provider";

    const mem0 = createMem0();

    const { textStream } = streamText({
        model: mem0("gpt-4-turbo", {
            user_id: "borat",
        }),
        prompt: "Suggest me a good car to buy! Why is it better than the other cars for me? Give options for every price range.",
    });

    for await (const textPart of textStream) {
        process.stdout.write(textPart);
    }
    ```

### 4. Generate Responses with Tools Call

    ```typescript
    import { generateText } from "ai";
    import { createMem0 } from "@mem0/vercel-ai-provider";
    import { z } from "zod";

    const mem0 = createMem0({
      provider: "anthropic",
      apiKey: "anthropic-api-key",
      mem0Config: {
        // Global User ID
        user_id: "borat"
      }
    });

    const prompt = "What the temperature in the city that I live in?"

    const result = await generateText({
      model: mem0('claude-3-5-sonnet-20240620'),
      tools: {
        weather: tool({
          description: 'Get the weather in a location',
          parameters: z.object({
            location: z.string().describe('The location to get the weather for'),
          }),
          execute: async ({ location }) => ({
            location,
            temperature: 72 + Math.floor(Math.random() * 21) - 10,
          }),
        }),
      },
      prompt: prompt,
    });

    console.log(result);
    ```

### 5. Get sources from memory

```typescript
const { text, sources } = await generateText({
    model: mem0("gpt-4-turbo"),
    prompt: "Suggest me a good car to buy!",
});

console.log(sources);
```

The same can be done for `streamText` as well.

## Graph Memory

Mem0 AI SDK now supports Graph Memory. You can enable it by setting `enable_graph` to `true` in the `mem0Config` object.

```typescript
const mem0 = createMem0({
  mem0Config: { enable_graph: true },
});
```

You can also pass `enable_graph` in the standalone functions. This includes `getMemories`, `retrieveMemories`, and `addMemories`.

```typescript
const memories = await getMemories(prompt, { user_id: "borat", mem0ApiKey: "m0-xxx", enable_graph: true });
```

The `getMemories` function will return an object with two keys: `results` and `relations`, if `enable_graph` is set to `true`. Otherwise, it will return an array of objects.

## Supported LLM Providers

| Provider | Configuration Value |
|----------|-------------------|
| OpenAI | openai |
| Anthropic | anthropic |
| Google | google |
| Groq | groq |

> **Note**: You can use `google` as provider for Gemini (Google) models. They are same and internally they use `@ai-sdk/google` package.

## Key Features

- `createMem0()`: Initializes a new Mem0 provider instance.
- `retrieveMemories()`: Retrieves memory context for prompts.
- `getMemories()`: Get memories from your profile in array format.
- `addMemories()`: Adds user memories to enhance contextual responses.

## Best Practices

1. **User Identification**: Use a unique `user_id` for consistent memory retrieval.
2. **Memory Cleanup**: Regularly clean up unused memory data.

    > **Note**: We also have support for `agent_id`, `app_id`, and `run_id`. Refer [Docs](/api-reference/memory/add-memories).

## Conclusion

Mem0’s Vercel AI SDK enables the creation of intelligent, context-aware applications with persistent memory and seamless integration.

## Help

- For more details on Vercel AI SDK, visit the [Vercel AI SDK documentation](https://sdk.vercel.ai/docs/introduction)
- [Mem0 Platform](https://app.mem0.ai/)
- If you need further assistance, please feel free to reach out to us through following methods:

<Snippet file="get-help.mdx" />


================================================
FILE: docs/open-source/multimodal-support.mdx
================================================
---
title: Multimodal Support
icon: "image"
iconType: "solid"
---

Mem0 extends its capabilities beyond text by supporting multimodal data, including images. Users can seamlessly integrate images into their interactions, allowing Mem0 to extract pertinent information from visual content and enrich the memory system.

## How It Works

When a user provides an image, Mem0 processes the image to extract textual information and relevant details, which are then added to the user's memory. This feature enhances the system's ability to understand and remember details based on visual inputs.

<Note>
To enable multimodal support, you must set `enable_vision = True` in your configuration. The `vision_details` parameter can be set to "auto" (default), "low", or "high" to control the level of detail in image processing.
</Note>

<CodeGroup>
```python Code
from mem0 import Memory

config = {
    "llm": {
        "provider": "openai",
        "config": {
            "enable_vision": True,
            "vision_details": "high"
        }
    }
}

client = Memory.from_config(config=config)

messages = [
    {
        "role": "user",
        "content": "Hi, my name is Alice."
    },
    {
        "role": "assistant",
        "content": "Nice to meet you, Alice! What do you like to eat?"
    },
    {
        "role": "user",
        "content": {
            "type": "image_url",
            "image_url": {
                "url": "https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg"
            }
        }
    },
]

# Calling the add method to ingest messages into the memory system
client.add(messages, user_id="alice")
```

```typescript TypeScript
import { Memory, Message } from "mem0ai/oss";

const client = new Memory();

const messages: Message[] = [
    {
        role: "user",
        content: "Hi, my name is Alice."
    },
    {
        role: "assistant",
        content: "Nice to meet you, Alice! What do you like to eat?"
    },
    {
        role: "user",
        content: {
            type: "image_url",
            image_url: {
                url: "https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg"
            }
        }
    },
]

await client.add(messages, { userId: "alice" })
```

```json Output
{
  "results": [
    {
      "memory": "Name is Alice",
      "event": "ADD",
      "id": "7ae113a3-3cb5-46e9-b6f7-486c36391847"
    },
    {
      "memory": "Likes large pizza with toppings including cherry tomatoes, black olives, green spinach, yellow bell peppers, diced ham, and sliced mushrooms",
      "event": "ADD",
      "id": "56545065-7dee-4acf-8bf2-a5b2535aabb3"
    }
  ]
}
```
</CodeGroup>

## Image Integration Methods

Mem0 allows you to add images to user interactions through two primary methods: by providing an image URL or by using a Base64-encoded image. Below are examples demonstrating each approach.

## 1. Using an Image URL (Recommended)

You can include an image by passing its direct URL. This method is simple and efficient for online images.

<CodeGroup>
```python
# Define the image URL
image_url = "https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg"

# Create the message dictionary with the image URL
image_message = {
    "role": "user",
    "content": {
        "type": "image_url",
        "image_url": {
            "url": image_url
        }
    }
}
```

```typescript TypeScript
import { Memory, Message } from "mem0ai/oss";

const client = new Memory();

const imageUrl = "https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg";

const imageMessage: Message = {
    role: "user",
    content: {
        type: "image_url",
        image_url: {
            url: imageUrl
        }
    }
}

await client.add([imageMessage], { userId: "alice" })
```
</CodeGroup>

## 2. Using Base64 Image Encoding for Local Files

For local images or scenarios where embedding the image directly is preferable, you can use a Base64-encoded string.

<CodeGroup>
```python Python
import base64

# Path to the image file
image_path = "path/to/your/image.jpg"

# Encode the image in Base64
with open(image_path, "rb") as image_file:
    base64_image = base64.b64encode(image_file.read()).decode("utf-8")

# Create the message dictionary with the Base64-encoded image
image_message = {
    "role": "user",
    "content": {
        "type": "image_url",
        "image_url": {
            "url": f"data:image/jpeg;base64,{base64_image}"
        }
    }
}
```

```typescript TypeScript
import { Memory, Message } from "mem0ai/oss";

const client = new Memory();

const imagePath = "path/to/your/image.jpg";

const base64Image = fs.readFileSync(imagePath, { encoding: 'base64' });

const imageMessage: Message = {
    role: "user",
    content: {
        type: "image_url",
        image_url: {
            url: `data:image/jpeg;base64,${base64Image}`
        }
    }
}

await client.add([imageMessage], { userId: "alice" })
```
</CodeGroup>

## 3. OpenAI-Compatible Message Format

You can also use the OpenAI-compatible format to combine text and images in a single message:

<CodeGroup>
```python Python
import base64

# Path to the image file
image_path = "path/to/your/image.jpg"

# Encode the image in Base64
with open(image_path, "rb") as image_file:
    base64_image = base64.b64encode(image_file.read()).decode("utf-8")

# Create the message using OpenAI-compatible format
message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "What is in this image?",
        },
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
        },
    ],
}

# Add the message to memory
client.add([message], user_id="alice")
```

```typescript TypeScript
import { Memory, Message } from "mem0ai/oss";

const client = new Memory();

const imagePath = "path/to/your/image.jpg";

const base64Image = fs.readFileSync(imagePath, { encoding: 'base64' });

const message: Message = {
    role: "user",
    content: [
        {
            type: "text",
            text: "What is in this image?",
        },
        {
            type: "image_url",
            image_url: {
                url: `data:image/jpeg;base64,${base64Image}`
            }
        },
    ],
}

await client.add([message], { userId: "alice" })
```
</CodeGroup>

This format allows you to combine text and images in a single message, making it easier to provide context along with visual content.

By utilizing these methods, you can effectively incorporate images into user interactions, enhancing the multimodal capabilities of your Mem0 instance.

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/open-source/node-quickstart.mdx
================================================
---
title: Node SDK Quickstart
description: 'Get started with Mem0 quickly!'
icon: "node"
iconType: "solid"
---

> Welcome to the Mem0 quickstart guide. This guide will help you get up and running with Mem0 in no time.

## Installation

To install Mem0, you can use npm. Run the following command in your terminal:

```bash
npm install mem0ai
```

## Basic Usage

### Initialize Mem0

<Tabs>
  <Tab title="Basic">
```typescript
import { Memory } from 'mem0ai/oss';

const memory = new Memory();
```
  </Tab>
  <Tab title="Advanced">
If you want to run Mem0 in production, initialize using the following method:

```typescript
import { Memory } from 'mem0ai/oss';

const memory = new Memory({
    version: 'v1.1',
    embedder: {
      provider: 'openai',
      config: {
        apiKey: process.env.OPENAI_API_KEY || '',
        model: 'text-embedding-3-small',
      },
    },
    vectorStore: {
      provider: 'memory',
      config: {
        collectionName: 'memories',
        dimension: 1536,
      },
    },
    llm: {
      provider: 'openai',
      config: {
        apiKey: process.env.OPENAI_API_KEY || '',
        model: 'gpt-4-turbo-preview',
      },
    },
    historyDbPath: 'memory.db',
  });
```
  </Tab>
</Tabs>


### Store a Memory

<CodeGroup>
```typescript Code
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]

await memory.add(messages, { userId: "alice", metadata: { category: "movie_recommendations" } });
```

```json Output
{
  "results": [
    {
      "id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
      "memory": "User is planning to watch a movie tonight.",
      "metadata": {
        "category": "movie_recommendations"
      }
    },
    {
      "id": "cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4",
      "memory": "User is not a big fan of thriller movies.",
      "metadata": {
        "category": "movie_recommendations"
      }
    },
    {
      "id": "475bde34-21e6-42ab-8bef-0ab84474f156",
      "memory": "User loves sci-fi movies.",
      "metadata": {
        "category": "movie_recommendations"
      }
    }
  ]
}
```
</CodeGroup>

### Retrieve Memories

<CodeGroup>
```typescript Code
// Get all memories
const allMemories = await memory.getAll({ userId: "alice" });
console.log(allMemories)
```

```json Output
{
  "results": [
    {
      "id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
      "memory": "User is planning to watch a movie tonight.",
      "hash": "1a271c007316c94377175ee80e746a19",
      "createdAt": "2025-02-27T16:33:20.557Z",
      "updatedAt": "2025-02-27T16:33:27.051Z",
      "metadata": {
        "category": "movie_recommendations"
      },
      "userId": "alice"
    },
    {
      "id": "475bde34-21e6-42ab-8bef-0ab84474f156",
      "memory": "User loves sci-fi movies.",
      "hash": "285d07801ae42054732314853e9eadd7",
      "createdAt": "2025-02-27T16:33:20.560Z",
      "updatedAt": undefined,
      "metadata": {
        "category": "movie_recommendations"
      },
      "userId": "alice"
    },
    {
      "id": "cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4",
      "memory": "User is not a big fan of thriller movies.",
      "hash": "285d07801ae42054732314853e9eadd7",
      "createdAt": "2025-02-27T16:33:20.560Z",
      "updatedAt": undefined,
      "metadata": {
        "category": "movie_recommendations"
      },
      "userId": "alice"
    }
  ]
}
```
</CodeGroup>


<br />

<CodeGroup>
```typescript Code
// Get a single memory by ID
const singleMemory = await memory.get('892db2ae-06d9-49e5-8b3e-585ef9b85b8e');
console.log(singleMemory);
```

```json Output
{
  "id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
  "memory": "User is planning to watch a movie tonight.",
  "hash": "1a271c007316c94377175ee80e746a19",
  "createdAt": "2025-02-27T16:33:20.557Z",
  "updatedAt": undefined,
  "metadata": {
    "category": "movie_recommendations"
  },
  "userId": "alice"
}
```
</CodeGroup>

### Search Memories

<CodeGroup>
```typescript Code
const result = await memory.search('What do you know about me?', { userId: "alice" });
console.log(result);
```

```json Output
{
  "results": [
    {
      "id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
      "memory": "User is planning to watch a movie tonight.",
      "hash": "1a271c007316c94377175ee80e746a19",
      "createdAt": "2025-02-27T16:33:20.557Z",
      "updatedAt": undefined,
      "score": 0.38920719231944799,
      "metadata": {
        "category": "movie_recommendations"
      },
      "userId": "alice"
    },
    {
      "id": "475bde34-21e6-42ab-8bef-0ab84474f156",
      "memory": "User loves sci-fi movies.",
      "hash": "285d07801ae42054732314853e9eadd7",
      "createdAt": "2025-02-27T16:33:20.560Z",
      "updatedAt": undefined,
      "score": 0.36869761478135689,
      "metadata": {
        "category": "movie_recommendations"
      },
      "userId": "alice"
    },
    {
      "id": "cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4",
      "memory": "User is not a big fan of thriller movies.",
      "hash": "285d07801ae42054732314853e9eadd7",
      "createdAt": "2025-02-27T16:33:20.560Z",
      "updatedAt": undefined,
      "score": 0.33855272141248272,
      "metadata": {
        "category": "movie_recommendations"
      },
      "userId": "alice"
    }
  ]
}
```
</CodeGroup>

### Update a Memory

<CodeGroup>
```typescript Code
const result = await memory.update(
  '892db2ae-06d9-49e5-8b3e-585ef9b85b8e',
  'I love India, it is my favorite country.'
);
console.log(result);
```

```json Output
{
  "message": "Memory updated successfully!"
}
```
</CodeGroup>

### Memory History

<CodeGroup>
```typescript Code
const history = await memory.history('892db2ae-06d9-49e5-8b3e-585ef9b85b8e');
console.log(history);
```

```json Output
[
  {
    "id": 39,
    "memoryId": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
    "previousValue": "User is planning to watch a movie tonight.",
    "newValue": "I love India, it is my favorite country.",
    "action": "UPDATE",
    "createdAt": "2025-02-27T16:33:20.557Z",
    "updatedAt": "2025-02-27T16:33:27.051Z",
    "isDeleted": 0
  },
  {
    "id": 37,
    "memoryId": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
    "previousValue": null,
    "newValue": "User is planning to watch a movie tonight.",
    "action": "ADD",
    "createdAt": "2025-02-27T16:33:20.557Z",
    "updatedAt": null,
    "isDeleted": 0
  }
]
```
</CodeGroup>

### Delete Memory

```typescript
// Delete a memory by id
await memory.delete('892db2ae-06d9-49e5-8b3e-585ef9b85b8e');

// Delete all memories for a user
await memory.deleteAll({ userId: "alice" });
```

### Reset Memory

```typescript
await memory.reset(); // Reset all memories
```

### History Store

Mem0 TypeScript SDK support history stores to run on a serverless environment:

We recommend using `Supabase` as a history store for serverless environments or disable history store to run on a serverless environment.

<CodeGroup>
```typescript Supabase
import { Memory } from 'mem0ai/oss';

const memory = new Memory({
  historyStore: {
    provider: 'supabase',
    config: {
      supabaseUrl: process.env.SUPABASE_URL || '',
      supabaseKey: process.env.SUPABASE_KEY || '',
      tableName: 'memory_history',
    },
  },
});
```

```typescript Disable History
import { Memory } from 'mem0ai/oss';

const memory = new Memory({
  disableHistory: true,
});
```
</CodeGroup>

Mem0 uses SQLite as a default history store.

#### Create Memory History Table in Supabase

You may need to create a memory history table in Supabase to store the history of memories. Use the following SQL command in `SQL Editor` on the Supabase project dashboard to create a memory history table:

```sql
create table memory_history (
  id text primary key,
  memory_id text not null,
  previous_value text,
  new_value text,
  action text not null,
  created_at timestamp with time zone default timezone('utc', now()),
  updated_at timestamp with time zone,
  is_deleted integer default 0
);
```

## Configuration Parameters

Mem0 offers extensive configuration options to customize its behavior according to your needs. These configurations span across different components like vector stores, language models, embedders, and graph stores.

<AccordionGroup>
<Accordion title="Vector Store Configuration">
| Parameter    | Description                     | Default     |
|-------------|---------------------------------|-------------|
| `provider`   | Vector store provider (e.g., "memory") | "memory"   |
| `host`       | Host address                    | "localhost" |
| `port`       | Port number                     | undefined       |
</Accordion>

<Accordion title="LLM Configuration">
| Parameter              | Description                                   | Provider          |
|-----------------------|-----------------------------------------------|-------------------|
| `provider`            | LLM provider (e.g., "openai", "anthropic")    | All              |
| `model`               | Model to use                                  | All              |
| `temperature`         | Temperature of the model                      | All              |
| `apiKey`             | API key to use                                | All              |
| `maxTokens`          | Tokens to generate                            | All              |
| `topP`               | Probability threshold for nucleus sampling    | All              |
| `topK`               | Number of highest probability tokens to keep  | All              |
| `openaiBaseUrl`     | Base URL for OpenAI API                      | OpenAI           |
</Accordion>

<Accordion title="Graph Store Configuration">
| Parameter    | Description                     | Default     |
|-------------|---------------------------------|-------------|
| `provider`   | Graph store provider (e.g., "neo4j") | "neo4j"    |
| `url`        | Connection URL                  | env.NEO4J_URL        |
| `username`   | Authentication username         | env.NEO4J_USERNAME        |
| `password`   | Authentication password         | env.NEO4J_PASSWORD        |
</Accordion>

<Accordion title="Embedder Configuration">
| Parameter    | Description                     | Default                      |
|-------------|---------------------------------|------------------------------|
| `provider`   | Embedding provider              | "openai"                     |
| `model`      | Embedding model to use          | "text-embedding-3-small"     |
| `apiKey`    | API key for embedding service   | None                        |
</Accordion>

<Accordion title="General Configuration">
| Parameter         | Description                          | Default                    |
|------------------|--------------------------------------|----------------------------|
| `historyDbPath` | Path to the history database         | "{mem0_dir}/history.db"    |
| `version`         | API version                          | "v1.0"                     |
| `customPrompt`   | Custom prompt for memory processing  | None                       |
</Accordion>

<Accordion title="History Table Configuration">
| Parameter         | Description                          | Default                    |
|------------------|--------------------------------------|----------------------------|
| `provider`       | History store provider               | "sqlite"                   |
| `config`         | History store configuration         | None (Defaults to SQLite)                      |
| `disableHistory` | Disable history store               | false                      |
</Accordion>

<Accordion title="Complete Configuration Example">
```typescript
const config = {
      version: 'v1.1',
      embedder: {
        provider: 'openai',
        config: {
          apiKey: process.env.OPENAI_API_KEY || '',
          model: 'text-embedding-3-small',
        },
      },
      vectorStore: {
        provider: 'memory',
        config: {
          collectionName: 'memories',
          dimension: 1536,
        },
      },
      llm: {
        provider: 'openai',
        config: {
          apiKey: process.env.OPENAI_API_KEY || '',
          model: 'gpt-4-turbo-preview',
        },
      },
      historyStore: {
        provider: 'supabase',
        config: {
          supabaseUrl: process.env.SUPABASE_URL || '',
          supabaseKey: process.env.SUPABASE_KEY || '',
          tableName: 'memories',
        },
      },
      disableHistory: false, // This is false by default
      customPrompt: "I'm a virtual assistant. I'm here to help you with your queries.",
    }
```
</Accordion>
</AccordionGroup>

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />


================================================
FILE: docs/open-source/overview.mdx
================================================
---
title: Overview
icon: "eye"
iconType: "solid"
---

Welcome to Mem0 Open Source - a powerful, self-hosted memory management solution for AI agents and assistants. With Mem0 OSS, you get full control over your infrastructure while maintaining complete customization flexibility.

We offer two SDKs for Python and Node.js.

Check out our [GitHub repository](https://mem0.dev/gd) to explore the source code.

<CardGroup cols={2}>
<Card title="Python SDK Guide" icon="python" href="/open-source/python-quickstart">
  Learn more about Mem0 OSS Python SDK
</Card>
<Card title="Node.js SDK Guide" icon="node" href="/open-source/node-quickstart">
  Learn more about Mem0 OSS Node.js SDK
</Card>
</CardGroup>

## Key Features

- **Full Infrastructure Control**: Host Mem0 on your own servers
- **Customizable Implementation**: Modify and extend functionality as needed
- **Local Development**: Perfect for development and testing
- **No Vendor Lock-in**: Own your data and infrastructure
- **Community Driven**: Benefit from and contribute to community improvements



================================================
FILE: docs/open-source/python-quickstart.mdx
================================================
---
title: Python SDK Quickstart
description: 'Get started with Mem0 quickly!'
icon: "python"
iconType: "solid"
---

> Welcome to the Mem0 quickstart guide. This guide will help you get up and running with Mem0 in no time.

## Installation

To install Mem0, you can use pip. Run the following command in your terminal:

```bash
pip install mem0ai
```

## Basic Usage

### Initialize Mem0

<Tabs>
  <Tab title="Basic">
```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key"

m = Memory()
```
  </Tab>
  <Tab title="Async">
```python
import os
from mem0 import AsyncMemory

os.environ["OPENAI_API_KEY"] = "your-api-key"

m = AsyncMemory()
```
  </Tab>
  <Tab title="Advanced">
If you want to run Mem0 in production, initialize using the following method:

Run Qdrant first:

```bash
docker pull qdrant/qdrant

docker run -p 6333:6333 -p 6334:6334 \
    -v $(pwd)/qdrant_storage:/qdrant/storage:z \
    qdrant/qdrant
```

Then, instantiate memory with qdrant server:

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key"

config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "host": "localhost",
            "port": 6333,
        }
    },
}

m = Memory.from_config(config)
```
  </Tab>

<Tab title="Advanced (Graph Memory)">

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key"

config = {
    "graph_store": {
        "provider": "neo4j",
        "config": {
            "url": "neo4j+s://---",
            "username": "neo4j",
            "password": "---"
        }
    }
}

m = Memory.from_config(config_dict=config)
```

</Tab>
</Tabs>


### Store a Memory

<CodeGroup>
```python Code
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]

# Store inferred memories (default behavior)
result = m.add(messages, user_id="alice", metadata={"category": "movie_recommendations"})

# Store memories with agent and run context
result = m.add(messages, user_id="alice", agent_id="movie-assistant", run_id="session-001", metadata={"category": "movie_recommendations"})

# Store raw messages without inference
# result = m.add(messages, user_id="alice", metadata={"category": "movie_recommendations"}, infer=False)
```

```json Output
{
    "results": [
        {
            "id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
            "memory": "User is planning to watch a movie tonight.",
            "metadata": {
                "category": "movie_recommendations"
            },
            "event": "ADD"
        },
        {
            "id": "cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4",
            "memory": "User is not a big fan of thriller movies.",
            "metadata": {
                "category": "movie_recommendations"
            },
            "event": "ADD"
        },
        {
            "id": "475bde34-21e6-42ab-8bef-0ab84474f156",
            "memory": "User loves sci-fi movies.",
            "metadata": {
                "category": "movie_recommendations"
            },
            "event": "ADD"
        }
    ]
}
```
</CodeGroup>

### Retrieve Memories

<CodeGroup>
```python Code
# Get all memories
all_memories = m.get_all(user_id="alice")
```

```json Output
{
    "results": [
            {
        "id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
        "memory": "User is planning to watch a movie tonight.",
        "hash": "1a271c007316c94377175ee80e746a19",
        "created_at": "2025-02-27T16:33:20.557Z",
        "updated_at": "2025-02-27T16:33:27.051Z",
        "metadata": {
            "category": "movie_recommendations"
        },
        "user_id": "alice"
        },
        {
        "id": "475bde34-21e6-42ab-8bef-0ab84474f156",
        "memory": "User loves sci-fi movies.",
        "hash": "285d07801ae42054732314853e9eadd7",
        "created_at": "2025-02-27T16:33:20.560Z",
        "updated_at": None,
        "metadata": {
            "category": "movie_recommendations"
        },
        "user_id": "alice"
        },
        {
        "id": "cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4",
        "memory": "User is not a big fan of thriller movies.",
        "hash": "285d07801ae42054732314853e9eadd7",
        "created_at": "2025-02-27T16:33:20.560Z",
        "updated_at": None,
        "metadata": {
            "category": "movie_recommendations"
        },
        "user_id": "alice"
        }
    ]
}
```
</CodeGroup>


<br />

<CodeGroup>
```python Code
# Get a single memory by ID
specific_memory = m.get("892db2ae-06d9-49e5-8b3e-585ef9b85b8e")
```

```json Output
{
  "id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
  "memory": "User is planning to watch a movie tonight.",
  "hash": "1a271c007316c94377175ee80e746a19",
  "created_at": "2025-02-27T16:33:20.557Z",
  "updated_at": None,
  "metadata": {
    "category": "movie_recommendations"
  },
  "user_id": "alice"
}
```
</CodeGroup>

### Search Memories

<CodeGroup>
```python Code
related_memories = m.search(query="What do you know about me?", user_id="alice")
```

```json Output
{
  "results": [
    {
      "id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
      "memory": "User is planning to watch a movie tonight.",
      "hash": "1a271c007316c94377175ee80e746a19",
      "created_at": "2025-02-27T16:33:20.557Z",
      "updated_at": None,
      "score": 0.38920719231944799,
      "metadata": {
        "category": "movie_recommendations"
      },
      "user_id": "alice"
    },
    {
      "id": "475bde34-21e6-42ab-8bef-0ab84474f156",
      "memory": "User loves sci-fi movies.",
      "hash": "285d07801ae42054732314853e9eadd7",
      "created_at": "2025-02-27T16:33:20.560Z",
      "updated_at": None,
      "score": 0.36869761478135689,
      "metadata": {
        "category": "movie_recommendations"
      },
      "user_id": "alice"
    },
    {
      "id": "cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4",
      "memory": "User is not a big fan of thriller movies.",
      "hash": "285d07801ae42054732314853e9eadd7",
      "created_at": "2025-02-27T16:33:20.560Z",
      "updated_at": None,
      "score": 0.33855272141248272,
      "metadata": {
        "category": "movie_recommendations"
      },
      "user_id": "alice"
    }
  ]
}
```
</CodeGroup>

### Update a Memory

<CodeGroup>
```python Code
result = m.update(memory_id="892db2ae-06d9-49e5-8b3e-585ef9b85b8e", data="I love India, it is my favorite country.")
```

```json Output
{'message': 'Memory updated successfully!'}
```
</CodeGroup>

### Memory History

<CodeGroup>
```python Code
history = m.history(memory_id="892db2ae-06d9-49e5-8b3e-585ef9b85b8e")
```

```json Output
[
  {
    "id": 39,
    "memory_id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
    "previous_value": "User is planning to watch a movie tonight.",
    "new_value": "I love India, it is my favorite country.",
    "action": "UPDATE",
    "created_at": "2025-02-27T16:33:20.557Z",
    "updated_at": "2025-02-27T16:33:27.051Z",
    "is_deleted": 0
  },
  {
    "id": 37,
    "memory_id": "892db2ae-06d9-49e5-8b3e-585ef9b85b8e",
    "previous_value": null,
    "new_value": "User is planning to watch a movie tonight.",
    "action": "ADD",
    "created_at": "2025-02-27T16:33:20.557Z",
    "updated_at": null,
    "is_deleted": 0
  }
]
```
</CodeGroup>

### Delete Memory

```python
# Delete a memory by id
m.delete(memory_id="892db2ae-06d9-49e5-8b3e-585ef9b85b8e")
# Delete all memories for a user
m.delete_all(user_id="alice")
```

### Reset Memory

```python
m.reset() # Reset all memories
```

## Advanced Memory Organization

Mem0 supports three key parameters for organizing memories:

- **`user_id`**: Organize memories by user identity
- **`agent_id`**: Organize memories by AI agent or assistant
- **`run_id`**: Organize memories by session, workflow, or execution context

### Using All Three Parameters

```python
# Store memories with full context
m.add("User prefers vegetarian food", 
      user_id="alice", 
      agent_id="diet-assistant", 
      run_id="consultation-001")

# Retrieve memories with different scopes
all_user_memories = m.get_all(user_id="alice")
agent_memories = m.get_all(user_id="alice", agent_id="diet-assistant")
session_memories = m.get_all(user_id="alice", run_id="consultation-001")
specific_memories = m.get_all(user_id="alice", agent_id="diet-assistant", run_id="consultation-001")

# Search with context
general_search = m.search("What do you know about me?", user_id="alice")
agent_search = m.search("What do you know about me?", user_id="alice", agent_id="diet-assistant")
session_search = m.search("What do you know about me?", user_id="alice", run_id="consultation-001")
```

## Configuration Parameters

Mem0 offers extensive configuration options to customize its behavior according to your needs. These configurations span across different components like vector stores, language models, embedders, and graph stores.

<AccordionGroup>
<Accordion title="Vector Store Configuration">
| Parameter    | Description                     | Default     |
|-------------|---------------------------------|-------------|
| `provider`   | Vector store provider (e.g., "qdrant") | "qdrant"   |
| `host`       | Host address                    | "localhost" |
| `port`       | Port number                     | 6333       |
</Accordion>

<Accordion title="LLM Configuration">
| Parameter              | Description                                   | Provider          |
|-----------------------|-----------------------------------------------|-------------------|
| `provider`            | LLM provider (e.g., "openai", "anthropic")    | All              |
| `model`               | Model to use                                  | All              |
| `temperature`         | Temperature of the model                      | All              |
| `api_key`             | API key to use                                | All              |
| `max_tokens`          | Tokens to generate                            | All              |
| `top_p`               | Probability threshold for nucleus sampling    | All              |
| `top_k`               | Number of highest probability tokens to keep  | All              |
| `http_client_proxies` | Allow proxy server settings                   | AzureOpenAI      |
| `models`              | List of models                                | Openrouter       |
| `route`               | Routing strategy                              | Openrouter       |
| `openrouter_base_url` | Base URL for Openrouter API                  | Openrouter       |
| `site_url`            | Site URL                                      | Openrouter       |
| `app_name`            | Application name                              | Openrouter       |
| `ollama_base_url`     | Base URL for Ollama API                      | Ollama           |
| `openai_base_url`     | Base URL for OpenAI API                      | OpenAI           |
| `azure_kwargs`        | Azure LLM args for initialization            | AzureOpenAI      |
| `deepseek_base_url`   | Base URL for DeepSeek API                    | DeepSeek         |
</Accordion>

<Accordion title="Embedder Configuration">
| Parameter    | Description                     | Default                      |
|-------------|---------------------------------|------------------------------|
| `provider`   | Embedding provider              | "openai"                     |
| `model`      | Embedding model to use          | "text-embedding-3-small"     |
| `api_key`    | API key for embedding service   | None                        |
</Accordion>

<Accordion title="Graph Store Configuration">
| Parameter    | Description                     | Default     |
|-------------|---------------------------------|-------------|
| `provider`   | Graph store provider (e.g., "neo4j") | "neo4j"    |
| `url`        | Connection URL                  | None        |
| `username`   | Authentication username         | None        |
| `password`   | Authentication password         | None        |
</Accordion>

<Accordion title="General Configuration">
| Parameter         | Description                          | Default                    |
|------------------|--------------------------------------|----------------------------|
| `history_db_path` | Path to the history database         | "{mem0_dir}/history.db"    |
| `version`         | API version                          | "v1.1"                     |
| `custom_fact_extraction_prompt`   | Custom prompt for memory processing  | None                       |
| `custom_update_memory_prompt` | Custom prompt for update memory | None                |
</Accordion>

<Accordion title="Complete Configuration Example">
```python
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "host": "localhost",
            "port": 6333
        }
    },
    "llm": {
        "provider": "openai",
        "config": {
            "api_key": "your-api-key",
            "model": "gpt-4"
        }
    },
    "embedder": {
        "provider": "openai",
        "config": {
            "api_key": "your-api-key",
            "model": "text-embedding-3-small"
        }
    },
    "graph_store": {
        "provider": "neo4j",
        "config": {
            "url": "neo4j+s://your-instance",
            "username": "neo4j",
            "password": "password"
        }
    },
    "history_db_path": "/path/to/history.db",
    "version": "v1.1",
    "custom_fact_extraction_prompt": "Optional custom prompt for fact extraction for memory",
    "custom_update_memory_prompt": "Optional custom prompt for update memory"
}
```
</Accordion>
</AccordionGroup>

## Run Mem0 Locally

Please refer to the example [Mem0 with Ollama](../examples/mem0-with-ollama) to run Mem0 locally.


## Chat Completion

Mem0 can be easily integrated into chat applications to enhance conversational agents with structured memory. Mem0's APIs are designed to be compatible with OpenAI's, with the goal of making it easy to leverage Mem0 in applications you may have already built.

If you have a `Mem0 API key`, you can use it to initialize the client. Alternatively, you can initialize Mem0 without an API key if you're using it locally.

Mem0 supports several language models (LLMs) through integration with various [providers](https://litellm.vercel.app/docs/providers).

## Use Mem0 OSS

```python
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "host": "localhost",
            "port": 6333,
        }
    },
}

client = Mem0(config=config)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "What's the capital of France?",
        }
    ],
    model="gpt-4o",
)
```

## Contributing

We welcome contributions to Mem0! Here's how you can contribute:

1. Fork the repository and create your branch from `main`.
2. Clone the forked repository to your local machine.
3. Install the project dependencies:

   ```bash
   poetry install
   ```

4. Install pre-commit hooks:

   ```bash
   pip install pre-commit  # If pre-commit is not already installed
   pre-commit install
   ```

5. Make your changes and ensure they adhere to the project's coding standards.

6. Run the tests locally:

   ```bash
   poetry run pytest
   ```

7. If all tests pass, commit your changes and push to your fork.
8. Open a pull request with a clear title and description.

Please make sure your code follows our coding conventions and is well-documented. We appreciate your contributions to make Mem0 better!


If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />


================================================
FILE: docs/open-source/features/async-memory.mdx
================================================
---
title: Async Memory
description: 'Asynchronous memory for Mem0'
icon: "bolt"
iconType: "solid"
---

## AsyncMemory

The `AsyncMemory` class is a direct asynchronous interface to Mem0's in-process memory operations. Unlike the memory, which interacts with an API, `AsyncMemory` works directly with the underlying storage systems. This makes it ideal for applications where you want to embed Mem0 directly into your codebase.

### Initialization

To use `AsyncMemory`, import it from the `mem0.memory` module:

```python Python
import asyncio
from mem0 import AsyncMemory

# Initialize with default configuration
memory = AsyncMemory()

# Or initialize with custom configuration
from mem0.configs.base import MemoryConfig
custom_config = MemoryConfig(
    # Your custom configuration here
)
memory = AsyncMemory(config=custom_config)
```

### Key Features

1. **Non-blocking Operations** - All memory operations use `asyncio` to avoid blocking the event loop
2. **Concurrent Processing** - Parallel execution of vector store and graph operations
3. **Efficient Resource Utilization** - Better handling of I/O bound operations
4. **Compatible with Async Frameworks** - Seamless integration with FastAPI, aiohttp, and other async frameworks

### Methods

All methods in `AsyncMemory` have the same parameters as the synchronous `Memory` class but are designed to be used with `async/await`.

#### Create memories

Add a new memory asynchronously:

```python Python
try:
    result = await memory.add(
        messages=[
            {"role": "user", "content": "I'm travelling to SF"},
            {"role": "assistant", "content": "That's great to hear!"}
        ],
        user_id="alice"
    )
    print("Memory added successfully:", result)
except Exception as e:
    print(f"Error adding memory: {e}")
```

#### Retrieve memories

Retrieve memories related to a query:

```python Python
try:
    results = await memory.search(
        query="Where am I travelling?",
        user_id="alice"
    )
    print("Found memories:", results)
except Exception as e:
    print(f"Error searching memories: {e}")
```

#### List memories

List all memories for a `user_id`, `agent_id`, and/or `run_id`:

```python Python
try:
    all_memories = await memory.get_all(user_id="alice")
    print(f"Retrieved {len(all_memories)} memories")
except Exception as e:
    print(f"Error retrieving memories: {e}")
```

#### Get specific memory

Retrieve a specific memory by its ID:

```python Python
try:
    specific_memory = await memory.get(memory_id="memory-id-here")
    print("Retrieved memory:", specific_memory)
except Exception as e:
    print(f"Error retrieving memory: {e}")
```

#### Update memory

Update an existing memory by ID:

```python Python
try:
    updated_memory = await memory.update(
        memory_id="memory-id-here",
        data="I'm travelling to Seattle"
    )
    print("Memory updated successfully:", updated_memory)
except Exception as e:
    print(f"Error updating memory: {e}")
```

#### Delete memory

Delete a specific memory by ID:

```python Python
try:
    result = await memory.delete(memory_id="memory-id-here")
    print("Memory deleted successfully")
except Exception as e:
    print(f"Error deleting memory: {e}")
```

#### Delete all memories

Delete all memories for a specific user, agent, or run:

```python Python
try:
    result = await memory.delete_all(user_id="alice")
    print("All memories deleted successfully")
except Exception as e:
    print(f"Error deleting memories: {e}")
```

<Note>
At least one filter (user_id, agent_id, or run_id) is required when using delete_all.
</Note>

### Advanced Memory Organization

AsyncMemory supports the same three-parameter organization system as the synchronous Memory class:

```python Python
# Store memories with full context
await memory.add(
    messages=[{"role": "user", "content": "I prefer vegetarian food"}],
    user_id="alice",
    agent_id="diet-assistant",
    run_id="consultation-001"
)

# Retrieve memories with different scopes
all_user_memories = await memory.get_all(user_id="alice")
agent_memories = await memory.get_all(user_id="alice", agent_id="diet-assistant")
session_memories = await memory.get_all(user_id="alice", run_id="consultation-001")
specific_memories = await memory.get_all(
    user_id="alice", 
    agent_id="diet-assistant", 
    run_id="consultation-001"
)

# Search with context
general_search = await memory.search("What do you know about me?", user_id="alice")
agent_search = await memory.search("What do you know about me?", user_id="alice", agent_id="diet-assistant")
session_search = await memory.search("What do you know about me?", user_id="alice", run_id="consultation-001")
```

#### Memory History

Get the history of changes for a specific memory:

```python Python
try:
    history = await memory.history(memory_id="memory-id-here")
    print("Memory history:", history)
except Exception as e:
    print(f"Error retrieving history: {e}")
```

### Example: Concurrent Usage with Other APIs

`AsyncMemory` can be effectively combined with other async operations. Here's an example showing how to use it alongside OpenAI API calls in separate threads:

```python Python
import asyncio
from openai import AsyncOpenAI
from mem0 import AsyncMemory

async_openai_client = AsyncOpenAI()
async_memory = AsyncMemory()

async def chat_with_memories(message: str, user_id: str = "default_user") -> str:
    try:
        # Retrieve relevant memories
        search_result = await async_memory.search(query=message, user_id=user_id, limit=3)
        relevant_memories = search_result["results"]
        memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_memories)
        
        # Generate Assistant response
        system_prompt = f"You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}"
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": message}]
        response = await async_openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages)
        assistant_response = response.choices[0].message.content

        # Create new memories from the conversation
        messages.append({"role": "assistant", "content": assistant_response})
        await async_memory.add(messages, user_id=user_id)

        return assistant_response
    except Exception as e:
        print(f"Error in chat_with_memories: {e}")
        return "I apologize, but I encountered an error processing your request."

async def async_main():
    print("Chat with AI (type 'exit' to quit)")
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() == 'exit':
            print("Goodbye!")
            break
        response = await chat_with_memories(user_input)
        print(f"AI: {response}")

def main():
    asyncio.run(async_main())

if __name__ == "__main__":
    main()
```

## Error Handling and Best Practices

### Common Error Types

When working with `AsyncMemory`, you may encounter these common errors:

#### Connection and Configuration Errors

```python Python
import asyncio
from mem0 import AsyncMemory
from mem0.configs.base import MemoryConfig

async def handle_initialization_errors():
    try:
        # Initialize with custom config
        config = MemoryConfig(
            vector_store={"provider": "chroma", "config": {"path": "./chroma_db"}},
            llm={"provider": "openai", "config": {"model": "gpt-4o-mini"}}
        )
        memory = AsyncMemory(config=config)
        print("AsyncMemory initialized successfully")
    except ValueError as e:
        print(f"Configuration error: {e}")
    except ConnectionError as e:
        print(f"Connection error: {e}")
    except Exception as e:
        print(f"Unexpected initialization error: {e}")

asyncio.run(handle_initialization_errors())
```

#### Memory Operation Errors

```python Python
async def handle_memory_operation_errors():
    memory = AsyncMemory()
    
    try:
        # Memory not found error
        result = await memory.get(memory_id="non-existent-id")
    except ValueError as e:
        print(f"Invalid memory ID: {e}")
    except Exception as e:
        print(f"Memory retrieval error: {e}")
    
    try:
        # Invalid search parameters
        results = await memory.search(query="", user_id="alice")
    except ValueError as e:
        print(f"Invalid search query: {e}")
    except Exception as e:
        print(f"Search error: {e}")
```

### Performance Optimization

#### Concurrent Operations

Take advantage of AsyncMemory's concurrent capabilities:

```python Python
async def batch_operations():
    memory = AsyncMemory()
    
    # Process multiple operations concurrently
    tasks = []
    for i in range(5):
        task = memory.add(
            messages=[{"role": "user", "content": f"Message {i}"}],
            user_id=f"user_{i}"
        )
        tasks.append(task)
    
    try:
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"Task {i} failed: {result}")
            else:
                print(f"Task {i} completed successfully")
    except Exception as e:
        print(f"Batch operation error: {e}")
```

#### Resource Management

Properly manage AsyncMemory lifecycle:

```python Python
import asyncio
from contextlib import asynccontextmanager

@asynccontextmanager
async def get_memory():
    memory = AsyncMemory()
    try:
        yield memory
    finally:
        # Clean up resources if needed
        pass

async def safe_memory_usage():
    async with get_memory() as memory:
        try:
            result = await memory.search("test query", user_id="alice")
            return result
        except Exception as e:
            print(f"Memory operation failed: {e}")
            return None
```

### Timeout and Retry Strategies

Implement timeout and retry logic for robustness:

```python Python
async def with_timeout_and_retry(operation, max_retries=3, timeout=10.0):
    for attempt in range(max_retries):
        try:
            result = await asyncio.wait_for(operation(), timeout=timeout)
            return result
        except asyncio.TimeoutError:
            print(f"Timeout on attempt {attempt + 1}")
        except Exception as e:
            print(f"Error on attempt {attempt + 1}: {e}")
        
        if attempt < max_retries - 1:
            await asyncio.sleep(2 ** attempt)  # Exponential backoff
    
    raise Exception(f"Operation failed after {max_retries} attempts")

# Usage example
async def robust_memory_search():
    memory = AsyncMemory()
    
    async def search_operation():
        return await memory.search("test query", user_id="alice")
    
    try:
        result = await with_timeout_and_retry(search_operation)
        print("Search successful:", result)
    except Exception as e:
        print(f"Search failed permanently: {e}")
```

### Integration with Async Frameworks

#### FastAPI Integration

```python Python
from fastapi import FastAPI, HTTPException
from mem0 import AsyncMemory
import asyncio

app = FastAPI()
memory = AsyncMemory()

@app.post("/memories/")
async def add_memory(messages: list, user_id: str):
    try:
        result = await memory.add(messages=messages, user_id=user_id)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/memories/search")
async def search_memories(query: str, user_id: str, limit: int = 10):
    try:
        result = await memory.search(query=query, user_id=user_id, limit=limit)
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Troubleshooting Guide

| Issue | Possible Causes | Solutions |
|-------|----------------|-----------|
| **Initialization fails** | Missing dependencies, invalid config | Check dependencies, validate configuration |
| **Slow operations** | Large datasets, network latency | Implement caching, optimize queries |
| **Memory not found** | Invalid memory ID, deleted memory | Validate IDs, implement existence checks |
| **Connection timeouts** | Network issues, server overload | Implement retry logic, check network |
| **Out of memory errors** | Large batch operations | Process in smaller batches |

### Monitoring and Logging

Add comprehensive logging to your async memory operations:

```python Python
import logging
import time
from functools import wraps

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def log_async_operation(operation_name):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            logger.info(f"Starting {operation_name}")
            try:
                result = await func(*args, **kwargs)
                duration = time.time() - start_time
                logger.info(f"{operation_name} completed in {duration:.2f}s")
                return result
            except Exception as e:
                duration = time.time() - start_time
                logger.error(f"{operation_name} failed after {duration:.2f}s: {e}")
                raise
        return wrapper
    return decorator

@log_async_operation("Memory Add")
async def logged_memory_add(memory, messages, user_id):
    return await memory.add(messages=messages, user_id=user_id)
```

If you have any questions or need further assistance, please don't hesitate to reach out:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/open-source/features/custom-fact-extraction-prompt.mdx
================================================
---
title: Custom Fact Extraction Prompt
description: 'Enhance your product experience by adding custom fact extraction prompt tailored to your needs'
icon: "pencil"
iconType: "solid"
---

## Introduction to Custom Fact Extraction Prompt

Custom fact extraction prompt allow you to tailor the behavior of your Mem0 instance to specific use cases or domains. 
By defining it, you can control how information is extracted from the user's message.

To create an effective custom fact extraction prompt:
1. Be specific about the information to extract.
2. Provide few-shot examples to guide the LLM.
3. Ensure examples follow the format shown below.

Example of a custom fact extraction prompt:

<CodeGroup>
```python Python
custom_fact_extraction_prompt = """
Please only extract entities containing customer support information, order details, and user information. 
Here are some few shot examples:

Input: Hi.
Output: {{"facts" : []}}

Input: The weather is nice today.
Output: {{"facts" : []}}

Input: My order #12345 hasn't arrived yet.
Output: {{"facts" : ["Order #12345 not received"]}}

Input: I'm John Doe, and I'd like to return the shoes I bought last week.
Output: {{"facts" : ["Customer name: John Doe", "Wants to return shoes", "Purchase made last week"]}}

Input: I ordered a red shirt, size medium, but received a blue one instead.
Output: {{"facts" : ["Ordered red shirt, size medium", "Received blue shirt instead"]}}

Return the facts and customer information in a json format as shown above.
"""
```

```typescript TypeScript
const customPrompt = `
Please only extract entities containing customer support information, order details, and user information. 
Here are some few shot examples:

Input: Hi.
Output: {"facts" : []}

Input: The weather is nice today.
Output: {"facts" : []}

Input: My order #12345 hasn't arrived yet.
Output: {"facts" : ["Order #12345 not received"]}

Input: I am John Doe, and I would like to return the shoes I bought last week.
Output: {"facts" : ["Customer name: John Doe", "Wants to return shoes", "Purchase made last week"]}

Input: I ordered a red shirt, size medium, but received a blue one instead.
Output: {"facts" : ["Ordered red shirt, size medium", "Received blue shirt instead"]}

Return the facts and customer information in a json format as shown above.
`;
```
</CodeGroup>

Here we initialize the custom fact extraction prompt in the config:

<CodeGroup>
```python Python
from mem0 import Memory

config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o",
            "temperature": 0.2,
            "max_tokens": 2000,
        }
    },
    "custom_fact_extraction_prompt": custom_fact_extraction_prompt,
    "version": "v1.1"
}

m = Memory.from_config(config_dict=config)
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  version: 'v1.1',
  llm: {
    provider: 'openai',
    config: {
      apiKey: process.env.OPENAI_API_KEY || '',
      model: 'gpt-4-turbo-preview',
      temperature: 0.2,
      maxTokens: 1500,
    },
  },
  customPrompt: customPrompt
};

const memory = new Memory(config);
```
</CodeGroup>

### Example 1

In this example, we are adding a memory of a user ordering a laptop. As seen in the output, the custom prompt is used to extract the relevant information from the user's message.

<CodeGroup>
```python Python
m.add("Yesterday, I ordered a laptop, the order id is 12345", user_id="alice")
```

```typescript TypeScript
await memory.add('Yesterday, I ordered a laptop, the order id is 12345', { userId: "user123" });
```

```json Output
{
  "results": [
    {
      "memory": "Ordered a laptop",
      "event": "ADD"
    },
    {
      "memory": "Order ID: 12345",
      "event": "ADD"
    },
    {
      "memory": "Order placed yesterday",
      "event": "ADD"
    }
  ],
  "relations": []
}
```
</CodeGroup>

### Example 2

In this example, we are adding a memory of a user liking to go on hikes. This add message is not specific to the use-case mentioned in the custom prompt. 
Hence, the memory is not added.

<CodeGroup>
```python Python
m.add("I like going to hikes", user_id="alice")
```

```typescript TypeScript
await memory.add('I like going to hikes', { userId: "user123" });
```

```json Output
{
  "results": [],
  "relations": []
}
```
</CodeGroup>

The custom fact extraction prompt will process both the user and assistant messages to extract relevant information according to the defined format.



================================================
FILE: docs/open-source/features/custom-update-memory-prompt.mdx
================================================
---
title: Custom Update Memory Prompt
icon: "pencil"
iconType: "solid"
---


Update memory prompt is a prompt used to determine the action to be performed on the memory. 
By customizing this prompt, you can control how the memory is updated.  


## Introduction  
Mem0 memory system compares the newly retrieved facts with the existing memory and determines the action to be performed on the memory.  
The kinds of actions are:
- Add
    - Add the newly retrieved facts to the memory.
- Update
    - Update the existing memory with the newly retrieved facts.
- Delete
    - Delete the existing memory.
- No Change
    - Do not make any changes to the memory.
  
### Example
Example of a custom update memory prompt:

<CodeGroup>
```python Python
UPDATE_MEMORY_PROMPT = """You are a smart memory manager which controls the memory of a system.
You can perform four operations: (1) add into the memory, (2) update the memory, (3) delete from the memory, and (4) no change.

Based on the above four operations, the memory will change.

Compare newly retrieved facts with the existing memory. For each new fact, decide whether to:
- ADD: Add it to the memory as a new element
- UPDATE: Update an existing memory element
- DELETE: Delete an existing memory element
- NONE: Make no change (if the fact is already present or irrelevant)

There are specific guidelines to select which operation to perform:

1. **Add**: If the retrieved facts contain new information not present in the memory, then you have to add it by generating a new ID in the id field.
- **Example**:
    - Old Memory:
        [
            {
                "id" : "0",
                "text" : "User is a software engineer"
            }
        ]
    - Retrieved facts: ["Name is John"]
    - New Memory:
        {
            "memory" : [
                {
                    "id" : "0",
                    "text" : "User is a software engineer",
                    "event" : "NONE"
                },
                {
                    "id" : "1",
                    "text" : "Name is John",
                    "event" : "ADD"
                }
            ]

        }

2. **Update**: If the retrieved facts contain information that is already present in the memory but the information is totally different, then you have to update it. 
If the retrieved fact contains information that conveys the same thing as the elements present in the memory, then you have to keep the fact which has the most information. 
Example (a) -- if the memory contains "User likes to play cricket" and the retrieved fact is "Loves to play cricket with friends", then update the memory with the retrieved facts.
Example (b) -- if the memory contains "Likes cheese pizza" and the retrieved fact is "Loves cheese pizza", then you do not need to update it because they convey the same information.
If the direction is to update the memory, then you have to update it.
Please keep in mind while updating you have to keep the same ID.
Please note to return the IDs in the output from the input IDs only and do not generate any new ID.
- **Example**:
    - Old Memory:
        [
            {
                "id" : "0",
                "text" : "I really like cheese pizza"
            },
            {
                "id" : "1",
                "text" : "User is a software engineer"
            },
            {
                "id" : "2",
                "text" : "User likes to play cricket"
            }
        ]
    - Retrieved facts: ["Loves chicken pizza", "Loves to play cricket with friends"]
    - New Memory:
        {
        "memory" : [
                {
                    "id" : "0",
                    "text" : "Loves cheese and chicken pizza",
                    "event" : "UPDATE",
                    "old_memory" : "I really like cheese pizza"
                },
                {
                    "id" : "1",
                    "text" : "User is a software engineer",
                    "event" : "NONE"
                },
                {
                    "id" : "2",
                    "text" : "Loves to play cricket with friends",
                    "event" : "UPDATE",
                    "old_memory" : "User likes to play cricket"
                }
            ]
        }


3. **Delete**: If the retrieved facts contain information that contradicts the information present in the memory, then you have to delete it. Or if the direction is to delete the memory, then you have to delete it.
Please note to return the IDs in the output from the input IDs only and do not generate any new ID.
- **Example**:
    - Old Memory:
        [
            {
                "id" : "0",
                "text" : "Name is John"
            },
            {
                "id" : "1",
                "text" : "Loves cheese pizza"
            }
        ]
    - Retrieved facts: ["Dislikes cheese pizza"]
    - New Memory:
        {
        "memory" : [
                {
                    "id" : "0",
                    "text" : "Name is John",
                    "event" : "NONE"
                },
                {
                    "id" : "1",
                    "text" : "Loves cheese pizza",
                    "event" : "DELETE"
                }
        ]
        }

4. **No Change**: If the retrieved facts contain information that is already present in the memory, then you do not need to make any changes.
- **Example**:
    - Old Memory:
        [
            {
                "id" : "0",
                "text" : "Name is John"
            },
            {
                "id" : "1",
                "text" : "Loves cheese pizza"
            }
        ]
    - Retrieved facts: ["Name is John"]
    - New Memory:
        {
        "memory" : [
                {
                    "id" : "0",
                    "text" : "Name is John",
                    "event" : "NONE"
                },
                {
                    "id" : "1",
                    "text" : "Loves cheese pizza",
                    "event" : "NONE"
                }
            ]
        }
"""
```
</CodeGroup>  

## Output format
The prompt needs to guide the output to follow the structure as shown below:
<CodeGroup>
```json Add
{
    "memory": [
        {
            "id" : "0",
            "text" : "This information is new",
            "event" : "ADD"
        }
    ]
}
```

```json Update
{
    "memory": [
        {
            "id" : "0",
            "text" : "This information replaces the old information",
            "event" : "UPDATE",
            "old_memory" : "Old information"
        }
    ]
}
```

```json Delete
{
    "memory": [
        {
            "id" : "0",
            "text" : "This information will be deleted",
            "event" : "DELETE"
        }
    ]
}
```

```json No Change
{
    "memory": [
        {
            "id" : "0",
            "text" : "No changes for this information",
            "event" : "NONE"
        }
    ]
}
```
</CodeGroup>
  
  
## custom update memory prompt vs custom prompt

| Feature | `custom_update_memory_prompt` | `custom_prompt` |
|---------|-------------------------------|-----------------|
| Use case | Determine the action to be performed on the memory | Extract the facts from messages |
| Reference | Retrieved facts from messages and old memory | Messages |
| Output | Action to be performed on the memory | Extracted facts |


================================================
FILE: docs/open-source/features/multimodal-support.mdx
================================================
---
title: Multimodal Support
description: Integrate images into your interactions with Mem0
icon: "image"
iconType: "solid"
---

Mem0 extends its capabilities beyond text by supporting multimodal data. With this feature, you can seamlessly integrate images into your interactions—allowing Mem0 to extract relevant information and context from visual content.

## How It Works

When you submit an image, Mem0:
1. **Processes the visual content** using advanced vision models
2. **Extracts textual information** and relevant details from the image
3. **Stores the extracted information** as searchable memories
4. **Maintains context** between visual and textual interactions

This enables more comprehensive understanding of user interactions that include both text and visual elements.

<CodeGroup>
```python Python
import os
from mem0 import Memory

client = Memory()

messages = [
    {
        "role": "user",
        "content": "Hi, my name is Alice."
    },
    {
        "role": "assistant",
        "content": "Nice to meet you, Alice! What do you like to eat?"
    },
    {
        "role": "user",
        "content": {
            "type": "image_url",
            "image_url": {
                "url": "https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg"
            }
        }
    },
]

# Calling the add method to ingest messages into the memory system
client.add(messages, user_id="alice")
```

```json Output
{
  "results": [
    {
      "memory": "Name is Alice",
      "event": "ADD",
      "id": "7ae113a3-3cb5-46e9-b6f7-486c36391847"
    },
    {
      "memory": "Likes large pizza with toppings including cherry tomatoes, black olives, green spinach, yellow bell peppers, diced ham, and sliced mushrooms",
      "event": "ADD",
      "id": "56545065-7dee-4acf-8bf2-a5b2535aabb3"
    }
  ]
}
```
</CodeGroup>

## Supported Image Formats

Mem0 supports common image formats:
- **JPEG/JPG** - Standard photos and images
- **PNG** - Images with transparency support  
- **WebP** - Modern web-optimized format
- **GIF** - Animated and static graphics

## Local Files vs URLs

### Using Image URLs
Images can be referenced via publicly accessible URLs:

```python
content = {
    "type": "image_url",
    "image_url": {
        "url": "https://example.com/my-image.jpg"
    }
}
```

### Using Local Files
For local images, convert them to base64 format:

<CodeGroup>
```python Python
import base64
from mem0 import Memory

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

client = Memory()

# Encode local image
base64_image = encode_image("path/to/your/image.jpg")

messages = [
    {
        "role": "user", 
        "content": [
            {
                "type": "text",
                "text": "What's in this image?"
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            }
        ]
    }
]

client.add(messages, user_id="alice")
```

```javascript JavaScript
import fs from 'fs';
import { Memory } from 'mem0ai';

function encodeImage(imagePath) {
    const imageBuffer = fs.readFileSync(imagePath);
    return imageBuffer.toString('base64');
}

const client = new Memory();

// Encode local image
const base64Image = encodeImage("path/to/your/image.jpg");

const messages = [
    {
        role: "user",
        content: [
            {
                type: "text", 
                text: "What's in this image?"
            },
            {
                type: "image_url",
                image_url: {
                    url: `data:image/jpeg;base64,${base64Image}`
                }
            }
        ]
    }
];

await client.add(messages, { user_id: "alice" });
```
</CodeGroup>

## Advanced Examples

### Restaurant Menu Analysis
```python
from mem0 import Memory

client = Memory()

messages = [
    {
        "role": "user",
        "content": "I'm looking at this restaurant menu. Help me remember my preferences."
    },
    {
        "role": "user", 
        "content": {
            "type": "image_url",
            "image_url": {
                "url": "https://example.com/restaurant-menu.jpg"
            }
        }
    },
    {
        "role": "user",
        "content": "I'm allergic to peanuts and prefer vegetarian options."
    }
]

result = client.add(messages, user_id="user123")
print(result)
```

### Document Analysis
```python
# Analyzing receipts, invoices, or documents
messages = [
    {
        "role": "user",
        "content": "Store this receipt information for my expense tracking."
    },
    {
        "role": "user",
        "content": {
            "type": "image_url", 
            "image_url": {
                "url": "https://example.com/receipt.jpg"
            }
        }
    }
]

client.add(messages, user_id="user123")
```

## File Size and Performance Considerations

### Image Size Limits
- **Maximum file size**: 20MB per image
- **Recommended size**: Under 5MB for optimal performance
- **Resolution**: Images are automatically resized if needed

### Performance Tips
1. **Compress large images** before sending to reduce processing time
2. **Use appropriate formats** - JPEG for photos, PNG for graphics with text
3. **Batch processing** - Send multiple images in separate requests for better reliability

## Error Handling

Handle common errors when working with images:

<CodeGroup>
```python Python
from mem0 import Memory
from mem0.exceptions import InvalidImageError, FileSizeError

client = Memory()

try:
    messages = [{
        "role": "user",
        "content": {
            "type": "image_url",
            "image_url": {"url": "https://example.com/image.jpg"}
        }
    }]
    
    result = client.add(messages, user_id="user123")
    print("Image processed successfully")
    
except InvalidImageError:
    print("Invalid image format or corrupted file")
except FileSizeError:
    print("Image file too large")
except Exception as e:
    print(f"Unexpected error: {e}")
```

```javascript JavaScript
import { Memory } from 'mem0ai';

const client = new Memory();

try {
    const messages = [{
        role: "user",
        content: {
            type: "image_url",
            image_url: { url: "https://example.com/image.jpg" }
        }
    }];
    
    const result = await client.add(messages, { user_id: "user123" });
    console.log("Image processed successfully");
    
} catch (error) {
    if (error.type === 'invalid_image') {
        console.log("Invalid image format or corrupted file");
    } else if (error.type === 'file_size_exceeded') {
        console.log("Image file too large");
    } else {
        console.log(`Unexpected error: ${error.message}`);
    }
}
```
</CodeGroup>

## Best Practices

### Image Selection
- **Use high-quality images** with clear, readable text and details
- **Ensure good lighting** in photos for better text extraction
- **Avoid heavily stylized fonts** that may be difficult to read

### Memory Context
- **Provide context** about what information you want extracted
- **Combine with text** to give Mem0 better understanding of the image's purpose
- **Be specific** about what aspects of the image are important

### Privacy and Security
- **Avoid sensitive information** in images (SSN, passwords, private data)
- **Use secure image hosting** for URLs to prevent unauthorized access
- **Consider local processing** for highly sensitive visual content

Using these methods, you can seamlessly incorporate various visual content types into your interactions, further enhancing Mem0's multimodal capabilities for more comprehensive memory management.

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/open-source/features/openai_compatibility.mdx
================================================
---
title: OpenAI Compatibility
icon: "code"
iconType: "solid"
---

Mem0 can be easily integrated into chat applications to enhance conversational agents with structured memory. Mem0's APIs are designed to be compatible with OpenAI's, with the goal of making it easy to leverage Mem0 in applications you may have already built.

If you have a `Mem0 API key`, you can use it to initialize the client. Alternatively, you can initialize Mem0 without an API key if you're using it locally.

Mem0 supports several language models (LLMs) through integration with various [providers](https://litellm.vercel.app/docs/providers).

## Use Mem0 Platform

```python
from mem0.proxy.main import Mem0

client = Mem0(api_key="m0-xxx")

# First interaction: Storing user preferences
messages = [
    {
        "role": "user",
        "content": "I love indian food but I cannot eat pizza since allergic to cheese."
    },
]
user_id = "alice"
chat_completion = client.chat.completions.create(
    messages=messages,
    model="gpt-4o-mini",
    user_id=user_id
)
# Memory saved after this will look like: "Loves Indian food. Allergic to cheese and cannot eat pizza."

# Second interaction: Leveraging stored memory
messages = [
    {
        "role": "user",
        "content": "Suggest restaurants in San Francisco to eat.",
    }
]

chat_completion = client.chat.completions.create(
    messages=messages,
    model="gpt-4o-mini",
    user_id=user_id
)
print(chat_completion.choices[0].message.content)
# Answer: You might enjoy Indian restaurants in San Francisco, such as Amber India, Dosa, or Curry Up Now, which offer delicious options without cheese.
```

In this example, you can see how the second response is tailored based on the information provided in the first interaction. Mem0 remembers the user's preference for Indian food and their cheese allergy, using this information to provide more relevant and personalized restaurant suggestions in San Francisco.

### Use Mem0 OSS

```python
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "host": "localhost",
            "port": 6333,
        }
    },
}

client = Mem0(config=config)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "What's the capital of France?",
        }
    ],
    model="gpt-4o",
)
```

## Mem0 Params for Chat Completion

- `user_id` (Optional[str]): Identifier for the user.

- `agent_id` (Optional[str]): Identifier for the agent.

- `run_id` (Optional[str]): Identifier for the run.

- `metadata` (Optional[dict]): Additional metadata to be stored with the memory.

- `filters` (Optional[dict]): Filters to apply when searching for relevant memories.

- `limit` (Optional[int]): Maximum number of relevant memories to retrieve. Default is 10.


Other parameters are similar to OpenAI's API, making it easy to integrate Mem0 into your existing applications.



================================================
FILE: docs/open-source/features/overview.mdx
================================================
---
title: Overview
description: 'Build powerful AI applications with self-improving memory using Mem0 open-source'
icon: "eye"
iconType: "solid"
---

## Welcome to Mem0 Open Source

Mem0 is a self-improving memory layer for LLM applications that enables personalized AI experiences while saving costs and delighting users. The open-source version gives you complete control over your memory infrastructure.

## Why Choose Mem0 Open Source?

Mem0 open-source provides a powerful, flexible foundation for AI memory management with these key advantages:

1. **Complete Control**: Deploy and manage your own memory infrastructure with full customization capabilities. Perfect for organizations that need data sovereignty and custom integrations.

2. **Flexible Architecture**: Choose from multiple vector databases (Pinecone, Qdrant, Weaviate, Chroma, PGVector), graph stores (Neo4j, Memgraph), and embedding models to fit your specific needs.

3. **Advanced Memory Organization**: Organize memories using `user_id`, `agent_id`, and `run_id` parameters for sophisticated multi-agent, multi-session applications with precise context control.

4. **Rich Integration Ecosystem**: Seamlessly integrate with popular frameworks like LangChain, LlamaIndex, AutoGen, CrewAI, and Vercel AI SDK.

## Core Features

### Memory Management
- **Synchronous & Asynchronous Operations**: Choose between sync and async memory operations based on your application needs
- **Smart Memory Retrieval**: Intelligent search and retrieval with semantic understanding
- **Memory Persistence**: Long-term storage with automatic optimization and cleanup

### Advanced Organization
- **User Context**: Organize memories by user for personalized experiences
- **Agent Isolation**: Separate memories by AI agent for specialized knowledge domains
- **Session Tracking**: Use run IDs to maintain context across different conversation sessions

### Flexible Storage
- **Vector Databases**: Support for Pinecone, Qdrant, Weaviate, Chroma, and PGVector
- **Graph Stores**: Neo4j and Memgraph integration for relationship-based memory
- **Embedding Models**: Multiple embedding providers for optimal performance

## Getting Started

Choose your preferred approach:

- **[Python Quickstart](../python-quickstart)**: Get started with Python SDK
- **[Node.js Quickstart](../node-quickstart)**: Use Mem0 with Node.js/TypeScript
- **[Examples](../examples)**: Explore real-world use cases and implementations

## Next Steps

- Explore [specific features](./async-memory) in detail
- Learn about [graph memory](../graph_memory/overview) capabilities
- Set up [vector databases](../components/vectordbs/overview) and [LLM integrations](../components/llms/overview)
- Check out our [examples](../examples) for practical implementations
- Join our [Discord community](https://mem0.dev/DiD) for support

We're excited to see what you'll build with Mem0 open-source. Let's create smarter, more personalized AI experiences together!



================================================
FILE: docs/open-source/features/rest-api.mdx
================================================
---
title: REST API Server
icon: "server"
iconType: "solid"
---

Mem0 provides a REST API server (written using FastAPI). Users can perform all operations through REST endpoints. The API also includes OpenAPI documentation, accessible at `/docs` when the server is running.

<Frame caption="APIs supported by Mem0 REST API Server">
    <img src="/images/rest-api-server.png"/>
</Frame>

## Features

- **Create memories:** Create memories based on messages for a user, agent, or run.
- **Retrieve memories:** Get all memories for a given user, agent, or run.
- **Search memories:** Search stored memories based on a query.
- **Update memories:** Update an existing memory.
- **Delete memories:** Delete a specific memory or all memories for a user, agent, or run.
- **Reset memories:** Reset all memories for a user, agent, or run.
- **OpenAPI Documentation:** Accessible via `/docs` endpoint.

## Running Locally

<Tabs>
    <Tab title="With Docker Compose">
        The Development Docker Compose comes pre-configured with postgres pgvector, neo4j and a `server/history/history.db` volume for the history database.

        The only required environment variable to run the server is `OPENAI_API_KEY`.

        1. Create a `.env` file in the `server/` directory and set your environment variables. For example:

        ```txt
        OPENAI_API_KEY=your-openai-api-key
        ```

        2. Run the Docker container using Docker Compose:

        ```bash
        cd server
        docker compose up
        ```

        3. Access the API at http://localhost:8888.

        4. Making changes to the server code or the library code will automatically reload the server.
    </Tab>

    <Tab title="With Docker">

        1. Create a `.env` file in the current directory and set your environment variables. For example:

        ```txt
        OPENAI_API_KEY=your-openai-api-key
        ```

        2. Either pull the docker image from docker hub or build the docker image locally.

        <Tabs>
            <Tab title="Pull from Docker Hub">

                ```bash
                docker pull mem0/mem0-api-server
                ```

            </Tab>

            <Tab title="Build Locally">

                ```bash
                docker build -t mem0-api-server .
                ```

            </Tab>
        </Tabs>

        3. Run the Docker container:

        ``` bash
        docker run -p 8000:8000 mem0-api-server --env-file .env
        ```

        4. Access the API at http://localhost:8000.

    </Tab>

    <Tab title="Without Docker">

        1. Create a `.env` file in the current directory and set your environment variables. For example:

        ```txt
        OPENAI_API_KEY=your-openai-api-key
        ```

        2. Install dependencies:

        ```bash
        pip install -r requirements.txt
        ```

        3. Start the FastAPI server:

        ```bash
        uvicorn main:app --reload
        ```

        4. Access the API at http://localhost:8000.

    </Tab>
</Tabs>

## Usage

Once the server is running (locally or via Docker), you can interact with it using any REST client or through your preferred programming language (e.g., Go, Java, etc.). You can test out the APIs using the OpenAPI documentation at [http://localhost:8000/docs](http://localhost:8000/docs) endpoint.



================================================
FILE: docs/open-source/graph_memory/features.mdx
================================================
---
title: Features
description: 'Graph Memory features'
icon: "list-check"
iconType: "solid"
---

Graph Memory is a powerful feature that allows users to create and utilize complex relationships between pieces of information.

## Graph Memory supports the following features:

### Using Custom Prompts

Users can specify a custom prompt that will be used to extract specific entities from the given input text.
This allows for more targeted and relevant information extraction based on the user's needs.
Here's an example of how to specify a custom prompt:

<CodeGroup>
    ```python Python
    from mem0 import Memory

    config = {
        "graph_store": {
            "provider": "neo4j",
            "config": {
                "url": "neo4j+s://xxx",
                "username": "neo4j",
                "password": "xxx"
            },
            "custom_prompt": "Please only extract entities containing sports related relationships and nothing else.",
        }
    }

    m = Memory.from_config(config_dict=config)
    ```

    ```typescript TypeScript
    import { Memory } from "mem0ai/oss";

    const config = {
        graphStore: {
            provider: "neo4j",
            config: {
                url: "neo4j+s://xxx",
                username: "neo4j",
                password: "xxx",
            },
            customPrompt: "Please only extract entities containing sports related relationships and nothing else.",
        }
    }

    const memory = new Memory(config);
    ```
</CodeGroup>

If you want to use a managed version of Mem0, please check out [Mem0](https://mem0.dev/pd). If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/open-source/graph_memory/overview.mdx
================================================
---
title: Overview
description: 'Enhance your memory system with graph-based knowledge representation and retrieval'
icon: "info"
iconType: "solid"
---

Mem0 now supports **Graph Memory**.
With Graph Memory, users can now create and utilize complex relationships between pieces of information, allowing for more nuanced and context-aware responses.
This integration enables users to leverage the strengths of both vector-based and graph-based approaches, resulting in more accurate and comprehensive information retrieval and generation.

<Note>
NodeSDK now supports Graph Memory. 🎉
</Note>

## Installation

To use Mem0 with Graph Memory support, install it using pip:

<CodeGroup>
```bash Python
pip install "mem0ai[graph]"
```

```bash TypeScript
npm install mem0ai
```
</CodeGroup>

This command installs Mem0 along with the necessary dependencies for graph functionality.

Try Graph Memory on Google Colab.
<a target="_blank" href="https://colab.research.google.com/drive/1PfIGVHnliIlG2v8cx0g45TF0US-jRPZ1?usp=sharing">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>


<iframe
width="100%"
height="400"
src="https://www.youtube.com/embed/u_ZAqNNVtXA"
title="YouTube video player"
frameborder="0"
allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
allowfullscreen
></iframe>

## Initialize Graph Memory

To initialize Graph Memory you'll need to set up your configuration with graph
store providers. Currently, we support [Neo4j](#initialize-neo4j), [Memgraph](#initialize-memgraph), [Neptune Analytics](#initialize-neptune-analytics), and [Kuzu](#initialize-kuzu) as graph store providers.


### Initialize Neo4j

You can setup [Neo4j](https://neo4j.com/) locally or use the hosted [Neo4j AuraDB](https://neo4j.com/product/auradb/).

<Note>If you are using Neo4j locally, then you need to install [APOC plugins](https://neo4j.com/labs/apoc/4.1/installation/).</Note>

User can also customize the LLM for Graph Memory from the [Supported LLM list](https://docs.mem0.ai/components/llms/overview) with three levels of configuration:

1. **Main Configuration**: If `llm` is set in the main config, it will be used for all graph operations.
2. **Graph Store Configuration**: If `llm` is set in the graph_store config, it will override the main config `llm` and be used specifically for graph operations.
3. **Default Configuration**: If no custom LLM is set, the default LLM (`gpt-4o-2024-08-06`) will be used for all graph operations.

Here's how you can do it:


<CodeGroup>
```python Python
from mem0 import Memory

config = {
    "graph_store": {
        "provider": "neo4j",
        "config": {
            "url": "neo4j+s://xxx",
            "username": "neo4j",
            "password": "xxx"
        }
    }
}

m = Memory.from_config(config_dict=config)
```

```typescript TypeScript
import { Memory } from "mem0ai/oss";

const config = {
    enableGraph: true,
    graphStore: {
        provider: "neo4j",
        config: {
            url: "neo4j+s://xxx",
            username: "neo4j",
            password: "xxx",
        }
    }
}

const memory = new Memory(config);
```

```python Python (Advanced)
config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o",
            "temperature": 0.2,
            "max_tokens": 2000,
        }
    },
    "graph_store": {
        "provider": "neo4j",
        "config": {
            "url": "neo4j+s://xxx",
            "username": "neo4j",
            "password": "xxx"
        },
        "llm" : {
            "provider": "openai",
            "config": {
                "model": "gpt-4o-mini",
                "temperature": 0.0,
            }
        }
    }
}

m = Memory.from_config(config_dict=config)
```

```typescript TypeScript (Advanced)
const config = {
    llm: {
        provider: "openai",
        config: {
            model: "gpt-4o",
            temperature: 0.2,
            max_tokens: 2000,
        }
    },
    enableGraph: true,
    graphStore: {
        provider: "neo4j",
        config: {
            url: "neo4j+s://xxx",
            username: "neo4j",
            password: "xxx",
        },
        llm: {
            provider: "openai",
            config: {
                model: "gpt-4o-mini",
                temperature: 0.0,
            }
        }
    }
}

const memory = new Memory(config);
```
</CodeGroup>

<Note>
If you are using NodeSDK, you need to pass `enableGraph` as `true` in the `config` object.
</Note>

### Initialize Memgraph

Run Memgraph with Docker:

```bash
docker run -p 7687:7687 memgraph/memgraph-mage:latest --schema-info-enabled=True
```

The `--schema-info-enabled` flag is set to `True` for more performant schema
generation.

Additional information can be found on [Memgraph
documentation](https://memgraph.com/docs).

User can also customize the LLM for Graph Memory from the [Supported LLM list](https://docs.mem0.ai/components/llms/overview) with three levels of configuration:

1. **Main Configuration**: If `llm` is set in the main config, it will be used for all graph operations.
2. **Graph Store Configuration**: If `llm` is set in the graph_store config, it will override the main config `llm` and be used specifically for graph operations.
3. **Default Configuration**: If no custom LLM is set, the default LLM (`gpt-4o-2024-08-06`) will be used for all graph operations.

Here's how you can do it:


<CodeGroup>
```python Python
from mem0 import Memory

config = {
    "graph_store": {
        "provider": "memgraph",
        "config": {
            "url": "bolt://localhost:7687",
            "username": "memgraph",
            "password": "xxx",
        },
    },
}

m = Memory.from_config(config_dict=config)
```

```python Python (Advanced)
config = {
    "embedder": {
        "provider": "openai",
        "config": {"model": "text-embedding-3-large", "embedding_dims": 1536},
    },
    "graph_store": {
        "provider": "memgraph",
        "config": {
            "url": "bolt://localhost:7687",
            "username": "memgraph",
            "password": "xxx"
        }
    }
}

m = Memory.from_config(config_dict=config)
```
</CodeGroup>

### Initialize Neptune Analytics

Mem0 now supports Amazon Neptune Analytics as a graph store provider. This integration allows you to use Neptune Analytics for storing and querying graph-based memories.

You can use Neptune Analytics as part of an Amazon tech stack [Setup AWS Bedrock, AOSS, and Neptune](https://docs.mem0.ai/examples/aws_example#aws-bedrock-and-aoss)

#### Instance Setup

Create an Amazon Neptune Analytics instance in your AWS account following the [AWS documentation](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/get-started.html).
- Public connectivity is not enabled by default, and if accessing from outside a VPC, it needs to be enabled.
- Once the Amazon Neptune Analytics instance is available, you will need the graph-identifier to connect.
- The Neptune Analytics instance must be created using the same vector dimensions as the embedding model creates. See: https://docs.aws.amazon.com/neptune-analytics/latest/userguide/vector-index.html

#### Attach Credentials

 Configure your AWS credentials with access to your Amazon Neptune Analytics resources by following the [Configuration and credentials precedence](https://docs.aws.amazon.com/cli/v1/userguide/cli-chap-configure.html#configure-precedence).
- For example, add your SSH access key session token via environment variables:
```bash
export AWS_ACCESS_KEY_ID=your-access-key
export AWS_SECRET_ACCESS_KEY=your-secret-key
export AWS_SESSION_TOKEN=your-session-token
export AWS_DEFAULT_REGION=your-region
```
- The IAM user or role making the request must have a policy attached that allows one of the following IAM actions in that neptune-graph:
  - neptune-graph:ReadDataViaQuery
  - neptune-graph:WriteDataViaQuery
  - neptune-graph:DeleteDataViaQuery

#### Usage

The Neptune memory store uses AWS LangChain Python API to connect to Neptune instances.  For additional configuration options for connecting to your Amazon Neptune Analytics instance see [AWS LangChain API documentation](https://python.langchain.com/api_reference/aws/graphs/langchain_aws.graphs.neptune_graph.NeptuneAnalyticsGraph.html).

<CodeGroup>
```python Python
from mem0 import Memory

# Provided neptune-graph instance must have the same vector dimensions as the embedder provider.
config = {
    "graph_store": {
        "provider": "neptune",
        "config": {
            "endpoint": "neptune-graph://<GRAPH_ID>",
        },
    },
}

m = Memory.from_config(config_dict=config)
```
</CodeGroup>

#### Troubleshooting

- For issues connecting to Amazon Neptune Analytics, please refer to the [Connecting to a graph guide](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/gettingStarted-connecting.html).

- For issues related to authentication, refer to the [boto3 client configuration options](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html).

- For more details on how to connect, configure, and use the graph_memory graph store, see the [Neptune Analytics example notebook](examples/graph-db-demo/neptune-analytics-example.ipynb).

### Initialize Kuzu

[Kuzu](https://kuzudb.com) is a fully local in-process graph database system that runs openCypher queries.
Kuzu comes embedded into the Python package and there is no additional setup required.

Kuzu needs a path to a file where it will store the graph database. For example:

<CodeGroup>
```python Python
config = {
    "graph_store": {
        "provider": "kuzu",
        "config": {
            "db": "/tmp/mem0-example.kuzu"
        }
    }
}
```
</CodeGroup>

Kuzu can also store its database in memory. Note that in this mode, all stored memories will be lost
after the program has finished executing.

<CodeGroup>
```python Python
config = {
    "graph_store": {
        "provider": "kuzu",
        "config": {
            "db": ":memory:"
        }
    }
}
```
</CodeGroup>

You can then use the above configuration in the usual way:

<CodeGroup>
```python Python
from mem0 import Memory

m = Memory.from_config(config_dict=config)
```
</CodeGroup>

## Graph Operations
Mem0's graph memory supports the following operations:

### Add Memories

<Note>
Mem0 with Graph Memory supports "user_id", "agent_id", and "run_id" parameters. You can use any combination of these to organize your memories. Use "userId", "agentId", and "runId" in NodeSDK.
</Note>

<CodeGroup>
```python Python
# Using only user_id
m.add("I like pizza", user_id="alice")

# Using both user_id and agent_id
m.add("I like pizza", user_id="alice", agent_id="food-assistant")

# Using all three parameters for maximum organization
m.add("I like pizza", user_id="alice", agent_id="food-assistant", run_id="session-123")
```

```typescript TypeScript
// Using only userId
memory.add("I like pizza", { userId: "alice" });

// Using both userId and agentId
memory.add("I like pizza", { userId: "alice", agentId: "food-assistant" });
```

```json Output
{'message': 'ok'}
```
</CodeGroup>


### Get all memories

<CodeGroup>
```python Python
# Get all memories for a user
m.get_all(user_id="alice")

# Get all memories for a specific agent belonging to a user
m.get_all(user_id="alice", agent_id="food-assistant")

# Get all memories for a specific run/session
m.get_all(user_id="alice", run_id="session-123")

# Get all memories for a specific agent and run combination
m.get_all(user_id="alice", agent_id="food-assistant", run_id="session-123")
```

```typescript TypeScript
// Get all memories for a user
memory.getAll({ userId: "alice" });

// Get all memories for a specific agent belonging to a user
memory.getAll({ userId: "alice", agentId: "food-assistant" });
```

```json Output
{
    'memories': [
        {
            'id': 'de69f426-0350-4101-9d0e-5055e34976a5',
            'memory': 'Likes pizza',
            'hash': '92128989705eef03ce31c462e198b47d',
            'metadata': None,
            'created_at': '2024-08-20T14:09:27.588719-07:00',
            'updated_at': None,
            'user_id': 'alice',
            'agent_id': 'food-assistant'
        }
    ],
    'entities': [
        {
            'source': 'alice',
            'relationship': 'likes',
            'target': 'pizza'
        }
    ]
}
```
</CodeGroup>

### Search Memories

<CodeGroup>
```python Python
# Search memories for a user
m.search("tell me my name.", user_id="alice")

# Search memories for a specific agent belonging to a user
m.search("tell me my name.", user_id="alice", agent_id="food-assistant")

# Search memories for a specific run/session
m.search("tell me my name.", user_id="alice", run_id="session-123")

# Search memories for a specific agent and run combination
m.search("tell me my name.", user_id="alice", agent_id="food-assistant", run_id="session-123")
```

```typescript TypeScript
// Search memories for a user
memory.search("tell me my name.", { userId: "alice" });

// Search memories for a specific agent belonging to a user
memory.search("tell me my name.", { userId: "alice", agentId: "food-assistant" });
```

```json Output
{
    'memories': [
        {
            'id': 'de69f426-0350-4101-9d0e-5055e34976a5',
            'memory': 'Likes pizza',
            'hash': '92128989705eef03ce31c462e198b47d',
            'metadata': None,
            'created_at': '2024-08-20T14:09:27.588719-07:00',
            'updated_at': None,
            'user_id': 'alice',
            'agent_id': 'food-assistant'
        }
    ],
    'entities': [
        {
            'source': 'alice',
            'relationship': 'likes',
            'target': 'pizza'
        }
    ]
}
```
</CodeGroup>


### Delete all Memories

<CodeGroup>
```python Python
# Delete all memories for a user
m.delete_all(user_id="alice")

# Delete all memories for a specific agent belonging to a user
m.delete_all(user_id="alice", agent_id="food-assistant")
```

```typescript TypeScript
// Delete all memories for a user
memory.deleteAll({ userId: "alice" });

// Delete all memories for a specific agent belonging to a user
memory.deleteAll({ userId: "alice", agentId: "food-assistant" });
```
</CodeGroup>

# Example Usage
Here's an example of how to use Mem0's graph operations:

1. First, we'll add some memories for a user named Alice.
2. Then, we'll visualize how the graph evolves as we add more memories.
3. You'll see how entities and relationships are automatically extracted and connected in the graph.

### Add Memories

Below are the steps to add memories and visualize the graph:

<Steps>
  <Step title="Add memory 'I like going to hikes'">

<CodeGroup>
```python Python
m.add("I like going to hikes", user_id="alice123")
```

```typescript TypeScript
memory.add("I like going to hikes", { userId: "alice123" });
```
</CodeGroup>
![Graph Memory Visualization](/images/graph_memory/graph_example1.png)

</Step>
<Step title="Add memory 'I love to play badminton'">

<CodeGroup>
```python Python
m.add("I love to play badminton", user_id="alice123")
```

```typescript TypeScript
memory.add("I love to play badminton", { userId: "alice123" });
```
</CodeGroup>

![Graph Memory Visualization](/images/graph_memory/graph_example2.png)

</Step>

<Step title="Add memory 'I hate playing badminton'">

<CodeGroup>
```python Python
m.add("I hate playing badminton", user_id="alice123")
```

```typescript TypeScript
memory.add("I hate playing badminton", { userId: "alice123" });
```
</CodeGroup>

![Graph Memory Visualization](/images/graph_memory/graph_example3.png)

</Step>

<Step title="Add memory 'My friend name is john and john has a dog named tommy'">

<CodeGroup>
```python Python
m.add("My friend name is john and john has a dog named tommy", user_id="alice123")
```

```typescript TypeScript
memory.add("My friend name is john and john has a dog named tommy", { userId: "alice123" });
```
</CodeGroup>

![Graph Memory Visualization](/images/graph_memory/graph_example4.png)

</Step>

<Step title="Add memory 'My name is Alice'">

<CodeGroup>
```python Python
m.add("My name is Alice", user_id="alice123")
```

```typescript TypeScript
memory.add("My name is Alice", { userId: "alice123" });
```
</CodeGroup>

![Graph Memory Visualization](/images/graph_memory/graph_example5.png)

</Step>

<Step title="Add memory 'John loves to hike and Harry loves to hike as well'">

<CodeGroup>
```python Python
m.add("John loves to hike and Harry loves to hike as well", user_id="alice123")
```

```typescript TypeScript
memory.add("John loves to hike and Harry loves to hike as well", { userId: "alice123" });
```
</CodeGroup>

![Graph Memory Visualization](/images/graph_memory/graph_example6.png)

</Step>

<Step title="Add memory 'My friend peter is the spiderman'">

<CodeGroup>
```python Python
m.add("My friend peter is the spiderman", user_id="alice123")
```

```typescript TypeScript
memory.add("My friend peter is the spiderman", { userId: "alice123" });
```
</CodeGroup>

![Graph Memory Visualization](/images/graph_memory/graph_example7.png)

</Step>

</Steps>


### Search Memories

<CodeGroup>
```python Python
m.search("What is my name?", user_id="alice123")
```

```typescript TypeScript
memory.search("What is my name?", { userId: "alice123" });
```

```json Output
{
    'memories': [...],
    'entities': [
        {'source': 'alice123', 'relation': 'dislikes_playing','destination': 'badminton'},
        {'source': 'alice123', 'relation': 'friend', 'destination': 'peter'},
        {'source': 'alice123', 'relation': 'friend', 'destination': 'john'},
        {'source': 'alice123', 'relation': 'has_name', 'destination': 'alice'},
        {'source': 'alice123', 'relation': 'likes', 'destination': 'hiking'}
    ]
}
```
</CodeGroup>

Below graph visualization shows what nodes and relationships are fetched from the graph for the provided query.

![Graph Memory Visualization](/images/graph_memory/graph_example8.png)

<CodeGroup>
```python Python
m.search("Who is spiderman?", user_id="alice123")
```

```typescript TypeScript
memory.search("Who is spiderman?", { userId: "alice123" });
```

```json Output
{
    'memories': [...],
    'entities': [
        {'source': 'peter', 'relation': 'identity','destination': 'spiderman'}
    ]
}
```
</CodeGroup>

![Graph Memory Visualization](/images/graph_memory/graph_example9.png)

> **Note:** The Graph Memory implementation is not standalone. You will be adding/retrieving memories to the vector store and the graph store simultaneously.

## Using Multiple Agents with Graph Memory


When working with multiple agents and sessions, you can use the "agent_id" and "run_id" parameters to organize memories by user, agent, and run context. This allows you to:

1. Create agent-specific knowledge graphs
2. Share common knowledge between agents
3. Isolate sensitive or specialized information to specific agents
4. Track conversation sessions and runs separately
5. Maintain context across different execution contexts

### Example: Multi-Agent Setup

<CodeGroup>
```python Python
# Add memories for different agents
m.add("I prefer Italian cuisine", user_id="bob", agent_id="food-assistant")
m.add("I'm allergic to peanuts", user_id="bob", agent_id="health-assistant")
m.add("I live in Seattle", user_id="bob")  # Shared across all agents

# Add memories for specific runs/sessions
m.add("Current session: discussing dinner plans", user_id="bob", agent_id="food-assistant", run_id="dinner-session-001")
m.add("Previous session: allergy consultation", user_id="bob", agent_id="health-assistant", run_id="health-session-001")

# Search within specific agent context
food_preferences = m.search("What food do I like?", user_id="bob", agent_id="food-assistant")
health_info = m.search("What are my allergies?", user_id="bob", agent_id="health-assistant")
location = m.search("Where do I live?", user_id="bob")  # Searches across all agents

# Search within specific run context
current_session = m.search("What are we discussing?", user_id="bob", run_id="dinner-session-001")
```

```typescript TypeScript
// Add memories for different agents
memory.add("I prefer Italian cuisine", { userId: "bob", agentId: "food-assistant" });
memory.add("I'm allergic to peanuts", { userId: "bob", agentId: "health-assistant" });
memory.add("I live in Seattle", { userId: "bob" });  // Shared across all agents

// Search within specific agent context
const foodPreferences = memory.search("What food do I like?", { userId: "bob", agentId: "food-assistant" });
const healthInfo = memory.search("What are my allergies?", { userId: "bob", agentId: "health-assistant" });
const location = memory.search("Where do I live?", { userId: "bob" });  // Searches across all agents
```
</CodeGroup>

If you want to use a managed version of Mem0, please check out [Mem0](https://mem0.dev/pd). If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/openmemory/integrations.mdx
================================================
---
title: MCP Client Integration Guide
icon: "plug"
iconType: "solid"
---

## Connecting an MCP Client

Once your OpenMemory server is running locally, you can connect any compatible MCP client to your personal memory stream. This enables a seamless memory layer integration for AI tools and agents.

Ensure the following environment variables are correctly set in your configuration files:

**In `/ui/.env`:**
```env
NEXT_PUBLIC_API_URL=http://localhost:8765
NEXT_PUBLIC_USER_ID=<user-id>
```

**In `/api/.env`:**
```env
OPENAI_API_KEY=sk-xxx
USER=<user-id>
```

These values define where your MCP server is running and which user's memory is accessed.

### MCP Client Setup

Use the following one step command to configure OpenMemory Local MCP to a client. The general command format is as follows:

```bash
npx @openmemory/install local http://localhost:8765/mcp/<client-name>/sse/<user-id> --client <client-name>
```

Replace `<client-name>` with the desired client name and `<user-id>` with the value specified in your environment variables.

### Example Commands for Supported Clients

| Client      | Command |
|-------------|---------|
| Claude      | `npx install-mcp http://localhost:8765/mcp/claude/sse/<user-id> --client claude` |
| Cursor      | `npx install-mcp http://localhost:8765/mcp/cursor/sse/<user-id> --client cursor` |
| Cline       | `npx install-mcp http://localhost:8765/mcp/cline/sse/<user-id> --client cline` |
| RooCline    | `npx install-mcp http://localhost:8765/mcp/roocline/sse/<user-id> --client roocline` |
| Windsurf    | `npx install-mcp http://localhost:8765/mcp/windsurf/sse/<user-id> --client windsurf` |
| Witsy       | `npx install-mcp http://localhost:8765/mcp/witsy/sse/<user-id> --client witsy` |
| Enconvo     | `npx install-mcp http://localhost:8765/mcp/enconvo/sse/<user-id> --client enconvo` |
| Augment     | `npx install-mcp http://localhost:8765/mcp/augment/sse/<user-id> --client augment` |

### What This Does

Running one of the above commands registers the specified MCP client and connects it to your OpenMemory server. This enables the client to stream and store contextual memory for the provided user ID.

The connection status and memory activity can be monitored via the OpenMemory UI at [http://localhost:3000](http://localhost:3000).


================================================
FILE: docs/openmemory/overview.mdx
================================================
---
title: Overview
icon: "info"
iconType: "solid"
---

## 🚀 Hosted OpenMemory MCP Now Available!

#### Sign Up Now - [app.openmemory.dev](https://app.openmemory.dev)

Everything you love about OpenMemory MCP but with zero setup.

✅ Works with all MCP-compatible tools (Claude Desktop, Cursor...)  
✅ Same standard memory ops: `add_memories`, `search_memory`, etc  
✅ One-click provisioning, no Docker required  
✅ Powered by Mem0  

Add shared, persistent, low-friction memory to your MCP-compatible clients in seconds.

### 🌟 Get Started Now
**Sign up and get your access key at [app.openmemory.dev](https://app.openmemory.dev)**

Example installation: `npx @openmemory/install --client claude --env OPENMEMORY_API_KEY=your-key`

OpenMemory is a local memory infrastructure powered by Mem0 that lets you carry your memory across any AI app. It provides a unified memory layer that stays with you, enabling agents and assistants to remember what matters across applications.

<img src="https://github.com/user-attachments/assets/3c701757-ad82-4afa-bfbe-e049c2b4320b" alt="OpenMemory UI" />

## What is the OpenMemory MCP Server

The OpenMemory MCP Server is a private, local-first memory server that creates a shared, persistent memory layer for your MCP-compatible tools. This runs entirely on your machine, enabling seamless context handoff across tools. Whether you're switching between development, planning, or debugging environments, your AI assistants can access relevant memory without needing repeated instructions.

The OpenMemory MCP Server ensures all memory stays local, structured, and under your control with no cloud sync or external storage.

## OpenMemory Easy Setup

### Prerequisites
- Docker
- OpenAI API Key

You can quickly run OpenMemory by running the following command:

```bash
curl -sL https://raw.githubusercontent.com/mem0ai/mem0/main/openmemory/run.sh | bash
```

You should set the `OPENAI_API_KEY` as a global environment variable:

```bash
export OPENAI_API_KEY=your_api_key
```

You can also set the `OPENAI_API_KEY` as a parameter to the script:

```bash
curl -sL https://raw.githubusercontent.com/mem0ai/mem0/main/openmemory/run.sh | OPENAI_API_KEY=your_api_key bash
```

This will start the OpenMemory server and the OpenMemory UI. Deleting the container will lead to the deletion of the memory store.
We suggest you follow the instructions [here](/openmemory/quickstart#setting-up-openmemory) to set up OpenMemory on your local machine, with more persistent memory store.

## How the OpenMemory MCP Server Works

Built around the Model Context Protocol (MCP), the OpenMemory MCP Server exposes a standardized set of memory tools:
- `add_memories`: Store new memory objects
- `search_memory`: Retrieve relevant memories
- `list_memories`: View all stored memory
- `delete_all_memories`: Clear memory entirely

Any MCP-compatible tool can connect to the server and use these APIs to persist and access memory.

## What It Enables

### Cross-Client Memory Access
Store context in Cursor and retrieve it later in Claude or Windsurf without repeating yourself.

### Fully Local Memory Store
All memory is stored on your machine. Nothing goes to the cloud. You maintain full ownership and control.

### Unified Memory UI
The built-in OpenMemory dashboard provides a central view of everything stored. Add, browse, delete and control memory access to clients directly from the dashboard.

## Supported Clients

The OpenMemory MCP Server is compatible with any client that supports the Model Context Protocol. This includes:
- Cursor
- Claude Desktop
- Windsurf
- Cline, and more.

As more AI systems adopt MCP, your private memory becomes more valuable.

## Real-World Examples

### Scenario 1: Cross-Tool Project Flow 
Define technical requirements of a project in Claude Desktop. Build in Cursor. Debug issues in Windsurf - all with shared context passed through OpenMemory.

### Scenario 2: Preferences That Persist 
Set your preferred code style or tone in one tool. When you switch to another MCP client, it can access those same preferences without redefining them.

### Scenario 3: Project Knowledge
Save important project details once, then access them from any compatible AI tool, no more repetitive explanations.

## Conclusion

The OpenMemory MCP Server brings memory to MCP-compatible tools without giving up control or privacy. It solves a foundational limitation in modern LLM workflows: the loss of context across tools, sessions, and environments.

By standardizing memory operations and keeping all data local, it reduces token overhead, improves performance, and unlocks more intelligent interactions across the growing ecosystem of AI assistants.

This is just the beginning. The MCP server is the first core layer in the OpenMemory platform - a broader effort to make memory portable, private, and interoperable across AI systems.

## Getting Started Today

- Repository: [GitHub](https://github.com/mem0ai/mem0/tree/main/openmemory)
- Join our community: [Discord](https://discord.gg/6PzXDgEjG5)

With OpenMemory, your AI memories stay private, portable, and under your control, exactly where they belong.

OpenMemory: Your memories, your control.

## Contributing

OpenMemory is open source and we welcome contributions. Please see the [CONTRIBUTING.md](https://github.com/mem0ai/mem0/blob/main/openmemory/CONTRIBUTING.md) file for more information.


================================================
FILE: docs/openmemory/quickstart.mdx
================================================
---
title: Quickstart
icon: "terminal"
iconType: "solid"
---

## 🚀 Hosted OpenMemory MCP Now Available!

#### Sign Up Now - [app.openmemory.dev](https://app.openmemory.dev)

Everything you love about OpenMemory MCP but with zero setup.

✅ Works with all MCP-compatible tools (Claude Desktop, Cursor...)  
✅ Same standard memory ops: `add_memories`, `search_memory`, etc  
✅ One-click provisioning, no Docker required  
✅ Powered by Mem0  

Add shared, persistent, low-friction memory to your MCP-compatible clients in seconds.

### 🌟 Get Started Now
**Sign up and get your access key at [app.openmemory.dev](https://app.openmemory.dev)**

Example installation: `npx @openmemory/install --client claude --env OPENMEMORY_API_KEY=your-key`

## Getting Started with Hosted OpenMemory

The fastest way to get started is with our hosted version - no setup required:

### 1. Get your API key
Visit [app.openmemory.dev](https://app.openmemory.dev) to sign up and get your `OPENMEMORY_API_KEY`.

### 2. Install and connect to your preferred client
Example commands (replace `your-key` with your actual API key):

For Claude Desktop: `npx @openmemory/install --client claude --env OPENMEMORY_API_KEY=your-key`

For Cursor: `npx @openmemory/install --client cursor --env OPENMEMORY_API_KEY=your-key`

For Windsurf: `npx @openmemory/install --client windsurf --env OPENMEMORY_API_KEY=your-key`

That's it! Your AI client now has persistent memory across sessions.

## Local Setup (Self-Hosted)

Prefer to run OpenMemory locally? Follow the instructions below for a self-hosted setup.

## OpenMemory Easy Setup

### Prerequisites
- Docker
- OpenAI API Key

You can quickly run OpenMemory by running the following command:

```bash
curl -sL https://raw.githubusercontent.com/mem0ai/mem0/main/openmemory/run.sh | bash
```

You should set the `OPENAI_API_KEY` as a global environment variable:

```bash
export OPENAI_API_KEY=your_api_key
```

You can also set the `OPENAI_API_KEY` as a parameter to the script:

```bash
curl -sL https://raw.githubusercontent.com/mem0ai/mem0/main/openmemory/run.sh | OPENAI_API_KEY=your_api_key bash
```

This will start the OpenMemory server and the OpenMemory UI. Deleting the container will lead to the deletion of the memory store.
We suggest you follow the instructions below to set up OpenMemory on your local machine, with more persistant memory store.

## Setting Up OpenMemory

Getting started with OpenMemory is straight forward and takes just a few minutes to set up on your local machine. Follow these steps:

### Getting started


### 1. First clone the repository and then follow the instructions:
```bash
# Clone the repository
git clone https://github.com/mem0ai/mem0.git
cd mem0/openmemory
```

### 2. Set Up Environment Variables

Before running the project, you need to configure environment variables for both the API and the UI.

You can do this in one of the following ways:

- **Manually**:  
  Create a `.env` file in each of the following directories:
  - `/api/.env`
  - `/ui/.env`

- **Using `.env.example` files**:  
  Copy and rename the example files:

  ```bash
  cp api/.env.example api/.env
  cp ui/.env.example ui/.env
  ```

 - **Using Makefile** (if supported):  
    Run:

   ```bash
   make env
   ```
- #### Example `/api/.env`

``` bash
OPENAI_API_KEY=sk-xxx
USER=<user-id> # The User Id you want to associate the memories with 
```
- #### Example `/ui/.env`

```bash
NEXT_PUBLIC_API_URL=http://localhost:8765
NEXT_PUBLIC_USER_ID=<user-id> # Same as the user id for environment variable in api
```

### 3. Build and Run the Project
You can run the project using the following two commands:
```bash
make build # builds the mcp server and ui
make up  # runs openmemory mcp server and ui
``` 

After running these commands, you will have:
- OpenMemory MCP server running at: http://localhost:8765 (API documentation available at http://localhost:8765/docs)
- OpenMemory UI running at: http://localhost:3000

#### UI not working on http://localhost:3000?

If the UI does not start properly on http://localhost:3000, try running it manually:

```bash
cd ui
pnpm install
pnpm dev
```


You can configure the MCP client using the following command (replace username with your username):

```bash
npx @openmemory/install local "http://localhost:8765/mcp/cursor/sse/username" --client cursor
```

The OpenMemory dashboard will be available at http://localhost:3000. From here, you can view and manage your memories, as well as check connection status with your MCP clients.

Once set up, OpenMemory runs locally on your machine, ensuring all your AI memories remain private and secure while being accessible across any compatible MCP client.

### Getting Started Today

- Github Repository: https://github.com/mem0ai/mem0/tree/main/openmemory



================================================
FILE: docs/platform/advanced-memory-operations.mdx
================================================
---
title: Advanced Memory Operations
description: 'Comprehensive guide to advanced memory operations and features'
icon: "gear"
iconType: "solid"
---

This guide covers advanced memory operations including complex filtering, batch operations, and detailed API usage. If you're just getting started, check out the [Quickstart](/platform/quickstart) first.

## Advanced Memory Creation

### Async Client (Python)

For asynchronous operations in Python, use the AsyncMemoryClient:

```python Python
import os
from mem0 import AsyncMemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"
client = AsyncMemoryClient()

async def main():
    messages = [
        {"role": "user", "content": "I'm travelling to SF"}
    ]
    response = await client.add(messages, user_id="john")
    print(response)

await main()
```

### Detailed Memory Creation Examples

#### Long-term memory with full context

<CodeGroup>

```python Python
messages = [
    {"role": "user", "content": "Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts."},
    {"role": "assistant", "content": "Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions."}
]

client.add(messages, user_id="alex", metadata={"food": "vegan"})
```

```javascript JavaScript
const messages = [
    {"role": "user", "content": "Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts."},
    {"role": "assistant", "content": "Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions."}
];
client.add(messages, { user_id: "alex", metadata: { food: "vegan" } })
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [
             {"role": "user", "content": "Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts."},
             {"role": "assistant", "content": "Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions."}
         ],
         "user_id": "alex",
         "metadata": {
             "food": "vegan"
         }
     }'
```

```json Output
{
    "results": [
        {
            "memory": "Name is Alex",
            "event": "ADD"
        },
        {
            "memory": "Is a vegetarian",
            "event": "ADD"
        },
        {
            "memory": "Is allergic to nuts",
            "event": "ADD"
        }
    ]
}
```

</CodeGroup>

<Note>
    When passing `user_id`, memories are primarily created based on user messages, but may be influenced by assistant messages for contextual understanding. For example, in a conversation about food preferences, both the user's stated preferences and their responses to the assistant's questions would form user memories. Similarly, when using `agent_id`, assistant messages are prioritized, but user messages might influence the agent's memories based on context.
    
    **Example:**
    ```
    User: My favorite cuisine is Italian
    Assistant: Nice! What about Indian cuisine?
    User: Don't like it much since I cannot eat spicy food
    
    Resulting user memories:
    memory1 - Likes Italian food
    memory2 - Doesn't like Indian food since cannot eat spicy 
    
    (memory2 comes from user's response about Indian cuisine)
    ```
</Note>

<Note>Metadata allows you to store structured information (location, timestamp, user state) with memories. Add it during creation to enable precise filtering and retrieval during searches.</Note>

#### Short-term memory for sessions

<CodeGroup>

```python Python
messages = [
    {"role": "user", "content": "I'm planning a trip to Japan next month."},
    {"role": "assistant", "content": "That's exciting, Alex! A trip to Japan next month sounds wonderful. Would you like some recommendations for vegetarian-friendly restaurants in Japan?"},
    {"role": "user", "content": "Yes, please! Especially in Tokyo."},
    {"role": "assistant", "content": "Great! I'll remember that you're interested in vegetarian restaurants in Tokyo for your upcoming trip. I'll prepare a list for you in our next interaction."}
]

client.add(messages, user_id="alex", run_id="trip-planning-2024")
```

```javascript JavaScript
const messages = [
    {"role": "user", "content": "I'm planning a trip to Japan next month."},
    {"role": "assistant", "content": "That's exciting, Alex! A trip to Japan next month sounds wonderful. Would you like some recommendations for vegetarian-friendly restaurants in Japan?"},
    {"role": "user", "content": "Yes, please! Especially in Tokyo."},
    {"role": "assistant", "content": "Great! I'll remember that you're interested in vegetarian restaurants in Tokyo for your upcoming trip. I'll prepare a list for you in our next interaction."}
];
client.add(messages, { user_id: "alex", run_id: "trip-planning-2024" })
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [
             {"role": "user", "content": "I'm planning a trip to Japan next month."},
             {"role": "assistant", "content": "That's exciting, Alex! A trip to Japan next month sounds wonderful. Would you like some recommendations for vegetarian-friendly restaurants in Japan?"},
             {"role": "user", "content": "Yes, please! Especially in Tokyo."},
             {"role": "assistant", "content": "Great! I'll remember that you're interested in vegetarian restaurants in Tokyo for your upcoming trip. I'll prepare a list for you in our next interaction."}
         ],
         "user_id": "alex",
         "run_id": "trip-planning-2024"
     }'
```

```json Output
{
  "results": [
    {
      "memory": "Planning a trip to Japan next month",
      "event": "ADD"
    },
    {
      "memory": "Interested in vegetarian restaurants in Tokyo",
      "event": "ADD"
    }
  ]
}
```

</CodeGroup>

#### Agent memories

<CodeGroup>

```python Python
messages = [
    {"role": "system", "content": "You are an AI tutor with a personality. Give yourself a name for the user."},
    {"role": "assistant", "content": "Understood. I'm an AI tutor with a personality. My name is Alice."}
]

client.add(messages, agent_id="ai-tutor")
```

```javascript JavaScript
const messages = [
    {"role": "system", "content": "You are an AI tutor with a personality. Give yourself a name for the user."},
    {"role": "assistant", "content": "Understood. I'm an AI tutor with a personality. My name is Alice."}
];
client.add(messages, { agent_id: "ai-tutor" })
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [
             {"role": "system", "content": "You are an AI tutor with a personality. Give yourself a name for the user."},
             {"role": "assistant", "content": "Understood. I'm an AI tutor with a personality. My name is Alice."}
         ],
         "agent_id": "ai-tutor"
     }'
```

</CodeGroup>

<Note>
    The `agent_id` retains memories exclusively based on messages generated by the assistant or those explicitly provided as input to the assistant. Messages outside these criteria are not stored as memory.
</Note>

#### Dual user and agent memories

When you provide both `user_id` and `agent_id`, Mem0 will store memories for both identifiers separately:
- Memories from messages with `"role": "user"` are automatically tagged with the provided `user_id`
- Memories from messages with `"role": "assistant"` are automatically tagged with the provided `agent_id`
- During retrieval, you can provide either `user_id` or `agent_id` to access the respective memories
- You can continuously enrich existing memory collections by adding new memories to the same `user_id` or `agent_id` in subsequent API calls, either together or separately, allowing for progressive memory building over time
- This dual-tagging approach enables personalized experiences for both users and AI agents in your application

<CodeGroup>

```python Python
messages = [
    {"role": "user", "content": "I'm travelling to San Francisco"},
    {"role": "assistant", "content": "That's great! I'm going to Dubai next month."},
]

client.add(messages=messages, user_id="user1", agent_id="agent1")
```

```javascript JavaScript
const messages = [
    {"role": "user", "content": "I'm travelling to San Francisco"},
    {"role": "assistant", "content": "That's great! I'm going to Dubai next month."},
]

client.add(messages, { user_id: "user1", agent_id: "agent1" })
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [
             {"role": "user", "content": "I'm travelling to San Francisco"},
             {"role": "assistant", "content": "That's great! I'm going to Dubai next month."},
         ],
         "user_id": "user1",
         "agent_id": "agent1"
     }'
```

```json Output
{
    "results": [
        {
            // memory from user1
            "id": "c57abfa2-f0ac-48af-896a-21728dbcecee0",
            "data": {"memory": "Travelling to San Francisco"},
            "event": "ADD"
        },
        {
            // memory from agent1
            "id": "0e8c003f-7db7-426a-9fdc-a46f9331a0c2",
            "data": {"memory": "Going to Dubai next month"},
            "event": "ADD"
        }
    ]
}
```

</CodeGroup>

## Advanced Search Operations

### Search with Custom Filters

Our advanced search allows you to set custom search filters. You can filter by user_id, agent_id, app_id, run_id, created_at, updated_at, categories, and text. The filters support logical operators (AND, OR) and comparison operators (in, gte, lte, gt, lt, ne, contains, icontains, `*`). The wildcard character (`*`) matches everything for a specific field.

Here you need to define `version` as `v2` in the search method.

#### Example 1: Search using user_id and agent_id filters

<CodeGroup>

```python Python
query = "What do you know about me?"
filters = {
   "OR":[
      {
         "user_id":"alex"
      },
      {
         "agent_id":{
            "in":[
               "travel-assistant",
               "customer-support"
            ]
         }
      }
   ]
}
client.search(query, version="v2", filters=filters)
```

```javascript JavaScript
const query = "What do you know about me?";
const filters = {
   "OR":[
      {
         "user_id":"alex"
      },
      {
         "agent_id":{
            "in":[
               "travel-assistant",
               "customer-support"
            ]
         }
      }
   ]
};
client.search(query, { version: "v2", filters })
    .then(results => console.log(results))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/search/?version=v2" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "query": "What do you know about me?",
         "filters": {
             "OR": [
                 {
                     "user_id": "alex"
                 },
                 {
                     "agent_id": {
                         "in": ["travel-assistant", "customer-support"]
                     }
                 }
             ]
         }
     }'
```

</CodeGroup>

#### Example 2: Search using date filters

<CodeGroup>
```python Python
query = "What do you know about me?"
filters = {
    "AND": [
        {"created_at": {"gte": "2024-07-20", "lte": "2024-07-10"}},
        {"user_id": "alex"}
    ]
}
client.search(query, version="v2", filters=filters)
```

```javascript JavaScript
const query = "What do you know about me?";
const filters = {
  "AND": [
    {"created_at": {"gte": "2024-07-20", "lte": "2024-07-10"}},
    {"user_id": "alex"}
  ]
};

client.search(query, { version: "v2", filters })
  .then(results => console.log(results))
  .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/search/?version=v2" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "query": "What do you know about me?",
         "filters": {
             "AND": [
                 {
                     "created_at": {
                         "gte": "2024-07-20",
                         "lte": "2024-07-10"
                     }
                 },
                 {
                     "user_id": "alex"
                 }
             ]
         }
     }'
```
</CodeGroup>

#### Example 3: Search using metadata and categories

<CodeGroup>
```python Python
query = "What do you know about me?"
filters = {
    "AND": [
        {"metadata": {"food": "vegan"}},
        {
         "categories":{
            "contains": "food_preferences"
         }
    }
    ]
}
client.search(query, version="v2", filters=filters)
```

```javascript JavaScript
const query = "What do you know about me?";
const filters = {
    "AND": [
        {"metadata": {"food": "vegan"}},
        {
            "categories": {
                "contains": "food_preferences"
            }
        }
    ]
};

client.search(query, { version: "v2", filters })
    .then(results => console.log(results))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/search/?version=v2" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "query": "What do you know about me?",
         "filters": {
             "AND": [
                 {
                     "metadata": {
                         "food": "vegan"
                     }
                 },
                 {
                     "categories": {
                         "contains": "food_preferences"
                     }
                 }
             ]
         }
     }'
```
</CodeGroup>

#### Example 4: Search using NOT filters

<CodeGroup>
```python Python
query = "What do you know about me?"
filters = {
    "NOT": [
        {
            "categories": {
                "contains": "food_preferences"
            }
        }
    ]
}
client.search(query, version="v2", filters=filters)
```

```javascript JavaScript
const query = "What do you know about me?";
const filters = {
    "NOT": [
        {
            "categories": {
                "contains": "food_preferences"
            }
        }
    ]
};

client.search(query, { version: "v2", filters })
    .then(results => console.log(results))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/search/?version=v2" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "query": "What do you know about me?",
         "filters": {
            "NOT": [
                {
                    "categories": {
                        "contains": "food_preferences"
                    }
                }
            ]
        }
     }'
```
</CodeGroup>

#### Example 5: Search using wildcard filters

<CodeGroup>
```python Python
query = "What do you know about me?"
filters = {
    "AND": [
        {
            "user_id": "alex"
        },
        {
            "run_id": "*"  # Matches all run_ids
        }
    ]
}
client.search(query, version="v2", filters=filters)
```

```javascript JavaScript
const query = "What do you know about me?";
const filters = {
    "AND": [
        {
            "user_id": "alex"
        },
        {
            "run_id": "*"  // Matches all run_ids
        }
    ]
};

client.search(query, { version: "v2", filters })
    .then(results => console.log(results))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/search/?version=v2" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "query": "What do you know about me?",
         "filters": {
             "AND": [
                 {
                     "user_id": "alex"
                 },
                 {
                     "run_id": "*"
                 }
             ]
         }
     }'
```
</CodeGroup>

## Advanced Retrieval Operations

### Get All Memories with Pagination

<Note> The `get_all` method supports two output formats: `v1.0` (default) and `v1.1`. To use the latest format, which provides more detailed information about each memory operation, set the `output_format` parameter to `v1.1`.</Note>

<Note> We're soon deprecating the default output format for get_all() method, which returned a list. Once the changes are live, paginated response will be the only supported format, with 100 memories per page by default. You can customize this using the `page` and `page_size` parameters.</Note>

#### Get all memories of a user

<CodeGroup>

```python Python
memories = client.get_all(user_id="alex", page=1, page_size=50)
```

```javascript JavaScript
client.getAll({ user_id: "alex", page: 1, page_size: 50 })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));
```

```bash cURL
curl -X GET "https://api.mem0.ai/v1/memories/?user_id=alex&page=1&page_size=50" \
     -H "Authorization: Token your-api-key"
```

```json Output (v1.1)
{
    "count": 204,
    "next": "https://api.mem0.ai/v1/memories/?user_id=alex&output_format=v1.1&page=2&page_size=50",
    "previous": null,
    "results": [
        {
            "id":"f38b689d-6b24-45b7-bced-17fbb4d8bac7",
            "memory":"Is a vegetarian and allergic to nuts.",
            "agent_id":"travel-assistant",
            "hash":"62bc074f56d1f909f1b4c2b639f56f6a",
            "metadata":null,
            "immutable": false,
            "expiration_date": null,
            "created_at":"2024-07-25T23:57:00.108347-07:00",
            "updated_at":"2024-07-25T23:57:00.108367-07:00",
            "categories":null
        }
    ]
}
```

</CodeGroup>

#### Get all memories by categories

You can filter memories by their categories when using get_all:

<CodeGroup>

```python Python
# Get memories with specific categories
memories = client.get_all(user_id="alex", categories=["likes"])

# Get memories with multiple categories
memories = client.get_all(user_id="alex", categories=["likes", "food_preferences"])

# Custom pagination with categories
memories = client.get_all(user_id="alex", categories=["likes"], page=1, page_size=50)

# Get memories with specific keywords
memories = client.get_all(user_id="alex", keywords="to play", page=1, page_size=50)
```

```javascript JavaScript
// Get memories with specific categories
client.getAll({ user_id: "alex", categories: ["likes"] })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));

// Get memories with multiple categories
client.getAll({ user_id: "alex", categories: ["likes", "food_preferences"] })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));

// Custom pagination with categories
client.getAll({ user_id: "alex", categories: ["likes"], page: 1, page_size: 50 })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));

// Get memories with specific keywords
client.getAll({ user_id: "alex", keywords: "to play", page: 1, page_size: 50 })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));
```

```bash cURL
# Get memories with specific categories
curl -X GET "https://api.mem0.ai/v1/memories/?user_id=alex&categories=likes" \
     -H "Authorization: Token your-api-key"

# Get memories with multiple categories
curl -X GET "https://api.mem0.ai/v1/memories/?user_id=alex&categories=likes,food_preferences" \
     -H "Authorization: Token your-api-key"

# Custom pagination with categories
curl -X GET "https://api.mem0.ai/v1/memories/?user_id=alex&categories=likes&page=1&page_size=50" \
     -H "Authorization: Token your-api-key"

# Get memories with specific keywords
curl -X GET "https://api.mem0.ai/v1/memories/?user_id=alex&keywords=to play&page=1&page_size=50" \
     -H "Authorization: Token your-api-key"
```

</CodeGroup>

#### Get all memories using custom filters

Our advanced retrieval allows you to set custom filters when fetching memories. You can filter by user_id, agent_id, app_id, run_id, created_at, updated_at, categories, and keywords. The filters support logical operators (AND, OR) and comparison operators (in, gte, lte, gt, lt, ne, contains, icontains, `*`). The wildcard character (`*`) matches everything for a specific field.

Here you need to define `version` as `v2` in the get_all method.

<CodeGroup>

```python Python
filters = {
   "AND":[
      {
         "user_id":"alex"
      },
      {
         "created_at":{
            "gte":"2024-07-01",
            "lte":"2024-07-31"
         }
      },
      {
         "categories":{
            "contains": "food_preferences"
         }
      }
   ]
}

# Default (No Pagination)
client.get_all(version="v2", filters=filters)

# Pagination (You can also use the page and page_size parameters)
client.get_all(version="v2", filters=filters, page=1, page_size=50)
```

```javascript JavaScript
const filters = {
   "AND":[
      {
         "user_id":"alex"
      },
      {
         "created_at":{
            "gte":"2024-07-01",
            "lte":"2024-07-31"
         }
      },
      {
         "categories":{
            "contains": "food_preferences"
         }
      }
   ]
};

// Default (No Pagination)
client.getAll({ version: "v2", filters })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));

// Pagination (You can also use the page and page_size parameters)
client.getAll({ version: "v2", filters, page: 1, page_size: 50 })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));
```

```bash cURL
# Default (No Pagination)
curl -X GET "https://api.mem0.ai/v1/memories/?version=v2" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "filters": {
             "AND": [
                {"user_id":"alex"},
                {"created_at":{
                    "gte":"2024-07-01",
                    "lte":"2024-07-31"
                }},
                {"categories":{
                    "contains": "food_preferences"
                }}
             ]
         }
     }'

# Pagination (You can also use the page and page_size parameters)
curl -X GET "https://api.mem0.ai/v1/memories/?version=v2&page=1&page_size=50" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "filters": {
             "AND": [
                {"user_id":"alex"},
                {"created_at":{
                    "gte":"2024-07-01",
                    "lte":"2024-07-31"
                }},
                {"categories":{
                    "contains": "food_preferences"
                }}
             ]
         }
     }'
```

</CodeGroup>

## Memory Management Operations

### Memory History

Get history of how a memory has changed over time.

<CodeGroup>

```python Python
# Add some message to create history
messages = [{"role": "user", "content": "I recently tried chicken and I loved it. I'm thinking of trying more non-vegetarian dishes.."}]
client.add(messages, user_id="alex")

# Add second message to update history
messages.append({'role': 'user', 'content': 'I turned vegetarian now.'})
client.add(messages, user_id="alex")

# Get history of how memory changed over time
memory_id = "<memory-id-here>"
history = client.history(memory_id)
```

```javascript JavaScript
// Add some message to create history
let messages = [{ role: "user", content: "I recently tried chicken and I loved it. I'm thinking of trying more non-vegetarian dishes.." }];
client.add(messages, { user_id: "alex" })
    .then(result => {
        // Add second message to update history
        messages.push({ role: 'user', content: 'I turned vegetarian now.' });
        return client.add(messages, { user_id: "alex" });
    })
    .then(result => {
        // Get history of how memory changed over time
        const memoryId = result.id; // Assuming the API returns the memory ID
        return client.history(memoryId);
    })
    .then(history => console.log(history))
    .catch(error => console.error(error));
```

```bash cURL
# First, add the initial memory
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [{"role": "user", "content": "I recently tried chicken and I loved it. I'm thinking of trying more non-vegetarian dishes.."}],
         "user_id": "alex"
     }'

# Then, update the memory
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [
             {"role": "user", "content": "I recently tried chicken and I loved it. I'm thinking of trying more non-vegetarian dishes.."},
             {"role": "user", "content": "I turned vegetarian now."}
         ],
         "user_id": "alex"
     }'

# Finally, get the history (replace <memory-id-here> with the actual memory ID)
curl -X GET "https://api.mem0.ai/v1/memories/<memory-id-here>/history/" \
     -H "Authorization: Token your-api-key"
```

```json Output
[
   {
      "id":"d6306e85-eaa6-400c-8c2f-ab994a8c4d09",
      "memory_id":"b163df0e-ebc8-4098-95df-3f70a733e198",
      "input":[
         {
            "role":"user",
            "content":"I recently tried chicken and I loved it. I'm thinking of trying more non-vegetarian dishes.."
         },
         {
            "role":"user",
            "content":"I turned vegetarian now."
         }
      ],
      "old_memory":"None",
      "new_memory":"Turned vegetarian.",
      "user_id":"alex",
      "event":"ADD",
      "metadata":"None",
      "created_at":"2024-07-26T01:02:41.737310-07:00",
      "updated_at":"2024-07-26T01:02:41.726073-07:00"
   }
]
```
</CodeGroup>

### Update Memory

Update a memory with new data. You can update the memory's text, metadata, or both.

<CodeGroup>

```python Python
client.update(
    memory_id="<memory-id-here>",
    text="I am now a vegetarian.", 
    metadata={"diet": "vegetarian"}
)
```

```javascript JavaScript
client.update("memory-id-here", { text: "I am now a vegetarian.", metadata: { diet: "vegetarian" } })
    .then(result => console.log(result))
    .catch(error => console.error(error));
```

```bash cURL
curl -X PUT "https://api.mem0.ai/v1/memories/<memory-id-here>" \
        -H "Authorization: Token your-api-key" \
        -H "Content-Type: application/json" \
        -d '{
            "message": "I recently tried chicken and I loved it. I'm thinking of trying more non-vegetarian dishes.."
        }'
```

```json Output
{
   "id":"c190ab1a-a2f1-4f6f-914a-495e9a16b76e",
   "memory":"I recently tried chicken and I loved it. I'm thinking of trying more non-vegetarian dishes..",
   "agent_id":"travel-assistant",
   "hash":"af1161983e03667063d1abb60e6d5c06",
   "metadata":"None",
   "created_at":"2024-07-30T22:46:40.455758-07:00",
   "updated_at":"2024-07-30T22:48:35.257828-07:00"
}
```

</CodeGroup>

## Batch Operations

### Batch Update Memories

Update multiple memories in a single API call. You can update up to 1000 memories at once.

<CodeGroup>
```python Python
update_memories = [
    {
        "memory_id": "285ed74b-6e05-4043-b16b-3abd5b533496",
        "text": "Watches football"
    },
    {
        "memory_id": "2c9bd859-d1b7-4d33-a6b8-94e0147c4f07",
        "text": "Loves to travel"
    }
]

response = client.batch_update(update_memories)
print(response)
```

```javascript JavaScript
const updateMemories = [
    {
        "memory_id": "285ed74b-6e05-4043-b16b-3abd5b533496",
        text: "Watches football"
    },
    {
        "memory_id": "2c9bd859-d1b7-4d33-a6b8-94e0147c4f07",
        text: "Loves to travel"
    }
];

client.batchUpdate(updateMemories)
    .then(response => console.log('Batch update response:', response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X PUT "https://api.mem0.ai/v1/memories/batch/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "memories": [
             {
                 "memory_id": "285ed74b-6e05-4043-b16b-3abd5b533496",
                 "text": "Watches football"
             },
             {
                 "memory_id": "2c9bd859-d1b7-4d33-a6b8-94e0147c4f07",
                 "text": "Loves to travel"
             }
         ]
     }'
```

```json Output
{
    "message": "Successfully updated 2 memories"
}
```
</CodeGroup>

### Batch Delete Memories

Delete multiple memories in a single API call. You can delete up to 1000 memories at once.

<CodeGroup>
```python Python
delete_memories = [
    {"memory_id": "285ed74b-6e05-4043-b16b-3abd5b533496"},
    {"memory_id": "2c9bd859-d1b7-4d33-a6b8-94e0147c4f07"}
]

response = client.batch_delete(delete_memories)
print(response)
```

```javascript JavaScript
const deleteMemories = [
    {"memory_id": "285ed74b-6e05-4043-b16b-3abd5b533496"},
    {"memory_id": "2c9bd859-d1b7-4d33-a6b8-94e0147c4f07"}
];

client.batchDelete(deleteMemories)
    .then(response => console.log('Batch delete response:', response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X DELETE "https://api.mem0.ai/v1/memories/batch/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "memory_ids": [
             {"memory_id": "285ed74b-6e05-4043-b16b-3abd5b533496"},
             {"memory_id": "2c9bd859-d1b7-4d33-a6b8-94e0147c4f07"}
         ]
     }'
```

```json Output
{
    "message": "Successfully deleted 2 memories"
}
```
</CodeGroup>

## Entity Management

### Get All Users

Get all users, agents, and runs which have memories associated with them.

<CodeGroup>

```python Python
client.users()
```

```javascript JavaScript
client.users()
    .then(users => console.log(users))
    .catch(error => console.error(error));
```

```bash cURL
curl -X GET "https://api.mem0.ai/v1/entities/" \
     -H "Authorization: Token your-api-key"
```

```json Output
[
    {
        "id": "1",
        "name": "user123",
        "created_at": "2024-07-17T16:47:23.899900-07:00",
        "updated_at": "2024-07-17T16:47:23.899918-07:00",
        "total_memories": 5,
        "owner": "alex",
        "metadata": {"foo": "bar"},
        "type": "user"
    },
    {
        "id": "2",
        "name": "travel-agent",
        "created_at": "2024-07-01T17:59:08.187250-07:00",
        "updated_at": "2024-07-01T17:59:08.187266-07:00",
        "total_memories": 10,
        "owner": "alex",
        "metadata": {"agent_id": "123"},
        "type": "agent"
    }
]
```

</CodeGroup>

### Delete Operations

Delete specific memory:

<CodeGroup>

```python Python
client.delete(memory_id)
```

```javascript JavaScript
client.delete("memory-id-here")
    .then(result => console.log(result))
    .catch(error => console.error(error));
```

```bash cURL
curl -X DELETE "https://api.mem0.ai/v1/memories/memory-id-here" \
     -H "Authorization: Token your-api-key"
```

</CodeGroup>

Delete all memories of a user:

<CodeGroup>

```python Python
client.delete_all(user_id="alex")
```

```javascript JavaScript
client.deleteAll({ user_id: "alex" })
    .then(result => console.log(result))
    .catch(error => console.error(error));
```

```bash cURL
curl -X DELETE "https://api.mem0.ai/v1/memories/?user_id=alex" \
     -H "Authorization: Token your-api-key"
```

</CodeGroup>

Delete specific user or agent:

<CodeGroup>
```python Python
# Delete specific user
client.delete_users(user_id="alex")

# Delete specific agent
# client.delete_users(agent_id="travel-assistant")
```

```javascript JavaScript
client.delete_users({ user_id: "alex" })
    .then(result => console.log(result))
    .catch(error => console.error(error));
```

```bash cURL
curl -X DELETE "https://api.mem0.ai/v2/entities/user/alex" \
     -H "Authorization: Token your-api-key"
```
</CodeGroup>

### Reset Client

<CodeGroup>

```python Python
client.reset()
```

```json Output
{'message': 'Client reset successful. All users and memories deleted.'}
```

</CodeGroup>

### Natural Language Delete

You can also delete memories using natural language commands:

<CodeGroup>

```python Python
messages = [
    {"role": "user", "content": "Delete all of my food preferences"}
]
client.add(messages, user_id="alex")
```

```javascript JavaScript
const messages = [
    {"role": "user", "content": "Delete all of my food preferences"}
]
client.add(messages, { user_id: "alex" })
    .then(result => console.log(result))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [{"role": "user", "content": "Delete all of my food preferences"}],
         "user_id": "alex"
     }'
```

</CodeGroup>

## Monitor Memory Operations

You can monitor memory operations on the platform dashboard:

![Mem0 Platform Activity](/images/platform/activity.png)

For more detailed information, see our [API Reference](/api-reference) or explore specific features in the [Platform Features](/platform/features/platform-overview) section.


================================================
FILE: docs/platform/overview.mdx
================================================
---
title: Overview
description: 'Empower your AI applications with long-term memory and personalization'
icon: "eye"
iconType: "solid"
---

## Welcome to Mem0 Platform

The Mem0 Platform is a managed service and the easiest way to add our powerful memory layer to your applications. 

## Why Choose Mem0 Platform?

Mem0 Platform offers a powerful, user-centric solution for AI memory management with a few key features:

1. **Simplified Development**: Integrate comprehensive memory capabilities with just 4 lines of code. Our API-first approach allows you to focus on building great features while we handle the complexities of memory management.

2. **Scalable Solution**: Whether you're working on a prototype or a production-ready system, Mem0 is designed to grow with your application. Our platform effortlessly scales to meet your evolving needs.

3. **Enhanced Performance**: Experience lightning-fast response times with sub-50ms latency, ensuring smooth and responsive user interactions in your AI applications.

4. **Insightful Dashboard**: Gain valuable insights and maintain full control over your AI's memory through our intuitive dashboard. Easily manage memories and access key user insights.


## Getting Started

Check out our [Platform Guide](/platform/quickstart) to start using Mem0 platform quickly.

## Next Steps

- Sign up to the [Mem0 Platform](https://mem0.dev/pd)
- Join our [Discord](https://mem0.dev/Did) with other developers and get support.

We're excited to see what you'll build with Mem0 Platform. Let's create smarter, more personalized AI experiences together!



================================================
FILE: docs/platform/quickstart.mdx
================================================
---
title: Quickstart
description: 'Get started with Mem0 Platform in minutes'
icon: "bolt"
iconType: "solid"
---

Get up and running with Mem0 Platform quickly. This guide covers the essential steps to start storing and retrieving memories.

## 1. Installation

<CodeGroup>
```bash pip
pip install mem0ai
```

```bash npm
npm install mem0ai
```
</CodeGroup>

## 2. API Key Setup

1. Sign in to [Mem0 Platform](https://mem0.dev/pd-api)
2. Copy your API Key from the dashboard

![Get API Key from Mem0 Platform](/images/platform/api-key.png)

## 3. Initialize Client

<CodeGroup>
```python Python
import os
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"
client = MemoryClient()
```

```javascript JavaScript
import MemoryClient from 'mem0ai';
const client = new MemoryClient({ apiKey: 'your-api-key' });
```
</CodeGroup>

## 4. Basic Operations

### Add Memories

Store user preferences and context:

<CodeGroup>
```python Python
messages = [
    {"role": "user", "content": "Hi, I'm Alex. I'm a vegetarian and allergic to nuts."},
    {"role": "assistant", "content": "Hello Alex! I'll remember your dietary preferences."}
]

result = client.add(messages, user_id="alex")
print(result)
```

```javascript JavaScript
const messages = [
    {"role": "user", "content": "Hi, I'm Alex. I'm a vegetarian and allergic to nuts."},
    {"role": "assistant", "content": "Hello Alex! I'll remember your dietary preferences."}
];

client.add(messages, { user_id: "alex" })
    .then(result => console.log(result))
    .catch(error => console.error(error));
```
</CodeGroup>

### Search Memories

Retrieve relevant memories based on queries:

<CodeGroup>
```python Python
query = "What should I cook for dinner?"
results = client.search(query, user_id="alex")
print(results)
```

```javascript JavaScript
const query = "What should I cook for dinner?";
client.search(query, { user_id: "alex" })
    .then(results => console.log(results))
    .catch(error => console.error(error));
```
</CodeGroup>

### Get All Memories

Fetch all memories for a user:

<CodeGroup>
```python Python
memories = client.get_all(user_id="alex")
print(memories)
```

```javascript JavaScript
client.getAll({ user_id: "alex" })
    .then(memories => console.log(memories))
    .catch(error => console.error(error));
```
</CodeGroup>

## 5. Memory Types

### User Memories
Long-term memories that persist across sessions:

<CodeGroup>
```python Python
client.add(messages, user_id="alex", metadata={"category": "preferences"})
```

```javascript JavaScript
client.add(messages, { user_id: "alex", metadata: { category: "preferences" } });
```
</CodeGroup>

### Session Memories
Short-term memories for specific conversations:

<CodeGroup>
```python Python
client.add(messages, user_id="alex", run_id="session-123")
```

```javascript JavaScript
client.add(messages, { user_id: "alex", run_id: "session-123" });
```
</CodeGroup>

### Agent Memories
Memories for AI assistants and agents:

<CodeGroup>
```python Python
client.add(messages, agent_id="support-bot")
```

```javascript JavaScript
client.add(messages, { agent_id: "support-bot" });
```
</CodeGroup>

## 6. Advanced Features

### Async Processing
Process memories in the background for faster responses:

<CodeGroup>
```python Python
client.add(messages, user_id="alex", async_mode=True)
```

```javascript JavaScript
client.add(messages, { user_id: "alex", async_mode: true });
```
</CodeGroup>

### Search with Filters
Filter results by categories and metadata:

<CodeGroup>
```python Python
results = client.search(
    "food preferences", 
    user_id="alex",
    categories=["preferences"],
    metadata={"category": "food"}
)
```

```javascript JavaScript
client.search("food preferences", {
    user_id: "alex",
    categories: ["preferences"], 
    metadata: { category: "food" }
});
```
</CodeGroup>

## TypeScript Example

<CodeGroup>
```typescript TypeScript
import MemoryClient, { Message, MemoryOptions } from 'mem0ai';

const client = new MemoryClient('your-api-key');

const messages: Message[] = [
    { role: "user", content: "I love Italian food" },
    { role: "assistant", content: "Noted! I'll remember your preference for Italian cuisine." }
];

const options: MemoryOptions = {
    user_id: "alex",
    metadata: { category: "food_preferences" }
};

client.add(messages, options)
    .then(result => console.log(result))
    .catch(error => console.error(error));
```
</CodeGroup>

## Next Steps

Now that you're up and running, explore more advanced features:

- **[Advanced Memory Operations](/core-concepts/memory-operations)** - Learn about filtering, updating, and managing memories
- **[Platform Features](/platform/features/platform-overview)** - Discover advanced platform capabilities
- **[API Reference](/api-reference)** - Complete API documentation

<Snippet file="get-help.mdx" />


================================================
FILE: docs/platform/features/advanced-retrieval.mdx
================================================
---
title: Advanced Retrieval
icon: "magnifying-glass"
iconType: "solid"
description: "Advanced memory search with keyword expansion, intelligent reranking, and precision filtering"
---

## What is Advanced Retrieval?

Advanced Retrieval gives you precise control over how memories are found and ranked. While basic search uses semantic similarity, these advanced options help you find exactly what you need, when you need it.

## Search Enhancement Options

### Keyword Search
**Expands results** to include memories with specific terms, names, and technical keywords.

<Tabs>
  <Tab title="When to Use">
- Searching for specific entities, names, or technical terms
- Need comprehensive coverage of a topic  
- Want broader recall even if some results are less relevant
- Working with domain-specific terminology
</Tab>
<Tab title="How it Works">
```python Python
# Find memories containing specific food-related terms
results = client.search(
    query="What foods should I avoid?",
    keyword_search=True,
    user_id="user123"
)

# Results might include:
# ✓ "Allergic to peanuts and shellfish"  
# ✓ "Lactose intolerant - avoid dairy"
# ✓ "Mentioned avoiding gluten last week"
```
  </Tab>
  <Tab title="Performance">
- **Latency**: ~10ms additional
- **Recall**: Significantly increased
- **Precision**: Slightly decreased
- **Best for**: Entity search, comprehensive coverage
</Tab>
</Tabs>

### Reranking  
**Reorders results** using deep semantic understanding to put the most relevant memories first.

<Tabs>
  <Tab title="When to Use">
- Need the most relevant result at the top
- Result order is critical for your application
- Want consistent quality across different queries
- Building user-facing features where accuracy matters
</Tab>
<Tab title="How it Works">
```python Python
# Get the most relevant travel plans first
results = client.search(
    query="What are my upcoming travel plans?",
    rerank=True,
    user_id="user123"
)

# Before reranking:        After reranking:
# 1. "Went to Paris"   →   1. "Tokyo trip next month"
# 2. "Tokyo trip next" →   2. "Need to book hotel in Tokyo"  
# 3. "Need hotel"      →   3. "Went to Paris last year"
```
</Tab>
<Tab title="Performance">
- **Latency**: 150-200ms additional
- **Accuracy**: Significantly improved
- **Ordering**: Much more relevant
- **Best for**: Top-N precision, user-facing results
</Tab>
</Tabs>

### Memory Filtering
**Filters results** to keep only the most precisely relevant memories.

<Tabs>
<Tab title="When to Use">
- Need highly specific, focused results
- Working with large datasets where noise is problematic  
- Quality over quantity is essential
- Building production or safety-critical applications
</Tab>
<Tab title="How it Works">
```python Python
# Get only the most relevant dietary restrictions
results = client.search(
    query="What are my dietary restrictions?",
    filter_memories=True,
    user_id="user123"
)

# Before filtering:           After filtering:
# • "Allergic to nuts"    →   • "Allergic to nuts"
# • "Likes Italian food"  →   • "Vegetarian diet"
# • "Vegetarian diet"     →   
# • "Eats dinner at 7pm"  →   
```
</Tab>
<Tab title="Performance">
- **Latency**: 200-300ms additional
- **Precision**: Maximized
- **Recall**: May be reduced
- **Best for**: Focused queries, production systems
</Tab>
</Tabs>

## Real-World Use Cases

<Tabs>
<Tab title="Personal AI Assistant">
```python Python
# Smart home assistant finding device preferences
results = client.search(
    query="How do I like my bedroom temperature?",
    keyword_search=True,    # Find specific temperature mentions
    rerank=True,           # Get most recent preferences first
    user_id="user123"
)

# Finds: "Keep bedroom at 68°F", "Too cold last night at 65°F", etc.
```
</Tab>
<Tab title="Customer Support">
```python Python
# Find specific product issues with high precision
results = client.search(
    query="Problems with premium subscription billing",
    keyword_search=True,     # Find "premium", "billing", "subscription"
    filter_memories=True,    # Only billing-related issues
    user_id="customer456"
)

# Returns only relevant billing problems, not general questions
```
</Tab>
<Tab title="Healthcare AI">
```python Python
# Critical medical information needs perfect accuracy
results = client.search(
    query="Patient allergies and contraindications",
    rerank=True,            # Most important info first
    filter_memories=True,   # Only medical restrictions
    user_id="patient789"
)

# Ensures critical allergy info appears first and filters out non-medical data
```
</Tab>
<Tab title="Learning Platform">
```python Python
# Find learning progress for specific topics
results = client.search(
    query="Python programming progress and difficulties",
    keyword_search=True,    # Find "Python", "programming", specific concepts
    rerank=True,           # Recent progress first
    user_id="student123"
)

# Gets comprehensive view of Python learning journey
```
</Tab>
</Tabs>

## Choosing the Right Combination

### Recommended Configurations

<CodeGroup>
```python Python
# Fast and broad - good for exploration
def quick_search(query, user_id):
    return client.search(
        query=query,
        keyword_search=True,
        user_id=user_id
    )

# Balanced - good for most applications  
def standard_search(query, user_id):
    return client.search(
        query=query,
        keyword_search=True,
        rerank=True,
        user_id=user_id
    )

# High precision - good for critical applications
def precise_search(query, user_id):
    return client.search(
        query=query,
        rerank=True,
        filter_memories=True,
        user_id=user_id
    )
```

```javascript JavaScript
// Fast and broad - good for exploration
function quickSearch(query, userId) {
    return client.search(query, {
        user_id: userId,
        keyword_search: true
    });
}

// Balanced - good for most applications
function standardSearch(query, userId) {
    return client.search(query, {
        user_id: userId,
        keyword_search: true,
        rerank: true
    });
}

// High precision - good for critical applications  
function preciseSearch(query, userId) {
    return client.search(query, {
        user_id: userId,
        rerank: true,
        filter_memories: true
    });
}
```
</CodeGroup>

## Best Practices

### ✅ Do
- **Start simple** with just one enhancement and measure impact
- **Use keyword search** for entity-heavy queries (names, places, technical terms)
- **Use reranking** when the top result quality matters most
- **Use filtering** for production systems where precision is critical
- **Handle empty results** gracefully when filtering is too aggressive
- **Monitor latency** and adjust based on your application's needs

### ❌ Don't
- Enable all options by default without measuring necessity
- Use filtering for broad exploratory queries
- Ignore latency impact in real-time applications
- Forget to handle cases where filtering returns no results
- Use advanced retrieval for simple, fast lookup scenarios

## Performance Guidelines

### Latency Expectations

```python Python
# Performance monitoring example
import time

start_time = time.time()
results = client.search(
    query="user preferences",
    keyword_search=True,  # +10ms
    rerank=True,         # +150ms
    filter_memories=True, # +250ms
    user_id="user123"
)
latency = time.time() - start_time
print(f"Search completed in {latency:.2f}s")  # ~0.41s expected
```

### Optimization Tips

1. **Cache frequent queries** to avoid repeated advanced processing
2. **Use session-specific search** with `run_id` to reduce search space
3. **Implement fallback logic** when filtering returns empty results
4. **Monitor and alert** on search latency patterns

---

**Ready to enhance your search?** Start with keyword search for broader coverage, add reranking for better ordering, and use filtering when precision is critical.

<Snippet file="get-help.mdx" />



================================================
FILE: docs/platform/features/async-client.mdx
================================================
---
title: Async Client
description: 'Asynchronous client for Mem0'
icon: "bolt"
iconType: "solid"
---

The `AsyncMemoryClient` is an asynchronous client for interacting with the Mem0 API. It provides similar functionality to the synchronous `MemoryClient` but allows for non-blocking operations, which can be beneficial in applications that require high concurrency.

## Initialization

To use the async client, you first need to initialize it:

<CodeGroup>

```python Python
import os
from mem0 import AsyncMemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

client = AsyncMemoryClient()
```

```javascript JavaScript
const { MemoryClient } = require('mem0ai');
const client = new MemoryClient({ apiKey: 'your-api-key'});
```

</CodeGroup>

## Methods

The `AsyncMemoryClient` provides the following methods:

### Add

Add a new memory asynchronously.

<CodeGroup>

```python Python
messages = [
    {"role": "user", "content": "Alice loves playing badminton"},
    {"role": "assistant", "content": "That's great! Alice is a fitness freak"},
]
await client.add(messages, user_id="alice")
```

```javascript JavaScript
const messages = [
    {"role": "user", "content": "Alice loves playing badminton"},
    {"role": "assistant", "content": "That's great! Alice is a fitness freak"},
];
await client.add(messages, { user_id: "alice" });
```

</CodeGroup>

### Search

Search for memories based on a query asynchronously.

<CodeGroup>

```python Python
await client.search("What is Alice's favorite sport?", user_id="alice")
```

```javascript JavaScript
await client.search("What is Alice's favorite sport?", { user_id: "alice" });
```

</CodeGroup>

### Get All

Retrieve all memories for a user asynchronously.

<CodeGroup>

```python Python
await client.get_all(user_id="alice")
```

```javascript JavaScript
await client.getAll({ user_id: "alice" });
```

</CodeGroup>

### Delete

Delete a specific memory asynchronously.

<CodeGroup>

```python Python
await client.delete(memory_id="memory-id-here")
```

```javascript JavaScript
await client.delete("memory-id-here");
```

</CodeGroup>

### Delete All

Delete all memories for a user asynchronously.

<CodeGroup>

```python Python
await client.delete_all(user_id="alice")
```

```javascript JavaScript
await client.deleteAll({ user_id: "alice" });
```

</CodeGroup>

### History

Get the history of a specific memory asynchronously.

<CodeGroup>

```python Python
await client.history(memory_id="memory-id-here")
```

```javascript JavaScript
await client.history("memory-id-here");
```

</CodeGroup>

### Users

Get all users, agents, and runs which have memories associated with them asynchronously.

<CodeGroup>

```python Python
await client.users()
```

```javascript JavaScript
await client.users();
```

</CodeGroup>

### Reset

Reset the client, deleting all users and memories asynchronously.

<CodeGroup>

```python Python
await client.reset()
```

```javascript JavaScript
await client.reset();
```

</CodeGroup>

## Conclusion

The `AsyncMemoryClient` provides a powerful way to interact with the Mem0 API asynchronously, allowing for more efficient and responsive applications. By using this client, you can perform memory operations without blocking your application's execution.

If you have any questions or need further assistance, please don't hesitate to reach out:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/platform/features/contextual-add.mdx
================================================
---
title: Contextual Memory Creation
icon: "square-plus"
iconType: "solid"
description: "Add messages with automatic context management - no manual history tracking required"
---

## What is Contextual Memory Creation?

Contextual memory creation automatically manages message history for you, so you can focus on building great AI experiences instead of tracking interactions manually. Simply send new messages, and Mem0 handles the context automatically.

<CodeGroup>
```python Python
# Just send new messages - Mem0 handles the context
messages = [
    {"role": "user", "content": "I love Italian food, especially pasta"},
    {"role": "assistant", "content": "Great! I'll remember your preference for Italian cuisine."}
]

client.add(messages, user_id="user123", version="v2")
```

```javascript JavaScript
// Just send new messages - Mem0 handles the context
const messages = [
    {"role": "user", "content": "I love Italian food, especially pasta"},
    {"role": "assistant", "content": "Great! I'll remember your preference for Italian cuisine."}
];

await client.add(messages, { user_id: "user123", version: "v2" });
```
</CodeGroup>

## Why Use Contextual Memory Creation?

- **Simple**: Send only new messages, no manual history tracking
- **Efficient**: Smaller payloads and faster processing
- **Automatic**: Context management handled by Mem0
- **Reliable**: No risk of missing interaction history
- **Scalable**: Works seamlessly as your application grows

## How It Works

### Basic Usage

<CodeGroup>
```python Python
# First interaction
messages1 = [
    {"role": "user", "content": "Hi, I'm Sarah from New York"},
    {"role": "assistant", "content": "Hello Sarah! Nice to meet you."}
]
client.add(messages1, user_id="sarah", version="v2")

# Later interaction - just send new messages
messages2 = [
    {"role": "user", "content": "I'm planning a trip to Italy next month"},
    {"role": "assistant", "content": "How exciting! Italy is beautiful this time of year."}
]
client.add(messages2, user_id="sarah", version="v2")
# Mem0 automatically knows Sarah is from New York and can use this context
```

```javascript JavaScript
// First interaction
const messages1 = [
    {"role": "user", "content": "Hi, I'm Sarah from New York"},
    {"role": "assistant", "content": "Hello Sarah! Nice to meet you."}
];
await client.add(messages1, { user_id: "sarah", version: "v2" });

// Later interaction - just send new messages
const messages2 = [
    {"role": "user", "content": "I'm planning a trip to Italy next month"},
    {"role": "assistant", "content": "How exciting! Italy is beautiful this time of year."}
];
await client.add(messages2, { user_id: "sarah", version: "v2" });
// Mem0 automatically knows Sarah is from New York and can use this context
```
</CodeGroup>

## Organization Strategies

Choose the right approach based on your application's needs:

### User-Level Memories (`user_id` only)

Best for: Personal preferences, profile information, long-term user data

<CodeGroup>
```python Python
# Persistent user memories across all interactions
messages = [
    {"role": "user", "content": "I'm allergic to nuts and dairy"},
    {"role": "assistant", "content": "I've noted your allergies for future reference."}
]

client.add(messages, user_id="user123", version="v2")
# This allergy info will be available in ALL future interactions
```

```javascript JavaScript
// Persistent user memories across all interactions
const messages = [
    {"role": "user", "content": "I'm allergic to nuts and dairy"},
    {"role": "assistant", "content": "I've noted your allergies for future reference."}
];

await client.add(messages, { user_id: "user123", version: "v2" });
// This allergy info will be available in ALL future interactions
```
</CodeGroup>

### Session-Specific Memories (`user_id` + `run_id`)

Best for: Task-specific context, separate interaction threads, project-based sessions

<CodeGroup>
```python Python
# Trip planning session
messages1 = [
    {"role": "user", "content": "I want to plan a 5-day trip to Tokyo"},
    {"role": "assistant", "content": "Perfect! Let's plan your Tokyo adventure."}
]
client.add(messages1, user_id="user123", run_id="tokyo-trip-2024", version="v2")

# Later in the same trip planning session
messages2 = [
    {"role": "user", "content": "I prefer staying near Shibuya"},
    {"role": "assistant", "content": "Great choice! Shibuya is very convenient."}
]
client.add(messages2, user_id="user123", run_id="tokyo-trip-2024", version="v2")

# Different session for work project (separate context)
work_messages = [
    {"role": "user", "content": "Let's discuss the Q4 marketing strategy"},
    {"role": "assistant", "content": "Sure! What are your main goals for Q4?"}
]
client.add(work_messages, user_id="user123", run_id="q4-marketing", version="v2")
```

```javascript JavaScript
// Trip planning session
const messages1 = [
    {"role": "user", "content": "I want to plan a 5-day trip to Tokyo"},
    {"role": "assistant", "content": "Perfect! Let's plan your Tokyo adventure."}
];
await client.add(messages1, { user_id: "user123", run_id: "tokyo-trip-2024", version: "v2" });

// Later in the same trip planning session
const messages2 = [
    {"role": "user", "content": "I prefer staying near Shibuya"},
    {"role": "assistant", "content": "Great choice! Shibuya is very convenient."}
];
await client.add(messages2, { user_id: "user123", run_id: "tokyo-trip-2024", version: "v2" });

// Different session for work project (separate context)
const workMessages = [
    {"role": "user", "content": "Let's discuss the Q4 marketing strategy"},
    {"role": "assistant", "content": "Sure! What are your main goals for Q4?"}
];
await client.add(workMessages, { user_id: "user123", run_id: "q4-marketing", version: "v2" });
```
</CodeGroup>

## Real-World Use Cases

<Tabs>
  <Tab title="Customer Support">
```python Python
# Support ticket context - keeps interaction focused
messages = [
    {"role": "user", "content": "My subscription isn't working"},
    {"role": "assistant", "content": "I can help with that. What specific issue are you experiencing?"},
    {"role": "user", "content": "I can't access premium features even though I paid"}
]

# Each support ticket gets its own run_id
client.add(messages, 
    user_id="customer123", 
    run_id="ticket-2024-001", 
    version="v2"
)
```
  </Tab>
  <Tab title="Personal AI Assistant">
```python Python
# Personal preferences (persistent across all interactions)
preference_messages = [
    {"role": "user", "content": "I prefer morning workouts and vegetarian meals"},
    {"role": "assistant", "content": "Got it! I'll keep your fitness and dietary preferences in mind."}
]

client.add(preference_messages, user_id="user456", version="v2")

# Daily planning session (session-specific)
planning_messages = [
    {"role": "user", "content": "Help me plan tomorrow's schedule"},
    {"role": "assistant", "content": "Of course! I'll consider your morning workout preference."}
]

client.add(planning_messages, 
    user_id="user456", 
    run_id="daily-plan-2024-01-15", 
    version="v2"
)
```
  </Tab>
  <Tab title="Educational Platform">
```python Python
# Student profile (persistent)
profile_messages = [
    {"role": "user", "content": "I'm studying computer science and struggle with math"},
    {"role": "assistant", "content": "I'll tailor explanations to help with math concepts."}
]

client.add(profile_messages, user_id="student789", version="v2")

# Specific lesson session
lesson_messages = [
    {"role": "user", "content": "Can you explain algorithms?"},
    {"role": "assistant", "content": "Sure! I'll explain algorithms with math-friendly examples."}
]

client.add(lesson_messages,
    user_id="student789",
    run_id="algorithms-lesson-1",
    version="v2"
)
```
  </Tab>
</Tabs>

## Best Practices

### ✅ Do
- **Organize by context scope**: Use `user_id` only for persistent data, add `run_id` for session-specific context
- **Keep messages focused** on the current interaction
- **Test with real interaction flows** to ensure context works as expected

### ❌ Don't
- Send duplicate messages or interaction history
- Forget to include `version="v2"` parameter
- Mix contextual and non-contextual approaches in the same application

## Troubleshooting

| Issue | Solution |
|-------|----------|
| **Context not working** | Ensure you're using `version="v2"` and consistent `user_id` |
| **Wrong context retrieved** | Check if you need separate `run_id` values for different interaction topics |
| **Missing interaction history** | Verify all messages in the interaction thread use the same `user_id` and `run_id` |
| **Too much irrelevant context** | Use more specific `run_id` values to separate different interaction types |


<Snippet file="get-help.mdx" />


================================================
FILE: docs/platform/features/criteria-retrieval.mdx
================================================
---
title: Criteria Retrieval
icon: "magnifying-glass-plus"
iconType: "solid"
---

Mem0’s **Criteria Retrieval** feature allows you to retrieve memories based on your defined criteria. It goes beyond generic semantic relevance and rank memories based on what matters to your application - emotional tone, intent, behavioral signals, or other custom traits.

Instead of just searching for "how similar a memory is to this query?", you can define what *relevance* really means for your project. For example:

- Prioritize joyful memories when building a wellness assistant
- Downrank negative memories in a productivity-focused agent
- Highlight curiosity in a tutoring agent

You define **criteria** - custom attributes like "joy", "negativity", "confidence", or "urgency", and assign weights to control how they influence scoring. When you `search`, Mem0 uses these to re-rank memories that are semantically relevant, favoring those that better match your intent.

This gives you nuanced, intent-aware memory search that adapts to your use case.



## When to Use Criteria Retrieval

Use Criteria Retrieval if:

- You’re building an agent that should react to **emotions** or **behavioral signals**
- You want to guide memory selection based on **context**, not just content
- You have domain-specific signals like "risk", "positivity", "confidence", etc. that shape recall



## Setting Up Criteria Retrieval

Let’s walk through how to configure and use Criteria Retrieval step by step.

### Initialize the Client

Before defining any criteria, make sure to initialize the `MemoryClient` with your credentials and project ID:

```python
from mem0 import MemoryClient

client = MemoryClient(
    api_key="your_mem0_api_key",
    org_id="your_organization_id",
    project_id="your_project_id"
)
```

### Define Your Criteria

Each criterion includes:
- A `name` (used in scoring)
- A `description` (interpreted by the LLM)
- A `weight` (how much it influences the final score)

```python
retrieval_criteria = [
    {
        "name": "joy",
        "description": "Measure the intensity of positive emotions such as happiness, excitement, or amusement expressed in the sentence. A higher score reflects greater joy.",
        "weight": 3
    },
    {
        "name": "curiosity",
        "description": "Assess the extent to which the sentence reflects inquisitiveness, interest in exploring new information, or asking questions. A higher score reflects stronger curiosity.",
        "weight": 2
    },
    {
        "name": "emotion",
        "description": "Evaluate the presence and depth of sadness or negative emotional tone, including expressions of disappointment, frustration, or sorrow. A higher score reflects greater sadness.",
        "weight": 1
    }
]
```

### Apply Criteria to Your Project

Once defined, register the criteria to your project:

```python
client.project.update(retrieval_criteria=retrieval_criteria)
```

Criteria apply project-wide. Once set, they affect all searches using `version="v2"`.


## Example Walkthrough

After setting up your criteria, you can use them to filter and retrieve memories. Here's an example:

### Add Memories

```python
messages = [
    {"role": "user", "content": "What a beautiful sunny day! I feel so refreshed and ready to take on anything!"},
    {"role": "user", "content": "I've always wondered how storms form—what triggers them in the atmosphere?"},
    {"role": "user", "content": "It's been raining for days, and it just makes everything feel heavier."},
    {"role": "user", "content": "Finally I get time to draw something today, after a long time!! I am super happy today."}
]

client.add(messages, user_id="alice")
```

### Run Standard vs. Criteria-Based Search

```python
# With criteria
filters = {
    "AND": [
        {"user_id": "alice"}
    ]
}
results_with_criteria = client.search(
    query="Why I am feeling happy today?",
    filters=filters,
    version="v2"
)

# Without criteria
results_without_criteria = client.search(
    query="Why I am feeling happy today?",
    user_id="alice"
)
```

### Compare Results

### Search Results (with Criteria)
```python
[
    {"memory": "User feels refreshed and ready to take on anything on a beautiful sunny day", "score": 0.666, ...},
    {"memory": "User finally has time to draw something after a long time", "score": 0.616, ...},
    {"memory": "User is happy today", "score": 0.500, ...},
    {"memory": "User is curious about how storms form and what triggers them in the atmosphere.", "score": 0.400, ...},
    {"memory": "It has been raining for days, making everything feel heavier.", "score": 0.116, ...}
]
```

### Search Results (without Criteria)
```python
[
    {"memory": "User is happy today", "score": 0.607, ...},
    {"memory": "User feels refreshed and ready to take on anything on a beautiful sunny day", "score": 0.512, ...},
    {"memory": "It has been raining for days, making everything feel heavier.", "score": 0.4617, ...},
    {"memory": "User is curious about how storms form and what triggers them in the atmosphere.", "score": 0.340, ...},
    {"memory": "User finally has time to draw something after a long time", "score": 0.336, ...},
]
```

## Search Results Comparison

1. **Memory Ordering**: With criteria, memories with high joy scores (like feeling refreshed and drawing) are ranked higher, while without criteria, the most relevant memory ("User is happy today") comes first.
2. **Score Distribution**: With criteria, scores are more spread out (0.116 to 0.666) and reflect the criteria weights, while without criteria, scores are more clustered (0.336 to 0.607) and based purely on relevance.
3. **Trait Sensitivity**: “Rainy day” content is penalized due to negative tone. “Storm curiosity” is recognized and scored accordingly.



## Key Differences vs. Standard Search

| Aspect                  | Standard Search                      | Criteria Retrieval                              |
|-------------------------|--------------------------------------|-------------------------------------------------|
| Ranking Logic           | Semantic similarity only             | Semantic + LLM-based criteria scoring           |
| Control Over Relevance  | None                                 | Fully customizable with weighted criteria       |
| Memory Reordering       | Static based on similarity           | Dynamically re-ranked by intent alignment       |
| Emotional Sensitivity   | No tone or trait awareness           | Incorporates emotion, tone, or custom behaviors |
| Version Required        | Defaults                             | `search(version="v2")`                          |

<Note>
If no criteria are defined for a project, `version="v2"` behaves like normal search.
</Note>



## Best Practices

- Choose **3–5 criteria** that reflect your application’s intent
- Make descriptions **clear and distinct**, those are interpreted by an LLM
- Use **stronger weights** to amplify impact of important traits
- Avoid redundant or ambiguous criteria (e.g. “positivity” + “joy”)
- Always handle empty result sets in your application logic



## How It Works

1. **Criteria Definition**: Define custom criteria with a name, description, and weight. These describe what matters in a memory (e.g., joy, urgency, empathy).
2. **Project Configuration**: Register these criteria using `project.update()`. They apply at the project level and influence all searches using `version="v2"`.
3. **Memory Retrieval**: When you perform a search with `version="v2"`, Mem0 first retrieves relevant memories based on the query and your defined criteria.
4. **Weighted Scoring**: Each retrieved memory is evaluated and scored against the defined criteria and weights.

This lets you prioritize memories that align with your agent’s goals and not just those that look similar to the query.

<Note>
Criteria retrieval is currently supported only in search v2. Make sure to use `version="v2"` when performing searches with custom criteria.
</Note>



## Summary

- Define what “relevant” means using criteria
- Apply them per project via `project.update()`
- Use `version="v2"` to activate criteria-aware search
- Build agents that reason not just with relevance, but **contextual importance**

---

Need help designing or tuning your criteria?

<Snippet file="get-help.mdx" />



================================================
FILE: docs/platform/features/custom-categories.mdx
================================================
---
title: Custom Categories
description: 'Enhance your product experience by adding custom categories tailored to your needs'
icon: "tags"
iconType: "solid"
---

## How to set custom categories?

You can now create custom categories tailored to your specific needs, instead of using the default categories such as travel, sports, music, and more (see [default categories](#default-categories) below). **When custom categories are provided, they will override the default categories.**

There are two ways to set custom categories:

### 1. Project Level

You can set custom categories at the project level, which will be applied to all memories added within that project. Mem0 will automatically assign relevant categories from your custom set to new memories based on their content. Setting custom categories at the project level will override the default categories.

Here's how to set custom categories:

<CodeGroup>
```python Code
import os
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

client = MemoryClient()

# Update custom categories
new_categories = [
    {"lifestyle_management_concerns": "Tracks daily routines, habits, hobbies and interests including cooking, time management and work-life balance"},
    {"seeking_structure": "Documents goals around creating routines, schedules, and organized systems in various life areas"},
    {"personal_information": "Basic information about the user including name, preferences, and personality traits"}
]

response = client.project.update(custom_categories=new_categories)
print(response)
```

```json Output
{
    "message": "Updated custom categories"
}
```
</CodeGroup>

This is how you will use these custom categories during the `add` API call:

<CodeGroup>
```python Code
messages = [
    {"role": "user", "content": "My name is Alice. I need help organizing my daily schedule better. I feel overwhelmed trying to balance work, exercise, and social life."},
    {"role": "assistant", "content": "I understand how overwhelming that can feel. Let's break this down together. What specific areas of your schedule feel most challenging to manage?"},
    {"role": "user", "content": "I want to be more productive at work, maintain a consistent workout routine, and still have energy for friends and hobbies."},
    {"role": "assistant", "content": "Those are great goals for better time management. What's one small change you could make to start improving your daily routine?"},
]

# Add memories with custom categories
client.add(messages, user_id="alice")
```

```python Memories with categories
# Following categories will be created for the memories added
Wants to have energy for friends and hobbies (lifestyle_management_concerns)
Wants to maintain a consistent workout routine (seeking_structure, lifestyle_management_concerns)
Wants to be more productive at work (lifestyle_management_concerns, seeking_structure)
Name is Alice (personal_information)
```
</CodeGroup>

You can also retrieve the current custom categories:

<CodeGroup>
```python Code
# Get current custom categories
categories = client.project.get(fields=["custom_categories"])
print(categories)
```

```json Output
{
  "custom_categories": [
    {"lifestyle_management_concerns": "Tracks daily routines, habits, hobbies and interests including cooking, time management and work-life balance"},
    {"seeking_structure": "Documents goals around creating routines, schedules, and organized systems in various life areas"},
    {"personal_information": "Basic information about the user including name, preferences, and personality traits"}
  ]
}

```
</CodeGroup>

These project-level categories will be automatically applied to all new memories added to the project.



### 2. During the `add` API call
You can also set custom categories during the `add` API call. This will override any project-level custom categories for that specific memory addition. For example, if you want to use different categories for food-related memories, you can provide custom categories like "food" and "user_preferences" in the `add` call. These custom categories will be used instead of the project-level categories when categorizing those specific memories.

<CodeGroup>
```python Code
import os
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

client = MemoryClient(api_key="<your_mem0_api_key>")

custom_categories = [
    {"seeking_structure": "Documents goals around creating routines, schedules, and organized systems in various life areas"},
    {"personal_information": "Basic information about the user including name, preferences, and personality traits"}
]

messages = [
    {"role": "user", "content": "My name is Alice. I need help organizing my daily schedule better. I feel overwhelmed trying to balance work, exercise, and social life."},
    {"role": "assistant", "content": "I understand how overwhelming that can feel. Let's break this down together. What specific areas of your schedule feel most challenging to manage?"},
    {"role": "user", "content": "I want to be more productive at work, maintain a consistent workout routine, and still have energy for friends and hobbies."},
    {"role": "assistant", "content": "Those are great goals for better time management. What's one small change you could make to start improving your daily routine?"},
]

client.add(messages, user_id="alice", custom_categories=custom_categories)
```

```python Memories with categories
# Following categories will be created for the memories added
Wants to have energy for friends and hobbies (seeking_structure)
Wants to maintain a consistent workout routine (seeking_structure)
Wants to be more productive at work (seeking_structure)
Name is Alice (personal_information)
```
</CodeGroup>

<Note>Providing more detailed and specific category descriptions will lead to more accurate and relevant memory categorization.</Note>


## Default Categories
Here is the list of **default categories**. If you don't specify any custom categories using the above methods, these will be used as default categories.
```
- personal_details
- family
- professional_details
- sports
- travel
- food
- music
- health
- technology
- hobbies
- fashion
- entertainment
- milestones
- user_preferences
- misc
```

<CodeGroup>
```python Code
import os
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

client = MemoryClient()

messages = [
    {"role": "user", "content": "Hi, my name is Alice."},
    {"role": "assistant", "content": "Hi Alice, what sports do you like to play?"},
    {"role": "user", "content": "I love playing badminton, football, and basketball. I'm quite athletic!"},
    {"role": "assistant", "content": "That's great! Alice seems to enjoy both individual sports like badminton and team sports like football and basketball."},
    {"role": "user", "content": "Sometimes, I also draw and sketch in my free time."},
    {"role": "assistant", "content": "That's cool! I'm sure you're good at it."}
]

# Add memories with default categories
client.add(messages, user_id='alice')
```

```python Memories with categories
# Following categories will be created for the memories added
Sometimes draws and sketches in free time (hobbies)
Is quite athletic (sports)
Loves playing badminton, football, and basketball (sports)
Name is Alice (personal_details)
```
</CodeGroup>

You can check whether default categories are being used by calling `project.get()`. If `custom_categories` returns `None`, it means the default categories are being used.

<CodeGroup>
```python Code
client.project.get(["custom_categories"])
```

```json Output
{
    'custom_categories': None
}
```
</CodeGroup>

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />


================================================
FILE: docs/platform/features/custom-instructions.mdx
================================================
---
title: Custom Instructions
description: 'Control how Mem0 extracts and stores memories using natural language guidelines'
icon: "pencil"
iconType: "solid"
---

## What are Custom Instructions?

Custom instructions are natural language guidelines that tell Mem0 exactly what information to extract and remember from conversations. Think of them as smart filters that ensure your AI application captures only the most relevant data for your specific use case.

<CodeGroup>
```python Python
# Simple example: Health app focusing on wellness
prompt = """
Extract only health and wellness information:
- Symptoms, medications, and treatments
- Exercise routines and dietary habits
- Doctor appointments and health goals

Exclude: Personal identifiers, financial data
"""

client.project.update(custom_instructions=prompt)
```

```javascript JavaScript
// Simple example: Health app focusing on wellness
const prompt = `
Extract only health and wellness information:
- Symptoms, medications, and treatments
- Exercise routines and dietary habits
- Doctor appointments and health goals

Exclude: Personal identifiers, financial data
`;

await client.project.update({ custom_instructions: prompt });
```
</CodeGroup>

## Why Use Custom Instructions?

- **Focus on What Matters**: Only capture information relevant to your application
- **Maintain Privacy**: Explicitly exclude sensitive data like passwords or personal identifiers
- **Ensure Consistency**: All memories follow the same extraction rules across your project
- **Improve Quality**: Filter out noise and irrelevant conversations

## How to Set Custom Instructions

### Basic Setup

<CodeGroup>
```python Python
# Set instructions for your project
client.project.update(custom_instructions="Your guidelines here...")

# Retrieve current instructions
response = client.project.get(fields=["custom_instructions"])
print(response["custom_instructions"])
```

```javascript JavaScript
// Set instructions for your project
await client.project.update({ custom_instructions: "Your guidelines here..." });

// Retrieve current instructions
const response = await client.project.get({ fields: ["custom_instructions"] });
console.log(response.custom_instructions);
```
</CodeGroup>

### Best Practice Template

Structure your instructions using this proven template:

```
Your Task: [Brief description of what to extract]

Information to Extract:
1. [Category 1]:
   - [Specific details]
   - [What to look for]

2. [Category 2]:
   - [Specific details]
   - [What to look for]

Guidelines:
- [Processing rules]
- [Quality requirements]

Exclude:
- [Sensitive data to avoid]
- [Irrelevant information]
```

## Real-World Examples

<Tabs>
  <Tab title="E-commerce Customer Support">
<CodeGroup>
```python Python
instructions = """
Extract customer service information for better support:

1. Product Issues:
   - Product names, SKUs, defects
   - Return/exchange requests
   - Quality complaints

2. Customer Preferences:
   - Preferred brands, sizes, colors
   - Shopping frequency and habits
   - Price sensitivity

3. Service Experience:
   - Satisfaction with support
   - Resolution time expectations
   - Communication preferences

Exclude: Payment card numbers, passwords, personal identifiers.
"""

client.project.update(custom_instructions=instructions)
```

```javascript JavaScript
const instructions = `
Extract customer service information for better support:

1. Product Issues:
   - Product names, SKUs, defects
   - Return/exchange requests
   - Quality complaints

2. Customer Preferences:
   - Preferred brands, sizes, colors
   - Shopping frequency and habits
   - Price sensitivity

3. Service Experience:
   - Satisfaction with support
   - Resolution time expectations
   - Communication preferences

Exclude: Payment card numbers, passwords, personal identifiers.
`;

await client.project.update({ custom_instructions: instructions });
```
</CodeGroup>
  </Tab>
  <Tab title="Personalized Learning Platform">
<CodeGroup>
```python Python
education_prompt = """
Extract learning-related information for personalized education:

1. Learning Progress:
   - Course completions and current modules
   - Skills acquired and improvement areas
   - Learning goals and objectives

2. Student Preferences:
   - Learning styles (visual, audio, hands-on)
   - Time availability and scheduling
   - Subject interests and career goals

3. Performance Data:
   - Assignment feedback and patterns
   - Areas of struggle or strength
   - Study habits and engagement

Exclude: Specific grades, personal identifiers, financial information.
"""

client.project.update(custom_instructions=education_prompt)
```

```javascript JavaScript
const educationPrompt = `
Extract learning-related information for personalized education:

1. Learning Progress:
   - Course completions and current modules
   - Skills acquired and improvement areas
   - Learning goals and objectives

2. Student Preferences:
   - Learning styles (visual, audio, hands-on)
   - Time availability and scheduling
   - Subject interests and career goals

3. Performance Data:
   - Assignment feedback and patterns
   - Areas of struggle or strength
   - Study habits and engagement

Exclude: Specific grades, personal identifiers, financial information.
`;

await client.project.update({ custom_instructions: educationPrompt });
```
</CodeGroup>
  </Tab>
  <Tab title="AI Financial Advisor">
<CodeGroup>
```python Python
finance_prompt = """
Extract financial planning information for advisory services:

1. Financial Goals:
   - Retirement and investment objectives
   - Risk tolerance and preferences
   - Short-term and long-term goals

2. Life Events:
   - Career and income changes
   - Family changes (marriage, children)
   - Major planned purchases

3. Investment Interests:
   - Asset allocation preferences
   - ESG or ethical investment interests
   - Previous investment experience

Exclude: Account numbers, SSNs, passwords, specific financial amounts.
"""

client.project.update(custom_instructions=finance_prompt)
```

```javascript JavaScript
const financePrompt = `
Extract financial planning information for advisory services:

1. Financial Goals:
   - Retirement and investment objectives
   - Risk tolerance and preferences
   - Short-term and long-term goals

2. Life Events:
   - Career and income changes
   - Family changes (marriage, children)
   - Major planned purchases

3. Investment Interests:
   - Asset allocation preferences
   - ESG or ethical investment interests
   - Previous investment experience

Exclude: Account numbers, SSNs, passwords, specific financial amounts.
`;

await client.project.update({ custom_instructions: financePrompt });
```
</CodeGroup>
  </Tab>
</Tabs>

## Advanced Techniques

### Conditional Processing

Handle different conversation types with conditional logic:

<CodeGroup>
```python Python
advanced_prompt = """
Extract information based on conversation context:

IF customer support conversation:
- Issue type, severity, resolution status
- Customer satisfaction indicators

IF sales conversation:
- Product interests, budget range
- Decision timeline and influencers

IF onboarding conversation:
- User experience level
- Feature interests and priorities

Always exclude personal identifiers and maintain professional context.
"""

client.project.update(custom_instructions=advanced_prompt)
```
</CodeGroup>

### Testing Your Instructions

Always test your custom instructions with real messages examples:

<CodeGroup>
```python Python
# Test with sample messages
messages = [
    {"role": "user", "content": "I'm having billing issues with my subscription"},
    {"role": "assistant", "content": "I can help with that. What's the specific problem?"},
    {"role": "user", "content": "I'm being charged twice each month"}
]

# Add the messages and check extracted memories
result = client.add(messages, user_id="test_user")
memories = client.get_all(user_id="test_user")

# Review if the right information was extracted
for memory in memories:
    print(f"Extracted: {memory['memory']}")
```
</CodeGroup>

## Best Practices

### ✅ Do
- **Be specific** about what information to extract
- **Use clear categories** to organize your instructions
- **Test with real conversations** before deploying
- **Explicitly state exclusions** for privacy and compliance
- **Start simple** and iterate based on results

### ❌ Don't
- Make instructions too long or complex
- Create conflicting rules within your guidelines
- Be overly restrictive (balance specificity with flexibility)
- Forget to exclude sensitive information
- Skip testing with diverse conversation examples

## Common Issues and Solutions

| Issue | Solution |
|-------|----------|
| **Instructions too long** | Break into focused categories, keep concise |
| **Missing important data** | Add specific examples of what to capture |
| **Capturing irrelevant info** | Strengthen exclusion rules and be more specific |
| **Inconsistent results** | Clarify guidelines and test with more examples |



================================================
FILE: docs/platform/features/direct-import.mdx
================================================
---
title: Direct Import
description: 'Bypass the memory deduction phase and directly store pre-defined memories for efficient retrieval'
icon: "arrow-right"
iconType: "solid"
---

## How to use Direct Import?
The Direct Import feature allows users to skip the memory deduction phase and directly input pre-defined memories into the system for storage and retrieval.
To enable this feature, you need to set the `infer` parameter to `False` in the `add` method.


<CodeGroup>


```python Python
messages = [
    {"role": "user", "content": "Alice loves playing badminton"},
    {"role": "assistant", "content": "That's great! Alice is a fitness freak"},
    {"role": "user", "content": "Alice mostly cook at home because of gym plan"},
]


client.add(messages, user_id="alice", infer=False)
```

```markdown Output
[]
```
</CodeGroup>

You can see that the output of add call is an empty list.

<Note> Only messages with the role "user" will be used for storage. Messages with roles such as "assistant" or "system" will be ignored during the storage process. </Note>


## How to retrieve memories?

You can retrieve memories using the `search` method.

<CodeGroup>

```python Python
client.search("What is Alice's favorite sport?", user_id="alice", output_format="v1.1")
```

```json Output
{
  "results": [
    {
      "id": "19d6d7aa-2454-4e58-96fc-e74d9e9f8dd1",
      "memory": "Alice loves playing badminton",
      "user_id": "pc123",
      "metadata": null,
      "categories": null,
      "created_at": "2024-10-15T21:52:11.474901-07:00",
      "updated_at": "2024-10-15T21:52:11.474912-07:00"
    }
  ]
}
```

</CodeGroup>

## How to retrieve all memories?

You can retrieve all memories using the `get_all` method.

<CodeGroup>

```python Python
client.get_all(query="What is Alice's favorite sport?", user_id="alice", output_format="v1.1")
```

```json Output
{
  "results": [
    {
      "id": "19d6d7aa-2454-4e58-96fc-e74d9e9f8dd1",
      "memory": "Alice loves playing badminton",
      "user_id": "pc123",
      "metadata": null,
      "categories": null,
      "created_at": "2024-10-15T21:52:11.474901-07:00",
      "updated_at": "2024-10-15T21:52:11.474912-07:00"
    },
    {
      "id": "8557f05d-7b3c-47e5-b409-9886f9e314fc",
      "memory": "Alice mostly cook at home because of gym plan",
      "user_id": "pc123",
      "metadata": null,
      "categories": null,
      "created_at": "2024-10-15T21:52:11.474929-07:00",
      "updated_at": "2024-10-15T21:52:11.474932-07:00"
    }
  ]
}
```

</CodeGroup>

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />


================================================
FILE: docs/platform/features/expiration-date.mdx
================================================
---
title: Expiration Date
description: 'Set time-bound memories in Mem0 with automatic expiration dates to manage temporal information effectively.'
icon: "clock"
iconType: "solid"
---

## Benefits of Memory Expiration

Setting expiration dates for memories offers several advantages:

• **Time-Sensitive Information Management**: Handle information that's only relevant for a specific time period.

• **Event-Based Memory**: Manage information related to upcoming events that becomes irrelevant after the event passes.

These benefits enable more sophisticated memory management for applications where temporal context matters.

## Setting Memory Expiration Date

You can set an expiration date for memories, after which they will no longer be retrieved in searches. This is useful for creating temporary memories or memories that are only relevant for a specific time period.

<CodeGroup>

```python Python
import datetime
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

messages = [
    {
        "role": "user", 
        "content": "I'll be in San Francisco until end of this month."
    }
]

# Set an expiration date for this memory
client.add(messages=messages, user_id="alex", expiration_date=str(datetime.datetime.now().date() + datetime.timedelta(days=30)))

# You can also use an explicit date string
client.add(messages=messages, user_id="alex", expiration_date="2023-08-31")
```

```javascript JavaScript
import MemoryClient from 'mem0ai';
const client = new MemoryClient({ apiKey: 'your-api-key' });

const messages = [
    {
        "role": "user", 
        "content": "I'll be in San Francisco until end of this month."
    }
];

// Set an expiration date 30 days from now
const expirationDate = new Date();
expirationDate.setDate(expirationDate.getDate() + 30);
client.add(messages, { 
    user_id: "alex", 
    expiration_date: expirationDate.toISOString().split('T')[0] 
})
    .then(response => console.log(response))
    .catch(error => console.error(error));

// You can also use an explicit date string
client.add(messages, { 
    user_id: "alex", 
    expiration_date: "2023-08-31" 
})
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [
             {
                "role": "user", 
                "content": "I'll be in San Francisco until end of this month."
            }
         ],
         "user_id": "alex",
         "expiration_date": "2023-08-31"
     }'
```

```json Output
{
    "results": [
        {
            "id": "a1b2c3d4-e5f6-4g7h-8i9j-k0l1m2n3o4p5",
            "data": {
                "memory": "In San Francisco until end of this month"
            },
            "event": "ADD"
        }
    ]
}
```

</CodeGroup>

<Note>
Once a memory reaches its expiration date, it won't be included in search or get results, though the data remains stored in the system.
</Note>

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/platform/features/feedback-mechanism.mdx
================================================
---
title: Feedback Mechanism
icon: "thumbs-up"
iconType: "solid"
---

Mem0's **Feedback Mechanism** allows you to provide feedback on the memories generated by your application. This feedback is used to improve the accuracy of the memories and the search results.

## How it works

The feedback mechanism is a simple API that allows you to provide feedback on the memories generated by your application. The feedback is stored in the database and is used to improve the accuracy of the memories and the search results. Over time, Mem0 continuously learns from this feedback, refining its memory generation and search capabilities for better performance.

## Give Feedback

You can give feedback on a memory by calling the `feedback` method on the Mem0 client.

<CodeGroup>

```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your_api_key")

client.feedback(memory_id="your-memory-id", feedback="NEGATIVE", feedback_reason="I don't like this memory because it is not relevant.")
```

```javascript JavaScript
import MemoryClient from 'mem0ai';

const client = new MemoryClient({ apiKey: 'your-api-key'});

client.feedback({
    memory_id: "your-memory-id", 
    feedback: "NEGATIVE", 
    feedback_reason: "I don't like this memory because it is not relevant."
})
```

</CodeGroup>

## Feedback Types

The `feedback` parameter can be one of the following values:

- `POSITIVE`: The memory is useful.
- `NEGATIVE`: The memory is not useful.
- `VERY_NEGATIVE`: The memory is not useful at all.

## Parameters

The `feedback` method accepts these parameters:

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `memory_id` | string | Yes | The ID of the memory to give feedback on |
| `feedback` | string | No | Type of feedback: `POSITIVE`, `NEGATIVE`, or `VERY_NEGATIVE` |
| `feedback_reason` | string | No | Optional explanation for the feedback |

<Note>
Pass `None` or `null` to the `feedback` and `feedback_reason` parameters to remove existing feedback for a memory.
</Note>

## Bulk Feedback Operations

For applications with high volumes of feedback, you can provide feedback on multiple memories at once:

<CodeGroup>

```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your_api_key")

# Bulk feedback example
feedback_data = [
    {
        "memory_id": "memory-1", 
        "feedback": "POSITIVE", 
        "feedback_reason": "Accurately captured the user's preference"
    },
    {
        "memory_id": "memory-2", 
        "feedback": "NEGATIVE", 
        "feedback_reason": "Contains outdated information"
    }
]

for item in feedback_data:
    client.feedback(**item)
```

```javascript JavaScript
import MemoryClient from 'mem0ai';

const client = new MemoryClient({ apiKey: 'your-api-key'});

// Bulk feedback example
const feedbackData = [
    {
        memory_id: "memory-1", 
        feedback: "POSITIVE", 
        feedback_reason: "Accurately captured the user's preference"
    },
    {
        memory_id: "memory-2", 
        feedback: "NEGATIVE", 
        feedback_reason: "Contains outdated information"
    }
];

for (const item of feedbackData) {
    await client.feedback(item);
}
```

</CodeGroup>

## Best Practices

### When to Provide Feedback
- **Immediately after memory retrieval** when you can assess relevance
- **During user interactions** when users explicitly indicate satisfaction/dissatisfaction
- **Through automated evaluation** using your application's success metrics

### Effective Feedback Reasons
Provide specific, actionable feedback reasons:

✅ **Good examples:**
- "Contains outdated contact information"
- "Accurately captured the user's dietary restrictions"
- "Irrelevant to the current conversation context"

❌ **Avoid vague reasons:**
- "Bad memory"
- "Wrong"
- "Not good"

### Feedback Strategy
1. **Be consistent** - Apply the same criteria across similar memories
2. **Be specific** - Detailed reasons help improve the system faster
3. **Monitor patterns** - Regular feedback analysis helps identify improvement areas

## Error Handling

Handle potential errors when submitting feedback:

<CodeGroup>

```python Python
from mem0 import MemoryClient
from mem0.exceptions import MemoryNotFoundError, APIError

client = MemoryClient(api_key="your_api_key")

try:
    client.feedback(
        memory_id="memory-123", 
        feedback="POSITIVE", 
        feedback_reason="Helpful context for user query"
    )
    print("Feedback submitted successfully")
except MemoryNotFoundError:
    print("Memory not found")
except APIError as e:
    print(f"API error: {e}")
except Exception as e:
    print(f"Unexpected error: {e}")
```

```javascript JavaScript
import MemoryClient from 'mem0ai';

const client = new MemoryClient({ apiKey: 'your-api-key'});

try {
    await client.feedback({
        memory_id: "memory-123", 
        feedback: "POSITIVE", 
        feedback_reason: "Helpful context for user query"
    });
    console.log("Feedback submitted successfully");
} catch (error) {
    if (error.status === 404) {
        console.log("Memory not found");
    } else {
        console.log(`Error: ${error.message}`);
    }
}
```

</CodeGroup>

## Feedback Analytics

Track the impact of your feedback by monitoring memory performance over time. Consider implementing:

- **Feedback completion rates** - What percentage of memories receive feedback
- **Feedback distribution** - Balance of positive vs. negative feedback  
- **Memory quality trends** - How accuracy improves with feedback volume
- **User satisfaction metrics** - Correlation between feedback and user experience




================================================
FILE: docs/platform/features/graph-memory.mdx
================================================
---
title: Graph Memory
icon: "circle-nodes"
iconType: "solid"
description: "Enable graph-based memory retrieval for more contextually relevant results"
---

## Overview

Graph Memory enhances memory pipeline by creating relationships between entities in your data. It builds a network of interconnected information for more contextually relevant search results.

This feature allows your AI applications to understand connections between entities, providing richer context for responses. It's ideal for applications needing relationship tracking and nuanced information retrieval across related memories.

## How Graph Memory Works

The Graph Memory feature analyzes how each entity connects and relates to each other. When enabled:

1. Mem0 automatically builds a graph representation of entities
2. Retrieval considers graph relationships between entities
3. Results include entities that may be contextually important even if they're not direct semantic matches

## Using Graph Memory

To use Graph Memory, you need to enable it in your API calls by setting the `enable_graph=True` parameter. You'll also need to specify `output_format="v1.1"` to receive the enriched response format.

### Adding Memories with Graph Memory

When adding new memories, enable Graph Memory to automatically build relationships with existing memories:

<CodeGroup>

```python Python
from mem0 import MemoryClient

client = MemoryClient(
    api_key="your-api-key",
    org_id="your-org-id",
    project_id="your-project-id"
)

messages = [
    {"role": "user", "content": "My name is Joseph"},
    {"role": "assistant", "content": "Hello Joseph, it's nice to meet you!"},
    {"role": "user", "content": "I'm from Seattle and I work as a software engineer"}
]

# Enable graph memory when adding
client.add(
    messages, 
    user_id="joseph", 
    version="v1", 
    enable_graph=True, 
    output_format="v1.1"
)
```

```javascript JavaScript
import { MemoryClient } from "mem0";

const client = new MemoryClient({
  apiKey: "your-api-key",
  org_id: "your-org-id",
  project_id: "your-project-id"
});

const messages = [
  { role: "user", content: "My name is Joseph" },
  { role: "assistant", content: "Hello Joseph, it's nice to meet you!" },
  { role: "user", content: "I'm from Seattle and I work as a software engineer" }
];

// Enable graph memory when adding
await client.add({
  messages,
  user_id: "joseph",
  version: "v1",
  enable_graph: true,
  output_format: "v1.1"
});
```

```json Output
{
  "results": [
    {
      "memory": "Name is Joseph",
      "event": "ADD",
      "id": "4a5a417a-fa10-43b5-8c53-a77c45e80438"
    },
    {
      "memory": "Is from Seattle",
      "event": "ADD",
      "id": "8d268d0f-5452-4714-b27d-ae46f676a49d"
    },
    {
      "memory": "Is a software engineer",
      "event": "ADD",
      "id": "5f0a184e-ddea-4fe6-9b92-692d6a901df8"
    }
  ]
}
```
</CodeGroup>

The graph memory would look like this:

<Frame>
  <img src="/images/graph-platform.png" alt="Graph Memory Visualization showing relationships between entities" />
</Frame>

<Caption>Graph Memory creates a network of relationships between entities, enabling more contextual retrieval</Caption>


<Note>
Response for the graph memory's `add` operation will not be available directly in the response. 
As adding graph memories is an asynchronous operation due to heavy processing, 
you can use the `get_all()` endpoint to retrieve the memory with the graph metadata.
</Note>


### Searching with Graph Memory

When searching memories, Graph Memory helps retrieve entities that are contextually important even if they're not direct semantic matches.

<CodeGroup>

```python Python
# Search with graph memory enabled
results = client.search(
    "what is my name?", 
    user_id="joseph", 
    enable_graph=True, 
    output_format="v1.1"
)

print(results)
```

```javascript JavaScript
// Search with graph memory enabled
const results = await client.search({
  query: "what is my name?",
  user_id: "joseph",
  enable_graph: true,
  output_format: "v1.1"
});

console.log(results);
```

```json Output
{
  "results": [
    {
      "id": "4a5a417a-fa10-43b5-8c53-a77c45e80438",
      "memory": "Name is Joseph",
      "user_id": "joseph",
      "metadata": null,
      "categories": ["personal_details"],
      "immutable": false,
      "created_at": "2025-03-19T09:09:00.146390-07:00",
      "updated_at": "2025-03-19T09:09:00.146404-07:00",
      "score": 0.3621795393335552
    },
    {
      "id": "8d268d0f-5452-4714-b27d-ae46f676a49d",
      "memory": "Is from Seattle",
      "user_id": "joseph",
      "metadata": null,
      "categories": ["personal_details"],
      "immutable": false,
      "created_at": "2025-03-19T09:09:00.170680-07:00",
      "updated_at": "2025-03-19T09:09:00.170692-07:00",
      "score": 0.31212713194651254
    }
  ],
  "relations": [
    {
      "source": "joseph",
      "source_type": "person",
      "relationship": "name",
      "target": "joseph",
      "target_type": "person",
      "score": 0.39
    }
  ]
}
```

</CodeGroup>

### Retrieving All Memories with Graph Memory

When retrieving all memories, Graph Memory provides additional relationship context:

<CodeGroup>

```python Python
# Get all memories with graph context
memories = client.get_all(
    user_id="joseph", 
    enable_graph=True, 
    output_format="v1.1"
)

print(memories)
```

```javascript JavaScript
// Get all memories with graph context
const memories = await client.getAll({
  user_id: "joseph",
  enable_graph: true,
  output_format: "v1.1"
});

console.log(memories);
```

```json Output
{
  "results": [
    {
      "id": "5f0a184e-ddea-4fe6-9b92-692d6a901df8",
      "memory": "Is a software engineer",
      "user_id": "joseph",
      "metadata": null,
      "categories": ["professional_details"],
      "immutable": false,
      "created_at": "2025-03-19T09:09:00.194116-07:00",
      "updated_at": "2025-03-19T09:09:00.194128-07:00",
    },
    {
      "id": "8d268d0f-5452-4714-b27d-ae46f676a49d",
      "memory": "Is from Seattle",
      "user_id": "joseph",
      "metadata": null,
      "categories": ["personal_details"],
      "immutable": false,
      "created_at": "2025-03-19T09:09:00.170680-07:00",
      "updated_at": "2025-03-19T09:09:00.170692-07:00",
    },
    {
      "id": "4a5a417a-fa10-43b5-8c53-a77c45e80438",
      "memory": "Name is Joseph",
      "user_id": "joseph",
      "metadata": null,
      "categories": ["personal_details"],
      "immutable": false,
      "created_at": "2025-03-19T09:09:00.146390-07:00",
      "updated_at": "2025-03-19T09:09:00.146404-07:00",
    }
  ],
  "relations": [
    {
      "source": "joseph",
      "source_type": "person",
      "relationship": "name",
      "target": "joseph",
      "target_type": "person"
    },
    {
      "source": "joseph",
      "source_type": "person",
      "relationship": "city",
      "target": "seattle",
      "target_type": "city"
    },
    {
      "source": "joseph",
      "source_type": "person",
      "relationship": "job",
      "target": "software engineer",
      "target_type": "job"
    }
  ]
}
```

</CodeGroup>

### Setting Graph Memory at Project Level

Instead of passing `enable_graph=True` to every add call, you can enable it once at the project level:

<CodeGroup>

```python Python
from mem0 import MemoryClient

client = MemoryClient(
    api_key="your-api-key",
    org_id="your-org-id",
    project_id="your-project-id"
)

# Enable graph memory for all operations in this project
client.project.update(enable_graph=True)

# Now all add operations will use graph memory by default
messages = [
    {"role": "user", "content": "My name is Joseph"},
    {"role": "assistant", "content": "Hello Joseph, it's nice to meet you!"},
    {"role": "user", "content": "I'm from Seattle and I work as a software engineer"}
]

client.add(
    messages,
    user_id="joseph",
    output_format="v1.1"
)
```

```javascript JavaScript
import { MemoryClient } from "mem0";

const client = new MemoryClient({
  apiKey: "your-api-key",
  org_id: "your-org-id",
  project_id: "your-project-id"
});

# Enable graph memory for all operations in this project
await client.updateProject({ enable_graph: true, version: "v1" });

# Now all add operations will use graph memory by default
const messages = [
  { role: "user", content: "My name is Joseph" },
  { role: "assistant", content: "Hello Joseph, it's nice to meet you!" },
  { role: "user", content: "I'm from Seattle and I work as a software engineer" }
];

await client.add({
  messages,
  user_id: "joseph",
  output_format: "v1.1"
});
```

</CodeGroup>


## Best Practices

- Enable Graph Memory for applications where understanding context and relationships between memories is important
- Graph Memory works best with a rich history of related conversations
- Consider Graph Memory for long-running assistants that need to track evolving information

## Performance Considerations

Graph Memory requires additional processing and may increase response times slightly for very large memory stores. However, for most use cases, the improved retrieval quality outweighs the minimal performance impact.

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />




================================================
FILE: docs/platform/features/group-chat.mdx
================================================
---
title: Group Chat
description: 'Enable multi-participant conversations with automatic memory attribution to individual speakers'
icon: "users"
iconType: "solid"
---

<Snippet file="paper-release.mdx" />

## Introduction to the Group Chat

## Overview

The Group Chat feature enables Mem0 to process conversations involving multiple participants and automatically attribute memories to individual speakers. This allows for precise tracking of each participant's preferences, characteristics, and contributions in collaborative discussions, team meetings, or multi-agent conversations.

When you provide messages with participant names, Mem0 automatically:
- Extracts memories from each participant's messages separately
- Attributes each memory to the correct speaker using their name as the `user_id` or `agent_id`
- Maintains individual memory profiles for each participant

## How Group Chat Works

Mem0 automatically detects group chat scenarios when messages contain a `name` field:

```json
{
  "role": "user",
  "name": "Alice",
  "content": "Hey team, I think we should use React for the frontend"
}
```

When names are present, Mem0:
- Formats messages as `"Alice (user): content"` for processing
- Extracts memories with proper attribution to each speaker
- Stores memories with the speaker's name as the `user_id` (for users) or `agent_id` (for assistants/agents)

### Memory Attribution Rules

- **User Messages**: The `name` field becomes the `user_id` in stored memories
- **Assistant/Agent Messages**: The `name` field becomes the `agent_id` in stored memories
- **Messages without names**: Fall back to standard processing using role as identifier

## Using Group Chat

### Basic Group Chat

Add memories from a multi-participant conversation:

<CodeGroup>

```python Python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

# Group chat with multiple users
messages = [
    {"role": "user", "name": "Alice", "content": "Hey team, I think we should use React for the frontend"},
    {"role": "user", "name": "Bob", "content": "I disagree, Vue.js would be better for our use case"},
    {"role": "user", "name": "Charlie", "content": "What about considering Angular? It has great enterprise support"},
    {"role": "assistant", "content": "All three frameworks have their merits. Let me summarize the pros and cons of each."}
]

response = client.add(
    messages,
    run_id="group_chat_1",
    output_format="v1.1",
    infer=True
)
print(response)
```

```json Output
{
  "results": [
    {
      "id": "4d82478a-8d50-47e6-9324-1f65efff5829",
      "event": "ADD",
      "memory": "prefers using React for the frontend"
    },
    {
      "id": "1d8b8f39-7b17-4d18-8632-ab1c64fa35b9",
      "event": "ADD",
      "memory": "prefers Vue.js for our use case"
    },
    {
      "id": "147559a8-c5f7-44d0-9418-91f53f7a89a4",
      "event": "ADD",
      "memory": "suggests considering Angular because it has great enterprise support"
    }
  ]
}
```

</CodeGroup>

## Retrieving Group Chat Memories

### Get All Memories for a Session

Retrieve all memories from a specific group chat session:

<CodeGroup>

```python Python
# Get all memories for a specific run_id
filters = {
    "AND": [
        {"user_id": "*"},
        {"run_id": "group_chat_1"}
    ]
}

all_memories = client.get_all(version="v2", filters=filters, page=1)
print(all_memories)
```

```json Output
[
    {
        "id": "147559a8-c5f7-44d0-9418-91f53f7a89a4",
        "memory": "suggests considering Angular because it has great enterprise support",
        "user_id": "charlie",
        "run_id": "group_chat_1",
        "created_at": "2025-06-21T05:51:11.007223-07:00",
        "updated_at": "2025-06-21T05:51:11.626562-07:00"
    },
    {
        "id": "1d8b8f39-7b17-4d18-8632-ab1c64fa35b9",
        "memory": "prefers Vue.js for our use case",
        "user_id": "bob",
        "run_id": "group_chat_1",
        "created_at": "2025-06-21T05:51:08.675301-07:00",
        "updated_at": "2025-06-21T05:51:09.319269-07:00",
    },
    {
        "id": "4d82478a-8d50-47e6-9324-1f65efff5829",
        "memory": "prefers using React for the frontend",
        "user_id": "alice",
        "run_id": "group_chat_1",
        "created_at": "2025-06-21T05:51:05.943223-07:00",
        "updated_at": "2025-06-21T05:51:06.982539-07:00",
    }
]
```

</CodeGroup>

### Get Memories for a Specific Participant

Retrieve memories from a specific participant in a group chat:

<CodeGroup>

```python Python
# Get memories for a specific participant
filters = {
    "AND": [
        {"user_id": "charlie"},
        {"run_id": "group_chat_1"}
    ]
}

charlie_memories = client.get_all(version="v2", filters=filters, page=1)
print(charlie_memories)
```

```json Output
[
    {
        "id": "147559a8-c5f7-44d0-9418-91f53f7a89a4",
        "memory": "suggests considering Angular because it has great enterprise support",
        "user_id": "charlie",
        "run_id": "group_chat_1",
        "created_at": "2025-06-21T05:51:11.007223-07:00",
        "updated_at": "2025-06-21T05:51:11.626562-07:00",

    }
]
```

</CodeGroup>

### Search Within Group Chat Context

Search for specific information within a group chat session:

<CodeGroup>

```python Python
# Search within group chat context
filters = {
    "AND": [
        {"user_id": "charlie"},
        {"run_id": "group_chat_1"}
    ]
}

search_response = client.search(
    query="What are the tasks?",
    filters=filters,
    version="v2"
)
print(search_response)
```

```json Output
[
    {
        "id": "147559a8-c5f7-44d0-9418-91f53f7a89a4",
        "memory": "suggests considering Angular because it has great enterprise support",
        "user_id": "charlie",
        "run_id": "group_chat_1",
        "created_at": "2025-06-21T05:51:11.007223-07:00",
        "updated_at": "2025-06-21T05:51:11.626562-07:00",
    }
]
```

</CodeGroup>

## Async Mode Support

Group chat also supports async processing for improved performance:

<CodeGroup>

```python Python
# Group chat with async mode
response = client.add(
    messages,
    run_id="groupchat_async",
    output_format="v1.1",
    infer=True,
    async_mode=True
)
print(response)
```

</CodeGroup>

## Message Format Requirements

### Required Fields

Each message in a group chat must include:

- `role`: The participant's role (`"user"`, `"assistant"`, `"agent"`)
- `content`: The message content
- `name`: The participant's name (required for group chat detection)

### Example Message Structure

```json
{
  "role": "user",
  "name": "Alice",
  "content": "I think we should use React for the frontend"
}
```
### Supported Roles

- **`user`**: Human participants (memories stored with `user_id`)
- **`assistant`**: AI assistants (memories stored with `agent_id`)

## Best Practices

1. **Consistent Naming**: Use consistent names for participants across sessions to maintain proper memory attribution.

2. **Clear Role Assignment**: Ensure each participant has the correct role (`user`, `assistant`, or `agent`) for proper memory categorization.

3. **Session Management**: Use meaningful `run_id` values to organize group chat sessions and enable easy retrieval.

4. **Memory Filtering**: Use filters to retrieve memories from specific participants or sessions when needed.

5. **Async Processing**: Use `async_mode=True` for large group conversations to improve performance.

6. **Search Context**: Leverage the search functionality to find specific information within group chat contexts.

## Use Cases

- **Team Meetings**: Track individual team member preferences and contributions
- **Customer Support**: Maintain separate memory profiles for different customers
- **Multi-Agent Systems**: Manage conversations with multiple AI assistants
- **Collaborative Projects**: Track individual preferences and expertise areas
- **Group Discussions**: Maintain context for each participant's viewpoints

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" /> 



================================================
FILE: docs/platform/features/memory-export.mdx
================================================
---
title: Memory Export
description: 'Export memories in a structured format using customizable Pydantic schemas'
icon: "file-export"
iconType: "solid"
---

## Overview

The Memory Export feature allows you to create structured exports of memories using customizable Pydantic schemas. This process enables you to transform your stored memories into specific data formats that match your needs. You can apply various filters to narrow down which memories to export and define exactly how the data should be structured.

## Creating a Memory Export

To create a memory export, you'll need to:
1. Define your schema structure
2. Submit an export job
3. Retrieve the exported data

### Define Schema

Here's an example schema for extracting professional profile information:

```json
{
    "$defs": {
        "EducationLevel": {
            "enum": ["high_school", "bachelors", "masters"],
            "title": "EducationLevel",
            "type": "string"
        },
        "EmploymentStatus": {
            "enum": ["full_time", "part_time", "student"],
            "title": "EmploymentStatus", 
            "type": "string"
        }
    },
    "properties": {
        "full_name": {
            "anyOf": [
                {
                    "maxLength": 100,
                    "minLength": 2,
                    "type": "string"
                },
                {
                    "type": "null"
                }
            ],
            "default": null,
            "description": "The professional's full name",
            "title": "Full Name"
        },
        "current_role": {
            "anyOf": [
                {
                    "type": "string"
                },
                {
                    "type": "null"
                }
            ],
            "default": null,
            "description": "Current job title or role",
            "title": "Current Role"
        }
    },
    "title": "ProfessionalProfile",
    "type": "object"
}
```

### Submit Export Job

You can optionally provide additional instructions to guide how memories are processed and structured during export using the `export_instructions` parameter.

<CodeGroup>

```python Python
# Basic export request
response = client.create_memory_export(
    schema=json_schema,
    user_id="alice"
)

# Export with custom instructions
export_instructions = """
1. Create a comprehensive profile with detailed information in each category
2. Only mark fields as "None" when absolutely no relevant information exists
3. Base all information directly on the user's memories
4. When contradictions exist, prioritize the most recent information
5. Clearly distinguish between factual statements and inferences
"""

# For create operation, using only user_id filter as requested
filters = {
    "AND": [
        {"user_id": "alex"}
    ]
}

response = client.create_memory_export(
    schema=json_schema,
    filters=filters,
    export_instructions=export_instructions  # Optional
)

print(response)
```

```javascript JavaScript
// Basic Export request
const response = await client.createMemoryExport({
    schema: json_schema,
    user_id: "alice"
});

// Export with custom instructions
const export_instructions = `
1. Create a comprehensive profile with detailed information in each category
2. Only mark fields as "None" when absolutely no relevant information exists
3. Base all information directly on the user's memories
4. When contradictions exist, prioritize the most recent information
5. Clearly distinguish between factual statements and inferences
`;

// For create operation, using only user_id filter as requested
const filters = {
    "AND": [
        {"user_id": "alex"}
    ]
}

const responseWithInstructions = await client.createMemoryExport({
    schema: json_schema,
    filters: filters,
    export_instructions: export_instructions
});

console.log(responseWithInstructions);
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/export/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "schema": {json_schema},
         "user_id": "alice",
         "export_instructions": "1. Create a comprehensive profile with detailed information\n2. Only mark fields as \"None\" when absolutely no relevant information exists"
     }'
```

```json Output
{
    "message": "Memory export request received. The export will be ready in a few seconds.",
    "id": "550e8400-e29b-41d4-a716-446655440000"
}
```

</CodeGroup>

### Retrieve Export

Once the export job is complete, you can retrieve the structured data in two ways:

#### Using Export ID

<CodeGroup>

```python Python
# Retrieve using export ID
response = client.get_memory_export(memory_export_id="550e8400-e29b-41d4-a716-446655440000")
print(response)
```

```javascript JavaScript
// Retrieve using export ID
const memory_export_id = "550e8400-e29b-41d4-a716-446655440000";

const response = await client.getMemoryExport({
    memory_export_id: memory_export_id
});

console.log(response);
```

```json Output
{
    "full_name": "John Doe",
    "current_role": "Senior Software Engineer",
    "years_experience": 8,
    "employment_status": "full_time",
    "education_level": "masters",
    "skills": ["Python", "AWS", "Machine Learning"]
}
```

</CodeGroup>

#### Using Filters

<CodeGroup>

```python Python
# Retrieve using filters
filters = {
    "AND": [
        {"created_at": {"gte": "2024-07-10", "lte": "2024-07-20"}},
        {"user_id": "alex"}
    ]
}

response = client.get_memory_export(filters=filters)
print(response)
```

```javascript JavaScript
// Retrieve using filters
const filters = {
    "AND": [
        {"created_at": {"gte": "2024-07-10", "lte": "2024-07-20"}},
        {"user_id": "alex"}
    ]
}

const response = await client.getMemoryExport({
    filters: filters
});

console.log(response);
```

```json Output
{
    "full_name": "John Doe",
    "current_role": "Senior Software Engineer",
    "years_experience": 8,
    "employment_status": "full_time",
    "education_level": "masters",
    "skills": ["Python", "AWS", "Machine Learning"]
}
```

</CodeGroup>

## Available Filters

You can apply various filters to customize which memories are included in the export:

- `user_id`: Filter memories by specific user
- `agent_id`: Filter memories by specific agent
- `run_id`: Filter memories by specific run
- `session_id`: Filter memories by specific session
- `created_at`: Filter memories by date

<Note>
The export process may take some time to complete, especially when dealing with a large number of memories or complex schemas.
</Note>

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/platform/features/multimodal-support.mdx
================================================
---
title: Multimodal Support
description: Integrate images and documents into your interactions with Mem0
icon: "image"
iconType: "solid"
---

Mem0 extends its capabilities beyond text by supporting multimodal data, including images and documents. With this feature, users can seamlessly integrate visual and document content into their interactions—allowing Mem0 to extract relevant information from various media types and enrich the memory system.

## How It Works

When a user submits an image or document, Mem0 processes it to extract textual information and other pertinent details. These details are then added to the user's memory, enhancing the system's ability to understand and recall multimodal inputs.

<CodeGroup>
```python Python
import os
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

client = MemoryClient()

messages = [
    {
        "role": "user",
        "content": "Hi, my name is Alice."
    },
    {
        "role": "assistant",
        "content": "Nice to meet you, Alice! What do you like to eat?"
    },
    {
        "role": "user",
        "content": {
            "type": "image_url",
            "image_url": {
                "url": "https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg"
            }
        }
    },
]

# Calling the add method to ingest messages into the memory system
client.add(messages, user_id="alice")
```

```typescript TypeScript
import MemoryClient from "mem0ai";

const client = new MemoryClient();

const messages = [
    {
        role: "user",
        content: "Hi, my name is Alice."
    },
    {
        role: "assistant",
        content: "Nice to meet you, Alice! What do you like to eat?"
    },
    {
        role: "user",
        content: {
            type: "image_url",
            image_url: {
                url: "https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg"
            }
        }
    },
]

await client.add(messages, { user_id: "alice" })
```

```json Output
{
  "results": [
    {
      "memory": "Name is Alice",
      "event": "ADD",
      "id": "7ae113a3-3cb5-46e9-b6f7-486c36391847"
    },
    {
      "memory": "Likes large pizza with toppings including cherry tomatoes, black olives, green spinach, yellow bell peppers, diced ham, and sliced mushrooms",
      "event": "ADD",
      "id": "56545065-7dee-4acf-8bf2-a5b2535aabb3"
    }
  ]
}
```
</CodeGroup>

## Supported Media Types

Mem0 currently supports the following media types:

1. **Images** - JPG, PNG, and other common image formats
2. **Documents** - MDX, TXT, and PDF files

## Integration Methods

### 1. Images

#### Using an Image URL (Recommended)

You can include an image by providing its direct URL. This method is simple and efficient for online images.

```python {2, 5-13}
# Define the image URL
image_url = "https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg"

# Create the message dictionary with the image URL
image_message = {
    "role": "user",
    "content": {
        "type": "image_url",
        "image_url": {
            "url": image_url
        }
    }
}
client.add([image_message], user_id="alice")
```

#### Using Base64 Image Encoding for Local Files

For local images—or when embedding the image directly is preferable—you can use a Base64-encoded string.

<CodeGroup>
```python Python
import base64

# Path to the image file
image_path = "path/to/your/image.jpg"

# Encode the image in Base64
with open(image_path, "rb") as image_file:
    base64_image = base64.b64encode(image_file.read()).decode("utf-8")

# Create the message dictionary with the Base64-encoded image
image_message = {
    "role": "user",
    "content": {
        "type": "image_url",
        "image_url": {
            "url": f"data:image/jpeg;base64,{base64_image}"
        }
    }
}
client.add([image_message], user_id="alice")
```

```typescript TypeScript
import MemoryClient from "mem0ai";
import fs from 'fs';

const imagePath = 'path/to/your/image.jpg';

const base64Image = fs.readFileSync(imagePath, { encoding: 'base64' });

const imageMessage = {
    role: "user",
    content: {
        type: "image_url",
        image_url: {
            url: `data:image/jpeg;base64,${base64Image}`
        }
    }
};

await client.add([imageMessage], { user_id: "alice" })
```
</CodeGroup>

### 2. Text Documents (MDX/TXT)

Mem0 supports both online and local text documents in MDX or TXT format.

#### Using a Document URL

```python
# Define the document URL
document_url = "https://www.w3.org/TR/2003/REC-PNG-20031110/iso_8859-1.txt"

# Create the message dictionary with the document URL
document_message = {
    "role": "user",
    "content": {
        "type": "mdx_url",
        "mdx_url": {
            "url": document_url
        }
    }
}
client.add([document_message], user_id="alice")
```

#### Using Base64 Encoding for Local Documents

```python
import base64

# Path to the document file
document_path = "path/to/your/document.txt"

# Function to convert file to Base64
def file_to_base64(file_path):
    with open(file_path, "rb") as file:
        return base64.b64encode(file.read()).decode('utf-8')

# Encode the document in Base64
base64_document = file_to_base64(document_path)

# Create the message dictionary with the Base64-encoded document
document_message = {
    "role": "user",
    "content": {
        "type": "mdx_url",
        "mdx_url": {
            "url": base64_document
        }
    }
}
client.add([document_message], user_id="alice")
```

### 3. PDF Documents

Mem0 supports PDF documents via URL.

```python
# Define the PDF URL
pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"

# Create the message dictionary with the PDF URL
pdf_message = {
    "role": "user",
    "content": {
        "type": "pdf_url",
        "pdf_url": {
            "url": pdf_url
        }
    }
}
client.add([pdf_message], user_id="alice")
```

## Complete Example with Multiple File Types

Here's a comprehensive example showing how to work with different file types:

```python
import base64
from mem0 import MemoryClient

client = MemoryClient()

def file_to_base64(file_path):
    with open(file_path, "rb") as file:
        return base64.b64encode(file.read()).decode('utf-8')

# Example 1: Using an image URL
image_message = {
    "role": "user",
    "content": {
        "type": "image_url",
        "image_url": {
            "url": "https://example.com/sample-image.jpg"
        }
    }
}

# Example 2: Using a text document URL
text_message = {
    "role": "user",
    "content": {
        "type": "mdx_url",
        "mdx_url": {
            "url": "https://www.w3.org/TR/2003/REC-PNG-20031110/iso_8859-1.txt"
        }
    }
}

# Example 3: Using a PDF URL
pdf_message = {
    "role": "user",
    "content": {
        "type": "pdf_url",
        "pdf_url": {
            "url": "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
        }
    }
}

# Add each message to the memory system
client.add([image_message], user_id="alice")
client.add([text_message], user_id="alice")
client.add([pdf_message], user_id="alice")
```

Using these methods, you can seamlessly incorporate various media types into your interactions, further enhancing Mem0's multimodal capabilities.

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/platform/features/platform-overview.mdx
================================================
---
title: Overview
icon: "info"
iconType: "solid"
---

Learn about the key features and capabilities that make Mem0 a powerful platform for memory management and retrieval.

## Core Features

<CardGroup>
  <Card title="Advanced Retrieval" icon="magnifying-glass" href="advanced-retrieval">
    Superior search results using state-of-the-art algorithms, including keyword search, reranking, and filtering capabilities.
  </Card>
  <Card title="Contextual Add" icon="square-plus" href="contextual-add">
    Only send your latest conversation history - we automatically retrieve the rest and generate properly contextualized memories.
  </Card>
  <Card title="Multimodal Support" icon="photo-film" href="multimodal-support">
    Process and analyze various types of content including images.
  </Card>
  <Card title="Memory Customization" icon="filter" href="selective-memory">
    Customize and curate stored memories to focus on relevant information while excluding unnecessary data, enabling improved accuracy, privacy control, and resource efficiency.
  </Card>
  <Card title="Custom Categories" icon="tags" href="custom-categories">
    Create and manage custom categories to organize memories based on your specific needs and requirements.
  </Card>
  <Card title="Custom Instructions" icon="list-check" href="custom-instructions">
    Define specific guidelines for your project to ensure consistent handling of information and requirements.
  </Card>
  <Card title="Direct Import" icon="message-bot" href="direct-import">
    Tailor the behavior of your Mem0 instance with custom prompts for specific use cases or domains.
  </Card>
  <Card title="Async Client" icon="bolt" href="async-client">
    Asynchronous client for non-blocking operations and high concurrency applications.
  </Card>
  <Card title="Memory Export" icon="file-export" href="memory-export">
    Export memories in structured formats using customizable Pydantic schemas.
  </Card>
  <Card title="Graph Memory" icon="circle-nodes" href="graph-memory">
    Add memories in the form of nodes and edges in a graph database and search for related memories.
  </Card>
</CardGroup>

## Getting Help

If you have any questions about these features or need assistance, our team is here to help:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/platform/features/selective-memory.mdx
================================================
---
title: Memory Customization
description: 'Mem0 supports customizing the memories you store, allowing you to focus on pertinent information while omitting irrelevant data.'
icon: "filter"
iconType: "solid"
---

## Benefits of Memory Customization

Memory customization offers several key benefits:

• **Focused Storage**: Store only relevant information for a streamlined system.

• **Improved Accuracy**: Curate memories for more accurate and relevant retrieval.

• **Enhanced Privacy**: Exclude sensitive information for better privacy control.

• **Resource Efficiency**: Optimize storage and processing by keeping only pertinent data.

• **Personalization**: Tailor the experience to individual user preferences.

• **Contextual Relevance**: Improve effectiveness in specialized domains or applications.

These benefits allow users to fine-tune their memory systems, creating a more powerful and personalized AI assistant experience.


## Memory Inclusion
Users can define specific kinds of memories to store. This feature enhances memory management by focusing on relevant information, resulting in a more efficient and personalized experience.
Here’s how you can do it:

```python
import os
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

m = MemoryClient()

# Define what to include
includes = "sports related things"

messages = [
    {"role": "user", "content": "Hi, my name is Alice and I love to play badminton"},
    {"role": "assistant", "content": "Nice to meet you, Alice! Badminton is a great sport."},
    {"role": "user", "content": "I love music festivals"},
    {"role": "assistant", "content": "Music festivals are exciting! Do you have a favorite one?"},
    {"role": "user", "content": "I love eating spicy food"},
    {"role": "assistant", "content": "Spicy food is delicious! What's your favorite spicy dish?"},
    {"role": "user", "content": "I love playing baseball with my friends"},
    {"role": "assistant", "content": "Baseball with friends sounds fun!"},
]
```

<CodeGroup>
```python Code
client.add(messages, user_id="alice", includes=includes)
```

```json Stored Memories
User's name is Alice.
Alice loves to play badminton.
User loves playing baseball with friends.
```
</CodeGroup>




## Memory Exclusion

In addition to specifying what to include, users can also define exclusion rules for their memory management. This feature allows for fine-tuning the memory system by instructing it to omit certain types of information.
Here’s how you can do it:

```python
from mem0 import MemoryClient

m = MemoryClient(api_key="xxx")

# Define what to exclude
excludes = "food preferences"

messages = [
    {"role": "user", "content": "Hi, my name is Alice and I love to play badminton"},
    {"role": "assistant", "content": "Nice to meet you, Alice! Badminton is a great sport."},
    {"role": "user", "content": "I love music festivals"},
    {"role": "assistant", "content": "Music festivals are exciting! Do you have a favorite one?"},
    {"role": "user", "content": "I love eating spicy food"},
    {"role": "assistant", "content": "Spicy food is delicious! What's your favorite spicy dish?"},
    {"role": "user", "content": "I love playing baseball with my friends"},
    {"role": "assistant", "content": "Baseball with friends sounds fun!"},
]
```

<CodeGroup>
```python Code
client.add(messages, user_id="alice", excludes=excludes)
```

```json Stored Memories
User's name is Alice.
Alice loves to play badminton.
Loves music festivals.
User loves playing baseball with friends.
```
</CodeGroup>



If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />


================================================
FILE: docs/platform/features/timestamp.mdx
================================================
---
title: Memory Timestamps
description: 'Add timestamps to your memories to maintain chronological accuracy and historical context'
icon: "clock"
iconType: "solid"
---

## Overview

The Memory Timestamps feature allows you to specify when a memory was created, regardless of when it's actually added to the system. This powerful capability enables you to:

- Maintain accurate chronological ordering of memories
- Import historical data with proper timestamps
- Create memories that reflect when events actually occurred
- Build timelines with precise temporal information

By leveraging custom timestamps, you can ensure that your memory system maintains an accurate representation of when information was generated or events occurred.

## Benefits of Custom Timestamps

Custom timestamps offer several important benefits:

• **Historical Accuracy**: Preserve the exact timing of past events and information.

• **Data Migration**: Seamlessly migrate existing data while maintaining original timestamps.

• **Time-Sensitive Analysis**: Enable time-based analysis and pattern recognition across memories.

• **Consistent Chronology**: Maintain proper ordering of memories for coherent storytelling.

## Using Custom Timestamps

When adding new memories, you can specify a custom timestamp to indicate when the memory was created. This timestamp will be used instead of the current time.

### Adding Memories with Custom Timestamps

<CodeGroup>

```python Python
import os
import time
from datetime import datetime, timedelta

from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

client = MemoryClient()

# Get the current time
current_time = datetime.now()

# Calculate 5 days ago
five_days_ago = current_time - timedelta(days=5)

# Convert to Unix timestamp (seconds since epoch)
unix_timestamp = int(five_days_ago.timestamp())

# Add memory with custom timestamp
messages = [
    {"role": "user", "content": "I'm travelling to SF"}
]
client.add(messages, user_id="user1", timestamp=unix_timestamp)
```

```javascript JavaScript
import MemoryClient from 'mem0ai';
const client = new MemoryClient({ apiKey: 'your-api-key' });

// Get the current time
const currentTime = new Date();

// Calculate 5 days ago
const fiveDaysAgo = new Date();
fiveDaysAgo.setDate(currentTime.getDate() - 5);

// Convert to Unix timestamp (seconds since epoch)
const unixTimestamp = Math.floor(fiveDaysAgo.getTime() / 1000);

// Add memory with custom timestamp
const messages = [
    {"role": "user", "content": "I'm travelling to SF"}
]
client.add(messages, { user_id: "user1", timestamp: unixTimestamp })
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

```bash cURL
curl -X POST "https://api.mem0.ai/v1/memories/" \
     -H "Authorization: Token your-api-key" \
     -H "Content-Type: application/json" \
     -d '{
         "messages": [{"role": "user", "content": "I'm travelling to SF"}],
         "user_id": "user1",
         "timestamp": 1721577600
     }'
```

```json Output
{
    "results": [
        {
            "id": "a1b2c3d4-e5f6-4g7h-8i9j-k0l1m2n3o4p5",
            "data": {"memory": "Travelling to SF"},
            "event": "ADD"
        }
    ]
}
```

</CodeGroup>

### Timestamp Format

When specifying a custom timestamp, you should provide a Unix timestamp (seconds since epoch). This is an integer representing the number of seconds that have elapsed since January 1, 1970 (UTC).

For example, to create a memory with a timestamp of January 1, 2023:

<CodeGroup>

```python Python
# January 1, 2023 timestamp
january_2023_timestamp = 1672531200  # Unix timestamp for 2023-01-01 00:00:00 UTC

messages = [
    {"role": "user", "content": "I'm travelling to SF"}
]
client.add(messages, user_id="user1", timestamp=january_2023_timestamp)
```

```javascript JavaScript
// January 1, 2023 timestamp
const january2023Timestamp = 1672531200;  // Unix timestamp for 2023-01-01 00:00:00 UTC

const messages = [
    {"role": "user", "content": "I'm travelling to SF"}
]
client.add(messages, { user_id: "user1", timestamp: january2023Timestamp })
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

</CodeGroup>

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />



================================================
FILE: docs/platform/features/webhooks.mdx
================================================
---
title: Webhooks
description: 'Configure and manage webhooks to receive real-time notifications about memory events'
icon: "webhook"
iconType: "solid"
---

## Overview

Webhooks enable real-time notifications for memory events in your Mem0 project. Webhooks are configured at the project level, meaning each webhook is tied to a specific project and receives events solely from that project. You can configure webhooks to send HTTP POST requests to your specified URLs whenever memories are created, updated, or deleted.

## Managing Webhooks

### Create Webhook

Create a webhook for your project; it will receive events only from that project:
<CodeGroup>

```python Python
import os
from mem0 import MemoryClient

os.environ["MEM0_API_KEY"] = "your-api-key"

client = MemoryClient()

# Create webhook in a specific project
webhook = client.create_webhook(
    url="https://your-app.com/webhook",
    name="Memory Logger",
    project_id="proj_123",
    event_types=["memory_add"]
)
print(webhook)
```

```javascript JavaScript
const { MemoryClient } = require('mem0ai');
const client = new MemoryClient({ apiKey: 'your-api-key'});

// Create webhook in a specific project
const webhook = await client.createWebhook({
    url: "https://your-app.com/webhook",
    name: "Memory Logger",
    projectId: "proj_123",
    eventTypes: ["memory_add"]
});
console.log(webhook);
```

```json Output
{
  "webhook_id": "wh_123",
  "name": "Memory Logger",
  "url": "https://your-app.com/webhook",
  "event_types": ["memory_add"],
  "project": "default-project",
  "is_active": true,
  "created_at": "2025-02-18T22:59:56.804993-08:00",
  "updated_at": "2025-02-18T23:06:41.479361-08:00"
}
```

</CodeGroup>

### Get Webhooks

Retrieve all webhooks for your project:

<CodeGroup>

```python Python
# Get webhooks for a specific project
webhooks = client.get_webhooks(project_id="proj_123")
print(webhooks)
```

```javascript JavaScript
// Get webhooks for a specific project
const webhooks = await client.getWebhooks({projectId: "proj_123"});
console.log(webhooks);
```

```json Output
[
    {
        "webhook_id": "wh_123",
        "url": "https://mem0.ai",
        "name": "mem0",
        "owner": "john",
        "event_types": ["memory_add"],
        "project": "default-project",
        "is_active": true,
        "created_at": "2025-02-18T22:59:56.804993-08:00",
        "updated_at": "2025-02-18T23:06:41.479361-08:00"
    }
]

```

</CodeGroup>

### Update Webhook

Update an existing webhook’s configuration by specifying its `webhook_id`:

<CodeGroup>

```python Python
# Update webhook for a specific project
updated_webhook = client.update_webhook(
    name="Updated Logger",
    url="https://your-app.com/new-webhook",
    event_types=["memory_update", "memory_add"],
    webhook_id="wh_123"
)
print(updated_webhook)
```

```javascript JavaScript
// Update webhook for a specific project
const updatedWebhook = await client.updateWebhook({
    name: "Updated Logger",
    url: "https://your-app.com/new-webhook",
    eventTypes: ["memory_update", "memory_add"],
    webhookId: "wh_123"
});
console.log(updatedWebhook);
```

```json Output
{
  "message": "Webhook updated successfully"
}
```

</CodeGroup>

### Delete Webhook

Delete a webhook by providing its `webhook_id`:

<CodeGroup>

```python Python
# Delete webhook from a specific project
response = client.delete_webhook(webhook_id="wh_123")
print(response)
```

```javascript JavaScript
// Delete webhook from a specific project
const response = await client.deleteWebhook({webhookId: "wh_123"});
console.log(response);
```

```json Output
{
  "message": "Webhook deleted successfully"
}
```

</CodeGroup>

## Event Types

Mem0 supports the following event types for webhooks:

- `memory_add`: Triggered when a memory is added.
- `memory_update`: Triggered when an existing memory is updated.
- `memory_delete`: Triggered when a memory is deleted.

## Webhook Payload

When a memory event occurs, Mem0 sends an HTTP POST request to your webhook URL with the following payload:

```json
{
    "event_details": {
        "id": "a1b2c3d4-e5f6-4g7h-8i9j-k0l1m2n3o4p5",
            "data": {
            "memory": "Name is Alex"
            },
        "event": "ADD"
    }
}
```

## Best Practices

1. **Implement Retry Logic**: Ensure your webhook endpoint can handle temporary failures by implementing retry logic.

2. **Verify Webhook Source**: Implement security measures to verify that webhook requests originate from Mem0.

3. **Process Events Asynchronously**: Process webhook events asynchronously to avoid timeouts and ensure reliable handling.

4. **Monitor Webhook Health**: Regularly review your webhook logs to ensure functionality and promptly address any delivery failures.

If you have any questions, please feel free to reach out to us using one of the following methods:

<Snippet file="get-help.mdx" />

