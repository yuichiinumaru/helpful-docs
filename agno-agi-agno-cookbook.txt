Directory structure:
└── agno-agi-agno/
    └── cookbook/
        ├── README.md
        ├── __init__.py
        ├── agent_concepts/
        │   ├── README.md
        │   ├── __init__.py
        │   ├── agentic_search/
        │   │   ├── __init__.py
        │   │   ├── agentic_rag.py
        │   │   ├── agentic_rag_infinity_reranker.py
        │   │   ├── agentic_rag_with_reasoning.py
        │   │   └── lightrag/
        │   │       ├── readme.md
        │   │       └── agentic_rag_with_lightrag.py
        │   ├── async/
        │   │   ├── __init__.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── data_analyst.py
        │   │   ├── delay.py
        │   │   ├── gather_agents.py
        │   │   ├── reasoning.py
        │   │   ├── structured_output.py
        │   │   └── tool_use.py
        │   ├── context/
        │   │   ├── __init__.py
        │   │   ├── add_context.py
        │   │   ├── agent_context.py
        │   │   └── context_in_instructions.py
        │   ├── events/
        │   │   ├── __init__.py
        │   │   ├── basic_agent_events.py
        │   │   └── reasoning_agent_events.py
        │   ├── knowledge/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── arxiv_kb.py
        │   │   ├── arxiv_kb_async.py
        │   │   ├── combined_kb.py
        │   │   ├── combined_kb_async.py
        │   │   ├── csv_kb.py
        │   │   ├── csv_kb_async.py
        │   │   ├── csv_url_kb.py
        │   │   ├── csv_url_kb_async.py
        │   │   ├── doc_kb.py
        │   │   ├── doc_kb_async.py
        │   │   ├── docx_kb.py
        │   │   ├── docx_kb_async.py
        │   │   ├── firecrawl_kb.py
        │   │   ├── firecrawl_kb_async.py
        │   │   ├── gcs_pdf_kb.py
        │   │   ├── gcs_pdf_kb_async.py
        │   │   ├── json_kb.py
        │   │   ├── json_kb_async.py
        │   │   ├── langchain_kb.py
        │   │   ├── llamaindex_kb.py
        │   │   ├── markdown_kb.py
        │   │   ├── markdown_kb_async.py
        │   │   ├── pdf_bytes_kb.py
        │   │   ├── pdf_bytes_kb_async.py
        │   │   ├── pdf_kb.py
        │   │   ├── pdf_kb_async.py
        │   │   ├── pdf_kb_password.py
        │   │   ├── pdf_kb_url_password.py
        │   │   ├── pdf_url_kb.py
        │   │   ├── pdf_url_kb_async.py
        │   │   ├── s3_pdf_kb.py
        │   │   ├── s3_pdf_kb_async.py
        │   │   ├── s3_text_kb.py
        │   │   ├── s3_text_kb_async.py
        │   │   ├── text_kb.py
        │   │   ├── text_kb_async.py
        │   │   ├── url_kb.py
        │   │   ├── url_kb_async.py
        │   │   ├── website_kb.py
        │   │   ├── website_kb_async.py
        │   │   ├── wikipedia_kb.py
        │   │   ├── youtube_kb.py
        │   │   ├── youtube_kb_async.py
        │   │   ├── chunking/
        │   │   │   ├── __init__.py
        │   │   │   ├── agentic_chunking.py
        │   │   │   ├── csv_row_chunking.py
        │   │   │   ├── default.py
        │   │   │   ├── document_chunking.py
        │   │   │   ├── fixed_size_chunking.py
        │   │   │   ├── recursive_chunking.py
        │   │   │   └── semantic_chunking.py
        │   │   ├── custom/
        │   │   │   ├── __init__.py
        │   │   │   ├── async_retriever.py
        │   │   │   └── retriever.py
        │   │   ├── embedders/
        │   │   │   ├── __init__.py
        │   │   │   ├── aws_bedrock_embedder.py
        │   │   │   ├── azure_embedder.py
        │   │   │   ├── cohere_embedder.py
        │   │   │   ├── fireworks_embedder.py
        │   │   │   ├── gemini_embedder.py
        │   │   │   ├── huggingface_embedder.py
        │   │   │   ├── jina_embedder.py
        │   │   │   ├── langdb_embedder.py
        │   │   │   ├── mistral_embedder.py
        │   │   │   ├── nebius_embedder.py
        │   │   │   ├── ollama_embedder.py
        │   │   │   ├── openai_embedder.py
        │   │   │   ├── qdrant_fastembed.py
        │   │   │   ├── sentence_transformer_embedder.py
        │   │   │   ├── together_embedder.py
        │   │   │   └── voyageai_embedder.py
        │   │   ├── filters/
        │   │   │   ├── __init__.py
        │   │   │   ├── filtering_chroma_db.py
        │   │   │   ├── filtering_lance_db.py
        │   │   │   ├── filtering_milvus.py
        │   │   │   ├── filtering_mongo_db.py
        │   │   │   ├── filtering_pgvector.py
        │   │   │   ├── filtering_pinecone.py
        │   │   │   ├── filtering_qdrant_db.py
        │   │   │   ├── filtering_surrealdb.py
        │   │   │   ├── filtering_traditional_RAG.py
        │   │   │   ├── filtering_weaviate.py
        │   │   │   ├── csv/
        │   │   │   │   ├── agentic_filtering.py
        │   │   │   │   ├── filtering.py
        │   │   │   │   └── filtering_on_load.py
        │   │   │   ├── csv_url/
        │   │   │   │   ├── agentic_filtering.py
        │   │   │   │   ├── filtering.py
        │   │   │   │   └── filtering_on_load.py
        │   │   │   ├── document/
        │   │   │   │   ├── agentic_filtering.py
        │   │   │   │   ├── filtering.py
        │   │   │   │   └── filtering_on_load.py
        │   │   │   ├── docx/
        │   │   │   │   ├── agentic_filtering.py
        │   │   │   │   ├── async_filtering.py
        │   │   │   │   ├── filtering.py
        │   │   │   │   └── filtering_on_load.py
        │   │   │   ├── json/
        │   │   │   │   ├── agentic_filtering.py
        │   │   │   │   ├── async_filtering.py
        │   │   │   │   ├── filtering.py
        │   │   │   │   └── filtering_on_load.py
        │   │   │   ├── pdf/
        │   │   │   │   ├── agentic_filtering.py
        │   │   │   │   ├── async_filtering.py
        │   │   │   │   ├── filtering.py
        │   │   │   │   └── filtering_on_load.py
        │   │   │   ├── pdf_url/
        │   │   │   │   ├── agentic_filtering.py
        │   │   │   │   ├── async_filtering.py
        │   │   │   │   ├── filtering.py
        │   │   │   │   └── filtering_on_load.py
        │   │   │   └── text/
        │   │   │       ├── agentic_filtering.py
        │   │   │       ├── async_filtering.py
        │   │   │       ├── filtering.py
        │   │   │       └── filtering_on_load.py
        │   │   ├── readers/
        │   │   │   ├── __init__.py
        │   │   │   ├── firecrawl_reader.py
        │   │   │   ├── json_reader.py
        │   │   │   ├── url_reader.py
        │   │   │   └── web_reader.py
        │   │   ├── search_type/
        │   │   │   ├── __init__.py
        │   │   │   ├── hybrid_search.py
        │   │   │   ├── keyword_search.py
        │   │   │   └── vector_search.py
        │   │   └── vector_dbs/
        │   │       ├── README.md
        │   │       ├── __init__.py
        │   │       ├── singlestore.py
        │   │       ├── upstash_db.py
        │   │       ├── cassandra_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_cassandra_db.py
        │   │       │   └── cassandra_db.py
        │   │       ├── chroma_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_chroma_db.py
        │   │       │   └── chroma_db.py
        │   │       ├── clickhouse_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_clickhouse.py
        │   │       │   └── clickhouse.py
        │   │       ├── couchbase_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_couchbase_db.py
        │   │       │   └── couchbase_db.py
        │   │       ├── lance_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_lance_db.py
        │   │       │   ├── lance_db.py
        │   │       │   ├── lance_db_hybrid_search.py
        │   │       │   └── remote_lance_db.py
        │   │       ├── milvus_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_milvus_db.py
        │   │       │   ├── async_milvus_db_hybrid_search.py
        │   │       │   ├── milvus_db.py
        │   │       │   └── milvus_db_hybrid_search.py
        │   │       ├── mongo_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_mongo_db.py
        │   │       │   ├── cosmos_mongodb_vcore.py
        │   │       │   ├── mongo_db.py
        │   │       │   └── mongo_db_hybrid_search.py
        │   │       ├── pgvector_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_pg_vector.py
        │   │       │   ├── pg_vector.py
        │   │       │   └── pgvector_hybrid_search.py
        │   │       ├── pinecone_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_pinecone_db.py
        │   │       │   └── pinecone_db.py
        │   │       ├── qdrant_db/
        │   │       │   ├── __init__.py
        │   │       │   ├── async_qdrant_db.py
        │   │       │   ├── qdrant_db.py
        │   │       │   └── qdrant_db_hybrid_search.py
        │   │       ├── surrealdb/
        │   │       │   ├── async_surreal_db.py
        │   │       │   └── surreal_db.py
        │   │       └── weaviate_db/
        │   │           ├── __init__.py
        │   │           ├── async_weaviate_db.py
        │   │           ├── weaviate_db.py
        │   │           ├── weaviate_db_hybrid_search.py
        │   │           └── weaviate_db_upsert.py
        │   ├── memory/
        │   │   ├── 00_builtin_memory.py
        │   │   ├── 01_standalone_memory.py
        │   │   ├── 02_persistent_memory.py
        │   │   ├── 03_memory_creation.py
        │   │   ├── 04_custom_memory_creation.py
        │   │   ├── 05_memory_search.py
        │   │   ├── 06_agent_with_memory.py
        │   │   ├── 07_agentic_memory.py
        │   │   ├── 08_agent_with_summaries.py
        │   │   ├── 09_agents_share_memory.py
        │   │   ├── 10_custom_memory.py
        │   │   ├── 11_multi_user_multi_session_chat.py
        │   │   ├── 12_multi_user_multi_session_chat_concurrent.py
        │   │   ├── 13_memory_references.py
        │   │   ├── 14_session_summary_references.py
        │   │   ├── 15_memory_demo.py
        │   │   ├── 16_custom_memory_instructions.py
        │   │   ├── 17_builtin_memory_with_session_summary.py
        │   │   ├── 18_persistent_memory_firestore.py
        │   │   ├── 19_share_memory_and_history_between_agents.py
        │   │   ├── __init__.py
        │   │   ├── playground.py
        │   │   ├── utils.py
        │   │   ├── db/
        │   │   │   ├── __init__.py
        │   │   │   ├── mongodb.py
        │   │   │   ├── postgres.py
        │   │   │   ├── redis_db.py
        │   │   │   └── sqlite.py
        │   │   └── integrations/
        │   │       ├── __init__.py
        │   │       ├── mem0_integration.py
        │   │       └── zep_integration.py
        │   ├── memory_legacy/
        │   │   ├── 01_builtin_memory.py
        │   │   ├── 02_persistent_memory.py
        │   │   ├── 03_memories_and_summaries.py
        │   │   ├── 04_persistent_memory_postgres.py
        │   │   ├── 05_memories_and_summaries_postgres.py
        │   │   ├── 06_memories_and_summaries_sqlite_async.py
        │   │   ├── 07_persistent_memory_mongodb.py
        │   │   ├── 08_mem0_memory.py
        │   │   ├── 09_using_other_models_for_memory.py
        │   │   └── __init__.py
        │   ├── multimodal/
        │   │   ├── __init__.py
        │   │   ├── audio_input_output.py
        │   │   ├── audio_multi_turn.py
        │   │   ├── audio_sentiment_analysis.py
        │   │   ├── audio_streaming.py
        │   │   ├── audio_to_text.py
        │   │   ├── generate_image_with_intermediate_steps.py
        │   │   ├── generate_video_using_models_lab.py
        │   │   ├── generate_video_using_replicate.py
        │   │   ├── image_to_audio.py
        │   │   ├── image_to_image_agent.py
        │   │   ├── image_to_structured_output.py
        │   │   ├── image_to_text.py
        │   │   ├── video_caption_agent.py
        │   │   └── video_to_shorts.py
        │   ├── other/
        │   │   ├── __init__.py
        │   │   ├── agent_extra_metrics.py
        │   │   ├── agent_metrics.py
        │   │   ├── datetime_instructions.py
        │   │   ├── debug_level.py
        │   │   ├── dynamic_instructions.py
        │   │   ├── dynamic_session_state.py
        │   │   ├── image_input_high_fidelity.py
        │   │   ├── input_as_dict.py
        │   │   ├── input_as_list.py
        │   │   ├── input_as_message.py
        │   │   ├── input_as_messages_list.py
        │   │   ├── instructions.py
        │   │   ├── instructions_via_function.py
        │   │   ├── intermediate_steps.py
        │   │   ├── location_instructions.py
        │   │   ├── output_model.py
        │   │   ├── parse_model.py
        │   │   ├── parse_model_ollama.py
        │   │   ├── parse_model_stream.py
        │   │   ├── response_as_variable.py
        │   │   ├── run_response_events.py
        │   │   ├── scenario_testing.py
        │   │   ├── success_criteria.py
        │   │   └── tool_call_limit.py
        │   ├── rag/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── agentic_rag_agent_ui.py
        │   │   ├── agentic_rag_lancedb.py
        │   │   ├── agentic_rag_pgvector.py
        │   │   ├── agentic_rag_with_reranking.py
        │   │   ├── local_rag_langchain_qdrant.py
        │   │   ├── rag_sentence_transformer.py
        │   │   ├── rag_with_lance_db_and_sqlite.py
        │   │   ├── traditional_rag_lancedb.py
        │   │   └── traditional_rag_pgvector.py
        │   ├── state/
        │   │   ├── __init__.py
        │   │   ├── last_n_session_messages.py
        │   │   ├── session_state.py
        │   │   ├── session_state_on_run.py
        │   │   ├── session_state_storage.py
        │   │   ├── session_state_user_id.py
        │   │   ├── shopping_list.py
        │   │   └── state_in_prompt.py
        │   ├── storage/
        │   │   └── firestore_storage.py
        │   ├── tool_concepts/
        │   │   ├── __init__.py
        │   │   ├── custom_tools/
        │   │   │   ├── __init__.py
        │   │   │   ├── add_tool_after_initialization.py
        │   │   │   ├── async_pre_and_post_hooks.py
        │   │   │   ├── async_tool_decorator.py
        │   │   │   ├── cache_tool_calls.py
        │   │   │   ├── complex_input_types.py
        │   │   │   ├── custom_tool_manipulate_session_state.py
        │   │   │   ├── human_in_the_loop.py
        │   │   │   ├── include_exclude_tools.py
        │   │   │   ├── pre_and_post_hooks.py
        │   │   │   ├── retry_tool_call.py
        │   │   │   ├── retry_tool_call_from_post_hook.py
        │   │   │   ├── stop_after_tool_call.py
        │   │   │   ├── stop_agent_exception.py
        │   │   │   ├── tool_decorator.py
        │   │   │   ├── tool_decorator_async.py
        │   │   │   ├── tool_decorator_with_hook.py
        │   │   │   ├── tool_decorator_with_instructions.py
        │   │   │   ├── tool_hook.py
        │   │   │   ├── tool_hook_async.py
        │   │   │   ├── tool_hook_with_state.py
        │   │   │   ├── tool_hook_with_state_nested.py
        │   │   │   ├── tool_hooks_nested.py
        │   │   │   └── tool_hooks_nested_async.py
        │   │   └── toolkits/
        │   │       ├── __init__.py
        │   │       ├── cache_tool_calls.py
        │   │       ├── include_exclude_tools.py
        │   │       ├── stop_after_tool_call_tools.py
        │   │       ├── tool_hook.py
        │   │       └── tool_hook_async.py
        │   └── user_control_flows/
        │       ├── __init__.py
        │       ├── agentic_user_input.py
        │       ├── confirmation_required.py
        │       ├── confirmation_required_async.py
        │       ├── confirmation_required_mixed_tools.py
        │       ├── confirmation_required_multiple_tools.py
        │       ├── confirmation_required_stream.py
        │       ├── confirmation_required_stream_async.py
        │       ├── confirmation_required_toolkit.py
        │       ├── confirmation_required_with_history.py
        │       ├── confirmation_required_with_run_id.py
        │       ├── external_tool_execution.py
        │       ├── external_tool_execution_async.py
        │       ├── external_tool_execution_async_responses.py
        │       ├── external_tool_execution_stream.py
        │       ├── external_tool_execution_stream_async.py
        │       ├── external_tool_execution_toolkit.py
        │       ├── user_input_required.py
        │       ├── user_input_required_all_fields.py
        │       ├── user_input_required_async.py
        │       ├── user_input_required_stream.py
        │       └── user_input_required_stream_async.py
        ├── agent_levels/
        │   ├── __init__.py
        │   ├── level_1_agent.py
        │   ├── level_2_agent.py
        │   ├── level_3_agent.py
        │   ├── level_4_team.py
        │   └── level_5_workflow.py
        ├── agents_from_scratch/
        │   ├── README.md
        │   ├── __init__.py
        │   ├── agent_with_knowledge.py
        │   ├── agent_with_storage.py
        │   ├── agent_with_tools.py
        │   ├── agno_assist.py
        │   ├── playground.py
        │   └── simple_agent.py
        ├── apps/
        │   ├── __init__.py
        │   ├── agui/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── agent_with_tool.py
        │   │   ├── basic.py
        │   │   ├── research_team.py
        │   │   └── structured_output.py
        │   ├── discord/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── agent_with_media.py
        │   │   ├── agent_with_user_memory.py
        │   │   └── basic.py
        │   ├── fastapi/
        │   │   ├── __init__.py
        │   │   ├── advanced.py
        │   │   ├── basic.py
        │   │   ├── image_generation_model.py
        │   │   ├── study_friend.py
        │   │   └── team.py
        │   ├── playground/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── agno_assist.py
        │   │   ├── audio_conversation_agent.py
        │   │   ├── azure_openai_agents.py
        │   │   ├── basic.py
        │   │   ├── blog_to_podcast.py
        │   │   ├── coding_agent.py
        │   │   ├── competitor_analysis.py
        │   │   ├── demo.py
        │   │   ├── gemini_agents.py
        │   │   ├── grok_agents.py
        │   │   ├── groq_agents.py
        │   │   ├── mcp_demo.py
        │   │   ├── multimodal_agents.py
        │   │   ├── ollama_agents.py
        │   │   ├── reasoning_demo.py
        │   │   ├── teams_demo.py
        │   │   ├── upload_files.py
        │   │   └── user_control_flows.py
        │   ├── slack/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── agent_with_user_memory.py
        │   │   ├── basic.py
        │   │   └── reasoning_agent.py
        │   └── whatsapp/
        │       ├── readme.md
        │       ├── __init__.py
        │       ├── agent_with_media.py
        │       ├── agent_with_user_memory.py
        │       ├── basic.py
        │       ├── image_generation_model.py
        │       ├── image_generation_tools.py
        │       ├── reasoning_agent.py
        │       └── study_friend.py
        ├── demo/
        │   ├── README.md
        │   ├── __init__.py
        │   ├── app_7777.py
        │   ├── app_8000.py
        │   ├── generate_requirements.sh
        │   ├── requirements.in
        │   ├── requirements.txt
        │   ├── agents/
        │   │   ├── __init__.py
        │   │   ├── agno_assist.py
        │   │   ├── basic.py
        │   │   ├── load_kb.py
        │   │   └── memory_agent.py
        │   ├── sql/
        │   │   ├── __init__.py
        │   │   ├── agents.py
        │   │   ├── load_f1_data.py
        │   │   ├── load_knowledge.py
        │   │   └── knowledge/
        │   │       ├── constructors_championship.json
        │   │       ├── drivers_championship.json
        │   │       ├── fastest_laps.json
        │   │       ├── race_results.json
        │   │       ├── race_wins.json
        │   │       └── sample_queries.sql
        │   └── teams/
        │       ├── __init__.py
        │       └── reasoning_finance_team.py
        ├── evals/
        │   ├── README.md
        │   ├── __init__.py
        │   ├── accuracy/
        │   │   ├── __init__.py
        │   │   ├── accuracy_9_11_bigger_or_9_99.py
        │   │   ├── accuracy_async.py
        │   │   ├── accuracy_basic.py
        │   │   ├── accuracy_team.py
        │   │   ├── accuracy_with_given_answer.py
        │   │   └── accuracy_with_tools.py
        │   ├── performance/
        │   │   ├── __init__.py
        │   │   ├── async_function.py
        │   │   ├── instantiation_agent.py
        │   │   ├── instantiation_agent_with_tool.py
        │   │   ├── instantiation_team.py
        │   │   ├── response_with_memory_updates.py
        │   │   ├── response_with_storage.py
        │   │   ├── simple_response.py
        │   │   ├── team_response_with_memory_and_reasoning.py
        │   │   ├── team_response_with_memory_multi_user.py
        │   │   ├── team_response_with_memory_simple.py
        │   │   └── other/
        │   │       ├── __init__.py
        │   │       ├── autogen_instantiation.py
        │   │       ├── crewai_instantiation.py
        │   │       ├── langgraph_instantiation.py
        │   │       ├── openai_agents_instantiation.py
        │   │       ├── pydantic_ai_instantiation.py
        │   │       └── smolagents_instantiation.py
        │   └── reliability/
        │       ├── __init__.py
        │       ├── reliability_async.py
        │       ├── multiple_tool_calls/
        │       │   ├── __init__.py
        │       │   └── openai/
        │       │       └── calculator.py
        │       ├── single_tool_calls/
        │       │   ├── __init__.py
        │       │   ├── google/
        │       │   │   └── calculator.py
        │       │   └── openai/
        │       │       └── calculator.py
        │       └── team/
        │           ├── __init__.py
        │           ├── google/
        │           │   ├── __init__.py
        │           │   └── company_info.py
        │           └── openai/
        │               ├── __init__.py
        │               └── company_info.py
        ├── examples/
        │   ├── __init__.py
        │   ├── a2a/
        │   │   ├── __init__.py
        │   │   └── basic_agent/
        │   │       ├── README.md
        │   │       ├── __init__.py
        │   │       ├── __main__.py
        │   │       ├── basic_agent.py
        │   │       └── client.py
        │   ├── agents/
        │   │   ├── __init__.py
        │   │   ├── agent_team.py
        │   │   ├── agent_with_instructions.py
        │   │   ├── agent_with_knowledge.py
        │   │   ├── agent_with_memory.py
        │   │   ├── agent_with_reasoning.py
        │   │   ├── agent_with_storage.py
        │   │   ├── agent_with_tools.py
        │   │   ├── agno_assist.py
        │   │   ├── agno_support_agent.py
        │   │   ├── airbnb_mcp.py
        │   │   ├── basic_agent.py
        │   │   ├── book_recommendation.py
        │   │   ├── competitor_analysis_agent.py
        │   │   ├── deep_knowledge.py
        │   │   ├── deep_research_agent_exa.py
        │   │   ├── fibonacci_agent.py
        │   │   ├── finance_agent.py
        │   │   ├── finance_agent_with_memory.py
        │   │   ├── legal_consultant.py
        │   │   ├── media_trend_analysis_agent.py
        │   │   ├── meeting_summarizer_agent.py
        │   │   ├── movie_recommedation.py
        │   │   ├── pydantic_model_as_input.py
        │   │   ├── readme_generator.py
        │   │   ├── reasoning_finance_agent.py
        │   │   ├── recipe_creator.py
        │   │   ├── recipe_rag_image.py
        │   │   ├── research_agent.py
        │   │   ├── research_agent_exa.py
        │   │   ├── shopping_partner.py
        │   │   ├── social_media_agent.py
        │   │   ├── startup_analyst_agent.py
        │   │   ├── study_partner.py
        │   │   ├── thinking_finance_agent.py
        │   │   ├── translation_agent.py
        │   │   ├── web_extraction_agent.py
        │   │   └── youtube_agent.py
        │   ├── multi_agent_reasoning/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── generate_requirements.sh
        │   │   ├── playground.py
        │   │   ├── reasoning_finance_team.py
        │   │   ├── requirements.in
        │   │   └── requirements.txt
        │   ├── streamlit_apps/
        │   │   ├── __init__.py
        │   │   ├── agentic_rag/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agentic_rag.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   └── utils.py
        │   │   ├── answer_engine/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── prompts.py
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   ├── test.py
        │   │   │   └── utils.py
        │   │   ├── chess_team/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   └── utils.py
        │   │   ├── deep_researcher/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   └── server.py
        │   │   ├── game_generator/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── app.py
        │   │   │   ├── game_generator.py
        │   │   │   └── requirements.txt
        │   │   ├── gemini-tutor/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── prompts.py
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   └── utils.py
        │   │   ├── geobuddy/
        │   │   │   ├── readme.md
        │   │   │   ├── __init__.py
        │   │   │   ├── app.py
        │   │   │   ├── geography_buddy.py
        │   │   │   └── requirements.txt
        │   │   ├── github_mcp_agent/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── requirements.in
        │   │   │   └── requirements.txt
        │   │   ├── github_repo_analyzer/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   └── utils.py
        │   │   ├── image_generation/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   └── utils.py
        │   │   ├── llama_tutor/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── prompts.py
        │   │   │   ├── requirements.txt
        │   │   │   ├── utils.py
        │   │   │   └── .env.example
        │   │   ├── mcp_agent/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── mcp_client.py
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   └── utils.py
        │   │   ├── medical_imaging/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── app.py
        │   │   │   ├── medical_agent.py
        │   │   │   └── requirements.txt
        │   │   ├── paperpal/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── app.py
        │   │   │   ├── requirements.txt
        │   │   │   └── technical_writer.py
        │   │   ├── parallel_world_builder/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── requirements.txt
        │   │   │   └── utils.py
        │   │   ├── podcast_generator/
        │   │   │   ├── readme.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── requirements.in
        │   │   │   └── requirements.txt
        │   │   ├── sql_agent/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── load_f1_data.py
        │   │   │   ├── load_knowledge.py
        │   │   │   ├── playground.py
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   ├── utils.py
        │   │   │   └── knowledge/
        │   │   │       ├── constructors_championship.json
        │   │   │       ├── drivers_championship.json
        │   │   │       ├── fastest_laps.json
        │   │   │       ├── race_results.json
        │   │   │       ├── race_wins.json
        │   │   │       └── sample_queries.sql
        │   │   ├── tic_tac_toe/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   └── utils.py
        │   │   ├── universal_agent_interface/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agents.py
        │   │   │   ├── app.py
        │   │   │   ├── css.py
        │   │   │   ├── generate_requirements.sh
        │   │   │   ├── load_knowledge.py
        │   │   │   ├── requirements.in
        │   │   │   ├── requirements.txt
        │   │   │   ├── tools.py
        │   │   │   ├── uagi.py
        │   │   │   └── utils.py
        │   │   └── vision_ai/
        │   │       ├── README.md
        │   │       ├── __init__.py
        │   │       ├── agents.py
        │   │       ├── app.py
        │   │       ├── generate_requirements.sh
        │   │       ├── prompt.py
        │   │       ├── requirements.in
        │   │       ├── requirements.txt
        │   │       └── utils.py
        │   ├── teams/
        │   │   ├── __init__.py
        │   │   ├── collaborate/
        │   │   │   ├── __init__.py
        │   │   │   └── collaboration_team.py
        │   │   ├── coordinate/
        │   │   │   ├── __init__.py
        │   │   │   ├── autonomous_startup_team.py
        │   │   │   ├── content_team.py
        │   │   │   ├── hackernews_team.py
        │   │   │   ├── news_agency_team.py
        │   │   │   ├── reasoning_team.py
        │   │   │   ├── skyplanner_mcp_team.py
        │   │   │   ├── tic_tac_toe_team.py
        │   │   │   ├── travel_planner_mcp_team.py
        │   │   │   └── werewolf_team.py
        │   │   └── route/
        │   │       ├── __init__.py
        │   │       ├── ai_customer_support_team.py
        │   │       ├── multi_language_team.py
        │   │       ├── multi_purpose_team.py
        │   │       ├── reasoning_team.py
        │   │       └── simple.py
        │   └── workflows_2/
        │       ├── __init__.py
        │       ├── company_analysis/
        │       │   ├── README.md
        │       │   ├── __init__.py
        │       │   ├── agents.py
        │       │   ├── models.py
        │       │   └── run_workflow.py
        │       ├── company_description/
        │       │   ├── README.md
        │       │   ├── __init__.py
        │       │   ├── agents.py
        │       │   ├── prompts.py
        │       │   └── run_workflow.py
        │       ├── customer_support/
        │       │   ├── README.md
        │       │   ├── __init__.py
        │       │   ├── agents.py
        │       │   └── run_workflow.py
        │       └── investment_analyst/
        │           ├── README.md
        │           ├── __init__.py
        │           ├── agents.py
        │           ├── models.py
        │           └── run_workflow.py
        ├── getting_started/
        │   ├── README.md
        │   ├── 01_basic_agent.py
        │   ├── 02_agent_with_tools.py
        │   ├── 03_agent_with_knowledge.py
        │   ├── 04_agent_with_storage.py
        │   ├── 05_agent_team.py
        │   ├── 06_structured_output.py
        │   ├── 07_write_your_own_tool.py
        │   ├── 08_research_agent_exa.py
        │   ├── 09_research_workflow.py
        │   ├── 10_image_agent.py
        │   ├── 11_generate_image.py
        │   ├── 12_generate_video.py
        │   ├── 13_audio_input_output.py
        │   ├── 14_agent_state.py
        │   ├── 15_agent_context.py
        │   ├── 16_agent_session.py
        │   ├── 17_user_memories_and_summaries.py
        │   ├── 18_retry_function_call.py
        │   ├── 19_human_in_the_loop.py
        │   ├── __init__.py
        │   └── readme_examples.py
        ├── hackathon/
        │   ├── README.md
        │   └── workshop/
        │       ├── __init__.py
        │       ├── agno_assist.py
        │       ├── agno_assist_voice.py
        │       └── playground.py
        ├── models/
        │   ├── __init__.py
        │   ├── aimlapi/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── image_agent.py
        │   │   ├── image_agent_bytes.py
        │   │   ├── image_agent_with_memory.py
        │   │   ├── structured_output.py
        │   │   └── tool_use.py
        │   ├── anthropic/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── code_execution.py
        │   │   ├── financial_analyst_thinking.py
        │   │   ├── image_input_bytes.py
        │   │   ├── image_input_file_upload.py
        │   │   ├── image_input_url.py
        │   │   ├── knowledge.py
        │   │   ├── mcp_connector.py
        │   │   ├── memory.py
        │   │   ├── pdf_input_bytes.py
        │   │   ├── pdf_input_file_upload.py
        │   │   ├── pdf_input_local.py
        │   │   ├── pdf_input_url.py
        │   │   ├── prompt_caching.py
        │   │   ├── prompt_caching_extended.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── structured_output_stream.py
        │   │   ├── thinking.py
        │   │   ├── thinking_stream.py
        │   │   ├── tool_use.py
        │   │   ├── tool_use_stream.py
        │   │   └── web_search.py
        │   ├── aws/
        │   │   ├── __init__.py
        │   │   ├── bedrock/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── async_basic.py
        │   │   │   ├── async_basic_stream.py
        │   │   │   ├── async_tool_use_stream.py
        │   │   │   ├── basic.py
        │   │   │   ├── basic_stream.py
        │   │   │   ├── image_agent_bytes.py
        │   │   │   ├── pdf_agent_bytes.py
        │   │   │   ├── structured_output.py
        │   │   │   ├── tool_use.py
        │   │   │   └── tool_use_stream.py
        │   │   └── claude/
        │   │       ├── README.md
        │   │       ├── __init__.py
        │   │       ├── async_basic.py
        │   │       ├── async_basic_stream.py
        │   │       ├── async_tool_use.py
        │   │       ├── basic.py
        │   │       ├── basic_stream.py
        │   │       ├── image_agent.py
        │   │       ├── knowledge.py
        │   │       ├── storage.py
        │   │       ├── structured_output.py
        │   │       └── tool_use.py
        │   ├── azure/
        │   │   ├── __init__.py
        │   │   ├── ai_foundry/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── async_basic.py
        │   │   │   ├── async_basic_stream.py
        │   │   │   ├── async_tool_use.py
        │   │   │   ├── basic.py
        │   │   │   ├── basic_stream.py
        │   │   │   ├── demo_cohere.py
        │   │   │   ├── demo_mistral.py
        │   │   │   ├── image_agent.py
        │   │   │   ├── image_agent_bytes.py
        │   │   │   ├── knowledge.py
        │   │   │   ├── storage.py
        │   │   │   ├── structured_output.py
        │   │   │   └── tool_use.py
        │   │   └── openai/
        │   │       ├── README.md
        │   │       ├── __init__.py
        │   │       ├── async_basic.py
        │   │       ├── async_basic_stream.py
        │   │       ├── basic.py
        │   │       ├── basic_stream.py
        │   │       ├── knowledge.py
        │   │       ├── storage.py
        │   │       ├── structured_output.py
        │   │       └── tool_use.py
        │   ├── cerebras/
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── async_tool_use_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── knowledge.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   └── tool_use_stream.py
        │   ├── cerebras_openai/
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── async_tool_use_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── knowledge.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   └── tool_use_stream.py
        │   ├── cohere/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_structured_output.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── image_agent.py
        │   │   ├── image_agent_bytes.py
        │   │   ├── image_agent_local_file.py
        │   │   ├── knowledge.py
        │   │   ├── memory.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   └── tool_use.py
        │   ├── dashscope/
        │   │   ├── README.md
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_image_agent.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── image_agent.py
        │   │   ├── image_agent_bytes.py
        │   │   ├── structured_output.py
        │   │   ├── thinking_agent.py
        │   │   └── tool_use.py
        │   ├── deepinfra/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── json_output.py
        │   │   └── tool_use.py
        │   ├── deepseek/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_streaming.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── structured_output.py
        │   │   └── tool_use.py
        │   ├── fireworks/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── structured_output.py
        │   │   └── tool_use.py
        │   ├── google/
        │   │   ├── __init__.py
        │   │   └── gemini/
        │   │       ├── README.md
        │   │       ├── __init__.py
        │   │       ├── agent_with_thinking_budget.py
        │   │       ├── async_basic.py
        │   │       ├── async_basic_stream.py
        │   │       ├── async_image_editing.py
        │   │       ├── async_image_generation.py
        │   │       ├── async_image_generation_stream.py
        │   │       ├── async_tool_use.py
        │   │       ├── audio_input_bytes_content.py
        │   │       ├── audio_input_file_upload.py
        │   │       ├── audio_input_local_file_upload.py
        │   │       ├── basic.py
        │   │       ├── basic_stream.py
        │   │       ├── file_upload_with_cache.py
        │   │       ├── grounding.py
        │   │       ├── image_editing.py
        │   │       ├── image_generation.py
        │   │       ├── image_generation_stream.py
        │   │       ├── image_input.py
        │   │       ├── image_input_file_upload.py
        │   │       ├── imagen_tool.py
        │   │       ├── imagen_tool_advanced.py
        │   │       ├── knowledge.py
        │   │       ├── pdf_input_file_upload.py
        │   │       ├── pdf_input_local.py
        │   │       ├── pdf_input_url.py
        │   │       ├── search.py
        │   │       ├── storage.py
        │   │       ├── storage_and_memory.py
        │   │       ├── structured_output.py
        │   │       ├── structured_output_stream.py
        │   │       ├── thinking_agent.py
        │   │       ├── thinking_agent_stream.py
        │   │       ├── tool_use.py
        │   │       ├── tool_use_stream.py
        │   │       ├── url_context.py
        │   │       ├── url_context_with_search.py
        │   │       ├── vertex_ai_search.py
        │   │       ├── vertexai.py
        │   │       ├── video_input_bytes_content.py
        │   │       ├── video_input_file_upload.py
        │   │       ├── video_input_local_file_upload.py
        │   │       └── video_input_youtube.py
        │   ├── groq/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── agent_team.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── browser_search.py
        │   │   ├── deep_knowledge.py
        │   │   ├── image_agent.py
        │   │   ├── knowledge.py
        │   │   ├── metrics.py
        │   │   ├── reasoning_agent.py
        │   │   ├── research_agent_exa.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   ├── transcription_agent.py
        │   │   ├── translation_agent.py
        │   │   └── reasoning/
        │   │       ├── __init__.py
        │   │       ├── basic.py
        │   │       ├── basic_stream.py
        │   │       ├── demo_deepseek_qwen.py
        │   │       ├── demo_qwen_2_5_32B.py
        │   │       └── finance_agent.py
        │   ├── huggingface/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── llama_essay_writer.py
        │   │   └── tool_use.py
        │   ├── ibm/
        │   │   ├── __init__.py
        │   │   └── watsonx/
        │   │       ├── README.md
        │   │       ├── __init__.py
        │   │       ├── async_basic.py
        │   │       ├── async_basic_stream.py
        │   │       ├── async_tool_use.py
        │   │       ├── basic.py
        │   │       ├── basic_stream.py
        │   │       ├── image_agent_bytes.py
        │   │       ├── knowledge.py
        │   │       ├── storage.py
        │   │       ├── structured_output.py
        │   │       └── tool_use.py
        │   ├── langdb/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── agent.py
        │   │   ├── agent_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── data_analyst.py
        │   │   ├── finance_agent.py
        │   │   ├── structured_output.py
        │   │   └── web_search.py
        │   ├── litellm/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── audio_input_agent.py
        │   │   ├── basic.py
        │   │   ├── basic_gpt.py
        │   │   ├── basic_stream.py
        │   │   ├── image_agent.py
        │   │   ├── image_agent_bytes.py
        │   │   ├── knowledge.py
        │   │   ├── memory.py
        │   │   ├── metrics.py
        │   │   ├── pdf_input_bytes.py
        │   │   ├── pdf_input_local.py
        │   │   ├── pdf_input_url.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   └── tool_use_stream.py
        │   ├── litellm_openai/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── audio_input_agent.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   └── tool_use.py
        │   ├── lmstudio/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── image_agent.py
        │   │   ├── knowledge.py
        │   │   ├── memory.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   └── tool_use_stream.py
        │   ├── meta/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── llama/
        │   │   │   ├── __init__.py
        │   │   │   ├── async_basic.py
        │   │   │   ├── async_basic_stream.py
        │   │   │   ├── async_knowledge.py
        │   │   │   ├── async_tool_use.py
        │   │   │   ├── async_tool_use_stream.py
        │   │   │   ├── basic.py
        │   │   │   ├── basic_stream.py
        │   │   │   ├── image_input_bytes.py
        │   │   │   ├── image_input_file.py
        │   │   │   ├── knowledge.py
        │   │   │   ├── memory.py
        │   │   │   ├── metrics.py
        │   │   │   ├── storage.py
        │   │   │   ├── structured_output.py
        │   │   │   ├── tool_use.py
        │   │   │   └── tool_use_stream.py
        │   │   └── llama_openai/
        │   │       ├── __init__.py
        │   │       ├── async_basic.py
        │   │       ├── async_basic_stream.py
        │   │       ├── async_tool_use.py
        │   │       ├── async_tool_use_stream.py
        │   │       ├── basic.py
        │   │       ├── basic_stream.py
        │   │       ├── image_input_bytes.py
        │   │       ├── image_input_file.py
        │   │       ├── knowledge.py
        │   │       ├── memory.py
        │   │       ├── metrics.py
        │   │       ├── storage.py
        │   │       ├── structured_output.py
        │   │       ├── tool_use.py
        │   │       └── tool_use_stream.py
        │   ├── mistral/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_structured_output.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── image_bytes_input_agent.py
        │   │   ├── image_compare_agent.py
        │   │   ├── image_file_input_agent.py
        │   │   ├── image_ocr_with_structured_output.py
        │   │   ├── image_transcribe_document_agent.py
        │   │   ├── memory.py
        │   │   ├── mistral_small.py
        │   │   ├── structured_output.py
        │   │   ├── structured_output_with_tool_use.py
        │   │   └── tool_use.py
        │   ├── nebius/
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── async_tool_use_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── knowledge.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   └── tool_use_stream.py
        │   ├── nvidia/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   └── tool_use.py
        │   ├── ollama/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── demo_deepseek_r1.py
        │   │   ├── demo_gemma.py
        │   │   ├── demo_phi4.py
        │   │   ├── demo_qwen.py
        │   │   ├── image_agent.py
        │   │   ├── knowledge.py
        │   │   ├── memory.py
        │   │   ├── set_client.py
        │   │   ├── set_temperature.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   └── tool_use_stream.py
        │   ├── ollama_tools/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── knowledge.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   └── tool_use_stream.py
        │   ├── openai/
        │   │   ├── __init__.py
        │   │   ├── chat/
        │   │   │   ├── README.md
        │   │   │   ├── __init__.py
        │   │   │   ├── agent_flex_tier.py
        │   │   │   ├── async_basic.py
        │   │   │   ├── async_basic_stream.py
        │   │   │   ├── async_structured_response_stream.py
        │   │   │   ├── async_tool_use.py
        │   │   │   ├── audio_input_agent.py
        │   │   │   ├── audio_input_and_output_multi_turn.py
        │   │   │   ├── audio_input_local_file_upload.py
        │   │   │   ├── audio_output_agent.py
        │   │   │   ├── audio_output_stream.py
        │   │   │   ├── basic.py
        │   │   │   ├── basic_stream.py
        │   │   │   ├── custom_role_map.py
        │   │   │   ├── generate_images.py
        │   │   │   ├── image_agent.py
        │   │   │   ├── image_agent_bytes.py
        │   │   │   ├── image_agent_with_memory.py
        │   │   │   ├── knowledge.py
        │   │   │   ├── memory.py
        │   │   │   ├── metrics.py
        │   │   │   ├── pdf_input_file_upload.py
        │   │   │   ├── pdf_input_local.py
        │   │   │   ├── pdf_input_url.py
        │   │   │   ├── reasoning_o3_mini.py
        │   │   │   ├── storage.py
        │   │   │   ├── structured_output.py
        │   │   │   ├── structured_output_stream.py
        │   │   │   ├── text_to_speech_agent.py
        │   │   │   ├── tool_use.py
        │   │   │   ├── tool_use_stream.py
        │   │   │   └── verbosity_control.py
        │   │   └── responses/
        │   │       ├── __init__.py
        │   │       ├── agent_flex_tier.py
        │   │       ├── async_basic.py
        │   │       ├── async_basic_stream.py
        │   │       ├── async_tool_use.py
        │   │       ├── basic.py
        │   │       ├── basic_stream.py
        │   │       ├── deep_research_agent.py
        │   │       ├── image_agent.py
        │   │       ├── image_agent_bytes.py
        │   │       ├── image_agent_with_memory.py
        │   │       ├── image_generation_agent.py
        │   │       ├── knowledge.py
        │   │       ├── memory.py
        │   │       ├── pdf_input_local.py
        │   │       ├── pdf_input_url.py
        │   │       ├── reasoning_o3_mini.py
        │   │       ├── storage.py
        │   │       ├── structured_output.py
        │   │       ├── tool_use.py
        │   │       ├── tool_use_gpt_5.py
        │   │       ├── tool_use_o3.py
        │   │       ├── tool_use_stream.py
        │   │       ├── verbosity_control.py
        │   │       └── websearch_builtin_tool.py
        │   ├── openrouter/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── structured_output.py
        │   │   └── tool_use.py
        │   ├── perplexity/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── knowledge.py
        │   │   ├── memory.py
        │   │   ├── structured_output.py
        │   │   └── web_search.py
        │   ├── portkey/
        │   │   ├── README.md
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── async_tool_use_stream.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── structured_output.py
        │   │   ├── tool_use.py
        │   │   └── tool_use_stream.py
        │   ├── sambanova/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── basic.py
        │   │   └── basic_stream.py
        │   ├── together/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── image_agent.py
        │   │   ├── image_agent_bytes.py
        │   │   ├── image_agent_with_memory.py
        │   │   ├── structured_output.py
        │   │   └── tool_use.py
        │   ├── vercel/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── image_agent.py
        │   │   ├── knowledge.py
        │   │   └── tool_use.py
        │   ├── vllm/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── async_basic.py
        │   │   ├── async_basic_stream.py
        │   │   ├── async_tool_use.py
        │   │   ├── basic.py
        │   │   ├── basic_stream.py
        │   │   ├── code_generation.py
        │   │   ├── memory.py
        │   │   ├── storage.py
        │   │   ├── structured_output.py
        │   │   └── tool_use.py
        │   └── xai/
        │       ├── README.md
        │       ├── __init__.py
        │       ├── async_basic.py
        │       ├── async_basic_stream.py
        │       ├── async_tool_use.py
        │       ├── basic.py
        │       ├── basic_stream.py
        │       ├── finance_agent.py
        │       ├── image_agent.py
        │       ├── image_agent_bytes.py
        │       ├── image_agent_with_memory.py
        │       ├── live_search_agent.py
        │       ├── live_search_agent_stream.py
        │       ├── reasoning_agent.py
        │       ├── structured_output.py
        │       └── tool_use.py
        ├── observability/
        │   ├── __init__.py
        │   ├── agent_ops.py
        │   ├── arize_phoenix_via_openinference.py
        │   ├── arize_phoenix_via_openinference_local.py
        │   ├── atla_op.py
        │   ├── langfuse_via_openinference.py
        │   ├── langfuse_via_openinference_response_model.py
        │   ├── langfuse_via_openlit.py
        │   ├── langsmith_via_openinference.py
        │   ├── langtrace_op.py
        │   ├── langwatch_op.py
        │   ├── weave_op.py
        │   └── teams/
        │       ├── langfuse_via_openinference_async_team.py
        │       └── langfuse_via_openinference_team.py
        ├── reasoning/
        │   ├── README.md
        │   ├── __init__.py
        │   ├── playground.py
        │   ├── agents/
        │   │   ├── __init__.py
        │   │   ├── analyse_treaty_of_versailles.py
        │   │   ├── capture_reasoning_content_default_COT.py
        │   │   ├── cerebras_llama_default_COT.py
        │   │   ├── default_chain_of_thought.py
        │   │   ├── fibonacci.py
        │   │   ├── finance_agent.py
        │   │   ├── ibm_watsonx_default_COT.py
        │   │   ├── is_9_11_bigger_than_9_9.py
        │   │   ├── life_in_500000_years.py
        │   │   ├── logical_puzzle.py
        │   │   ├── mathematical_proof.py
        │   │   ├── mistral_reasoning_cot.py
        │   │   ├── plan_itenerary.py
        │   │   ├── python_101_curriculum.py
        │   │   ├── scientific_research.py
        │   │   ├── ship_of_theseus.py
        │   │   ├── strawberry.py
        │   │   └── trolley_problem.py
        │   ├── models/
        │   │   ├── __init__.py
        │   │   ├── azure_ai_foundry/
        │   │   │   ├── __init__.py
        │   │   │   └── reasoning_model_deepseek.py
        │   │   ├── azure_openai/
        │   │   │   ├── __init__.py
        │   │   │   ├── o1.py
        │   │   │   ├── o3_mini_with_tools.py
        │   │   │   ├── o4_mini.py
        │   │   │   └── reasoning_model_gpt_4_1.py
        │   │   ├── deepseek/
        │   │   │   ├── 9_11_or_9_9.py
        │   │   │   ├── __init__.py
        │   │   │   ├── analyse_treaty_of_versailles.py
        │   │   │   ├── ethical_dilemma.py
        │   │   │   ├── fibonacci.py
        │   │   │   ├── finance_agent.py
        │   │   │   ├── life_in_500000_years.py
        │   │   │   ├── logical_puzzle.py
        │   │   │   ├── mathematical_proof.py
        │   │   │   ├── plan_itenerary.py
        │   │   │   ├── python_101_curriculum.py
        │   │   │   ├── scientific_research.py
        │   │   │   ├── ship_of_theseus.py
        │   │   │   ├── strawberry.py
        │   │   │   └── trolley_problem.py
        │   │   ├── groq/
        │   │   │   ├── 9_11_or_9_9.py
        │   │   │   ├── __init__.py
        │   │   │   └── deepseek_plus_claude.py
        │   │   ├── ollama/
        │   │   │   ├── __init__.py
        │   │   │   └── reasoning_model_deepseek.py
        │   │   ├── openai/
        │   │   │   ├── __init__.py
        │   │   │   ├── o1_pro.py
        │   │   │   ├── o3_mini.py
        │   │   │   ├── o3_mini_with_tools.py
        │   │   │   ├── o4_mini.py
        │   │   │   ├── reasoning_effort.py
        │   │   │   ├── reasoning_model_gpt_4_1.py
        │   │   │   └── reasoning_summary.py
        │   │   └── xai/
        │   │       ├── __init__.py
        │   │       └── reasoning_effort.py
        │   ├── teams/
        │   │   ├── __init__.py
        │   │   ├── finance_team_chain_of_thought.py
        │   │   ├── knowledge_tool_team.py
        │   │   └── reasoning_finance_team.py
        │   └── tools/
        │       ├── __init__.py
        │       ├── azure_openai_reasoning_tools.py
        │       ├── capture_reasoning_content_knowledge_tools.py
        │       ├── capture_reasoning_content_reasoning_tools.py
        │       ├── capture_reasoning_content_thinking_tools.py
        │       ├── cerebras_llama_reasoning_tools.py
        │       ├── claude_reasoning_tools.py
        │       ├── claude_thinking_tools.py
        │       ├── gemini_finance_agent.py
        │       ├── gemini_reasoning_tools.py
        │       ├── groq_llama_finance_agent.py
        │       ├── ibm_watsonx_reasoning_tools.py
        │       ├── knowledge_tools.py
        │       ├── llama_reasoning_tools.py
        │       ├── ollama_reasoning_tools.py
        │       ├── openai_reasoning_tools.py
        │       ├── reasoning_tools.py
        │       ├── thinking_playground.py
        │       └── vercel_reasoning_tools.py
        ├── scripts/
        │   ├── _utils.sh
        │   ├── cookbook_runner.py
        │   ├── format.bat
        │   ├── format.sh
        │   ├── run_cassandra.bat
        │   ├── run_cassandra.sh
        │   ├── run_clickhouse.bat
        │   ├── run_clickhouse.sh
        │   ├── run_couchbase.bat
        │   ├── run_couchbase.sh
        │   ├── run_mongodb.bat
        │   ├── run_mongodb.sh
        │   ├── run_mysql.bat
        │   ├── run_mysql.sh
        │   ├── run_pgvector.bat
        │   ├── run_pgvector.sh
        │   ├── run_qdrant.bat
        │   ├── run_qdrant.sh
        │   ├── run_redis.bat
        │   ├── run_redis.sh
        │   ├── run_singlestore.bat
        │   ├── run_singlestore.sh
        │   ├── run_surrealdb.bat
        │   ├── run_surrealdb.sh
        │   ├── run_weviate.bat
        │   ├── run_weviate.sh
        │   ├── lightrag-init/
        │   │   ├── docker-compose.yml
        │   │   ├── example.env
        │   │   ├── run_lightrag.sh
        │   │   └── stop_lightrag.sh
        │   └── mysql-init/
        │       └── init.sql
        ├── storage/
        │   ├── __init__.py
        │   ├── dynamodb_storage/
        │   │   ├── __init__.py
        │   │   ├── dynamodb_storage_for_agent.py
        │   │   ├── dynamodb_storage_for_team.py
        │   │   └── dynamodb_storage_for_workflow.py
        │   ├── examples/
        │   │   ├── multi_user_multi_session.py
        │   │   └── sqlite_storage.py
        │   ├── gcs_storage/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   └── gcs_json_storage_for_agent.py
        │   ├── in_memory_storage/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── in_memory_storage_for_agent.py
        │   │   ├── in_memory_storage_for_team.py
        │   │   ├── in_memory_storage_for_workflow.py
        │   │   └── in_memory_storage_for_workflows_2.py
        │   ├── json_storage/
        │   │   ├── __init__.py
        │   │   ├── json_storage_for_agent.py
        │   │   ├── json_storage_for_team.py
        │   │   ├── json_storage_for_workflow.py
        │   │   └── json_storage_for_workflows_2.py
        │   ├── mongodb_storage/
        │   │   ├── __init__.py
        │   │   ├── mongodb_storage_for_agent.py
        │   │   ├── mongodb_storage_for_team.py
        │   │   └── mongodb_storage_for_workflow.py
        │   ├── mysql_storage/
        │   │   ├── __init__.py
        │   │   ├── mysql_storage_for_agent.py
        │   │   ├── mysql_storage_for_team.py
        │   │   └── mysql_storage_for_workflow.py
        │   ├── postgres_storage/
        │   │   ├── __init__.py
        │   │   ├── postgres_storage_for_agent.py
        │   │   ├── postgres_storage_for_team.py
        │   │   ├── postgres_storage_for_workflow.py
        │   │   └── postgres_storage_for_workflow_2.py
        │   ├── redis_storage/
        │   │   ├── __init__.py
        │   │   ├── redis_storage_for_agent.py
        │   │   ├── redis_storage_for_team.py
        │   │   ├── redis_storage_for_workflow.py
        │   │   └── redis_storage_for_workflow_2.py
        │   ├── singlestore_storage/
        │   │   ├── __init__.py
        │   │   ├── singlestore_storage_for_agent.py
        │   │   ├── singlestore_storage_for_team.py
        │   │   └── singlestore_storage_for_workflow.py
        │   ├── sqllite_storage/
        │   │   ├── __init__.py
        │   │   ├── sqlite_storage_for_agent.py
        │   │   ├── sqlite_storage_for_team.py
        │   │   ├── sqlite_storage_for_workflow.py
        │   │   └── sqlite_storage_for_workflow_2.py
        │   └── yaml_storage/
        │       ├── __init__.py
        │       ├── yaml_storage_for_agent.py
        │       ├── yaml_storage_for_team.py
        │       ├── yaml_storage_for_workflow.py
        │       └── yaml_storage_for_workflow_2.py
        ├── teams/
        │   ├── __init__.py
        │   ├── medical_history.txt
        │   ├── pydantic_model_as_input.py
        │   ├── reasoning_multi_purpose_team.py
        │   ├── response_as_variable.py
        │   ├── streaming.py
        │   ├── streaming_async.py
        │   ├── structured_output_streaming.py
        │   ├── team_events.py
        │   ├── team_events_route.py
        │   ├── team_metrics.py
        │   ├── team_with_agentic_knowledge_filters.py
        │   ├── team_with_custom_tools.py
        │   ├── team_with_knowledge.py
        │   ├── team_with_knowledge_filters.py
        │   ├── team_with_local_agentic_rag.py
        │   ├── team_with_nested_shared_state.py
        │   ├── team_with_output_model.py
        │   ├── team_with_parser_model.py
        │   ├── team_with_shared_state.py
        │   ├── team_with_storage.py
        │   ├── team_with_tool_hooks.py
        │   ├── team_with_tools.py
        │   ├── memory/
        │   │   ├── 01_chat_history.py
        │   │   ├── 02_persistent_chat_history.py
        │   │   ├── 03_user_memories.py
        │   │   ├── 04_agentic_context.py
        │   │   ├── 05_team_manages_memory.py
        │   │   ├── 06_team_session_summaries.py
        │   │   ├── 07_multiple_teams.py
        │   │   ├── __init__.py
        │   │   └── utils.py
        │   └── modes/
        │       ├── __init__.py
        │       ├── collaborate.py
        │       ├── coordinate.py
        │       └── route.py
        ├── tools/
        │   ├── __init__.py
        │   ├── agentql_tools.py
        │   ├── airflow_tools.py
        │   ├── apify_tools.py
        │   ├── arxiv_tools.py
        │   ├── aws_lambda_tools.py
        │   ├── aws_ses_tools.py
        │   ├── baidusearch_tools.py
        │   ├── bitbucket_tools.py
        │   ├── brandfetch_tools.py
        │   ├── bravesearch_tools.py
        │   ├── brightdata_tools.py
        │   ├── browserbase_tools.py
        │   ├── calcom_tools.py
        │   ├── calculator_tools.py
        │   ├── cartesia_tools.py
        │   ├── clickup_tools.py
        │   ├── composio_tools.py
        │   ├── confluence_tools.py
        │   ├── crawl4ai_tools.py
        │   ├── csv_tools.py
        │   ├── custom_api_tools.py
        │   ├── custom_async_tools.py
        │   ├── custom_tools.py
        │   ├── dalle_tools.py
        │   ├── daytona_tools.py
        │   ├── desi_vocal_tools.py
        │   ├── discord_tools.py
        │   ├── docker_tools.py
        │   ├── duckdb_tools.py
        │   ├── duckduckgo_tools.py
        │   ├── e2b_tools.py
        │   ├── elevenlabs_tools.py
        │   ├── email_tools.py
        │   ├── evm_tools.py
        │   ├── exa_tools.py
        │   ├── fal_tools.py
        │   ├── file_tools.py
        │   ├── financial_datasets_tools.py
        │   ├── firecrawl_tools.py
        │   ├── giphy_tools.py
        │   ├── github_tools.py
        │   ├── gmail_tools.py
        │   ├── google_bigquery_tools.py
        │   ├── google_maps_tools.py
        │   ├── googlecalendar_tools.py
        │   ├── googlesearch_tools.py
        │   ├── googlesheets_tools.py
        │   ├── hackernews_tools.py
        │   ├── jinareader_tools.py
        │   ├── jira_tools.py
        │   ├── linear_tools.py
        │   ├── linkup_tools.py
        │   ├── lumalabs_tools.py
        │   ├── mcp_tools.py
        │   ├── mem0_tools.py
        │   ├── memori_tools.py
        │   ├── mlx_transcribe_tools.py
        │   ├── models_lab_tools.py
        │   ├── moviepy_video_tools.py
        │   ├── multiple_tools.py
        │   ├── neo4j_tools.py
        │   ├── newspaper4k_tools.py
        │   ├── newspaper_tools.py
        │   ├── openbb_tools.py
        │   ├── opencv_tools.py
        │   ├── openweather_tools.py
        │   ├── oxylabs_tools.py
        │   ├── pandas_tools.py
        │   ├── postgres_tools.py
        │   ├── pubmed_tools.py
        │   ├── python_function.py
        │   ├── python_function_as_tool.py
        │   ├── python_tools.py
        │   ├── reddit_tools.py
        │   ├── replicate_tools.py
        │   ├── resend_tools.py
        │   ├── scrapegraph_tools.py
        │   ├── searxng_tools.py
        │   ├── serpapi_tools.py
        │   ├── serper_tools.py
        │   ├── shell_tools.py
        │   ├── slack_tools.py
        │   ├── sleep_tools.py
        │   ├── spider_tools.py
        │   ├── sql_tools.py
        │   ├── tavily_tools.py
        │   ├── telegram_tools.py
        │   ├── todoist_tools.py
        │   ├── tool_calls_accesing_agent.py
        │   ├── trafilatura_tools.py
        │   ├── trello_tools.py
        │   ├── twilio_tools.py
        │   ├── valyu_tools.py
        │   ├── visualization_tools.py
        │   ├── web_tools.py
        │   ├── webbrowser_tools.py
        │   ├── webex_tools.py
        │   ├── website_tools.py
        │   ├── website_tools_knowledge.py
        │   ├── whatsapp_tools.py
        │   ├── wikipedia_tools.py
        │   ├── x_tools.py
        │   ├── yfinance_tools.py
        │   ├── youtube_tools.py
        │   ├── zendesk_tools.py
        │   ├── zep_async_tools.py
        │   ├── zep_tools.py
        │   ├── zoom_tools.py
        │   ├── async/
        │   │   ├── __init__.py
        │   │   ├── groq-demo.py
        │   │   └── openai-demo.py
        │   ├── mcp/
        │   │   ├── README.md
        │   │   ├── __init__.py
        │   │   ├── airbnb.py
        │   │   ├── brave.py
        │   │   ├── cli.py
        │   │   ├── filesystem.py
        │   │   ├── gibsonai.py
        │   │   ├── github.py
        │   │   ├── graphiti.py
        │   │   ├── groq_mcp.py
        │   │   ├── include_exclude_tools.py
        │   │   ├── include_tools.py
        │   │   ├── mem0.py
        │   │   ├── multiple_servers.py
        │   │   ├── notion_mcp_agent.py
        │   │   ├── oxylabs.py
        │   │   ├── pipedream_auth.py
        │   │   ├── pipedream_google_calendar.py
        │   │   ├── pipedream_linkedin.py
        │   │   ├── pipedream_slack.py
        │   │   ├── qdrant.py
        │   │   ├── sequential_thinking.py
        │   │   ├── stagehand.py
        │   │   ├── stripe.py
        │   │   ├── supabase.py
        │   │   ├── local_server/
        │   │   │   ├── client.py
        │   │   │   └── server.py
        │   │   ├── sse_transport/
        │   │   │   ├── README.md
        │   │   │   ├── client.py
        │   │   │   └── server.py
        │   │   └── streamable_http_transport/
        │   │       ├── README.md
        │   │       ├── client.py
        │   │       └── server.py
        │   └── models/
        │       ├── __init__.py
        │       ├── azure_openai_tools.py
        │       ├── gemini_video_generation.py
        │       ├── morph.py
        │       ├── nebius_tools.py
        │       └── openai_tools.py
        ├── workflows/
        │   ├── __init__.py
        │   ├── async_blog_post_generator.py
        │   ├── async_hackernews_reporter.py
        │   ├── blog_post_generator.py
        │   ├── employee_recruiter.py
        │   ├── hackernews_reporter.py
        │   ├── investment_report_generator.py
        │   ├── personalized_email_generator.py
        │   ├── reddit_post_generator.py
        │   ├── self_evaluating_content_creator.py
        │   ├── simple_cache_workflow.py
        │   ├── startup_idea_validator.py
        │   ├── team_workflow.py
        │   ├── workflows_playground.py
        │   ├── content_creator/
        │   │   ├── readme.md
        │   │   ├── __init__.py
        │   │   ├── config.py
        │   │   ├── prompts.py
        │   │   ├── requirements.txt
        │   │   ├── scheduler.py
        │   │   └── workflow.py
        │   └── product_manager/
        │       ├── __init__.py
        │       ├── meeting_notes.txt
        │       └── product_manager.py
        └── workflows_2/
            ├── README.md
            ├── __init__.py
            ├── blog_post_generator.py
            ├── employee_recruiter.py
            ├── employee_recruiter_async_stream.py
            ├── fastapi_demo.py
            ├── investment_report_generator.py
            ├── playground_demo.py
            ├── startup_idea_validator.py
            ├── async/
            │   ├── __init__.py
            │   ├── 01_basic_workflows/
            │   │   ├── __init__.py
            │   │   ├── function_instead_of_steps.py
            │   │   ├── function_instead_of_steps_stream.py
            │   │   ├── sequence_of_functions_and_agents.py
            │   │   ├── sequence_of_functions_and_agents_stream.py
            │   │   ├── sequence_of_steps.py
            │   │   ├── sequence_of_steps_stream.py
            │   │   ├── step_with_function_additional_data.py
            │   │   ├── step_with_function_stream.py
            │   │   └── workflow_using_steps.py
            │   ├── 02_workflows_conditional_execution/
            │   │   ├── __init__.py
            │   │   ├── condition_and_parallel_steps.py
            │   │   ├── condition_and_parallel_steps_stream.py
            │   │   ├── condition_steps_workflow_stream.py
            │   │   └── condition_with_list_of_steps.py
            │   ├── 03_workflows_loop_execution/
            │   │   ├── __init__.py
            │   │   ├── loop_steps_workflow.py
            │   │   ├── loop_steps_workflow_stream.py
            │   │   └── loop_with_parallel_steps_stream.py
            │   ├── 04_workflows_parallel_execution/
            │   │   ├── __init__.py
            │   │   ├── parallel_and_condition_steps_stream.py
            │   │   ├── parallel_steps_workflow.py
            │   │   └── parallel_steps_workflow_stream.py
            │   ├── 05_workflows_conditional_branching/
            │   │   ├── __init__.py
            │   │   ├── router_steps_workflow.py
            │   │   ├── router_steps_workflow_stream.py
            │   │   ├── router_with_loop_steps.py
            │   │   └── selector_for_image_video_generation_pipeline.py
            │   └── 06_workflows_advanced_concepts/
            │       ├── __init__.py
            │       └── background_execution.py
            └── sync/
                ├── __init__.py
                ├── 01_basic_workflows/
                │   ├── __init__.py
                │   ├── basic_workflow.py
                │   ├── function_instead_of_steps.py
                │   ├── function_instead_of_steps_stream.py
                │   ├── sequence_of_functions_and_agents.py
                │   ├── sequence_of_functions_and_agents_stream.py
                │   ├── sequence_of_steps.py
                │   ├── sequence_of_steps_stream.py
                │   ├── step_with_function.py
                │   ├── step_with_function_additional_data.py
                │   ├── step_with_function_stream.py
                │   ├── workflow_using_steps.py
                │   ├── workflow_using_steps_nested.py
                │   └── workflow_with_file_input.py
                ├── 02_workflows_conditional_execution/
                │   ├── __init__.py
                │   ├── condition_and_parallel_steps.py
                │   ├── condition_and_parallel_steps_stream.py
                │   ├── condition_steps_workflow_stream.py
                │   └── condition_with_list_of_steps.py
                ├── 03_workflows_loop_execution/
                │   ├── __init__.py
                │   ├── loop_steps_workflow.py
                │   ├── loop_steps_workflow_stream.py
                │   ├── loop_with_parallel_steps.py
                │   └── loop_with_parallel_steps_stream.py
                ├── 04_workflows_parallel_execution/
                │   ├── __init__.py
                │   ├── parallel_and_condition_steps_stream.py
                │   ├── parallel_steps_workflow.py
                │   └── parallel_steps_workflow_stream.py
                ├── 05_workflows_conditional_branching/
                │   ├── __init__.py
                │   ├── router_steps_workflow.py
                │   ├── router_steps_workflow_stream.py
                │   ├── router_with_loop_steps.py
                │   └── selector_for_image_video_generation_pipelines.py
                └── 06_workflows_advanced_concepts/
                    ├── __init__.py
                    ├── access_multiple_previous_step_output_stream_2.py
                    ├── access_multiple_previous_steps_output_stream_1.py
                    ├── early_stop_workflow_with_agents.py
                    ├── early_stop_workflow_with_condition.py
                    ├── early_stop_workflow_with_loop.py
                    ├── early_stop_workflow_with_parallel.py
                    ├── early_stop_workflow_with_router.py
                    ├── early_stop_workflow_with_step.py
                    ├── early_stop_workflow_with_steps.py
                    ├── pydantic_model_as_input.py
                    ├── run_stream_with_debug_mode.py
                    ├── shared_session_state_with_agent.py
                    ├── shared_session_state_with_team.py
                    ├── store_events_and_events_to_skip_in_a_workflow.py
                    ├── structured_io_at_each_level_agent.py
                    ├── structured_io_at_each_level_agent_stream.py
                    ├── structured_io_at_each_level_function_1.py
                    ├── structured_io_at_each_level_function_2.py
                    ├── structured_io_at_each_level_team.py
                    ├── structured_io_at_each_level_team_stream.py
                    ├── workflow_metrics_on_run_response.py
                    └── workflow_with_image_input.py

================================================
FILE: cookbook/README.md
================================================
# Agno Cookbooks

## Getting Started

The getting started guide walks through the basics of building Agents with Agno. Recipes build on each other, introducing new concepts and capabilities.

## Agent Concepts

The concepts cookbook walks through the core concepts of Agno.

- [Async](./agent_concepts/async)
- [RAG](./agent_concepts/rag)
- [Knowledge](./agent_concepts/knowledge)
- [Memory](./agent_concepts/memory)
- [Storage](storage)
- [Tools](./tools)
- [Reasoning](./reasoning)
- [Vector DBs](./agent_concepts/knowledge/vector_dbs)
- [Multi-modal Agents](./agent_concepts/multimodal)
- [Agent Teams](./teams)
- [Other](./agent_concepts/other)

## Examples

The examples cookbook contains real world examples of building agents with Agno.

## Playground

The playground cookbook contains examples of interacting with agents using the Agno Agent UI.

## Workflows

The workflows cookbook contains examples of building workflows with Agno.

## Scripts

Just a place to store setup scripts like `run_pgvector.sh` etc

## Setup

### Create and activate a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### Install libraries

```shell
pip install -U openai agno  # And all other packages you might need
```

### Export your keys

```shell
export OPENAI_API_KEY=***
export GOOGLE_API_KEY=***
```

## Run a cookbook

```shell
python cookbook/.../example.py
```



================================================
FILE: cookbook/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/README.md
================================================
# Agent Concepts

Application of several agent concepts using Agno.

## Overview

### Async

Async refers to agents built with `async def` support, allowing them to seamlessly integrate into asynchronous Python applications. While async agents are not inherently parallel, they allow better handling of I/O-bound operations, improving responsiveness in Python apps.

For examples of using async agents, see /cookbook/agent_concepts/async/.

### Hybrid Search

Hybrid Search combines multiple search paradigms—such as vector similarity search and traditional keyword-based search—to retrieve the most relevant results for a given query. This approach ensures that agents can find both semantically similar results and exact keyword matches, improving accuracy and context-awareness in diverse use cases.

Hybrid search examples can be found under `/cookbook/agent_concepts/hybrid_search/`

### Knowledge

Agents use a knowledge base to supplement their training data with domain expertise.
Knowledge is stored in a vector database and provides agents with business context at query time, helping them respond in a context-aware manner.

Examples of Agents with knowledge can be found under `/cookbook/agent_concepts/knowledge/`

### Memory

Agno provides 3 types of memory for Agents:

1. Chat History: The message history of the session. Agno will store the sessions in a database for you, and retrieve them when you resume a session.
2. User Memories: Notes and insights about the user, this helps the model personalize the response to the user.
3. Summaries: A summary of the conversation, which is added to the prompt when chat history gets too long.

Examples of Agents using different memory types can be found under `/cookbook/agent_concepts/memory/`

### Multimodal

In addition to text, Agno agents support image, audio, and video inputs and can generate image and audio outputs.

Examples with multimodal input and outputs using Agno can be found under `/cookbook/agent_concepts/storage/`

### RAG

RAG (Retrieval-Augmented Generation) integrates external data sources with AI's generation processes to produce context-aware, accurate, and relevant responses. It leverages vector databases for retrieved information and enhances agent memory components like chat history and summaries to provide coherent and informed answers.

Examples of agentic RAG can be found under `/cookbook/agent_concepts/rag/`

### Reasoning

Reasoning is an *experimental feature* that enables an Agent to think through a problem step-by-step before jumping into a response. The Agent works through different ideas, validating and correcting as needed. Once it reaches a final answer, it will validate and provide a response.

Examples of agentic shwowing their reasoning can be found under `/cookbook/agent_concepts/reasoning/`

### Storage

Agents use storage to persist sessions and session state by storing them in a database.

Agents come with built-in memory, but it only lasts while the session is active. To continue conversations across sessions, we store agent sessions in a database like Sqllite or PostgreSQL.

Examples of using storage with Agno agents can be found under `/cookbook/agent_concepts/storage/`

### Teams

Multiple agents can be combined to form a team and complete complicated tasks as a cohesive unit.

Examples of using agent teams with Agno can be found under `/cookbook/agent_concepts/teams/`

### Tools

Agents use tools to take actions and interact with external systems. A tool is a function that an Agent can use to achieve a task. For example: searching the web, running SQL, sending an email or calling APIs.

Examples of using tools with Agno agents can be found under `/cookbook/agent_concepts/tools/`

### Vector DB's

Vector databases enable us to store information as embeddings and search for “results similar” to our input query using cosine similarity or full text search. These results are then provided to the Agent as context so it can respond in a context-aware manner using Retrieval Augmented Generation (RAG).

Examples of using vector databases with Agno can be found under `/cookbook/agent_concepts/vector_dbs/`



================================================
FILE: cookbook/agent_concepts/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/agentic_search/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/agentic_search/agentic_rag.py
================================================
"""This cookbook shows how to implement Agentic RAG using Hybrid Search and Reranking.
1. Run: `pip install agno anthropic cohere lancedb tantivy sqlalchemy` to install the dependencies
2. Export your ANTHROPIC_API_KEY and CO_API_KEY
3. Run: `python cookbook/agent_concepts/agentic_search/agentic_rag.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.cohere import CohereEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.anthropic import Claude
from agno.reranker.cohere import CohereReranker
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base, loaded with documents from a URL
knowledge_base = UrlKnowledge(
    urls=["https://docs.agno.com/introduction/agents.md"],
    # Use LanceDB as the vector database, store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge_base,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
    ],
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, comment after first run
    # knowledge_base.load(recreate=True)
    agent.print_response("What are Agents?", stream=True)



================================================
FILE: cookbook/agent_concepts/agentic_search/agentic_rag_infinity_reranker.py
================================================
"""This cookbook shows how to implement Agentic RAG using Infinity Reranker.

Infinity is a high-performance inference server for text-embeddings, reranking, and classification models.
It provides fast and efficient reranking capabilities for RAG applications.

## Setup Instructions:

### 1. Install Dependencies
Run: `pip install agno anthropic infinity-client lancedb`

### 2. Set up Infinity Server
You have several options to deploy Infinity:

#### Local Installation
```bash
# Install infinity
pip install "infinity-emb[all]"

# Run infinity server with reranking model
infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997
```
Wait for the engine to start.

# For better performance, you can use larger models:
# BAAI/bge-reranker-large
# BAAI/bge-reranker-v2-m3
# ms-marco-MiniLM-L-12-v2


### 3. Export API Keys
```bash
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

### 4. Run the Example
```bash
python cookbook/agent_concepts/agentic_search/agentic_rag_infinity_reranker.py
```

## About Infinity Reranker:
- Provides fast, local reranking without external API calls
- Supports multiple state-of-the-art reranking models
- Can be deployed on GPU for better performance
- Offers both sync and async reranking capabilities
- More deployment options: https://michaelfeil.eu/infinity/0.0.76/deploy/
"""

from agno.agent import Agent
from agno.embedder.cohere import CohereEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.anthropic import Claude
from agno.reranker.infinity import InfinityReranker
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base, loaded with documents from a URL
knowledge_base = UrlKnowledge(
    urls=[
        "https://docs.agno.com/introduction/agents.md",
        "https://docs.agno.com/agents/tools.md",
        "https://docs.agno.com/agents/knowledge.md",
    ],
    # Use LanceDB as the vector database, store embeddings in the `agno_docs_infinity` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_infinity",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        # Use Infinity reranker for local, fast reranking
        reranker=InfinityReranker(
            model="BAAI/bge-reranker-base",  # You can change this to other models
            host="localhost",
            port=7997,
            top_n=5,  # Return top 5 reranked documents
        ),
    ),
)

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge_base,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
        "Provide detailed and accurate information based on the retrieved documents.",
    ],
    markdown=True,
)


def test_infinity_connection():
    """Test if Infinity server is running and accessible"""
    try:
        from infinity_client import Client

        client = Client(base_url="http://localhost:7997")
        print("✅ Successfully connected to Infinity server at localhost:7997")
        return True
    except Exception as e:
        print(f"❌ Failed to connect to Infinity server: {e}")
        print(
            "\nPlease make sure Infinity server is running. See setup instructions above."
        )
        return False


if __name__ == "__main__":
    print("🚀 Agentic RAG with Infinity Reranker Example")
    print("=" * 50)

    # Test Infinity connection first
    if not test_infinity_connection():
        exit(1)

    print("\n📚 Loading knowledge base...")
    # Load the knowledge base, comment after first run
    knowledge_base.load(recreate=True)
    print("✅ Knowledge base loaded successfully!")

    print("\n🤖 Starting agent interaction...")
    print("=" * 50)

    # Example questions to test the reranking capabilities
    questions = [
        "What are Agents and how do they work?",
        "How do I use tools with agents?",
        "What is the difference between knowledge and tools?",
    ]

    for i, question in enumerate(questions, 1):
        print(f"\n🔍 Question {i}: {question}")
        print("-" * 40)
        agent.print_response(question, stream=True)
        print("\n" + "=" * 50)

    print("\n🎉 Example completed!")
    print("\nThe Infinity reranker helped improve the relevance of retrieved documents")
    print("by reranking them based on semantic similarity to your queries.")



================================================
FILE: cookbook/agent_concepts/agentic_search/agentic_rag_with_reasoning.py
================================================
"""This cookbook shows how to implement Agentic RAG with Reasoning.
1. Run: `pip install agno anthropic cohere lancedb tantivy sqlalchemy` to install the dependencies
2. Export your ANTHROPIC_API_KEY and CO_API_KEY
3. Run: `python cookbook/agent_concepts/agentic_search/agentic_rag_with_reasoning.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.cohere import CohereEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.anthropic import Claude
from agno.reranker.cohere import CohereReranker
from agno.tools.reasoning import ReasoningTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base, loaded with documents from a URL
knowledge_base = UrlKnowledge(
    urls=["https://docs.agno.com/introduction/agents.md"],
    # Use LanceDB as the vector database, store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge_base,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
    ],
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, comment after first run
    # knowledge_base.load(recreate=True)
    agent.print_response(
        "What are Agents?",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/agent_concepts/agentic_search/lightrag/readme.md
================================================
# LightRAG with Agno 🔍

This cookbook demonstrates how to implement **Agentic RAG** (Retrieval-Augmented Generation) using [LightRAG](https://github.com/HKUDS/LightRAG) integrated with Agno. LightRAG provides a fast, graph-based RAG system that enhances document retrieval and knowledge querying capabilities.

---

## 🌟 Features

- **Agentic Search** → Agno agents can intelligently search and retrieve relevant information
- **Real-time Knowledge Updates** → Dynamic document loading and knowledge base updates
- **Multi-modal Support** → Works with various document formats (PDF, Markdown, etc.)

---

## 📖 What You'll Learn

The example demonstrates how to:

1. **Create a Knowledge Base** that connects to a hosted LightRAG Server
2. **Load Documents** from URLs or local files  
3. **Configure an Agent** with agentic search capabilities

---

## 📋 Prerequisites

Before getting started, ensure you have:

- **Python 3.8+** installed
- **Docker & Docker Compose** for infrastructure setup
- **OpenAI API key** for LLM and embedding services

---

## 🚀 Quick Start

### Step 1: Configure Environment

Edit the provided environment file:

```bash
cookbook/agent_concepts/agentic_search/lightrag/.env
```

Add your API keys as required. For detailed configuration, visit the official [LightRAG documentation](https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md).

> **Note:** The example uses OpenAI for both LLM and embedding provision.

**Key configurations to update:**
```env
LLM_BINDING_API_KEY=your_openai_api_key_here
EMBEDDING_BINDING_API_KEY=your_embedding_api_key_here
```

### Step 2: Deploy Infrastructure  

Deploy the LightRAG server using the provided Docker Compose file:

```bash
cd cookbook/agent_concepts/agentic_search/lightrag/
docker-compose up -d
```

✅ **Server will be available at:** `http://localhost:9621`

### Step 3: Run the Example

Execute the main example script:

```bash
python cookbook/agent_concepts/agentic_search/lightrag/agentic_rag_with_lightrag.py
```

---

## ⚠️ Important Note

**Document Processing Time:** Loading documents into the LightRAG server requires processing time. 

**Recommended workflow:**
1. Run the `knowledge_base.load()` function
2. Navigate to your LightRAG server (`http://localhost:9621`)
3. Monitor the file processing status
4. Once processing is complete, proceed with your queries

---

## 🔗 Additional Resources

- [LightRAG Official Repository](https://github.com/HKUDS/LightRAG)
- [LightRAG API Documentation](https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md)
- [Agno Documentation](https://agno.ai)




================================================
FILE: cookbook/agent_concepts/agentic_search/lightrag/agentic_rag_with_lightrag.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.light_rag import LightRagKnowledgeBase, lightrag_retriever
from agno.models.anthropic import Claude

# Create a knowledge base, loaded with documents from a URL
knowledge_base = LightRagKnowledgeBase(
    lightrag_server_url="http://localhost:9621",
    path="tmp/",  # Load documents from a local directory
    urls=["https://docs.agno.com/introduction/agents.md"],  # Load documents from a URL
)

# Load the knowledge base with the documents from the local directory and the URL
asyncio.run(knowledge_base.load())

# Load the knowledge base with the text
asyncio.run(
    knowledge_base.load_text(text="Dogs and cats are not pets, they are friends.")
)


# Create an agent with the knowledge base and the retriever
agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge_base,
    retriever=lightrag_retriever,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
        "Use the async_search method to search the knowledge base.",
    ],
    markdown=True,
)


asyncio.run(
    agent.aprint_response(
        "Which candidates are available for the role of a Senior Software Engineer?"
    )
)
asyncio.run(agent.aprint_response("What are Agno Agents?"))



================================================
FILE: cookbook/agent_concepts/async/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/async/basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
    markdown=True,
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe."))



================================================
FILE: cookbook/agent_concepts/async/basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
    markdown=True,
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", stream=True))



================================================
FILE: cookbook/agent_concepts/async/data_analyst.py
================================================
"""Run `pip install duckdb` to install dependencies."""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckdb import DuckDbTools

duckdb_tools = DuckDbTools(
    create_tables=False, export_tables=False, summarize_tables=False
)
duckdb_tools.create_table_from_path(
    path="https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
    table="movies",
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[duckdb_tools],
    markdown=True,
    show_tool_calls=True,
    additional_context=dedent("""\
    You have access to the following tables:
    - movies: contains information about movies from IMDB.
    """),
)
asyncio.run(agent.aprint_response("What is the average rating of movies?"))



================================================
FILE: cookbook/agent_concepts/async/delay.py
================================================
import asyncio

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

providers = ["openai", "anthropic", "ollama", "cohere", "google"]
instructions = [
    "Your task is to write a well researched report on AI providers.",
    "The report should be unbiased and factual.",
]


async def get_agent(delay, provider):
    agent = Agent(
        model=OpenAIChat(id="gpt-4"),
        instructions=instructions,
        tools=[DuckDuckGoTools()],
    )
    await asyncio.sleep(delay)
    response: RunResponse = await agent.arun(
        f"Write a report on the following AI provider: {provider}"
    )
    return response


async def get_reports():
    tasks = []
    for delay, provider in enumerate(providers):
        delay = delay * 2
        tasks.append(get_agent(delay, provider))

    results = await asyncio.gather(*tasks)
    return results


async def main():
    results = await get_reports()
    for result in results:
        print("************")
        pprint(result.content)
        print("************")
        print("\n")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/agent_concepts/async/gather_agents.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

providers = ["openai", "anthropic", "ollama", "cohere", "google"]
instructions = [
    "Your task is to write a well researched report on AI providers.",
    "The report should be unbiased and factual.",
]


async def get_reports():
    tasks = []
    for provider in providers:
        agent = Agent(
            model=OpenAIChat(id="gpt-4"),
            instructions=instructions,
            tools=[DuckDuckGoTools()],
        )
        tasks.append(
            agent.arun(f"Write a report on the following AI provider: {provider}")
        )

    results = await asyncio.gather(*tasks)
    return results


async def main():
    results = await get_reports()
    for result in results:
        print("************")
        pprint(result.content)
        print("************")
        print("\n")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/agent_concepts/async/reasoning.py
================================================
import asyncio

from agno.agent import Agent
from agno.cli.console import console
from agno.models.openai import OpenAIChat

task = "9.11 and 9.9 -- which is bigger?"

regular_agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

console.rule("[bold green]Regular Agent[/bold green]")
asyncio.run(regular_agent.aprint_response(task, stream=True))
console.rule("[bold yellow]Reasoning Agent[/bold yellow]")
asyncio.run(
    reasoning_agent.aprint_response(task, stream=True, show_full_reasoning=True)
)



================================================
FILE: cookbook/agent_concepts/async/structured_output.py
================================================
import asyncio
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o-2024-08-06"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)


# Get the response in a variable
# json_mode_response: RunResponse = json_mode_agent.arun("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.arun("New York")
# pprint(structured_output_response.content)

asyncio.run(structured_output_agent.aprint_response("New York"))
asyncio.run(json_mode_agent.aprint_response("New York"))



================================================
FILE: cookbook/agent_concepts/async/tool_use.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in UK and in USA?"))



================================================
FILE: cookbook/agent_concepts/context/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/context/add_context.py
================================================
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

    Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)


# Create a Context-Aware Agent that can access real-time HackerNews data
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Each function in the context is resolved when the agent is run,
    # think of it as dependency injection for Agents
    context={"top_hackernews_stories": get_top_hackernews_stories},
    # We can add the entire context dictionary to the user message
    add_context=True,
    markdown=True,
)

# Example usage
agent.print_response(
    "Summarize the top stories on HackerNews and identify any interesting trends.",
    stream=True,
)



================================================
FILE: cookbook/agent_concepts/context/agent_context.py
================================================
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

    Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)


# Create a Context-Aware Agent that can access real-time HackerNews data
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Each function in the context is evaluated at runtime
    context={"top_hackernews_stories": get_top_hackernews_stories},
    # Alternatively, you can manually add the context to the instructions
    instructions=dedent("""\
        You are an insightful tech trend observer! 📰

        Here are the top stories on HackerNews:
        {top_hackernews_stories}\
    """),
    # add_state_in_messages will make the `top_hackernews_stories` variable
    # available in the instructions
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response(
    "Summarize the top stories on HackerNews and identify any interesting trends.",
    stream=True,
)



================================================
FILE: cookbook/agent_concepts/context/context_in_instructions.py
================================================
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_upcoming_spacex_launches(num_launches: int = 5) -> str:
    url = "https://api.spacexdata.com/v5/launches/upcoming"
    launches = httpx.get(url).json()
    launches = sorted(launches, key=lambda x: x["date_unix"])[:num_launches]
    return json.dumps(launches, indent=4)


# Create an Agent that has access to real-time SpaceX data
agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    # Each function in the context is evaluated at runtime
    context={"upcoming_spacex_launches": get_upcoming_spacex_launches},
    description=dedent("""\
        You are a cosmic analyst and spaceflight enthusiast. 🚀

        Here are the next SpaceX launches:
        {upcoming_spacex_launches}\
    """),
    # add_state_in_messages will make the `upcoming_spacex_launches` variable
    # available in the description and instructions
    add_state_in_messages=True,
    markdown=True,
)

agent.print_response(
    "Tell me about the upcoming SpaceX missions.",
    stream=True,
)



================================================
FILE: cookbook/agent_concepts/events/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/events/basic_agent_events.py
================================================
import asyncio

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    agent_id="finance-agent",
    name="Finance Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
)


async def run_agent_with_events(prompt: str):
    content_started = False
    async for run_response_event in await finance_agent.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        if run_response_event.event in [RunEvent.run_started, RunEvent.run_completed]:
            print(f"\nEVENT: {run_response_event.event}")

        if run_response_event.event in [RunEvent.tool_call_started]:
            print(f"\nEVENT: {run_response_event.event}")
            print(f"TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_response_event.tool.tool_args}")

        if run_response_event.event in [RunEvent.tool_call_completed]:
            print(f"\nEVENT: {run_response_event.event}")
            print(f"TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TOOL CALL RESULT: {run_response_event.tool.result}")

        if run_response_event.event in [RunEvent.run_response_content]:
            if not content_started:
                print("\nCONTENT:")
                content_started = True
            else:
                print(run_response_event.content, end="")


if __name__ == "__main__":
    asyncio.run(
        run_agent_with_events(
            "What is the price of Apple stock?",
        )
    )



================================================
FILE: cookbook/agent_concepts/events/reasoning_agent_events.py
================================================
import asyncio

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat

finance_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
)


async def run_agent_with_events(prompt: str):
    content_started = False
    async for run_response_event in await finance_agent.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        if run_response_event.event in [RunEvent.run_started, RunEvent.run_completed]:
            print(f"\nEVENT: {run_response_event.event}")

        if run_response_event.event in [RunEvent.reasoning_started]:
            print(f"\nEVENT: {run_response_event.event}")

        if run_response_event.event in [RunEvent.reasoning_step]:
            print(f"\nEVENT: {run_response_event.event}")
            print(f"REASONING CONTENT: {run_response_event.reasoning_content}")

        if run_response_event.event in [RunEvent.reasoning_completed]:
            print(f"\nEVENT: {run_response_event.event}")

        if run_response_event.event in [RunEvent.run_response_content]:
            if not content_started:
                print("\nCONTENT:")
                content_started = True
            else:
                print(run_response_event.content, end="")


if __name__ == "__main__":
    task = (
        "Analyze the key factors that led to the signing of the Treaty of Versailles in 1919. "
        "Discuss the political, economic, and social impacts of the treaty on Germany and how it "
        "contributed to the onset of World War II. Provide a nuanced assessment that includes "
        "multiple historical perspectives."
    )
    asyncio.run(
        run_agent_with_events(
            task,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/README.md
================================================
# Agent Knowledge

**Knowledge Base:** is information that the Agent can search to improve its responses. This directory contains a series of cookbooks that demonstrate how to build a knowledge base for the Agent.

> Note: Fork and clone this repository if needed

### 1. Create a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Install libraries

```shell
pip install -U pgvector "psycopg[binary]" sqlalchemy openai agno
```

### 3. Run PgVector

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

- Run using a helper script

```shell
./cookbook/scripts/run_pgvector.sh
```

- OR run using the docker run command

```shell
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16
```

### 4. Test Knowledge Cookbooks

Eg: PDF URL Knowledge Base

- Install libraries

```shell
pip install -U pypdf bs4
```

- Run the PDF URL script

```shell
python cookbook/agent_concepts/knowledge/pdf_url.py
```



================================================
FILE: cookbook/agent_concepts/knowledge/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/arxiv_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.arxiv import ArxivKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the ArXiv documents
knowledge_base = ArxivKnowledgeBase(
    queries=["Generative AI", "Machine Learning"],
    # Table name: ai.arxiv_documents
    vector_db=PgVector(
        table_name="arxiv_documents",
        db_url=db_url,
    ),
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Ask the agent about the knowledge base
agent.print_response(
    "Ask me about generative ai from the knowledge base", markdown=True
)



================================================
FILE: cookbook/agent_concepts/knowledge/arxiv_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.arxiv import ArxivKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "arxive-reader"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

# Create a knowledge base with the ArXiv documents
knowledge_base = ArxivKnowledgeBase(
    queries=["Generative AI", "Machine Learning"], vector_db=vector_db
)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "Ask me about generative ai from the knowledge base", markdown=True
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/combined_kb.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.combined import CombinedKnowledgeBase
from agno.knowledge.csv import CSVKnowledgeBase
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.knowledge.website import WebsiteKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create CSV knowledge base
csv_kb = CSVKnowledgeBase(
    path=Path("data/csvs"),
    vector_db=PgVector(
        table_name="csv_documents",
        db_url=db_url,
    ),
)

# Create PDF URL knowledge base
pdf_url_kb = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="pdf_documents",
        db_url=db_url,
    ),
)

# Create Website knowledge base
website_kb = WebsiteKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    max_links=10,
    vector_db=PgVector(
        table_name="website_documents",
        db_url=db_url,
    ),
)

# Create Local PDF knowledge base
local_pdf_kb = PDFKnowledgeBase(
    path="data/pdfs",
    vector_db=PgVector(
        table_name="pdf_documents",
        db_url=db_url,
    ),
)

# Combine knowledge bases
knowledge_base = CombinedKnowledgeBase(
    sources=[
        csv_kb,
        pdf_url_kb,
        website_kb,
        local_pdf_kb,
    ],
    vector_db=PgVector(
        table_name="combined_documents",
        db_url=db_url,
    ),
)

# Initialize the Agent with the combined knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

knowledge_base.load(recreate=False)

# Use the agent
agent.print_response("Ask me about something from the knowledge base", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/combined_kb_async.py
================================================
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.combined import CombinedKnowledgeBase
from agno.knowledge.csv import CSVKnowledgeBase
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.knowledge.website import WebsiteKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create CSV knowledge base
csv_kb = CSVKnowledgeBase(
    path=Path("data/csvs"),
    vector_db=PgVector(
        table_name="csv_documents",
        db_url=db_url,
    ),
)

# Create PDF URL knowledge base
pdf_url_kb = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="pdf_documents",
        db_url=db_url,
    ),
)

# Create Website knowledge base
website_kb = WebsiteKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    max_links=10,
    vector_db=PgVector(
        table_name="website_documents",
        db_url=db_url,
    ),
)

# Create Local PDF knowledge base
local_pdf_kb = PDFKnowledgeBase(
    path="data/pdfs",
    vector_db=PgVector(
        table_name="pdf_documents",
        db_url=db_url,
    ),
)

# Combine knowledge bases
knowledge_base = CombinedKnowledgeBase(
    sources=[
        csv_kb,
        pdf_url_kb,
        website_kb,
        local_pdf_kb,
    ],
    vector_db=PgVector(
        table_name="combined_documents",
        db_url=db_url,
    ),
)

# Initialize the Agent with the combined knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "Ask me about something from the knowledge base", markdown=True
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/csv_kb.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.csv import CSVKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = CSVKnowledgeBase(
    path=Path("data/csvs"),
    vector_db=PgVector(
        table_name="csv_documents",
        db_url=db_url,
    ),
    num_documents=5,  # Number of documents to return on search
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Initialize the Agent with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Use the agent
agent.print_response("Ask me about something from the knowledge base", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/csv_kb_async.py
================================================
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.csv import CSVKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "csv-reader"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")


knowledge_base = CSVKnowledgeBase(
    path=Path("data/csv"),
    vector_db=vector_db,
    num_documents=5,  # Number of documents to return on search
)

# Initialize the Agent with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("What is the csv file about", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/csv_url_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.csv_url import CSVUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = CSVUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/csvs/employees.csv"],
    vector_db=PgVector(table_name="csv_documents", db_url=db_url),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "What is the average salary of employees in the Marketing department?",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/csv_url_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.csv_url import CSVUrlKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "csv-reader"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")


knowledge_base = CSVUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv"],
    vector_db=vector_db,
    num_documents=5,  # Number of documents to return on search
)

# Initialize the Agent with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(
        agent.aprint_response("What genre of movies are present here?", markdown=True)
    )



================================================
FILE: cookbook/agent_concepts/knowledge/doc_kb.py
================================================
from agno.agent import Agent
from agno.document.base import Document
from agno.knowledge.document import DocumentKnowledgeBase
from agno.vectordb.pgvector import PgVector

fun_facts = """
- Earth is the third planet from the Sun and the only known astronomical object to support life.
- Approximately 71% of Earth's surface is covered by water, with the Pacific Ocean being the largest.
- The Earth's atmosphere is composed mainly of nitrogen (78%) and oxygen (21%), with traces of other gases.
- Earth rotates on its axis once every 24 hours, leading to the cycle of day and night.
- The planet has one natural satellite, the Moon, which influences tides and stabilizes Earth's axial tilt.
- Earth's tectonic plates are constantly shifting, leading to geological activities like earthquakes and volcanic eruptions.
- The highest point on Earth is Mount Everest, standing at 8,848 meters (29,029 feet) above sea level.
- The deepest part of the ocean is the Mariana Trench, reaching depths of over 11,000 meters (36,000 feet).
- Earth has a diverse range of ecosystems, from rainforests and deserts to coral reefs and tundras.
- The planet's magnetic field protects life by deflecting harmful solar radiation and cosmic rays.
"""


# Load documents from the data/docs directory
documents = [Document(content=fun_facts)]

# Database connection URL
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the loaded documents
knowledge_base = DocumentKnowledgeBase(
    documents=documents,
    vector_db=PgVector(
        table_name="documents",
        db_url=db_url,
    ),
)

# Load the knowledge base
knowledge_base.load(recreate=False)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
)

# Ask the agent about the knowledge base
agent.print_response(
    "Ask me about something from the knowledge base about earth", markdown=True
)



================================================
FILE: cookbook/agent_concepts/knowledge/doc_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.document.base import Document
from agno.knowledge.document import DocumentKnowledgeBase
from agno.vectordb.pgvector import PgVector

fun_facts = """
- Earth is the third planet from the Sun and the only known astronomical object to support life.
- Approximately 71% of Earth's surface is covered by water, with the Pacific Ocean being the largest.
- The Earth's atmosphere is composed mainly of nitrogen (78%) and oxygen (21%), with traces of other gases.
- Earth rotates on its axis once every 24 hours, leading to the cycle of day and night.
- The planet has one natural satellite, the Moon, which influences tides and stabilizes Earth's axial tilt.
- Earth's tectonic plates are constantly shifting, leading to geological activities like earthquakes and volcanic eruptions.
- The highest point on Earth is Mount Everest, standing at 8,848 meters (29,029 feet) above sea level.
- The deepest part of the ocean is the Mariana Trench, reaching depths of over 11,000 meters (36,000 feet).
- Earth has a diverse range of ecosystems, from rainforests and deserts to coral reefs and tundras.
- The planet's magnetic field protects life by deflecting harmful solar radiation and cosmic rays.
"""


# Load documents from the data/docs directory
documents = [Document(content=fun_facts)]

# Database connection URL
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the loaded documents
knowledge_base = DocumentKnowledgeBase(
    documents=documents,
    vector_db=PgVector(
        table_name="documents",
        db_url=db_url,
    ),
)


async def main():
    # Load the knowledge base
    await knowledge_base.aload(recreate=False)

    # Create an agent with the knowledge base
    agent = Agent(
        knowledge=knowledge_base,
    )

    # Ask the agent about the knowledge base
    await agent.aprint_response(
        "Ask me about something from the knowledge base about earth", markdown=True
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/agent_concepts/knowledge/docx_kb.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.docx import DocxKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the DOCX files from the data/docs directory
knowledge_base = DocxKnowledgeBase(
    path=Path("tmp/docs"),
    vector_db=PgVector(
        table_name="docx_documents",
        db_url=db_url,
    ),
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Ask the agent about the knowledge base
agent.print_response("What docs do you have in your knowledge base?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/docx_kb_async.py
================================================
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.docx import DocxKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base with the DOCX files from the data/docs directory
knowledge_base = DocxKnowledgeBase(
    path=Path("tmp/docs"),
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="docx_reader",
        search_type=SearchType.hybrid,
    ),
)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    asyncio.run(knowledge_base.aload(recreate=False))

    asyncio.run(
        agent.aprint_response(
            "What docs do you have in your knowledge base?", markdown=True
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/firecrawl_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.firecrawl import FireCrawlKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "website-content"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

# Create a knowledge base with the seed URLs
knowledge_base = FireCrawlKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    vector_db=vector_db,
)

# Create an agent with the knowledge base
agent = Agent(knowledge=knowledge_base, search_knowledge=True, debug_mode=True)

if __name__ == "__main__":
    # Comment out after first run
    knowledge_base.load(recreate=False)

    agent.print_response("How does agno work?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/firecrawl_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.firecrawl import FireCrawlKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "website-content"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

# Create a knowledge base with the seed URLs
knowledge_base = FireCrawlKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    vector_db=vector_db,
)

# Create an agent with the knowledge base
agent = Agent(knowledge=knowledge_base, search_knowledge=True, debug_mode=True)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    asyncio.run(agent.aprint_response("How does agno work?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/gcs_pdf_kb.py
================================================
"""
This agent answers questions using knowledge from a PDF stored in a Google Cloud Storage (GCS) bucket.

Setup Steps:
1. Install required libraries: agno, google-cloud-storage, psycopg-binary (for PostgreSQL vector DB).
2. Set up your GCS bucket and upload your PDF file.
3. For public GCS buckets: No authentication needed, just set the bucket and PDF path.
4. For private GCS buckets:
   - Grant the service account Storage Object Viewer access to the bucket via Google Cloud Console
   - Export GOOGLE_APPLICATION_CREDENTIALS with the path to your service account JSON before running the script
5. Update 'bucket_name' and 'blob_name' in the script to your PDF's location.
6. Run the script to load the knowledge base and ask questions.
"""

from agno.agent import Agent
from agno.knowledge.gcs.pdf import GCSPDFKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = GCSPDFKnowledgeBase(
    bucket_name="your-gcs-bucket",
    blob_name="path/to/your.pdf",
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(knowledge=knowledge_base, search_knowledge=True)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/gcs_pdf_kb_async.py
================================================
"""
This agent answers questions using knowledge from a PDF stored in a Google Cloud Storage (GCS) bucket.

Setup Steps:
1. Install required libraries: agno, google-cloud-storage, psycopg-binary (for PostgreSQL vector DB).
2. Set up your GCS bucket and upload your PDF file.
3. For public GCS buckets: No authentication needed, just set the bucket and PDF path.
4. For private GCS buckets:
   - Grant the service account Storage Object Viewer access to the bucket via Google Cloud Console
   - Export GOOGLE_APPLICATION_CREDENTIALS with the path to your service account JSON before running the script
5. Update 'bucket_name' and 'blob_name' in the script to your PDF's location.
6. Run the script to load the knowledge base and ask questions.
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.gcs.pdf import GCSPDFKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = GCSPDFKnowledgeBase(
    bucket_name="your-gcs-bucket",
    blob_name="path/to/your.pdf",
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
agent = Agent(knowledge=knowledge_base, search_knowledge=True)

if __name__ == "__main__":
    asyncio.run(knowledge_base.aload(recreate=False))  # Comment out after first run

    asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/json_kb.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.json import JSONKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = JSONKnowledgeBase(
    path=Path("tmp/docs"),
    vector_db=PgVector(
        table_name="json_documents",
        db_url=db_url,
    ),
    num_documents=5,  # Number of documents to return on search
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Initialize the Agent with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Use the agent
agent.print_response("Ask me about something from the knowledge base", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/json_kb_async.py
================================================
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.json import JSONKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "json-reader"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

knowledge_base = JSONKnowledgeBase(
    path=Path("tmp/docs"),
    vector_db=vector_db,
    num_documents=5,  # Number of documents to return on search
)

# Initialize the Agent with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "Ask anything from the json knowledge base", markdown=True
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/langchain_kb.py
================================================
"""
pip install langchain langchain-community langchain-openai langchain-chroma agno
"""

import pathlib

from agno.agent import Agent
from agno.knowledge.langchain import LangChainKnowledgeBase
from langchain.text_splitter import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings

# Define the directory where the Chroma database is located
chroma_db_dir = pathlib.Path("./chroma_db")

# Define the path to the document to be loaded into the knowledge base
state_of_the_union = pathlib.Path("data/demo/state_of_the_union.txt")

# Load the document
raw_documents = TextLoader(str(state_of_the_union)).load()

# Split the document into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)

# Embed each chunk and load it into the vector store
Chroma.from_documents(
    documents, OpenAIEmbeddings(), persist_directory=str(chroma_db_dir)
)

# Get the vector database
db = Chroma(embedding_function=OpenAIEmbeddings(), persist_directory=str(chroma_db_dir))

# Create a retriever from the vector store
retriever = db.as_retriever()

# Create a knowledge base from the vector store
knowledge_base = LangChainKnowledgeBase(retriever=retriever)

# Create an agent with the knowledge base
agent = Agent(knowledge=knowledge_base)

# Use the agent to ask a question and print a response.
agent.print_response("What did the president say?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/llamaindex_kb.py
================================================
"""
pip install llama-index-core llama-index-readers-file llama-index-embeddings-openai agno
"""

from pathlib import Path
from shutil import rmtree

import httpx
from agno.agent import Agent
from agno.knowledge.llamaindex import LlamaIndexKnowledgeBase
from llama_index.core import (
    SimpleDirectoryReader,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever

data_dir = Path(__file__).parent.parent.parent.joinpath("wip", "data", "paul_graham")
if data_dir.is_dir():
    rmtree(path=data_dir, ignore_errors=True)
data_dir.mkdir(parents=True, exist_ok=True)

url = "https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt"
file_path = data_dir.joinpath("paul_graham_essay.txt")
response = httpx.get(url)
if response.status_code == 200:
    with open(file_path, "wb") as file:
        file.write(response.content)
    print(f"File downloaded and saved as {file_path}")
else:
    print("Failed to download the file")


documents = SimpleDirectoryReader(str(data_dir)).load_data()

splitter = SentenceSplitter(chunk_size=1024)

nodes = splitter.get_nodes_from_documents(documents)

storage_context = StorageContext.from_defaults()

index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)

retriever = VectorIndexRetriever(index)

# Create a knowledge base from the vector store
knowledge_base = LlamaIndexKnowledgeBase(retriever=retriever)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    debug_mode=True,
    show_tool_calls=True,
)

# Use the agent to ask a question and print a response.
agent.print_response(
    "Explain what this text means: low end eats the high end", markdown=True
)



================================================
FILE: cookbook/agent_concepts/knowledge/markdown_kb.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.markdown import MarkdownKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


knowledge_base = MarkdownKnowledgeBase(
    path=Path("README.md"),  # Path to your markdown file(s)
    vector_db=PgVector(
        table_name="markdown_documents",
        db_url=db_url,
    ),
    num_documents=5,  # Number of documents to return on search
)

# Load the knowledge base
knowledge_base.load(recreate=False)

# Initialize the Assistant with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Ask the agent about the documents in the knowledge base
agent.print_response(
    "What is a good way to get started with Agno?",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/markdown_kb_async.py
================================================
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.markdown import MarkdownKnowledgeBase
from agno.vectordb.pgvector.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


knowledge_base = MarkdownKnowledgeBase(
    path=Path("README.md"),  # Path to your markdown file(s)
    vector_db=PgVector(
        table_name="markdown_documents",
        db_url=db_url,
    ),
    num_documents=5,  # Number of documents to return on search
)


# Initialize the Assistant with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # This is only needed during the first run
    asyncio.run(knowledge_base.aload(recreate=False))

    asyncio.run(
        agent.aprint_response(
            "What knowledge is available in my knowledge base?",
            markdown=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/pdf_bytes_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFBytesKnowledgeBase
from agno.vectordb.lancedb import LanceDb

vector_db = LanceDb(
    table_name="recipes_async",
    uri="tmp/lancedb",
)

with open("data/pdfs/ThaiRecipes.pdf", "rb") as f:
    pdf_bytes = f.read()

knowledge_base = PDFBytesKnowledgeBase(
    pdfs=[pdf_bytes],
    vector_db=vector_db,
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Tom Kha Gai?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/pdf_bytes_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf import PDFBytesKnowledgeBase
from agno.vectordb.lancedb import LanceDb

vector_db = LanceDb(
    table_name="recipes_async",
    uri="tmp/lancedb",
)

with open("data/pdfs/ThaiRecipes.pdf", "rb") as f:
    pdf_bytes = f.read()

knowledge_base = PDFBytesKnowledgeBase(
    pdfs=[pdf_bytes],
    vector_db=vector_db,
)

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/pdf_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase, PDFReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the PDFs from the data/pdfs directory
knowledge_base = PDFKnowledgeBase(
    path="data/pdfs",
    vector_db=PgVector(
        table_name="pdf_documents",
        # Can inspect database via psql e.g. "psql -h localhost -p 5432 -U ai -d ai"
        db_url=db_url,
    ),
    reader=PDFReader(chunk=True),
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Ask the agent about the knowledge base
agent.print_response("Ask me about something from the knowledge base", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/pdf_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase, PDFReader
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "pdf-reader"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

# Create a knowledge base with the PDFs from the data/pdfs directory
knowledge_base = PDFKnowledgeBase(
    path="data/pdf",  # for password-protected PDFs, use path=[{"path": "tmp/ThaiRecipes_protected.pdf", "password": "ThaiRecipes"}],
    vector_db=vector_db,
    reader=PDFReader(chunk=True),
)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/pdf_kb_password.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import download_file
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
download_file(
    "https://agno-public.s3.us-east-1.amazonaws.com/recipes/ThaiRecipes_protected.pdf",
    "ThaiRecipes_protected.pdf",
)

# Create a knowledge base with simplified password handling
knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": "ThaiRecipes_protected.pdf",
            "password": "ThaiRecipes",
        }
    ],
    vector_db=PgVector(
        table_name="pdf_documents_password",
        db_url=db_url,
    ),
)
# Load the knowledge base
knowledge_base.load(recreate=True)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    show_tool_calls=True,
)

agent.print_response("Give me the recipe for pad thai")



================================================
FILE: cookbook/agent_concepts/knowledge/pdf_kb_url_password.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with simplified password handling
knowledge_base = PDFUrlKnowledgeBase(
    urls=[
        {
            "url": "https://agno-public.s3.us-east-1.amazonaws.com/recipes/ThaiRecipes_protected.pdf",
            "password": "ThaiRecipes",
        }
    ],
    vector_db=PgVector(
        table_name="pdf_documents_password",
        db_url=db_url,
    ),
)
# Load the knowledge base
knowledge_base.load(recreate=True)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    show_tool_calls=True,
)

agent.print_response("Give me the recipe for pad thai")



================================================
FILE: cookbook/agent_concepts/knowledge/pdf_url_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/pdf_url_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase, PDFUrlReader
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "pdf-url-reader"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

# Create a knowledge base with the PDFs from the data/pdfs directory
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
    reader=PDFUrlReader(chunk=True),
)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/s3_pdf_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.s3.pdf import S3PDFKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = S3PDFKnowledgeBase(
    bucket_name="agno-public",
    key="recipes/ThaiRecipes.pdf",
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(knowledge=knowledge_base, search_knowledge=True)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/s3_pdf_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.s3.pdf import S3PDFKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = S3PDFKnowledgeBase(
    bucket_name="agno-public",
    key="recipes/ThaiRecipes.pdf",
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)


agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(agent.knowledge.aload(recreate=True))

    asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/s3_text_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.s3.text import S3TextKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = S3TextKnowledgeBase(
    bucket_name="agno-public",
    key="recipes/recipes.docx",
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(knowledge=knowledge_base, search_knowledge=True)
agent.print_response("How to make Hummus?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/s3_text_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.s3.text import S3TextKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = S3TextKnowledgeBase(
    bucket_name="agno-public",
    key="recipes/recipes.docx",
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)


agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(agent.knowledge.aload(recreate=True))

    asyncio.run(agent.aprint_response("How to make Hummus?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/text_kb.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.text import TextKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


# Initialize the TextKnowledgeBase
knowledge_base = TextKnowledgeBase(
    path=Path("data/docs"),  # Table name: ai.text_documents
    vector_db=PgVector(
        table_name="text_documents",
        db_url=db_url,
    ),
    num_documents=5,  # Number of documents to return on search
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Initialize the Assistant with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Use the agent
agent.print_response("Ask me about something from the knowledge base", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/text_kb_async.py
================================================
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.text import TextKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "essay-txt"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

# Initialize the TextKnowledgeBase
knowledge_base = TextKnowledgeBase(
    path=Path("tmp/docs"),
    vector_db=vector_db,
    num_documents=5,
)

# Initialize the Assistant with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    asyncio.run(
        agent.aprint_response(
            "What knowledge is available in my knowledge base?", markdown=True
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/url_kb.py
================================================
"""Agent with Knowledge - An agent that can search a knowledge base

Install dependencies: `pip install openai lancedb tantivy agno`
"""

from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths
cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

# Initialize knowledge base
agent_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/introduction"],
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
    ),
)

agent_with_knowledge = Agent(
    name="Agent with Knowledge",
    model=OpenAIChat(id="gpt-4o"),
    knowledge=agent_knowledge,
    show_tool_calls=True,
    markdown=True,
)

if __name__ == "__main__":
    # Comment out after first run
    agent_knowledge.load()

    agent_with_knowledge.print_response(
        "Tell me about teams with context to agno", stream=True
    )



================================================
FILE: cookbook/agent_concepts/knowledge/url_kb_async.py
================================================
"""Agent with Knowledge - An agent that can search a knowledge base

Install dependencies: `pip install openai lancedb tantivy agno`
"""

import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths
cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

# Initialize knowledge base
agent_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/introduction"],
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
    ),
)

agent_with_knowledge = Agent(
    name="Agent with Knowledge",
    model=OpenAIChat(id="gpt-4o"),
    knowledge=agent_knowledge,
    show_tool_calls=True,
    markdown=True,
)

if __name__ == "__main__":
    asyncio.run(agent_knowledge.aload(recreate=False))

    asyncio.run(
        agent_with_knowledge.aprint_response(
            "Tell me about teams with context to agno", stream=True
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/website_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.website import WebsiteKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the seed URLs
knowledge_base = WebsiteKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    # Number of links to follow from the seed URLs
    max_links=10,
    # Table name: ai.website_documents
    vector_db=PgVector(
        table_name="website_documents",
        db_url=db_url,
    ),
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Ask the agent about the knowledge base
agent.print_response("How does agno work?")



================================================
FILE: cookbook/agent_concepts/knowledge/website_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.website import WebsiteKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "website-content"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")


# Create a knowledge base with the seed URLs
knowledge_base = WebsiteKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    # Number of links to follow from the seed URLs
    max_links=5,
    # Table name: ai.website_documents
    vector_db=vector_db,
)

# Create an agent with the knowledge base
agent = Agent(knowledge=knowledge_base, search_knowledge=True, debug_mode=True)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How does agno work?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/wikipedia_kb.py
================================================
from agno.agent import Agent
from agno.knowledge.wikipedia import WikipediaKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the PDFs from the data/pdfs directory
knowledge_base = WikipediaKnowledgeBase(
    topics=["Manchester United", "Real Madrid"],
    # Table name: ai.wikipedia_documents
    vector_db=PgVector(
        table_name="wikipedia_documents",
        db_url=db_url,
    ),
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Ask the agent about the knowledge base
agent.print_response(
    "Which team is objectively better, Manchester United or Real Madrid?"
)



================================================
FILE: cookbook/agent_concepts/knowledge/youtube_kb.py
================================================
from os import getenv

from agno.agent import Agent
from agno.knowledge.youtube import YouTubeKnowledgeBase, YouTubeReader
from agno.vectordb.qdrant import Qdrant

api_key = getenv("QDRANT_API_KEY")
qdrant_url = getenv("QDRANT_URL")

vector_db = Qdrant(collection="youtube-agno", url=qdrant_url, api_key=api_key)

knowledge_base = YouTubeKnowledgeBase(
    urls=["https://www.youtube.com/watch?v=CDC3GOuJyZ0"],
    vector_db=vector_db,
    reader=YouTubeReader(chunk=True),
)
knowledge_base.load(recreate=False)  # only once, comment it out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "What is the major focus of the knowledge provided?", markdown=True
)



================================================
FILE: cookbook/agent_concepts/knowledge/youtube_kb_async.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.youtube import YouTubeKnowledgeBase, YouTubeReader
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "youtube-reader"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

knowledge_base = YouTubeKnowledgeBase(
    urls=[
        "https://www.youtube.com/watch?v=CDC3GOuJyZ0",
        "https://www.youtube.com/watch?v=JbF_8g1EXj4",
    ],
    vector_db=vector_db,
    reader=YouTubeReader(chunk=True),
)

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "What is the major focus of the knowledge provided in both the videos, explain briefly.",
            markdown=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/chunking/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/chunking/agentic_chunking.py
================================================
from agno.agent import Agent
from agno.document.chunking.agentic import AgenticChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_agentic_chunking", db_url=db_url),
    chunking_strategy=AgenticChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/chunking/csv_row_chunking.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.document.chunking.row import RowChunking
from agno.knowledge.csv_url import CSVUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = CSVUrlKnowledgeBase(
    urls=[
        "https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
    ],
    vector_db=PgVector(
        table_name="imdb_movies_row_chunking",
        db_url=db_url,
    ),
    chunking_strategy=RowChunking(),
)
# Load the knowledge base
knowledge_base.load(recreate=False)

# Initialize the Agent with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Use the agent
agent.print_response("Tell me about the movie Guardians of the Galaxy", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/chunking/default.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/chunking/document_chunking.py
================================================
from agno.agent import Agent
from agno.document.chunking.document import DocumentChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_document_chunking", db_url=db_url),
    chunking_strategy=DocumentChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/chunking/fixed_size_chunking.py
================================================
from agno.agent import Agent
from agno.document.chunking.fixed import FixedSizeChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_fixed_size_chunking", db_url=db_url),
    chunking_strategy=FixedSizeChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/chunking/recursive_chunking.py
================================================
from agno.agent import Agent
from agno.document.chunking.recursive import RecursiveChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_recursive_chunking", db_url=db_url),
    chunking_strategy=RecursiveChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/chunking/semantic_chunking.py
================================================
from agno.agent import Agent
from agno.document.chunking.semantic import SemanticChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_semantic_chunking", db_url=db_url),
    chunking_strategy=SemanticChunking(similarity_threshold=0.5),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/custom/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/custom/async_retriever.py
================================================
import asyncio
from typing import Optional

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.qdrant import Qdrant
from qdrant_client import AsyncQdrantClient

# ---------------------------------------------------------
# This section loads the knowledge base. Skip if your knowledge base was populated elsewhere.
# Define the embedder
embedder = OpenAIEmbedder(id="text-embedding-3-small")
# Initialize vector database connection
vector_db = Qdrant(
    collection="thai-recipes", url="http://localhost:6333", embedder=embedder
)
# Load the knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Load the knowledge base
# knowledge_base.aload(recreate=True)  # Comment out after first run
# Knowledge base is now loaded
# ---------------------------------------------------------


# Define the custom async retriever
# This is the function that the agent will use to retrieve documents
async def retriever(
    query: str, agent: Optional[Agent] = None, num_documents: int = 5, **kwargs
) -> Optional[list[dict]]:
    """
    Custom async retriever function to search the vector database for relevant documents.

    Args:
        query (str): The search query string
        agent (Agent): The agent instance making the query
        num_documents (int): Number of documents to retrieve (default: 5)
        **kwargs: Additional keyword arguments

    Returns:
        Optional[list[dict]]: List of retrieved documents or None if search fails
    """
    try:
        qdrant_client = AsyncQdrantClient(url="http://localhost:6333")
        query_embedding = embedder.get_embedding(query)
        results = await qdrant_client.query_points(
            collection_name="thai-recipes",
            query=query_embedding,
            limit=num_documents,
        )
        results_dict = results.model_dump()
        if "points" in results_dict:
            return results_dict["points"]
        else:
            return None
    except Exception as e:
        print(f"Error during vector database search: {str(e)}")
        return None


async def amain():
    """Async main function to demonstrate agent usage."""
    # Initialize agent with custom retriever
    # Remember to set search_knowledge=True to use agentic_rag or add_reference=True for traditional RAG
    # search_knowledge=True is default when you add a knowledge base but is needed here
    agent = Agent(
        retriever=retriever,
        search_knowledge=True,
        instructions="Search the knowledge base for information",
        show_tool_calls=True,
    )

    # Load the knowledge base (uncomment for first run)
    await knowledge_base.aload(recreate=True)

    # Example query
    query = "List down the ingredients to make Massaman Gai"
    await agent.aprint_response(query, markdown=True)


def main():
    """Synchronous wrapper for main function"""
    asyncio.run(amain())


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/agent_concepts/knowledge/custom/retriever.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.qdrant import Qdrant
from qdrant_client import QdrantClient

# ---------------------------------------------------------
# This section loads the knowledge base. Skip if your knowledge base was populated elsewhere.
# Define the embedder
embedder = OpenAIEmbedder(id="text-embedding-3-small")
# Initialize vector database connection
vector_db = Qdrant(
    collection="thai-recipes", url="http://localhost:6333", embedder=embedder
)
# Load the knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Load the knowledge base
knowledge_base.load(recreate=True)  # Comment out after first run
# Knowledge base is now loaded
# ---------------------------------------------------------


# Define the custom retriever
# This is the function that the agent will use to retrieve documents
def retriever(
    query: str, agent: Optional[Agent] = None, num_documents: int = 5, **kwargs
) -> Optional[list[dict]]:
    """
    Custom retriever function to search the vector database for relevant documents.

    Args:
        query (str): The search query string
        agent (Agent): The agent instance making the query
        num_documents (int): Number of documents to retrieve (default: 5)
        **kwargs: Additional keyword arguments

    Returns:
        Optional[list[dict]]: List of retrieved documents or None if search fails
    """
    try:
        qdrant_client = QdrantClient(url="http://localhost:6333")
        query_embedding = embedder.get_embedding(query)
        results = qdrant_client.query_points(
            collection_name="thai-recipes",
            query=query_embedding,
            limit=num_documents,
        )
        results_dict = results.model_dump()
        if "points" in results_dict:
            return results_dict["points"]
        else:
            return None
    except Exception as e:
        print(f"Error during vector database search: {str(e)}")
        return None


def main():
    """Main function to demonstrate agent usage."""
    # Initialize agent with custom retriever
    # Remember to set search_knowledge=True to use agentic_rag or add_reference=True for traditional RAG
    # search_knowledge=True is default when you add a knowledge base but is needed here
    agent = Agent(
        retriever=retriever,
        search_knowledge=True,
        instructions="Search the knowledge base for information",
        show_tool_calls=True,
    )

    # Example query
    query = "List down the ingredients to make Massaman Gai"
    agent.print_response(query, markdown=True)


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/embedders/aws_bedrock_embedder.py
================================================
from agno.document.reader.pdf_reader import PDFUrlReader
from agno.embedder.aws_bedrock import AwsBedrockEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

embeddings = AwsBedrockEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)
# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    reader=PDFUrlReader(
        chunk_size=2048
    ),  # Required because cohere has a fixed size of 2048
    vector_db=PgVector(
        table_name="recipes",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        embedder=AwsBedrockEmbedder(),
    ),
)
knowledge_base.load(recreate=False)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/azure_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.azure_openai import AzureOpenAIEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = AzureOpenAIEmbedder(id="text-embedding-3-small").get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="azure_openai_embeddings",
        embedder=AzureOpenAIEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/cohere_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.cohere import CohereEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = CohereEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)
# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="cohere_embeddings",
        embedder=CohereEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/fireworks_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.fireworks import FireworksEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = FireworksEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="fireworks_embeddings",
        embedder=FireworksEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/gemini_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.google import GeminiEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = GeminiEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="gemini_embeddings",
        embedder=GeminiEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/huggingface_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.huggingface import HuggingfaceCustomEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = HuggingfaceCustomEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="huggingface_embeddings",
        embedder=HuggingfaceCustomEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/jina_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.jina import JinaEmbedder
from agno.vectordb.pgvector import PgVector

# Basic usage - automatically loads from JINA_API_KEY environment variable
embeddings = JinaEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

custom_embedder = JinaEmbedder(
    dimensions=1024,
    late_chunking=True,  # Improved processing for long documents
    timeout=30.0,  # Request timeout in seconds
)

# Get embedding with usage information
embedding, usage = custom_embedder.get_embedding_and_usage(
    "Advanced text processing with Jina embeddings and late chunking."
)
print(f"Embedding dimensions: {len(embedding)}")
if usage:
    print(f"Usage info: {usage}")

# Example usage with AgentKnowledge
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="jina_embeddings",
        embedder=JinaEmbedder(
            late_chunking=True,  # Better handling of long documents
            timeout=30.0,  # Configure request timeout
        ),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/langdb_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.langdb import LangDBEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = LangDBEmbedder().get_embedding("Embed me")

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="langdb_embeddings",
        embedder=LangDBEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/mistral_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.mistral import MistralEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = MistralEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="mistral_embeddings",
        embedder=MistralEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/nebius_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.nebius import NebiusEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = NebiusEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="nebius_embeddings",
        embedder=NebiusEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/ollama_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.ollama import OllamaEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = OllamaEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="ollama_embeddings",
        embedder=OllamaEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/openai_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.openai import OpenAIEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = OpenAIEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="openai_embeddings",
        embedder=OpenAIEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/qdrant_fastembed.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.fastembed import FastEmbedEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = FastEmbedEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="qdrant_embeddings",
        embedder=FastEmbedEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/sentence_transformer_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.sentence_transformer import SentenceTransformerEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = SentenceTransformerEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="sentence_transformer_embeddings",
        embedder=SentenceTransformerEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/together_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.together import TogetherEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = TogetherEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="together_embeddings",
        embedder=TogetherEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/embedders/voyageai_embedder.py
================================================
from agno.agent import AgentKnowledge
from agno.embedder.voyageai import VoyageAIEmbedder
from agno.vectordb.pgvector import PgVector

embeddings = VoyageAIEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge_base = AgentKnowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="voyageai_embeddings",
        embedder=VoyageAIEmbedder(),
    ),
    num_documents=2,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_chroma_db.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.chroma import ChromaDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize ChromaDB
vector_db = ChromaDb(collection="recipes", path="tmp/chromadb", persistent_client=True)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_lance_db.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_milvus.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.milvus import Milvus

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize Milvus vector db
vector_db = Milvus(
    collection="recipes",
    uri="tmp/milvus.db",
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_mongo_db.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.mongodb import MongoDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=MongoDb(
        collection_name="filters",
        db_url=mdb_connection_string,
        search_index_name="filters",
    ),
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_pgvector.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.pgvector import PgVector

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize PgVector
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

vector_db = PgVector(table_name="recipes", db_url=db_url)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_pinecone.py
================================================
from os import getenv

from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.pineconedb import PineconeDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize Pinecone
api_key = getenv("PINECONE_API_KEY")
index_name = "thai-recipe-index"

vector_db = PineconeDb(
    name=index_name,
    dimension=1536,
    metric="cosine",
    spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
    api_key=api_key,
)


# Step 1: Initialize knowledge base with documents and metadata
knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True, upsert=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "hey"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_qdrant_db.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.qdrant import Qdrant

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

COLLECTION_NAME = "filtering-cv"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")
# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_surrealdb.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import Surreal

# SurrealDB connection parameters
SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Create a client
client = Surreal(url=SURREALDB_URL)
client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

vector_db = SurrealDb(
    client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
)


# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_traditional_RAG.py
================================================
"""
User-Level Knowledge Filtering Example

This cookbook demonstrates how to use knowledge filters to restrict knowledge base searches to specific users, document types, or any other metadata attributes.

Key concepts demonstrated:
1. Loading documents with user-specific metadata
2. Filtering knowledge base searches by user ID
3. Combining multiple filter criteria
4. Comparing results across different filter combinations

You can pass filters in the following ways:
1. If you pass on Agent only, we use that for all runs
2. If you pass on run/print_response only, we use that for that run
3. If you pass on both, we override with the filters passed on run/print_response for that run
"""

from agno.agent import Agent
from agno.knowledge.text import TextKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.TXT
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = TextKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=False,
    add_references=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)

# Query for Jordan Mitchell's experience and skills
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)

# # Option 2: Filters on the run/print_response
# agent = Agent(
#     knowledge=knowledge_base,
#     add_references=True,
#     search_knowledge=False,
# )

# # Query for Taylor Brooks as a candidate
# agent.print_response(
#     "Tell me about Taylor Brooks as a candidate",
#     knowledge_filters={"user_id": "taylor_brooks"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/filtering_weaviate.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

vector_db = Weaviate(
    collection="recipes",
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=False,  # Set to False if using Weaviate Cloud and True if using local instance
)

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/csv/agentic_filtering.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.csv import CSVKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample sales files and get their paths
downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------

knowledge_base = CSVKnowledgeBase(
    path=[
        {
            "path": downloaded_csv_paths[0],
            "metadata": {
                "data_type": "sales",
                "quarter": "Q1",
                "year": 2024,
                "region": "north_america",
                "currency": "USD",
            },
        },
        {
            "path": downloaded_csv_paths[1],
            "metadata": {
                "data_type": "sales",
                "year": 2024,
                "region": "europe",
                "currency": "EUR",
            },
        },
        {
            "path": downloaded_csv_paths[2],
            "metadata": {
                "data_type": "survey",
                "survey_type": "customer_satisfaction",
                "year": 2024,
                "target_demographic": "mixed",
            },
        },
        {
            "path": downloaded_csv_paths[3],
            "metadata": {
                "data_type": "financial",
                "sector": "technology",
                "year": 2024,
                "report_type": "quarterly_earnings",
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with Agent using filters from query automatically
# -----------------------------------------------------------------------------------

# Enable agentic filtering
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

agent.print_response(
    "Tell me about revenue performance and top selling products in the region north_america and data_type sales",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/csv/filtering.py
================================================
from agno.agent import Agent
from agno.knowledge.csv import CSVKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample sales documents and get their paths
downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# -----------------------------------------------------------------------------
knowledge_base = CSVKnowledgeBase(
    path=[
        {
            "path": downloaded_csv_paths[0],
            "metadata": {
                "data_type": "sales",
                "quarter": "Q1",
                "year": 2024,
                "region": "north_america",
                "currency": "USD",
            },
        },
        {
            "path": downloaded_csv_paths[1],
            "metadata": {
                "data_type": "sales",
                "year": 2024,
                "region": "europe",
                "currency": "EUR",
            },
        },
        {
            "path": downloaded_csv_paths[2],
            "metadata": {
                "data_type": "survey",
                "survey_type": "customer_satisfaction",
                "year": 2024,
                "target_demographic": "mixed",
            },
        },
        {
            "path": downloaded_csv_paths[3],
            "metadata": {
                "data_type": "financial",
                "sector": "technology",
                "year": 2024,
                "report_type": "quarterly_earnings",
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
na_sales = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

na_sales.print_response(
    "Revenue performance and top selling products",
    knowledge_filters={"region": "north_america", "data_type": "sales"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/csv/filtering_on_load.py
================================================
from agno.agent import Agent
from agno.knowledge.csv import CSVKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample sales files and get their paths
downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering

# Initialize the PDFKnowledgeBase
knowledge_base = CSVKnowledgeBase(
    vector_db=vector_db,
    num_documents=5,
)

knowledge_base.load_document(
    path=downloaded_csv_paths[0],
    metadata={
        "data_type": "sales",
        "quarter": "Q1",
        "year": 2024,
        "region": "north_america",
        "currency": "USD",
    },
    recreate=True,  # Set to True only for the first run, then set to False
)

knowledge_base.load_document(
    path=downloaded_csv_paths[1],
    metadata={
        "data_type": "sales",
        "year": 2024,
        "region": "europe",
        "currency": "EUR",
    },
)

knowledge_base.load_document(
    path=downloaded_csv_paths[2],
    metadata={
        "data_type": "survey",
        "survey_type": "customer_satisfaction",
        "year": 2024,
        "target_demographic": "mixed",
    },
)

knowledge_base.load_document(
    path=downloaded_csv_paths[3],
    metadata={
        "data_type": "financial",
        "sector": "technology",
        "year": 2024,
        "report_type": "quarterly_earnings",
    },
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"region": "north_america", "data_type": "sales"},
)
agent.print_response(
    "Revenue performance and top selling products",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/csv_url/agentic_filtering.py
================================================
from agno.agent import Agent
from agno.knowledge.csv_url import CSVUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with URLs and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include cuisine type, source, region, or any other attributes

knowledge_base = CSVUrlKnowledgeBase(
    urls=[
        {
            "url": "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/filters_1.csv",
            "metadata": {
                "data_type": "sales",
                "quarter": "Q1",
                "year": 2024,
                "region": "north_america",
                "currency": "USD",
            },
        },
        {
            "url": "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/filters_2.csv",
            "metadata": {
                "data_type": "sales",
                "year": 2024,
                "region": "europe",
                "currency": "EUR",
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

agent.print_response(
    "Tell me about revenue performance and top selling products for region 'north_america' and data_type 'sales'",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/csv_url/filtering.py
================================================
from agno.agent import Agent
from agno.knowledge.csv_url import CSVUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with URLs and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include cuisine type, source, region, or any other attributes

knowledge_base = CSVUrlKnowledgeBase(
    urls=[
        {
            "url": "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/filters_1.csv",
            "metadata": {
                "data_type": "sales",
                "quarter": "Q1",
                "year": 2024,
                "region": "north_america",
                "currency": "USD",
            },
        },
        {
            "url": "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/filters_2.csv",
            "metadata": {
                "data_type": "sales",
                "year": 2024,
                "region": "europe",
                "currency": "EUR",
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"region": "north_america", "data_type": "sales"},
)

agent.print_response(
    "Revenue performance and top selling products",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/csv_url/filtering_on_load.py
================================================
from agno.agent import Agent
from agno.knowledge.csv_url import CSVUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

# Initialize knowledge base
knowledge_base = CSVUrlKnowledgeBase(
    vector_db=vector_db,
)

knowledge_base.load_document(
    url="https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/filters_1.csv",
    metadata={
        "data_type": "sales",
        "quarter": "Q1",
        "year": 2024,
        "region": "north_america",
        "currency": "USD",
    },
    recreate=False,  # only use at the first run, True/False
)

knowledge_base.load_document(
    url="https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/filters_2.csv",
    metadata={
        "data_type": "sales",
        "year": 2024,
        "region": "europe",
        "currency": "EUR",
    },
)


agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"region": "north_america", "data_type": "sales"},
)
agent.print_response(
    "Revenue performance and top selling products",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/document/agentic_filtering.py
================================================
from agno.agent import Agent
from agno.document import Document
from agno.knowledge.document import DocumentKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="documents",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Create sample documents with different metadata
sample_documents = [
    Document(
        name="user_profile_jordan",
        id="doc_1",
        content="Jordan Mitchell is a Senior Software Engineer with 8 years of experience in full-stack development. Expertise in Python, JavaScript, React, and Node.js. Led multiple projects and mentored junior developers.",
    ),
    Document(
        name="user_profile_taylor",
        id="doc_2",
        content="Taylor Brooks is a Product Manager with 5 years of experience in agile development and product strategy. Skilled in user research, roadmap planning, and cross-functional team leadership.",
    ),
    Document(
        name="user_profile_morgan",
        id="doc_3",
        content="Morgan Lee is a UX Designer with 6 years of experience in user interface design and user experience research. Proficient in Figma, Adobe Creative Suite, and user testing methodologies.",
    ),
    Document(
        name="company_policy_remote",
        id="doc_4",
        content="Remote Work Policy: Employees are allowed to work from home up to 3 days per week. All remote work must be approved by direct supervisor and requires secure VPN connection.",
    ),
    Document(
        name="company_policy_vacation",
        id="doc_5",
        content="Vacation Policy: Full-time employees accrue 2.5 days of vacation per month. Maximum accrual is 30 days. Vacation requests must be submitted at least 2 weeks in advance.",
    ),
]

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach additional metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = DocumentKnowledgeBase(
    documents=[
        {
            "document": sample_documents[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "year": 2025,
                "status": "active",
            },
        },
        {
            "document": sample_documents[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "year": 2025,
                "status": "active",
            },
        },
        {
            "document": sample_documents[2],
            "metadata": {
                "user_id": "morgan_lee",
                "year": 2025,
                "status": "active",
            },
        },
        {
            "document": sample_documents[3],
            "metadata": {
                "created_by": "hr_department",
                "year": 2025,
                "status": "current",
            },
        },
        {
            "document": sample_documents[4],
            "metadata": {
                "created_by": "hr_department",
                "year": 2025,
                "status": "current",
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills with user_id jordan_mitchell",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/document/filtering.py
================================================
from agno.agent import Agent
from agno.document import Document
from agno.knowledge.document import DocumentKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="documents",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Create sample documents with different metadata
sample_documents = [
    Document(
        name="user_profile_jordan",
        id="doc_1",
        content="Jordan Mitchell is a Senior Software Engineer with 8 years of experience in full-stack development. Expertise in Python, JavaScript, React, and Node.js. Led multiple projects and mentored junior developers.",
    ),
    Document(
        name="user_profile_taylor",
        id="doc_2",
        content="Taylor Brooks is a Product Manager with 5 years of experience in agile development and product strategy. Skilled in user research, roadmap planning, and cross-functional team leadership.",
    ),
    Document(
        name="user_profile_morgan",
        id="doc_3",
        content="Morgan Lee is a UX Designer with 6 years of experience in user interface design and user experience research. Proficient in Figma, Adobe Creative Suite, and user testing methodologies.",
    ),
    Document(
        name="company_policy_remote",
        id="doc_4",
        content="Remote Work Policy: Employees are allowed to work from home up to 3 days per week. All remote work must be approved by direct supervisor and requires secure VPN connection.",
    ),
    Document(
        name="company_policy_vacation",
        id="doc_5",
        content="Vacation Policy: Full-time employees accrue 2.5 days of vacation per month. Maximum accrual is 30 days. Vacation requests must be submitted at least 2 weeks in advance.",
    ),
]

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach additional metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = DocumentKnowledgeBase(
    documents=[
        {
            "document": sample_documents[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "year": 2025,
                "status": "active",
            },
        },
        {
            "document": sample_documents[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "year": 2025,
                "status": "active",
            },
        },
        {
            "document": sample_documents[2],
            "metadata": {
                "user_id": "morgan_lee",
                "year": 2025,
                "status": "active",
            },
        },
        {
            "document": sample_documents[3],
            "metadata": {
                "created_by": "hr_department",
                "year": 2025,
                "status": "current",
            },
        },
        {
            "document": sample_documents[4],
            "metadata": {
                "created_by": "hr_department",
                "year": 2025,
                "status": "current",
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/document/filtering_on_load.py
================================================
from agno.agent import Agent
from agno.document import Document
from agno.knowledge.document import DocumentKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="documents",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Create sample documents with different metadata
sample_documents = [
    Document(
        name="user_profile_jordan",
        id="doc_1",
        content="Jordan Mitchell is a Senior Software Engineer with 8 years of experience in full-stack development. Expertise in Python, JavaScript, React, and Node.js. Led multiple projects and mentored junior developers.",
    ),
    Document(
        name="user_profile_taylor",
        id="doc_2",
        content="Taylor Brooks is a Product Manager with 5 years of experience in agile development and product strategy. Skilled in user research, roadmap planning, and cross-functional team leadership.",
    ),
    Document(
        name="user_profile_morgan",
        id="doc_3",
        content="Morgan Lee is a UX Designer with 6 years of experience in user interface design and user experience research. Proficient in Figma, Adobe Creative Suite, and user testing methodologies.",
    ),
    Document(
        name="company_policy_remote",
        id="doc_4",
        content="Remote Work Policy: Employees are allowed to work from home up to 3 days per week. All remote work must be approved by direct supervisor and requires secure VPN connection.",
    ),
    Document(
        name="company_policy_vacation",
        id="doc_5",
        content="Vacation Policy: Full-time employees accrue 2.5 days of vacation per month. Maximum accrual is 30 days. Vacation requests must be submitted at least 2 weeks in advance.",
    ),
]

# Step 1: Initialize knowledge base and load documents with metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

# Initialize the DocumentKnowledgeBase
knowledge_base = DocumentKnowledgeBase(
    vector_db=vector_db,
    num_documents=5,
)

# Load first document with Jordan Mitchell metadata
knowledge_base.load_document(
    document=sample_documents[0],
    metadata={"user_id": "jordan_mitchell", "year": 2025, "status": "active"},
    recreate=True,  # Set to True only for the first run, then set to False
)

# Load second document with Taylor Brooks metadata
knowledge_base.load_document(
    document=sample_documents[1],
    metadata={"user_id": "taylor_brooks", "year": 2025, "status": "active"},
)

# Load third document with Morgan Lee metadata
knowledge_base.load_document(
    document=sample_documents[2],
    metadata={"user_id": "morgan_lee", "year": 2025, "status": "active"},
)

# Load fourth document with HR department metadata
knowledge_base.load_document(
    document=sample_documents[3],
    metadata={"created_by": "hr_department", "year": 2025, "status": "current"},
)

# Load fifth document with HR department metadata
knowledge_base.load_document(
    document=sample_documents[4],
    metadata={"created_by": "hr_department", "year": 2025, "status": "current"},
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/docx/agentic_filtering.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.docx import DocxKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.DOCX
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = DocxKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with Agent using filters from query automatically
# -----------------------------------------------------------------------------------

# Enable agentic filtering
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

# Query for Jordan Mitchell's experience and skills with filters in query so that Agent can automatically pick them up
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills with jordan_mitchell as user id and document type cv",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/docx/async_filtering.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.docx import DocxKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.DOCX
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = DocxKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Load all documents into the vector database
    asyncio.run(knowledge_base.aload(recreate=True))

    # Query for Jordan Mitchell's experience and skills
    asyncio.run(
        agent.aprint_response(
            "Tell me about Jordan Mitchell's experience and skills",
            knowledge_filters={"user_id": "jordan_mitchell"},
            markdown=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/docx/filtering.py
================================================
"""
User-Level Knowledge Filtering Example

This cookbook demonstrates how to use knowledge filters to restrict knowledge base searches to specific users, document types, or any other metadata attributes.

Key concepts demonstrated:
1. Loading documents with user-specific metadata
2. Filtering knowledge base searches by user ID
3. Combining multiple filter criteria
4. Comparing results across different filter combinations

You can pass filters in the following ways:
1. If you pass on Agent only, we use that for all runs
2. If you pass on run/print_response only, we use that for that run
3. If you pass on both, we override with the filters passed on run/print_response for that run
"""

from agno.agent import Agent
from agno.knowledge.docx import DocxKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.DOCX
)

# Initialize LanceDB
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",
)

# Now use the downloaded paths in knowledge base initialization
knowledge_base = DocxKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)


# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)

# Query for Jordan Mitchell's experience and skills
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)

# # Option 2: Filters on the run/print_response
# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )

# # Query for Taylor Brooks as a candidate
# agent.print_response(
#     "Tell me about Taylor Brooks as a candidate",
#     knowledge_filters={"user_id": "taylor_brooks"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/docx/filtering_on_load.py
================================================
from agno.agent import Agent
from agno.knowledge.docx import DocxKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.DOCX
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

# Initialize the DocxKnowledgeBase
knowledge_base = DocxKnowledgeBase(
    vector_db=vector_db,
    num_documents=5,
)

knowledge_base.load_document(
    path=downloaded_cv_paths[0],
    metadata={"user_id": "jordan_mitchell", "document_type": "cv", "year": 2025},
    recreate=True,  # Set to True only for the first run, then set to False
)

# Load second document with user_2 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[1],
    metadata={"user_id": "taylor_brooks", "document_type": "cv", "year": 2025},
)

# Load second document with user_3 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[2],
    metadata={"user_id": "morgan_lee", "document_type": "cv", "year": 2025},
)

# Load second document with user_4 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[3],
    metadata={"user_id": "casey_jordan", "document_type": "cv", "year": 2025},
)

# Load second document with user_5 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[4],
    metadata={"user_id": "alex_rivera", "document_type": "cv", "year": 2025},
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
# Uncomment the example you want to run

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)

# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )
# agent.print_response(
#     "Tell me about Jordan Mitchell's experience and skills",
#     knowledge_filters = {"user_id": "jordan_mitchell"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/json/agentic_filtering.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.json import JSONKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.JSON
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = JSONKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with Agent using filters from query automatically
# -----------------------------------------------------------------------------------

# Enable agentic filtering
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

# Query for Jordan Mitchell's experience and skills with filters in query so that Agent can automatically pick them up
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills with jordan_mitchell as user id and document type cv",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/json/async_filtering.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.json import JSONKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.JSON
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = JSONKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Load all documents into the vector database
    asyncio.run(knowledge_base.aload(recreate=True))

    # Query for Alex Rivera's experience and skills
    asyncio.run(
        agent.aprint_response(
            "Tell me about Jordan Mitchell's experience and skills",
            knowledge_filters={"user_id": "jordan_mitchell"},
            markdown=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/json/filtering.py
================================================
"""
User-Level Knowledge Filtering Example

This cookbook demonstrates how to use knowledge filters to restrict knowledge base searches to specific users, document types, or any other metadata attributes.

Key concepts demonstrated:
1. Loading documents with user-specific metadata
2. Filtering knowledge base searches by user ID
3. Combining multiple filter criteria
4. Comparing results across different filter combinations

You can pass filters in the following ways:
1. If you pass on Agent only, we use that for all runs
2. If you pass on run/print_response only, we use that for that run
3. If you pass on both, we override with the filters passed on run/print_response for that run
"""

from agno.agent import Agent
from agno.knowledge.json import JSONKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.JSON
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = JSONKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)

# Query for Jordan Mitchell' experience and skills
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)

# # Option 2: Filters on the run/print_response
# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )

# # Query for Taylor Brooks as a candidate
# agent.print_response(
#     "Tell me about Taylor Brooks as a candidate",
#     knowledge_filters={"user_id": "taylor_brooks"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/json/filtering_on_load.py
================================================
from agno.agent import Agent
from agno.knowledge.json import JSONKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.JSON
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

# Initialize the JSONKnowledgeBase
knowledge_base = JSONKnowledgeBase(
    vector_db=vector_db,
    num_documents=5,
)

knowledge_base.load_document(
    path=downloaded_cv_paths[0],
    metadata={"user_id": "jordan_mitchell", "document_type": "cv", "year": 2025},
    recreate=True,  # Set to True only for the first run, then set to False
)

# Load second document with user_2 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[1],
    metadata={"user_id": "taylor_brooks", "document_type": "cv", "year": 2025},
)

# Load second document with user_3 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[2],
    metadata={"user_id": "morgan_lee", "document_type": "cv", "year": 2025},
)

# Load second document with user_4 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[3],
    metadata={"user_id": "casey_jordan", "document_type": "cv", "year": 2025},
)

# Load second document with user_5 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[4],
    metadata={"user_id": "alex_rivera", "document_type": "cv", "year": 2025},
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
# Uncomment the example you want to run

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)

# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )
# agent.print_response(
#     "Tell me about Jordan Mitchell's experience and skills",
#     knowledge_filters = {"user_id": "jordan_mitchell"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/pdf/agentic_filtering.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with Agent using filters from query automatically
# -----------------------------------------------------------------------------------

# Enable agentic filtering
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

# Query for Jordan Mitchell's experience and skills with filters in query so that Agent can automatically pick them up
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills with jordan_mitchell as user id and document type cv",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/pdf/async_filtering.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Load all documents into the vector database
    asyncio.run(knowledge_base.aload(recreate=True))

    # Query for Alex Rivera's experience and skills
    asyncio.run(
        agent.aprint_response(
            "Tell me about Jordan Mitchell's experience and skills",
            knowledge_filters={"user_id": "jordan_mitchell"},
            markdown=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/pdf/filtering.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/pdf/filtering_on_load.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

# Initialize the PDFKnowledgeBase
knowledge_base = PDFKnowledgeBase(
    vector_db=vector_db,
    num_documents=5,
)

knowledge_base.load_document(
    path=downloaded_cv_paths[0],
    metadata={"user_id": "jordan_mitchell", "document_type": "cv", "year": 2025},
    recreate=True,  # Set to True only for the first run, then set to False
)

# Load second document with user_2 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[1],
    metadata={"user_id": "taylor_brooks", "document_type": "cv", "year": 2025},
)

# Load second document with user_3 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[2],
    metadata={"user_id": "morgan_lee", "document_type": "cv", "year": 2025},
)

# Load second document with user_4 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[3],
    metadata={"user_id": "casey_jordan", "document_type": "cv", "year": 2025},
)

# Load second document with user_5 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[4],
    metadata={"user_id": "alex_rivera", "document_type": "cv", "year": 2025},
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
# Uncomment the example you want to run

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)

# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )
# agent.print_response(
#     "Tell me about Jordan Mitchell's experience and skills",
#     knowledge_filters = {"user_id": "jordan_mitchell"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/pdf_url/agentic_filtering.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFUrlKnowledgeBase(
    urls=[
        {
            "url": "https://agno-public.s3.amazonaws.com/recipes/thai_recipes_short.pdf",
            "metadata": {
                "cuisine": "Thai",
                "source": "Thai Cookbook",
                "region": "Southeast Asia",
            },
        },
        {
            "url": "https://agno-public.s3.amazonaws.com/recipes/cape_recipes_short_2.pdf",
            "metadata": {
                "cuisine": "Cape",
                "source": "Cape Cookbook",
                "region": "South Africa",
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with Agent using filters from query automatically
# -----------------------------------------------------------------------------------

# Enable agentic filtering
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

# Query for Jordan Mitchell's experience and skills with filters in query so that Agent can automatically pick them up
agent.print_response(
    "How to make Pad Thai, refer from document with cuisine Thai and source Thai Cookbook",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/pdf_url/async_filtering.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = PDFUrlKnowledgeBase(
    urls=[
        {
            "url": "https://agno-public.s3.amazonaws.com/recipes/thai_recipes_short.pdf",
            "metadata": {
                "cuisine": "Thai",
                "source": "Thai Cookbook",
                "region": "Southeast Asia",
            },
        },
        {
            "url": "https://agno-public.s3.amazonaws.com/recipes/cape_recipes_short_2.pdf",
            "metadata": {
                "cuisine": "Cape",
                "source": "Cape Cookbook",
                "region": "South Africa",
            },
        },
    ],
    vector_db=vector_db,
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Load all documents into the vector database
    asyncio.run(knowledge_base.aload(recreate=True))

    # Query for Alex Rivera's experience and skills
    asyncio.run(
        agent.aprint_response(
            "Tell me how to make Pad Thai",
            knowledge_filters={"cuisine": "Thai"},
            markdown=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/pdf_url/filtering.py
================================================
"""
User-Level Knowledge Filtering Example with PDF URLs

This cookbook demonstrates how to use knowledge filters with PDF documents accessed via URLs,
showing how to restrict knowledge base searches to specific cuisines, sources, or any other metadata attributes.

Key concepts demonstrated:
1. Loading PDF documents from URLs with specific metadata
2. Filtering knowledge base searches by cuisine type
3. Combining multiple filter criteria
4. Comparing results across different filter combinations

You can pass filters in the following ways:
1. If you pass on Agent only, we use that for all runs
2. If you pass on run/print_response only, we use that for that run
3. If you pass on both, we override with the filters passed on run/print_response for that run
"""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with URLs and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include cuisine type, source, region, or any other attributes

knowledge_base = PDFUrlKnowledgeBase(
    urls=[
        {
            "url": "https://agno-public.s3.amazonaws.com/recipes/thai_recipes_short.pdf",
            "metadata": {
                "cuisine": "Thai",
                "source": "Thai Cookbook",
                "region": "Southeast Asia",
            },
        },
        {
            "url": "https://agno-public.s3.amazonaws.com/recipes/cape_recipes_short_2.pdf",
            "metadata": {
                "cuisine": "Cape",
                "source": "Cape Cookbook",
                "region": "South Africa",
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    # This will only return information from documents with Thai cuisine
    knowledge_filters={"cuisine": "Thai"},
)

# Query for Thai recipes
agent.print_response(
    "Tell me how to make Pad Thai",
    markdown=True,
)

# # Option 2: Filters on the run/print_response
# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )

# # Query for Cape Malay recipes
# agent.print_response(
#     "Tell me how to make Cape Malay Curry",
#     knowledge_filters={"cuisine": "Cape"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/pdf_url/filtering_on_load.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

# Initialize knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    vector_db=vector_db,
)

knowledge_base.load_document(
    url="https://agno-public.s3.amazonaws.com/recipes/thai_recipes_short.pdf",
    metadata={"cuisine": "Thai", "source": "Thai Cookbook"},
    recreate=False,  # only use at the first run, True/False
)

knowledge_base.load_document(
    url="https://agno-public.s3.amazonaws.com/recipes/cape_recipes_short_2.pdf",
    metadata={"cuisine": "Cape", "source": "Cape Cookbook"},
)


# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={
        "cuisine": "Thai"
    },  # This will only return information from documents associated with Thai cuisine
)
agent.print_response(
    "Tell me how to make Pad Thai",
    markdown=True,
)

# # # Option 2: Filters on the run/print_response
# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )
# agent.print_response(
#     "Tell me how to make Cape Malay Curry",
#     knowledge_filters={"cuisine": "Cape"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/text/agentic_filtering.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.text import TextKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.TXT
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = TextKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with Agent using filters from query automatically
# -----------------------------------------------------------------------------------

# Enable agentic filtering
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

# Query for Jordan Mitchell's experience and skills with filters in query so that Agent can automatically pick them up
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills with jordan_mitchell as user id and document type cv",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/filters/text/async_filtering.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.text import TextKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.TXT
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = TextKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Load all documents into the vector database
    asyncio.run(knowledge_base.aload(recreate=True))

    # Query for Alex Rivera's experience and skills
    asyncio.run(
        agent.aprint_response(
            "Tell me about Jordan Mitchell's experience and skills",
            knowledge_filters={"user_id": "jordan_mitchell"},
            markdown=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/text/filtering.py
================================================
"""
User-Level Knowledge Filtering Example

This cookbook demonstrates how to use knowledge filters to restrict knowledge base searches to specific users, document types, or any other metadata attributes.

Key concepts demonstrated:
1. Loading documents with user-specific metadata
2. Filtering knowledge base searches by user ID
3. Combining multiple filter criteria
4. Comparing results across different filter combinations

You can pass filters in the following ways:
1. If you pass on Agent only, we use that for all runs
2. If you pass on run/print_response only, we use that for that run
3. If you pass on both, we override with the filters passed on run/print_response for that run
"""

from agno.agent import Agent
from agno.knowledge.text import TextKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.TXT
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge_base = TextKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge_base.load(recreate=True)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)

# Query for Jordan Mitchell's experience and skills
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)

# # Option 2: Filters on the run/print_response
# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )

# # Query for Taylor Brooks as a candidate
# agent.print_response(
#     "Tell me about Taylor Brooks as a candidate",
#     knowledge_filters={"user_id": "taylor_brooks"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/filters/text/filtering_on_load.py
================================================
from agno.agent import Agent
from agno.knowledge.text import TextKnowledgeBase
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.TXT
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

# Initialize the TextKnowledgeBase
knowledge_base = TextKnowledgeBase(
    vector_db=vector_db,
    num_documents=5,
)

knowledge_base.load_document(
    path=downloaded_cv_paths[0],
    metadata={"user_id": "jordan_mitchell", "document_type": "cv", "year": 2025},
    recreate=True,  # Set to True only for the first run, then set to False
)

# Load second document with user_2 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[1],
    metadata={"user_id": "taylor_brooks", "document_type": "cv", "year": 2025},
)

# Load second document with user_3 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[2],
    metadata={"user_id": "morgan_lee", "document_type": "cv", "year": 2025},
)

# Load second document with user_4 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[3],
    metadata={"user_id": "casey_jordan", "document_type": "cv", "year": 2025},
)

# Load second document with user_5 metadata
knowledge_base.load_document(
    path=downloaded_cv_paths[4],
    metadata={"user_id": "alex_rivera", "document_type": "cv", "year": 2025},
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
# Uncomment the example you want to run

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)
agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    markdown=True,
)

# agent = Agent(
#     knowledge=knowledge_base,
#     search_knowledge=True,
# )
# agent.print_response(
#     "Tell me about Jordan Mitchell's experience and skills",
#     knowledge_filters = {"user_id": "jordan_mitchell"},
#     markdown=True,
# )



================================================
FILE: cookbook/agent_concepts/knowledge/readers/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/readers/firecrawl_reader.py
================================================
import os

from agno.document.reader.firecrawl_reader import FirecrawlReader

api_key = os.getenv("FIRECRAWL_API_KEY")

reader = FirecrawlReader(
    api_key=api_key,
    mode="scrape",
    chunk=True,
    # for crawling
    # params={
    #     'limit': 5,
    #     'scrapeOptions': {'formats': ['markdown']}
    # }
    # for scraping
    params={"formats": ["markdown"]},
)

try:
    print("Starting scrape...")
    documents = reader.read("https://github.com/agno-agi/agno")

    if documents:
        for doc in documents:
            print(doc.name)
            print(doc.content)
            print(f"Content length: {len(doc.content)}")
            print("-" * 80)
    else:
        print("No documents were returned")

except Exception as e:
    print(f"Error type: {type(e)}")
    print(f"Error occurred: {str(e)}")



================================================
FILE: cookbook/agent_concepts/knowledge/readers/json_reader.py
================================================
import json
from pathlib import Path

from agno.document.reader.json_reader import JSONReader

reader = JSONReader()

json_path = Path("tmp/test.json")
test_data = {"key": "value"}
json_path.write_text(json.dumps(test_data))

try:
    print("Starting read...")
    documents = reader.read(json_path)

    if documents:
        for doc in documents:
            print(doc.name)
            print(doc.content)
            print(f"Content length: {len(doc.content)}")
            print("-" * 80)
    else:
        print("No documents were returned")

except Exception as e:
    print(f"Error type: {type(e)}")
    print(f"Error occurred: {str(e)}")



================================================
FILE: cookbook/agent_concepts/knowledge/readers/url_reader.py
================================================
from agno.document.chunking.recursive import RecursiveChunking
from agno.document.reader.url_reader import URLReader

reader = URLReader(chunking_strategy=RecursiveChunking(chunk_size=1000))

try:
    print("Starting read...")
    documents = reader.read("https://docs.agno.com/llms-full.txt")

    if documents:
        for doc in documents:
            print(doc.name)
            print(doc.content)
            print(f"Content length: {len(doc.content)}")
            print("-" * 80)
    else:
        print("No documents were returned")

except Exception as e:
    print(f"Error type: {type(e)}")
    print(f"Error occurred: {str(e)}")



================================================
FILE: cookbook/agent_concepts/knowledge/readers/web_reader.py
================================================
from agno.document.reader.website_reader import WebsiteReader

reader = WebsiteReader(max_depth=3, max_links=10)

try:
    print("Starting read...")
    documents = reader.read("https://docs.agno.com/introduction")
    if documents:
        for doc in documents:
            print(doc.name)
            print(doc.content)
            print(f"Content length: {len(doc.content)}")
            print("-" * 80)
    else:
        print("No documents were returned")

except Exception as e:
    print(f"Error type: {type(e)}")
    print(f"Error occurred: {str(e)}")



================================================
FILE: cookbook/agent_concepts/knowledge/search_type/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/search_type/hybrid_search.py
================================================
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Load knowledge base using hybrid search
hybrid_db = PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.hybrid)
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=hybrid_db,
)

# Load data (only required for first time)
knowledge_base.load(recreate=True, upsert=True)

# Run a hybrid search query
results = hybrid_db.search("chicken coconut soup", limit=5)
print("Hybrid Search Results:", results)



================================================
FILE: cookbook/agent_concepts/knowledge/search_type/keyword_search.py
================================================
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Load knowledge base using keyword search
keyword_db = PgVector(
    table_name="recipes", db_url=db_url, search_type=SearchType.keyword
)
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=keyword_db,
)

# Load data (only required for first time)
knowledge_base.load(recreate=True, upsert=True)

# Run a keyword-based query
results = keyword_db.search("chicken coconut soup", limit=5)
print("Keyword Search Results:", results)



================================================
FILE: cookbook/agent_concepts/knowledge/search_type/vector_search.py
================================================
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Load knowledge base using vector search
vector_db = PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.vector)
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Load data (only required for first time)
knowledge_base.load(recreate=True, upsert=True)

# Run a vector-based query
results = vector_db.search("chicken coconut soup", limit=5)
print("Vector Search Results:", results)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/README.md
================================================
## Vector DBs
Vector databases enable us to store information as embeddings and search for “results similar” to our input query using cosine similarity or full text search.

## Setup

### Create a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### Install libraries

```shell
pip install -U qdrant-client pypdf openai agno
```

## Test your VectorDB

### Cassandra DB

```shell
python cookbook/vector_dbs/cassandra_db.py
```


### ChromaDB

```shell
python cookbook/vector_dbs/chroma_db.py
```

### Clickhouse

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

- Run using a helper script

```shell
./cookbook/run_clickhouse.sh
```

- OR run using the docker run command

```shell
docker run -d \
  -e CLICKHOUSE_DB=ai \
  -e CLICKHOUSE_USER=ai \
  -e CLICKHOUSE_PASSWORD=ai \
  -e CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1 \
  -v clickhouse_data:/var/lib/clickhouse/ \
  -v clickhouse_log:/var/log/clickhouse-server/ \
  -p 8123:8123 \
  -p 9000:9000 \
  --ulimit nofile=262144:262144 \
  --name clickhouse-server \
  clickhouse/clickhouse-server
```

#### Run the agent

```shell
python cookbook/vector_dbs/clickhouse.py
```

### LanceDB

```shell
python cookbook/vector_dbs/lance_db.py
```

### PgVector

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

- Run using a helper script

```shell
./cookbook/run_pgvector.sh
```

- OR run using the docker run command

```shell
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16
```

```shell
python cookbook/vector_dbs/pg_vector.py
```

### Mem0

```shell
python cookbook/vector_dbs/mem0.py
```

### Milvus

```shell
python cookbook/vector_dbs/milvus.py
```

### Pinecone DB

```shell
python cookbook/vector_dbs/pinecone_db.py
```

### Singlestore

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

#### Run the setup script
```shell
./cookbook/scripts/run_singlestore.sh
```

#### Create the database

- Visit http://localhost:8080 and login with username: `root` and password: `admin`
- Create the database with your choice of name. Default setup script requires AGNO as database name. `CREATE DATABASE your_database_name;`

#### Add credentials

- For SingleStore

```shell
export SINGLESTORE_HOST="localhost"
export SINGLESTORE_PORT="3306"
export SINGLESTORE_USERNAME="root"
export SINGLESTORE_PASSWORD="admin"
export SINGLESTORE_DATABASE="your_database_name"
export SINGLESTORE_SSL_CA=".certs/singlestore_bundle.pem"
```

- Set your OPENAI_API_KEY

```shell
export OPENAI_API_KEY="sk-..."
```

#### Run Agent

```shell
python cookbook/vector_dbs/singlestore.py
```


### Qdrant

```shell
docker run -p 6333:6333 -p 6334:6334 -v $(pwd)/qdrant_storage:/qdrant/storage:z qdrant/qdrant
```

```shell
python cookbook/vector_dbs/qdrant_db.py
```

### Weaviate

```shell
./cookbook/scripts/run_weviate.sh
```

```shell
python cookbook/vector_dbs/weaviate_db.py
```



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/singlestore.py
================================================
"""
# Run the setup script
```shell
./cookbook/scripts/run_singlestore.sh
```

# Create the database

- Visit http://localhost:8080 and login with `root` and `admin`
- Create the database with your choice of name. Default setup script requires AGNO as database name. `CREATE DATABASE your_database_name;`
"""

from os import getenv

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.singlestore import SingleStore
from sqlalchemy.engine import create_engine

USERNAME = getenv("SINGLESTORE_USERNAME")
PASSWORD = getenv("SINGLESTORE_PASSWORD")
HOST = getenv("SINGLESTORE_HOST")
PORT = getenv("SINGLESTORE_PORT")
DATABASE = getenv("SINGLESTORE_DATABASE")
SSL_CERT = getenv("SINGLESTORE_SSL_CERT", None)

db_url = (
    f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
)
if SSL_CERT:
    db_url += f"&ssl_ca={SSL_CERT}&ssl_verify_cert=true"

db_engine = create_engine(db_url)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=SingleStore(
        collection="recipes",
        db_engine=db_engine,
        schema=DATABASE,
    ),
)

# Comment out after first run
knowledge_base.load(recreate=False)

agent = Agent(
    knowledge=knowledge_base,
    # Show tool calls in the response
    show_tool_calls=True,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

agent.print_response("How do I make pad thai?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/upstash_db.py
================================================
# install upstash-vector - `uv pip install upstash-vector`
# Add OPENAI_API_KEY to your environment variables for the agent response

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.upstashdb.upstashdb import UpstashVectorDb

# How to connect to an Upstash Vector index
# - Create a new index in Upstash Console with the correct dimension
# - Fetch the URL and token from Upstash Console
# - Replace the values below or use environment variables

# Initialize Upstash DB
vector_db = UpstashVectorDb(
    url="UPSTASH_VECTOR_REST_URL",
    token="UPSTASH_VECTOR_REST_TOKEN",
)

# Create a new PDFUrlKnowledgeBase
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Load the knowledge base - after first run, comment out
knowledge_base.load(recreate=False, upsert=True)

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("What are some tips for cooking glass noodles?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/cassandra_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/cassandra_db/async_cassandra_db.py
================================================
import asyncio

from agno.agent import Agent
from agno.embedder.mistral import MistralEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.mistral import MistralChat
from agno.vectordb.cassandra import Cassandra

try:
    from cassandra.cluster import Cluster  # type: ignore
except (ImportError, ModuleNotFoundError):
    raise ImportError(
        "Could not import cassandra-driver python package.Please install it with pip install cassandra-driver."
    )

cluster = Cluster()

session = cluster.connect()
session.execute(
    """
    CREATE KEYSPACE IF NOT EXISTS testkeyspace
    WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }
    """
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=Cassandra(
        table_name="recipes",
        keyspace="testkeyspace",
        session=session,
        embedder=MistralEmbedder(),
    ),
)

agent = Agent(
    model=MistralChat(),
    knowledge=knowledge_base,
    show_tool_calls=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "What are the health benefits of Khao Niew Dam Piek Maphrao Awn?",
            markdown=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/cassandra_db/cassandra_db.py
================================================
from agno.agent import Agent
from agno.embedder.mistral import MistralEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.mistral import MistralChat
from agno.vectordb.cassandra import Cassandra

try:
    from cassandra.cluster import Cluster  # type: ignore
except (ImportError, ModuleNotFoundError):
    raise ImportError(
        "Could not import cassandra-driver python package.Please install it with pip install cassandra-driver."
    )

cluster = Cluster()

session = cluster.connect()
session.execute(
    """
    CREATE KEYSPACE IF NOT EXISTS testkeyspace
    WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }
    """
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=Cassandra(
        table_name="recipes",
        keyspace="testkeyspace",
        session=session,
        embedder=MistralEmbedder(),
    ),
)


knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=MistralChat(),
    knowledge=knowledge_base,
    show_tool_calls=True,
)

agent.print_response(
    "What are the health benefits of Khao Niew Dam Piek Maphrao Awn?",
    markdown=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/chroma_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/chroma_db/async_chroma_db.py
================================================
# install chromadb - `pip install chromadb`

import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.chroma import ChromaDb

# Initialize ChromaDB
vector_db = ChromaDb(collection="recipes", path="tmp/chromadb", persistent_client=True)

# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/chroma_db/chroma_db.py
================================================
# install chromadb - `pip install chromadb`

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.chroma import ChromaDb

# Initialize ChromaDB
vector_db = ChromaDb(collection="recipes", path="tmp/chromadb", persistent_client=True)

# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

knowledge_base.load(recreate=False)  # Comment out after first run

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("Show me how to make Tom Kha Gai", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/clickhouse_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/clickhouse_db/async_clickhouse.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.vectordb.clickhouse import Clickhouse

agent = Agent(
    storage=SqliteAgentStorage(table_name="recipe_agent"),
    knowledge=PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=Clickhouse(
            table_name="recipe_documents",
            host="localhost",
            port=8123,
            username="ai",
            password="ai",
        ),
    ),
    # Show tool calls in the response
    show_tool_calls=True,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(agent.knowledge.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/clickhouse_db/clickhouse.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.vectordb.clickhouse import Clickhouse

agent = Agent(
    storage=SqliteAgentStorage(table_name="recipe_agent"),
    knowledge=PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=Clickhouse(
            table_name="recipe_documents",
            host="localhost",
            port=8123,
            username="ai",
            password="ai",
        ),
    ),
    # Show tool calls in the response
    show_tool_calls=True,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)
# Comment out after first run
agent.knowledge.load(recreate=False)  # type: ignore

agent.print_response("How do I make pad thai?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/couchbase_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/couchbase_db/async_couchbase_db.py
================================================
"""
Couchbase Vector DB Example
==========================

Setup Couchbase Cluster (Local via Docker):
-------------------------------------------
1. Run Couchbase locally:

   docker run -d --name couchbase-server \
     -p 8091-8096:8091-8096 \
     -p 11210:11210 \
     -e COUCHBASE_ADMINISTRATOR_USERNAME=Administrator \
     -e COUCHBASE_ADMINISTRATOR_PASSWORD=password \
     couchbase:latest

2. Access the Couchbase UI at: http://localhost:8091
   (Login with the username and password above)

3. Create a new cluster. You can select "Finish with defaults".

4. Create a bucket named 'recipe_bucket', a scope 'recipe_scope', and a collection 'recipes'.

Managed Couchbase (Capella):
----------------------------
- For a managed cluster, use Couchbase Capella: https://cloud.couchbase.com/
- Follow Capella's UI to create a database, bucket, scope, and collection as above.

Environment Variables (export before running):
----------------------------------------------
Create a shell script (e.g., set_couchbase_env.sh):

    export COUCHBASE_USER="Administrator"
    export COUCHBASE_PASSWORD="password"
    export COUCHBASE_CONNECTION_STRING="couchbase://localhost"
    export OPENAI_API_KEY="<your-openai-api-key>"

# For Capella, set COUCHBASE_CONNECTION_STRING to the Capella connection string.

Install couchbase-sdk:
----------------------
    pip install couchbase
"""

import asyncio
import os
import time

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.couchbase import CouchbaseSearch
from couchbase.auth import PasswordAuthenticator
from couchbase.management.search import SearchIndex
from couchbase.options import ClusterOptions, KnownConfigProfiles

# Couchbase connection settings
username = os.getenv("COUCHBASE_USER")  # Replace with your username
password = os.getenv("COUCHBASE_PASSWORD")  # Replace with your password
connection_string = os.getenv("COUCHBASE_CONNECTION_STRING")

# Create cluster options with authentication
auth = PasswordAuthenticator(username, password)
cluster_options = ClusterOptions(auth)
cluster_options.apply_profile(KnownConfigProfiles.WanDevelopment)

# Define the vector search index
search_index = SearchIndex(
    name="vector_search",
    source_type="gocbcore",
    idx_type="fulltext-index",
    source_name="recipe_bucket",
    plan_params={"index_partitions": 1, "num_replicas": 0},
    params={
        "doc_config": {
            "docid_prefix_delim": "",
            "docid_regexp": "",
            "mode": "scope.collection.type_field",
            "type_field": "type",
        },
        "mapping": {
            "default_analyzer": "standard",
            "default_datetime_parser": "dateTimeOptional",
            "index_dynamic": True,
            "store_dynamic": True,
            "default_mapping": {"dynamic": True, "enabled": False},
            "types": {
                "recipe_scope.recipes": {
                    "dynamic": False,
                    "enabled": True,
                    "properties": {
                        "content": {
                            "enabled": True,
                            "fields": [
                                {
                                    "docvalues": True,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "content",
                                    "store": True,
                                    "type": "text",
                                }
                            ],
                        },
                        "embedding": {
                            "enabled": True,
                            "dynamic": False,
                            "fields": [
                                {
                                    "vector_index_optimized_for": "recall",
                                    "docvalues": True,
                                    "dims": 3072,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "embedding",
                                    "similarity": "dot_product",
                                    "store": True,
                                    "type": "vector",
                                }
                            ],
                        },
                        "meta": {
                            "dynamic": True,
                            "enabled": True,
                            "properties": {
                                "name": {
                                    "enabled": True,
                                    "fields": [
                                        {
                                            "docvalues": True,
                                            "include_in_all": False,
                                            "include_term_vectors": False,
                                            "index": True,
                                            "name": "name",
                                            "store": True,
                                            "analyzer": "keyword",
                                            "type": "text",
                                        }
                                    ],
                                }
                            },
                        },
                    },
                }
            },
        },
    },
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=CouchbaseSearch(
        bucket_name="recipe_bucket",
        scope_name="recipe_scope",
        collection_name="recipes",
        couchbase_connection_string=connection_string,
        cluster_options=cluster_options,
        search_index=search_index,
        embedder=OpenAIEmbedder(
            id="text-embedding-3-large",
            dimensions=3072,
            api_key=os.getenv("OPENAI_API_KEY"),
        ),
        wait_until_index_ready=60,
        overwrite=True,
    ),
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)


async def run_agent():
    await knowledge_base.aload(recreate=True)
    time.sleep(5)  # wait for the vector index to be sync with kv
    await agent.aprint_response("How to make Thai curry?", markdown=True)


if __name__ == "__main__":
    # Comment out after the first run
    asyncio.run(run_agent())



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/couchbase_db/couchbase_db.py
================================================
"""
Couchbase Vector DB Example
==========================

Setup Couchbase Cluster (Local via Docker):
-------------------------------------------
1. Run Couchbase locally:

   docker run -d --name couchbase-server \
     -p 8091-8096:8091-8096 \
     -p 11210:11210 \
     -e COUCHBASE_ADMINISTRATOR_USERNAME=Administrator \
     -e COUCHBASE_ADMINISTRATOR_PASSWORD=password \
     couchbase:latest

2. Access the Couchbase UI at: http://localhost:8091
   (Login with the username and password above)

3. Create a new cluster. You can select "Finish with defaults".

4. Create a bucket named 'recipe_bucket', a scope 'recipe_scope', and a collection 'recipes'.

Managed Couchbase (Capella):
----------------------------
- For a managed cluster, use Couchbase Capella: https://cloud.couchbase.com/
- Follow Capella's UI to create a database, bucket, scope, and collection as above.

Environment Variables (export before running):
----------------------------------------------
Create a shell script (e.g., set_couchbase_env.sh):

    export COUCHBASE_USER="Administrator"
    export COUCHBASE_PASSWORD="password"
    export COUCHBASE_CONNECTION_STRING="couchbase://localhost"
    export OPENAI_API_KEY="<your-openai-api-key>"

# For Capella, set COUCHBASE_CONNECTION_STRING to the Capella connection string.

Install couchbase-sdk:
----------------------
    pip install couchbase
"""

import os
import time

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.couchbase import CouchbaseSearch
from couchbase.auth import PasswordAuthenticator
from couchbase.management.search import SearchIndex
from couchbase.options import ClusterOptions, KnownConfigProfiles

# Couchbase connection settings
username = os.getenv("COUCHBASE_USER")  # Replace with your username
password = os.getenv("COUCHBASE_PASSWORD")  # Replace with your password
connection_string = os.getenv("COUCHBASE_CONNECTION_STRING")

# Create cluster options with authentication
auth = PasswordAuthenticator(username, password)
cluster_options = ClusterOptions(auth)
cluster_options.apply_profile(KnownConfigProfiles.WanDevelopment)

# Define the vector search index
search_index = SearchIndex(
    name="vector_search",
    source_type="gocbcore",
    idx_type="fulltext-index",
    source_name="recipe_bucket",
    plan_params={"index_partitions": 1, "num_replicas": 0},
    params={
        "doc_config": {
            "docid_prefix_delim": "",
            "docid_regexp": "",
            "mode": "scope.collection.type_field",
            "type_field": "type",
        },
        "mapping": {
            "default_analyzer": "standard",
            "default_datetime_parser": "dateTimeOptional",
            "index_dynamic": True,
            "store_dynamic": True,
            "default_mapping": {"dynamic": True, "enabled": False},
            "types": {
                "recipe_scope.recipes": {
                    "dynamic": False,
                    "enabled": True,
                    "properties": {
                        "content": {
                            "enabled": True,
                            "fields": [
                                {
                                    "docvalues": True,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "content",
                                    "store": True,
                                    "type": "text",
                                }
                            ],
                        },
                        "embedding": {
                            "enabled": True,
                            "dynamic": False,
                            "fields": [
                                {
                                    "vector_index_optimized_for": "recall",
                                    "docvalues": True,
                                    "dims": 3072,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "embedding",
                                    "similarity": "dot_product",
                                    "store": True,
                                    "type": "vector",
                                }
                            ],
                        },
                        "meta": {
                            "dynamic": True,
                            "enabled": True,
                            "properties": {
                                "name": {
                                    "enabled": True,
                                    "fields": [
                                        {
                                            "docvalues": True,
                                            "include_in_all": False,
                                            "include_term_vectors": False,
                                            "index": True,
                                            "name": "name",
                                            "store": True,
                                            "analyzer": "keyword",
                                            "type": "text",
                                        }
                                    ],
                                }
                            },
                        },
                    },
                }
            },
        },
    },
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=CouchbaseSearch(
        bucket_name="recipe_bucket",
        scope_name="recipe_scope",
        collection_name="recipes",
        couchbase_connection_string=connection_string,
        cluster_options=cluster_options,
        search_index=search_index,
        embedder=OpenAIEmbedder(
            id="text-embedding-3-large",
            dimensions=3072,
            api_key=os.getenv("OPENAI_API_KEY"),
        ),
        wait_until_index_ready=60,
        overwrite=True,
    ),
)

# Load the knowledge base
knowledge_base.load(recreate=True)

time.sleep(20)  # wait for the vector index to be sync with kv
# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/lance_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/lance_db/async_lance_db.py
================================================
# install lancedb - `pip install lancedb`

import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)

if __name__ == "__main__":
    asyncio.run(knowledge_base.aload(recreate=False))  # Comment out after first run

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/lance_db/lance_db.py
================================================
# install lancedb - `pip install lancedb`

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

knowledge_base.load(recreate=False)  # Comment out after first run

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("How to make Tom Kha Gai", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/lance_db/lance_db_hybrid_search.py
================================================
import typer
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb
from agno.vectordb.search import SearchType
from rich.prompt import Prompt

vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",
    search_type=SearchType.hybrid,
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)


def lancedb_agent(user: str = "user"):
    agent = Agent(
        user_id=user,
        knowledge=knowledge_base,
        search_knowledge=True,
        show_tool_calls=True,
    )

    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        agent.print_response(message)


if __name__ == "__main__":
    # Comment out after first run
    knowledge_base.load(recreate=False)

    typer.run(lancedb_agent)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/lance_db/remote_lance_db.py
================================================
"""
This example shows how to use a remote LanceDB database.

- Set URI obtained from https://cloud.lancedb.com/
- Export `LANCEDB_API_KEY` OR set `api_key` in the `LanceDb` constructor
"""

# install lancedb - `pip install lancedb`

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb

# Initialize Remote LanceDB
vector_db = LanceDb(
    table_name="recipes",
    uri="<URI>",
    # api_key="<API_KEY>",
)

# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

knowledge_base.load(recreate=False)  # Comment out after first run

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("How to make Tom Kha Gai", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/milvus_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/milvus_db/async_milvus_db.py
================================================
# install pymilvus - `pip install pymilvus`

import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.milvus import Milvus

# Initialize Milvus

# Set the uri and token for your Milvus server.
# - If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
# - If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use "<your_username>:<your_password>" as the token, otherwise don't set the token.
# - If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.
vector_db = Milvus(
    collection="recipes",
    uri="tmp/milvus.db",
)
# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/milvus_db/async_milvus_db_hybrid_search.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.milvus import Milvus, SearchType

# Initialize Milvus

# Set the uri and token for your Milvus server.
# - If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
# - If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use "<your_username>:<your_password>" as the token, otherwise don't set the token.
# - If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.
vector_db = Milvus(
    collection="recipes", uri="tmp/milvus.db", search_type=SearchType.hybrid
)
# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)


if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=True))  # Comment out after first run

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/milvus_db/milvus_db.py
================================================
# install pymilvus - `pip install pymilvus`

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.milvus import Milvus

# Initialize Milvus

# Set the uri and token for your Milvus server.
# - If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
# - If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use "<your_username>:<your_password>" as the token, otherwise don't set the token.
# - If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.
vector_db = Milvus(
    collection="recipes",
    uri="tmp/milvus.db",
)
# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

knowledge_base.load(recreate=False)  # Comment out after first run

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("How to make Tom Kha Gai", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/milvus_db/milvus_db_hybrid_search.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.milvus import Milvus, SearchType

# Initialize Milvus

# Set the uri and token for your Milvus server.
# - If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
# - If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use "<your_username>:<your_password>" as the token, otherwise don't set the token.
# - If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.
vector_db = Milvus(
    collection="recipes", uri="tmp/milvus.db", search_type=SearchType.hybrid
)
# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

knowledge_base.load(recreate=True)  # Comment out after first run

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("How to make Tom Kha Gai", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/mongo_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/mongo_db/async_mongo_db.py
================================================
"""
1. Create a MongoDB Atlas Account:
   - Go to https://www.mongodb.com/cloud/atlas/register
   - Sign up for a free account

2. Create a New Cluster:
   - Click "Build a Database"
   - Choose the FREE tier (M0)
   - Select your preferred cloud provider and region
   - Click "Create Cluster"

3. Set Up Database Access:
   - Follow the instructions in the MongoDB Atlas UI
   - Create a username and password
   - Click "Add New Database User"

5. Get Connection String:
   - Select "Drivers" as connection method
   - Select "Python" as driver
   - Copy the connection string

7. Test Connection:
   - Use the connection string in your code
   - Ensure pymongo is installed: pip install "pymongo[srv]"
   - Test with a simple query to verify connectivity

Alternatively to test locally, you can run a docker container

docker run -p 27017:27017 -d --name mongodb-container --rm -v ./tmp/mongo-data:/data/db mongodb/mongodb-atlas-local:8.0.3
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.mongodb import MongoDb

# MongoDB Atlas connection string
"""
Example connection strings:
"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
"mongodb://localhost:27017/agno?authSource=admin"
"""
mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=MongoDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
    ),
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)

if __name__ == "__main__":
    # Comment out after the first run
    asyncio.run(knowledge_base.aload(recreate=False))

    asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/mongo_db/cosmos_mongodb_vcore.py
================================================
import urllib.parse

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.mongodb import MongoDb

"""
Example connection strings:
"mongodb+srv://<username>:<encoded_password>@cluster0.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000"
"""

mdb_connection_string = f"mongodb+srv://<username>:<encoded_password>@cluster0.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=MongoDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
        search_index_name="recipes",
        cosmos_compatibility=True,
    ),
)

# Comment out after first run
knowledge_base.load(recreate=True)

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/mongo_db/mongo_db.py
================================================
"""
1. Create a MongoDB Atlas Account:
   - Go to https://www.mongodb.com/cloud/atlas/register
   - Sign up for a free account

2. Create a New Cluster:
   - Click "Build a Database"
   - Choose the FREE tier (M0)
   - Select your preferred cloud provider and region
   - Click "Create Cluster"

3. Set Up Database Access:
   - Follow the instructions in the MongoDB Atlas UI
   - Create a username and password
   - Click "Add New Database User"

5. Get Connection String:
   - Select "Drivers" as connection method
   - Select "Python" as driver
   - Copy the connection string

7. Test Connection:
   - Use the connection string in your code
   - Ensure pymongo is installed: pip install "pymongo[srv]"
   - Test with a simple query to verify connectivity

Alternatively to test locally, you can run a docker container

docker run -p 27017:27017 -d --name mongodb-container --rm -v ./tmp/mongo-data:/data/db mongodb/mongodb-atlas-local:8.0.3
"""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.mongodb import MongoDb

# MongoDB Atlas connection string
"""
Example connection strings:
"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
"mongodb://localhost:27017/agno?authSource=admin"
"""
mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=MongoDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
        search_index_name="recipes",
    ),
)

# Comment out after first run
knowledge_base.load(recreate=False)

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)

agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/mongo_db/mongo_db_hybrid_search.py
================================================
import typer
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.mongodb import MongoDb
from agno.vectordb.search import SearchType
from rich.prompt import Prompt

# MongoDB Atlas connection string
"""
Example connection strings:
"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
"mongodb://localhost:27017/agno?authSource=admin"
"""
mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

vector_db = MongoDb(
    collection_name="recipes",
    db_url=mdb_connection_string,
    search_index_name="recipes",
    search_type=SearchType.hybrid,
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)


def mongodb_agent(user: str = "user"):
    agent = Agent(
        user_id=user,
        knowledge=knowledge_base,
        search_knowledge=True,
        show_tool_calls=True,
    )

    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        agent.print_response(message)


if __name__ == "__main__":
    # Comment out after first run
    knowledge_base.load(recreate=True)

    typer.run(mongodb_agent)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/pgvector_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/pgvector_db/async_pg_vector.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

vector_db = PgVector(table_name="recipes", db_url=db_url)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

agent = Agent(knowledge=knowledge_base, show_tool_calls=True)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/pgvector_db/pg_vector.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

vector_db = PgVector(table_name="recipes", db_url=db_url)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/pgvector_db/pgvector_hybrid_search.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes", db_url=db_url, search_type=SearchType.hybrid
    ),
)
# Load the knowledge base: Comment out after first run
knowledge_base.load(recreate=False)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    search_knowledge=True,
    read_chat_history=True,
    show_tool_calls=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
agent.print_response("What was my last question?", stream=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/pinecone_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/pinecone_db/async_pinecone_db.py
================================================
import asyncio
from os import getenv

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pineconedb import PineconeDb

api_key = getenv("PINECONE_API_KEY")
index_name = "thai-recipe-index"

vector_db = PineconeDb(
    name=index_name,
    dimension=1536,
    metric="cosine",
    spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
    api_key=api_key,
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

agent = Agent(
    knowledge=knowledge_base,
    # Show tool calls in the response
    show_tool_calls=True,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False, upsert=True))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/pinecone_db/pinecone_db.py
================================================
from os import getenv

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pineconedb import PineconeDb

api_key = getenv("PINECONE_API_KEY")
index_name = "thai-recipe-index"

vector_db = PineconeDb(
    name=index_name,
    dimension=1536,
    metric="cosine",
    spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
    api_key=api_key,
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Comment out after first run
knowledge_base.load(recreate=False, upsert=True)

agent = Agent(
    knowledge=knowledge_base,
    # Show tool calls in the response
    show_tool_calls=True,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

agent.print_response("How do I make pad thai?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/qdrant_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/qdrant_db/async_qdrant_db.py
================================================
import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "thai-recipes"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

agent = Agent(knowledge=knowledge_base, show_tool_calls=True)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/qdrant_db/qdrant_db.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "thai-recipes"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

knowledge_base.load(recreate=False)  # Comment out after first run

# Create and use the agent
agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/qdrant_db/qdrant_db_hybrid_search.py
================================================
import typer
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.qdrant import Qdrant
from agno.vectordb.search import SearchType
from rich.prompt import Prompt

COLLECTION_NAME = "thai-recipes"

vector_db = Qdrant(
    collection=COLLECTION_NAME,
    url="http://localhost:6333",
    search_type=SearchType.hybrid,
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)


def qdrantdb_agent(user: str = "user"):
    agent = Agent(
        user_id=user,
        knowledge=knowledge_base,
        search_knowledge=True,
        show_tool_calls=True,
    )

    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        agent.print_response(message)


if __name__ == "__main__":
    # Comment out after first run
    knowledge_base.load(recreate=True)

    typer.run(qdrantdb_agent)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/surrealdb/async_surreal_db.py
================================================
# Run SurrealDB in a container before running this script
# docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root

import asyncio

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import AsyncSurreal

# SurrealDB connection parameters
SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

# Create a client
client = AsyncSurreal(url=SURREALDB_URL)

surrealdb = SurrealDb(
    async_client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
)


async def async_demo():
    """Demonstrate asynchronous usage of SurrealDb"""

    await client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
    await client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=surrealdb,
        embedder=OpenAIEmbedder(),
    )

    await knowledge_base.aload(recreate=True)

    agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
    await agent.aprint_response(
        "What are the 3 categories of Thai SELECT is given to restaurants overseas?",
        markdown=True,
    )


if __name__ == "__main__":
    # Run asynchronous demo
    print("\nRunning asynchronous demo...")
    asyncio.run(async_demo())



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/surrealdb/surreal_db.py
================================================
# Run SurrealDB in a container before running this script
# docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import Surreal

# SurrealDB connection parameters
SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

# Create a client
client = Surreal(url=SURREALDB_URL)
client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

surrealdb = SurrealDb(
    client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
)


def sync_demo():
    """Demonstrate synchronous usage of SurrealDb"""
    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=surrealdb,
        embedder=OpenAIEmbedder(),
    )

    # Load data synchronously
    knowledge_base.load(recreate=True)

    # Create agent and query synchronously
    agent = Agent(knowledge=knowledge_base, show_tool_calls=True)
    agent.print_response(
        "What are the 3 categories of Thai SELECT is given to restaurants overseas?",
        markdown=True,
    )


if __name__ == "__main__":
    # Run synchronous demo
    print("Running synchronous demo...")
    sync_demo()



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/weaviate_db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/weaviate_db/async_weaviate_db.py
================================================
"""
This example demonstrates using Weaviate as a vector database for semantic search.

Installation:
    pip install weaviate-client

You can use either Weaviate Cloud or a local instance.

Weaviate Cloud Setup:
1. Create account at https://console.weaviate.cloud/
2. Create a cluster and copy the "REST endpoint" and "Admin" API Key. Then set environment variables:
    export WCD_URL="your-cluster-url" 
    export WCD_API_KEY="your-api-key"

Local Development Setup:
1. Install Docker from https://docs.docker.com/get-docker/
2. Run Weaviate locally:
    docker run -d \
        -p 8080:8080 \
        -p 50051:50051 \
        --name weaviate \
        cr.weaviate.io/semitechnologies/weaviate:1.28.4
   or use the script `cookbook/scripts/run_weviate.sh` to start a local instance.
3. Remember to set `local=True` on the Weaviate instantiation.
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

vector_db = Weaviate(
    collection="recipes_async",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=True,  # Set to False if using Weaviate Cloud and True if using local instance
)
# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    show_tool_calls=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=False))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/weaviate_db/weaviate_db.py
================================================
"""
This example demonstrates using Weaviate as a vector database for semantic search.

Installation:
    pip install weaviate-client

You can use either Weaviate Cloud or a local instance.

Weaviate Cloud Setup:
1. Create account at https://console.weaviate.cloud/
2. Create a cluster and copy the "REST endpoint" and "Admin" API Key. Then set environment variables:
    export WCD_URL="your-cluster-url" 
    export WCD_API_KEY="your-api-key"

Local Development Setup:
1. Install Docker from https://docs.docker.com/get-docker/
2. Run Weaviate locally:
    docker run -d \
        -p 8080:8080 \
        -p 50051:50051 \
        --name weaviate \
        cr.weaviate.io/semitechnologies/weaviate:1.28.4
   or use the script `cookbook/scripts/run_weviate.sh` to start a local instance.
3. Remember to set `local=True` on the Weaviate instantiation.
"""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

vector_db = Weaviate(
    collection="recipes",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=True,  # Set to False if using Weaviate Cloud and True if using local instance
)
# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)
knowledge_base.load(recreate=False)  # Comment out after first run

# Create and use the agent
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/weaviate_db/weaviate_db_hybrid_search.py
================================================
import typer
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate
from rich.prompt import Prompt

vector_db = Weaviate(
    collection="recipes",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=False,  # Set to True if using Weaviate Cloud and False if using local instance
    # Adjust alpha for hybrid search (0.0-1.0, default is 0.5), where 0 is pure keyword search, 1 is pure vector search
    hybrid_search_alpha=0.6,
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)


def weaviate_agent(user: str = "user"):
    agent = Agent(
        user_id=user,
        knowledge=knowledge_base,
        search_knowledge=True,
        show_tool_calls=True,
    )

    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        agent.print_response(message)


if __name__ == "__main__":
    # Comment out after first run
    knowledge_base.load(recreate=True)

    typer.run(weaviate_agent)



================================================
FILE: cookbook/agent_concepts/knowledge/vector_dbs/weaviate_db/weaviate_db_upsert.py
================================================
"""
This example demonstrates using Weaviate as a vector database.

Installation:
    pip install weaviate-client

You can use either Weaviate Cloud or a local instance.

Weaviate Cloud Setup:
1. Create account at https://console.weaviate.cloud/
2. Create a cluster and copy the "REST endpoint" and "Admin" API Key. Then set environment variables:
    export WCD_URL="your-cluster-url" 
    export WCD_API_KEY="your-api-key"

Local Development Setup:
1. Install Docker from https://docs.docker.com/get-docker/
2. Run Weaviate locally:
    docker run -d \
        -p 8080:8080 \
        -p 50051:50051 \
        --name weaviate \
        cr.weaviate.io/semitechnologies/weaviate:1.28.4
   or use the script `cookbook/scripts/run_weviate.sh` to start a local instance.
3. Remember to set `local=True` on the Weaviate instantiation.
"""

from agno.document import Document
from agno.embedder.sentence_transformer import SentenceTransformerEmbedder
from agno.knowledge.document import DocumentKnowledgeBase
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.utils.log import set_log_level_to_debug
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

embedder = SentenceTransformerEmbedder()

vector_db = Weaviate(
    collection="recipes",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    embedder=embedder,
    local=True,  # Set to False if using Weaviate Cloud and True if using local instance
)
# Create knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

vector_db.drop()
set_log_level_to_debug()

knowledge_base.load(recreate=False, upsert=True)

print(
    "Knowledge base loaded with PDF content. Loading the same data again will not recreate it."
)
knowledge_base.load(recreate=False, upsert=True)

print("First example finished. Now dropping the knowledge base.")
vector_db.drop()

doc1 = Document(content="my first content", name="doc1")
doc1_modified = Document(content="my first content corrected", name="doc1")
doc2 = Document(content="my second content", name="doc2")

knowledge_base = DocumentKnowledgeBase(
    documents=[doc1, doc2],
    vector_db=vector_db,
)
knowledge_base_changed = DocumentKnowledgeBase(
    documents=[doc1_modified, doc2],
    vector_db=vector_db,
)

print("\n\nStart second example. Load initial data...")
knowledge_base.load(recreate=False, upsert=True)
print("\nNow uploading the changed data...")
knowledge_base_changed.load(recreate=False, upsert=True)
print("Example finished. Now dropping the knowledge base.")
vector_db.drop()



================================================
FILE: cookbook/agent_concepts/memory/00_builtin_memory.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Set add_history_to_messages=true to add the previous chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_runs=3,
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
)

# -*- Create a run
agent.print_response("Share a 2 sentence horror story", stream=True)
# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)

# -*- Ask a follow up question that continues the conversation
agent.print_response("What was my first message?", stream=True)
# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)



================================================
FILE: cookbook/agent_concepts/memory/01_standalone_memory.py
================================================
"""
How to add, get, delete, and replace user memories manually
"""

from agno.memory.v2 import Memory, UserMemory
from rich.pretty import pprint

memory = Memory()

# Add a memory for the default user
memory.add_user_memory(
    memory=UserMemory(memory="The user's name is John Doe", topics=["name"]),
)
print("Memories:")
pprint(memory.memories)

# Add memories for Jane Doe
jane_doe_id = "jane_doe@example.com"
print(f"\nUser: {jane_doe_id}")
memory_id_1 = memory.add_user_memory(
    memory=UserMemory(memory="The user's name is Jane Doe", topics=["name"]),
    user_id=jane_doe_id,
)
memory_id_2 = memory.add_user_memory(
    memory=UserMemory(memory="She likes to play tennis", topics=["hobbies"]),
    user_id=jane_doe_id,
)
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)

# Delete a memory
print("\nDeleting memory")
memory.delete_user_memory(user_id=jane_doe_id, memory_id=memory_id_2)
print("Memory deleted\n")
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)

# Replace a memory
print("\nReplacing memory")
memory.replace_user_memory(
    memory_id=memory_id_1,
    memory=UserMemory(memory="The user's name is Jane Mary Doe", topics=["name"]),
    user_id=jane_doe_id,
)
print("Memory replaced")
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/02_persistent_memory.py
================================================
"""
This example shows how to use the Memory class to create a persistent memory.

Every time you run this, the `Memory` object will be re-initialized from the DB.
"""

from typing import List

from agno.memory.v2.db.schema import MemoryRow
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.memory.v2.schema import UserMemory
from rich.pretty import pprint

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

# Clear the DB
memory_db.clear()

memory = Memory(db=memory_db)

john_doe_id = "john_doe@example.com"

# Run 1
memory.add_user_memory(
    memory=UserMemory(memory="The user's name is John Doe", topics=["name"]),
    user_id=john_doe_id,
)

# Run this the 2nd time
memory.add_user_memory(
    memory=UserMemory(
        memory="The user works at a software company called Agno", topics=["work"]
    ),
    user_id=john_doe_id,
)

memories: List[MemoryRow] = memory_db.read_memories()
print("All memories:")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/03_memory_creation.py
================================================
"""
Create user memories with an Agent by providing a either text or a list of messages.
"""

from agno.memory.v2 import Memory
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.models.google import Gemini
from agno.models.message import Message
from rich.pretty import pprint

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")
# Reset for this example
memory_db.clear()

memory = Memory(model=Gemini(id="gemini-2.0-flash-exp"), db=memory_db)

john_doe_id = "john_doe@example.com"

memory.create_user_memories(
    message="""\
I enjoy hiking in the mountains on weekends,
reading science fiction novels before bed,
cooking new recipes from different cultures,
playing chess with friends,
and attending live music concerts whenever possible.
Photography has become a recent passion of mine, especially capturing landscapes and street scenes.
I also like to meditate in the mornings and practice yoga to stay centered.
""",
    user_id=john_doe_id,
)


memories = memory.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)

jane_doe_id = "jane_doe@example.com"
# Send a history of messages and add memories
memory.create_user_memories(
    messages=[
        Message(role="user", content="My name is Jane Doe"),
        Message(role="assistant", content="That is great!"),
        Message(role="user", content="I like to play chess"),
        Message(role="assistant", content="That is great!"),
    ],
    user_id=jane_doe_id,
)

memories = memory.get_user_memories(user_id=jane_doe_id)
print("Jane Doe's memories:")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/04_custom_memory_creation.py
================================================
"""
Create user memories with an Agent by providing a either text or a list of messages.
"""

from agno.memory.v2 import Memory
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.manager import MemoryManager
from agno.models.anthropic.claude import Claude
from agno.models.google import Gemini
from agno.models.message import Message
from rich.pretty import pprint

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")
# Reset for this example
memory_db.clear()

memory = Memory(
    model=Gemini(id="gemini-2.0-flash-exp"),
    memory_manager=MemoryManager(
        model=Gemini(id="gemini-2.0-flash-exp"),
        memory_capture_instructions="""\
                    Memories should only include details about the user's academic interests.
                    Only include which subjects they are interested in.
                    Ignore names, hobbies, and personal interests.
                    """,
    ),
    db=memory_db,
)

john_doe_id = "john_doe@example.com"

memory.create_user_memories(
    message="""\
My name is John Doe.

I enjoy hiking in the mountains on weekends,
reading science fiction novels before bed,
cooking new recipes from different cultures,
playing chess with friends.

I am interested to learn about the history of the universe and other astronomical topics.
""",
    user_id=john_doe_id,
)


memories = memory.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)


# Use default memory manager
memory = Memory(model=Claude(id="claude-3-5-sonnet-latest"), db=memory_db)
jane_doe_id = "jane_doe@example.com"

# Send a history of messages and add memories
memory.create_user_memories(
    messages=[
        Message(role="user", content="Hi, how are you?"),
        Message(role="assistant", content="I'm good, thank you!"),
        Message(role="user", content="What are you capable of?"),
        Message(
            role="assistant",
            content="I can help you with your homework and answer questions about the universe.",
        ),
        Message(role="user", content="My name is Jane Doe"),
        Message(role="user", content="I like to play chess"),
        Message(
            role="user",
            content="Actually, forget that I like to play chess. I more enjoy playing table top games like dungeons and dragons",
        ),
        Message(
            role="user",
            content="I'm also interested in learning about the history of the universe and other astronomical topics.",
        ),
        Message(role="assistant", content="That is great!"),
        Message(
            role="user",
            content="I am really interested in physics. Tell me about quantum mechanics?",
        ),
    ],
    user_id=jane_doe_id,
)

memories = memory.get_user_memories(user_id=jane_doe_id)
print("Jane Doe's memories:")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/05_memory_search.py
================================================
"""
How to search for user memories using different retrieval methods

- last_n: Retrieves the last n memories
- first_n: Retrieves the first n memories
- semantic: Retrieves memories using semantic search
"""

from agno.memory.v2 import Memory, UserMemory
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.models.google.gemini import Gemini
from rich.pretty import pprint

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")
# Reset for this example
memory_db.clear()

memory = Memory(model=Gemini(id="gemini-2.0-flash-exp"), db=memory_db)

john_doe_id = "john_doe@example.com"
memory.add_user_memory(
    memory=UserMemory(memory="The user enjoys hiking in the mountains on weekends"),
    user_id=john_doe_id,
)
memory.add_user_memory(
    memory=UserMemory(
        memory="The user enjoys reading science fiction novels before bed"
    ),
    user_id=john_doe_id,
)
print("John Doe's memories:")
pprint(memory.memories)

memories = memory.search_user_memories(
    user_id=john_doe_id, limit=1, retrieval_method="last_n"
)
print("\nJohn Doe's last_n memories:")
pprint(memories)

memories = memory.search_user_memories(
    user_id=john_doe_id, limit=1, retrieval_method="first_n"
)
print("\nJohn Doe's first_n memories:")
pprint(memories)

memories = memory.search_user_memories(
    user_id=john_doe_id,
    query="What does the user like to do on weekends?",
    retrieval_method="agentic",
)
print("\nJohn Doe's memories similar to the query (agentic):")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/06_agent_with_memory.py
================================================
"""
This example shows you how to use persistent memory with an Agent.

After each run, user memories are created/updated.

To enable this, set `enable_user_memories=True` in the Agent config.
"""

from uuid import uuid4

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from rich.pretty import pprint
from utils import print_chat_history

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

# No need to set the model, it gets set by the agent to the agent's model
memory = Memory(db=memory_db)

# Reset the memory for this example
memory.clear()

session_id = str(uuid4())
john_doe_id = "john_doe@example.com"

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    memory=memory,
    storage=SqliteStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    enable_user_memories=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
    session_id=session_id,
)

agent.print_response(
    "What are my hobbies?", stream=True, user_id=john_doe_id, session_id=session_id
)

# -*- Print the chat history
session_runs = memory.runs[session_id]
print_chat_history(session_runs)

memories = memory.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)

agent.print_response(
    "Ok i dont like hiking anymore, i like to play soccer instead.",
    stream=True,
    user_id=john_doe_id,
    session_id=session_id,
)

# -*- Print the chat history
session_runs = memory.runs[session_id]
print_chat_history(session_runs)

# You can also get the user memories from the agent
memories = agent.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/07_agentic_memory.py
================================================
"""
This example shows you how to use persistent memory with an Agent.

During each run the Agent can create/update/delete user memories.

To enable this, set `enable_agentic_memory=True` in the Agent config.
"""

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

# No need to set the model, it gets set by the agent to the agent's model
memory = Memory(db=memory_db)

# Reset the memory for this example
memory.clear()

john_doe_id = "john_doe@example.com"

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    memory=memory,
    enable_agentic_memory=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
)

agent.print_response("What are my hobbies?", stream=True, user_id=john_doe_id)

memories = memory.get_user_memories(user_id=john_doe_id)
print("Memories about John Doe:")
pprint(memories)


agent.print_response(
    "Remove all existing memories of me.",
    stream=True,
    user_id=john_doe_id,
)

memories = memory.get_user_memories(user_id=john_doe_id)
print("Memories about John Doe:")
pprint(memories)

agent.print_response(
    "My name is John Doe and I like to paint.", stream=True, user_id=john_doe_id
)

memories = memory.get_user_memories(user_id=john_doe_id)
print("Memories about John Doe:")
pprint(memories)


agent.print_response(
    "I don't pain anymore, i draw instead.", stream=True, user_id=john_doe_id
)

memories = memory.get_user_memories(user_id=john_doe_id)

print("Memories about John Doe:")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/08_agent_with_summaries.py
================================================
"""
This example demonstrates how to create session summaries.

To enable this, set `enable_session_summaries=True` in the Agent config.
"""

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.memory.v2.summarizer import SessionSummarizer
from agno.models.anthropic.claude import Claude
from rich.pretty import pprint

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(
    db=memory_db,
    summarizer=SessionSummarizer(model=Claude(id="claude-3-5-sonnet-20241022")),
)

# Reset the memory for this example
memory.clear()

# No session and user ID is specified, so they are generated automatically
agent = Agent(
    model=Claude(id="claude-3-5-sonnet-20241022"),
    memory=memory,
    enable_user_memories=True,
    enable_session_summaries=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
)

agent.print_response(
    "What are my hobbies?",
    stream=True,
)


memories = memory.get_user_memories()
print("John Doe's memories:")
pprint(memories)
session_summary = agent.get_session_summary()
pprint(session_summary)


# Now lets do a new session with a different user
session_id_2 = "1002"
mark_gonzales_id = "mark@example.com"

agent.print_response(
    "My name is Mark Gonzales and I like anime and video games.",
    stream=True,
    user_id=mark_gonzales_id,
    session_id=session_id_2,
)

agent.print_response(
    "What are my hobbies?",
    stream=True,
    user_id=mark_gonzales_id,
    session_id=session_id_2,
)


memories = memory.get_user_memories(user_id=mark_gonzales_id)
print("Mark Gonzales's memories:")
pprint(memories)

# We can get the session summary from memory as well
session_summary = memory.get_session_summary(
    session_id=session_id_2, user_id=mark_gonzales_id
)
pprint(session_summary)



================================================
FILE: cookbook/agent_concepts/memory/09_agents_share_memory.py
================================================
"""
In this example, we have two agents that share the same memory.
"""

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.google.gemini import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

# No need to set the model, it gets set by the agent to the agent's model
memory = Memory(db=memory_db)

# Reset the memory for this example
memory.clear()

john_doe_id = "john_doe@example.com"

chat_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    description="You are a helpful assistant that can chat with users",
    memory=memory,
    enable_user_memories=True,
)

chat_agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
)

chat_agent.print_response("What are my hobbies?", stream=True, user_id=john_doe_id)


research_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    description="You are a research assistant that can help users with their research questions",
    tools=[DuckDuckGoTools(cache_results=True)],
    memory=memory,
    enable_user_memories=True,
)

research_agent.print_response(
    "I love asking questions about quantum computing. What is the latest news on quantum computing?",
    stream=True,
    user_id=john_doe_id,
)

memories = memory.get_user_memories(user_id=john_doe_id)
print("Memories about John Doe:")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/10_custom_memory.py
================================================
"""
This example shows how you can configure the Memory Manager and Summarizer models individually.

In this example, we use OpenRouter and LLama 3.3-70b-instruct for the memory manager and Claude 3.5 Sonnet for the summarizer. And we use Gemini for the Agent.

We also set custom system prompts for the memory manager and summarizer. You can either override the entire system prompt or add additional instructions which is added to the end of the system prompt.
"""

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory, MemoryManager, SessionSummarizer
from agno.models.anthropic.claude import Claude
from agno.models.google.gemini import Gemini
from agno.models.openrouter.openrouter import OpenRouter
from rich.pretty import pprint

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")


# You can also override the entire `system_message` for the memory manager
memory_manager = MemoryManager(
    model=OpenRouter(id="meta-llama/llama-3.3-70b-instruct"),
    additional_instructions="""
    IMPORTANT: Don't store any memories about the user's name. Just say "The User" instead of referencing the user's name.
    """,
)

# You can also override the entire `system_message` for the session summarizer
session_summarizer = SessionSummarizer(
    model=Claude(id="claude-3-5-sonnet-20241022"),
    additional_instructions="""
    Make the summary very informal and conversational.
    """,
)

memory = Memory(
    db=memory_db,
    memory_manager=memory_manager,
    summarizer=session_summarizer,
)

# Reset the memory for this example
memory.clear()

john_doe_id = "john_doe@example.com"

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    memory=memory,
    enable_user_memories=True,
    enable_session_summaries=True,
    user_id=john_doe_id,
)

agent.print_response(
    "My name is John Doe and I like to swim and play soccer.", stream=True
)

agent.print_response("I dont like to swim", stream=True)


memories = memory.get_user_memories(user_id=john_doe_id)

print("John Doe's memories:")
pprint(memories)

summary = agent.get_session_summary()
print("Session summary:")
pprint(summary)



================================================
FILE: cookbook/agent_concepts/memory/11_multi_user_multi_session_chat.py
================================================
"""
This example demonstrates how to run a multi-user, multi-session chat.

In this example, we have 3 users and 4 sessions.

User 1 has 2 sessions.
User 2 has 1 session.
User 3 has 1 session.
"""

import asyncio

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic.claude import Claude
from agno.models.google.gemini import Gemini
from agno.storage.sqlite import SqliteStorage

agent_storage = SqliteStorage(
    table_name="agent_sessions", db_file="tmp/persistent_memory.db"
)
memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(model=Claude(id="claude-3-5-sonnet-20241022"), db=memory_db)

# Reset the memory for this example
memory.clear()

user_1_id = "user_1@example.com"
user_2_id = "user_2@example.com"
user_3_id = "user_3@example.com"

user_1_session_1_id = "user_1_session_1"
user_1_session_2_id = "user_1_session_2"
user_2_session_1_id = "user_2_session_1"
user_3_session_1_id = "user_3_session_1"

chat_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    storage=agent_storage,
    memory=memory,
    enable_user_memories=True,
)


async def run_chat_agent():
    await chat_agent.aprint_response(
        "My name is Mark Gonzales and I like anime and video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )
    await chat_agent.aprint_response(
        "I also enjoy reading manga and playing video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )

    # Chat with user 1 - Session 2
    await chat_agent.aprint_response(
        "I'm going to the movies tonight.",
        user_id=user_1_id,
        session_id=user_1_session_2_id,
    )

    # Chat with user 2
    await chat_agent.aprint_response(
        "Hi my name is John Doe.", user_id=user_2_id, session_id=user_2_session_1_id
    )
    await chat_agent.aprint_response(
        "I'm planning to hike this weekend.",
        user_id=user_2_id,
        session_id=user_2_session_1_id,
    )

    # Chat with user 3
    await chat_agent.aprint_response(
        "Hi my name is Jane Smith.", user_id=user_3_id, session_id=user_3_session_1_id
    )
    await chat_agent.aprint_response(
        "I'm going to the gym tomorrow.",
        user_id=user_3_id,
        session_id=user_3_session_1_id,
    )

    # Continue the conversation with user 1
    # The agent should take into account all memories of user 1.
    await chat_agent.aprint_response(
        "What do you suggest I do this weekend?",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )


if __name__ == "__main__":
    # Chat with user 1 - Session 1
    asyncio.run(run_chat_agent())

    user_1_memories = memory.get_user_memories(user_id=user_1_id)
    print("User 1's memories:")
    for i, m in enumerate(user_1_memories):
        print(f"{i}: {m.memory}")

    user_2_memories = memory.get_user_memories(user_id=user_2_id)
    print("User 2's memories:")
    for i, m in enumerate(user_2_memories):
        print(f"{i}: {m.memory}")

    user_3_memories = memory.get_user_memories(user_id=user_3_id)
    print("User 3's memories:")
    for i, m in enumerate(user_3_memories):
        print(f"{i}: {m.memory}")



================================================
FILE: cookbook/agent_concepts/memory/12_multi_user_multi_session_chat_concurrent.py
================================================
"""
This example shows how to run a multi-user, multi-session chat concurrently.

In this example, we have 3 users and 4 sessions.

User 1 has 2 sessions.
User 2 has 1 session.
User 3 has 1 session.
"""

import asyncio

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic.claude import Claude
from agno.models.google.gemini import Gemini
from agno.storage.sqlite import SqliteStorage

agent_storage = SqliteStorage(
    table_name="agent_sessions", db_file="tmp/persistent_memory.db"
)
memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(model=Claude(id="claude-3-5-sonnet-20241022"), db=memory_db)

# Reset the memory for this example
memory.clear()

user_1_id = "user_1@example.com"
user_2_id = "user_2@example.com"
user_3_id = "user_3@example.com"

user_1_session_1_id = "user_1_session_1"
user_1_session_2_id = "user_1_session_2"
user_2_session_1_id = "user_2_session_1"
user_3_session_1_id = "user_3_session_1"

chat_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    storage=agent_storage,
    memory=memory,
    enable_user_memories=True,
)


async def user_1_conversation():
    """Handle conversation with user 1 across multiple sessions"""
    # User 1 - Session 1
    await chat_agent.arun(
        "My name is Mark Gonzales and I like anime and video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )
    await chat_agent.arun(
        "I also enjoy reading manga and playing video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )

    # User 1 - Session 2
    await chat_agent.arun(
        "I'm going to the movies tonight.",
        user_id=user_1_id,
        session_id=user_1_session_2_id,
    )

    # Continue the conversation in session 1
    await chat_agent.arun(
        "What do you suggest I do this weekend?",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )

    print("User 1 Done")


async def user_2_conversation():
    """Handle conversation with user 2"""
    await chat_agent.arun(
        "Hi my name is John Doe.", user_id=user_2_id, session_id=user_2_session_1_id
    )
    await chat_agent.arun(
        "I'm planning to hike this weekend.",
        user_id=user_2_id,
        session_id=user_2_session_1_id,
    )
    print("User 2 Done")


async def user_3_conversation():
    """Handle conversation with user 3"""
    await chat_agent.arun(
        "Hi my name is Jane Smith.", user_id=user_3_id, session_id=user_3_session_1_id
    )
    await chat_agent.arun(
        "I'm going to the gym tomorrow.",
        user_id=user_3_id,
        session_id=user_3_session_1_id,
    )
    print("User 3 Done")


async def run_concurrent_chat_agent():
    """Run all user conversations concurrently"""
    await asyncio.gather(
        user_1_conversation(), user_2_conversation(), user_3_conversation()
    )


if __name__ == "__main__":
    # Run all conversations concurrently
    asyncio.run(run_concurrent_chat_agent())

    user_1_memories = memory.get_user_memories(user_id=user_1_id)
    print("User 1's memories:")
    for i, m in enumerate(user_1_memories):
        print(f"{i}: {m.memory}")

    user_2_memories = memory.get_user_memories(user_id=user_2_id)
    print("User 2's memories:")
    for i, m in enumerate(user_2_memories):
        print(f"{i}: {m.memory}")

    user_3_memories = memory.get_user_memories(user_id=user_3_id)
    print("User 3's memories:")
    for i, m in enumerate(user_3_memories):
        print(f"{i}: {m.memory}")



================================================
FILE: cookbook/agent_concepts/memory/13_memory_references.py
================================================
"""
This example shows how to use the `add_memory_references` parameter in the Agent config to
add references to the user memories to the Agent.
"""

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory, UserMemory
from agno.models.google.gemini import Gemini

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(db=memory_db)

memory.add_user_memory(
    memory=UserMemory(memory="I like to play soccer", topics=["soccer"]),
    user_id="john_doe@example.com",
)

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    memory=memory,
    add_memory_references=True,  # Add pre existing memories to the Agent but don't create new ones
)

# Alternatively, you can create/update user memories but not add them to the Agent
# agent = Agent(
#     model=Gemini(id="gemini-2.0-flash-exp"),
#     memory=memory,
#     enable_user_memories=True,
#     add_memory_references=False,
# )

agent.print_response("What are my hobbies?", user_id="john_doe@example.com")



================================================
FILE: cookbook/agent_concepts/memory/14_session_summary_references.py
================================================
"""
This example shows how to use the `add_session_summary_references` parameter in the Agent config to
add references to the session summaries to the Agent.

Start the postgres db locally on Docker by running: cookbook/scripts/run_pgvector.sh

Note: Session summaries are stored in the storage table along with the session, not the memory table.
"""

from agno.agent.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.google.gemini import Gemini
from agno.storage.postgres import PostgresStorage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory_db = PostgresMemoryDb(table_name="memory", db_url=db_url)

# Reset for this example
memory_db.clear()

memory = Memory(db=memory_db)

user_id = "john_doe@example.com"
session_id = "session_summaries"

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    memory=memory,
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    enable_session_summaries=True,
    session_id=session_id,
)

# This will create a new session summary
agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    user_id=user_id,
)

# You can use existing session summaries from session storage without creating or updating any new ones.
agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    memory=memory,
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    add_session_summary_references=True,
    session_id=session_id,
)

# agent = Agent(
#     model=Gemini(id="gemini-2.0-flash-exp"),
#     memory=memory,
#     storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/persistent_memory.db"),
#     enable_session_summaries=True,
#     add_session_summary_references=False,
# )

agent.print_response("What are my hobbies?", user_id=user_id)



================================================
FILE: cookbook/agent_concepts/memory/15_memory_demo.py
================================================
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

# UserId for the memories
user_id = "ava"
# Database file for memory and storage
db_file = "tmp/agent.db"

# Initialize memory.v2
memory = Memory(
    # Use any model for creating memories
    model=OpenAIChat(id="gpt-4.1"),
    db=SqliteMemoryDb(table_name="user_memories", db_file=db_file),
)
# Initialize storage
storage = SqliteStorage(table_name="agent_sessions", db_file=db_file)

# Initialize Agent
memory_agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    # Store memories in a database
    memory=memory,
    # Give the Agent the ability to update memories
    enable_agentic_memory=True,
    # OR - Run the MemoryManager after each response
    enable_user_memories=True,
    # Store the chat history in the database
    storage=storage,
    # Add the chat history to the messages
    add_history_to_messages=True,
    # Number of history runs
    num_history_runs=3,
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# memory.clear()
print("Memories about Ava:")
pprint(memory.get_user_memories(user_id=user_id))

memory_agent.print_response(
    "My name is Ava and I like to ski.",
    user_id=user_id,
    stream=True,
    stream_intermediate_steps=True,
)
print("Memories about Ava:")
pprint(memory.get_user_memories(user_id=user_id))

memory_agent.print_response(
    "I live in san francisco, where should i move within a 4 hour drive?",
    user_id=user_id,
    stream=True,
    stream_intermediate_steps=True,
)
print("Memories about Ava:")
pprint(memory.get_user_memories(user_id=user_id))



================================================
FILE: cookbook/agent_concepts/memory/16_custom_memory_instructions.py
================================================
"""
Create user memories with an Agent by providing a either text or a list of messages.
"""

from textwrap import dedent

from agno.memory.v2 import Memory
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.manager import MemoryManager
from agno.models.google import Gemini
from rich.pretty import pprint

memory = Memory(
    db=SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db"),
    memory_manager=MemoryManager(
        model=Gemini(id="gemini-2.0-flash-001"),
        memory_capture_instructions=dedent("""\
            Memories should only include details about the user's academic interests.
            Ignore names, hobbies, and personal interests.
            """),
    ),
)
# Reset the memory for this example
memory.clear()

user_id = "ava@ava.com"

memory.create_user_memories(
    message=dedent("""\
    My name is Ava and I like to ski.
    I live in San Francisco and study geometric neuron architecture.
    """),
    user_id=user_id,
)


memories = memory.get_user_memories(user_id=user_id)
print("Ava's memories:")
pprint(memories)



================================================
FILE: cookbook/agent_concepts/memory/17_builtin_memory_with_session_summary.py
================================================
from agno.agent import Agent
from agno.memory.v2 import Memory, SessionSummarizer
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

# You can also override the entire `system_message` for the session summarizer if you wanted
session_summarizer = SessionSummarizer(
    model=OpenAIChat(id="gpt-4o-mini"),
    additional_instructions="""
    Make the summary a points-wise list of a summarised version of each message in the conversation.
    """,
)

memory = Memory(
    summarizer=session_summarizer,
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    memory=memory,
    # Set add_history_to_messages=true to add the previous chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_runs=3,
    # Let the agent summarize the session after every run
    enable_session_summaries=True,
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner. Keep responses very short and concise.",
)

agent.print_response("Hello! How are you today?", stream=True)

agent.print_response("Explain what an LLM is.", stream=True)

agent.print_response(
    "I'm thinking about learning a new programming language. Any suggestions?",
    stream=True,
)

agent.print_response("Tell me an interesting fact about space.", stream=True)


# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)

agent.print_response("What have we been talking about?", stream=True)

# -*- Print the messages used for the last response (only the last 3 is kept in history)
pprint([m.model_dump(include={"role", "content"}) for m in agent.run_response.messages])

# We can get the session summary from memory as well
session_summary = agent.get_session_summary()
pprint(session_summary)



================================================
FILE: cookbook/agent_concepts/memory/18_persistent_memory_firestore.py
================================================
"""
This example shows how to run a multi-user, multi-session chat concurrently with Firestore.

In this example, we have 3 users and 4 sessions.

User 1 has 2 sessions.
User 2 has 1 session.
User 3 has 1 session.

Steps:
1. Ensure you have firestore enabled in your gcloud project. See: https://cloud.google.com/firestore/docs/create-database-server-client-library
2. Run: `pip install openai google-cloud-firestore agno` to install dependencies
3. Set up authentication:
   - Option 1: Run `gcloud auth application-default login`
   - Option 2: Set GOOGLE_APPLICATION_CREDENTIALS environment variable to your service account key
   - Option 3: If running on GCP, it will use the default service account
4. Make sure your project has Firestore API enabled
"""

import asyncio

from agno.agent.agent import Agent
from agno.memory.v2.db.firestore import FirestoreMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic.claude import Claude
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.storage.firestore import FirestoreStorage

agent_storage = FirestoreStorage(
    db_name="memory",
    project_id="forward-vial-458917-s8",
    collection_name="agno_sessions",
)
memory_db = FirestoreMemoryDb(
    db_name="memory", project_id="forward-vial-458917-s8", collection_name="agno_memory"
)

memory = Memory(model=Claude(id="claude-3-5-sonnet-20241022"), db=memory_db)

# Reset the memory for this example
memory.clear()

user_1_id = "user_1@example.com"
user_2_id = "user_2@example.com"
user_3_id = "user_3@example.com"

user_1_session_1_id = "user_1_session_1"
user_1_session_2_id = "user_1_session_2"
user_2_session_1_id = "user_2_session_1"
user_3_session_1_id = "user_3_session_1"

chat_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=agent_storage,
    memory=memory,
    enable_user_memories=True,
)


async def user_1_conversation():
    """Handle conversation with user 1 across multiple sessions"""
    # User 1 - Session 1
    await chat_agent.arun(
        "My name is Mark Gonzales and I like anime and video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )
    await chat_agent.arun(
        "I also enjoy reading manga and playing video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )

    # User 1 - Session 2
    await chat_agent.arun(
        "I'm going to the movies tonight.",
        user_id=user_1_id,
        session_id=user_1_session_2_id,
    )

    # Continue the conversation in session 1
    await chat_agent.arun(
        "What do you suggest I do this weekend?",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )

    print("User 1 Done")


async def user_2_conversation():
    """Handle conversation with user 2"""
    await chat_agent.arun(
        "Hi my name is John Doe.", user_id=user_2_id, session_id=user_2_session_1_id
    )
    await chat_agent.arun(
        "I'm planning to hike this weekend.",
        user_id=user_2_id,
        session_id=user_2_session_1_id,
    )
    print("User 2 Done")


async def user_3_conversation():
    """Handle conversation with user 3"""
    await chat_agent.arun(
        "Hi my name is Jane Smith.", user_id=user_3_id, session_id=user_3_session_1_id
    )
    await chat_agent.arun(
        "I'm going to the gym tomorrow.",
        user_id=user_3_id,
        session_id=user_3_session_1_id,
    )
    print("User 3 Done")


async def run_concurrent_chat_agent():
    """Run all user conversations concurrently"""
    await asyncio.gather(
        user_1_conversation(), user_2_conversation(), user_3_conversation()
    )


if __name__ == "__main__":
    # Run all conversations concurrently
    asyncio.run(run_concurrent_chat_agent())

    user_1_memories = memory.get_user_memories(user_id=user_1_id)
    print("User 1's memories:")
    for i, m in enumerate(user_1_memories):
        print(f"{i}: {m.memory}")

    user_2_memories = memory.get_user_memories(user_id=user_2_id)
    print("User 2's memories:")
    for i, m in enumerate(user_2_memories):
        print(f"{i}: {m.memory}")

    user_3_memories = memory.get_user_memories(user_id=user_3_id)
    print("User 3's memories:")
    for i, m in enumerate(user_3_memories):
        print(f"{i}: {m.memory}")



================================================
FILE: cookbook/agent_concepts/memory/19_share_memory_and_history_between_agents.py
================================================
from uuid import uuid4

from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai.chat import OpenAIChat
from agno.storage.sqlite import SqliteStorage

agent_storage = SqliteStorage(
    table_name="agent_sessions", db_file="tmp/agent_sessions.db"
)
memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")
memory = Memory(db=memory_db)

# Reset the memory for this example
memory.clear()


session_id = str(uuid4())
user_id = "john_doe@example.com"

agent_1 = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are really friendly and helpful.",
    storage=agent_storage,
    memory=memory,
    add_history_to_messages=True,
    enable_user_memories=True,
)

agent_2 = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are really grumpy and mean.",
    storage=agent_storage,
    memory=memory,
    add_history_to_messages=True,
    enable_user_memories=True,
)

agent_1.print_response(
    "Hi! My name is John Doe.", session_id=session_id, user_id=user_id
)

agent_2.print_response("What is my name?", session_id=session_id, user_id=user_id)

agent_2.print_response(
    "I like to hike in the mountains on weekends.",
    session_id=session_id,
    user_id=user_id,
)

agent_1.print_response("What are my hobbies?", session_id=session_id, user_id=user_id)

agent_1.print_response(
    "What have we been discussing? Give me bullet points.",
    session_id=session_id,
    user_id=user_id,
)



================================================
FILE: cookbook/agent_concepts/memory/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/memory/playground.py
================================================
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools

# Database file for memory and storage
db_file = "tmp/agent.db"

# Initialize memory.v2
memory = Memory(
    # Use any model for creating memories
    model=OpenAIChat(id="gpt-4.1"),
    db=SqliteMemoryDb(table_name="user_memories", db_file=db_file),
    delete_memories=True,
    clear_memories=True,
)
# Initialize storage
storage = SqliteStorage(table_name="agent_sessions", db_file=db_file)

# Initialize Agent
agent = Agent(
    name="Memory Agent",
    model=OpenAIChat(id="gpt-4.1"),
    # Store memories in a database
    memory=memory,
    # Give the Agent the ability to update memories
    enable_agentic_memory=True,
    # Store the chat history in the database
    storage=storage,
    # Add chat history to the messages
    add_history_to_messages=True,
    num_history_runs=3,
    # Give the agent a tool to access chat history
    read_chat_history=True,
    # Add datetime to the instructions
    add_datetime_to_instructions=True,
    # Use markdown for the response
    markdown=True,
    # Add a tool to search the web
    tools=[DuckDuckGoTools()],
)


playground = Playground(
    agents=[
        agent,
    ],
    app_id="memory-playground-app",
    name="Memory Playground",
)
app = playground.get_app()

if __name__ == "__main__":
    # Start the playground server
    playground.serve(
        app="playground:app",
        host="localhost",
        port=7777,
        reload=True,
    )



================================================
FILE: cookbook/agent_concepts/memory/utils.py
================================================
import json
from typing import List

from agno.run.response import RunResponse
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel

console = Console()


def print_chat_history(session_runs: List[RunResponse]):
    # -*- Print history
    messages = []
    for run in session_runs:
        for m in run.messages:
            if m.role == "system" and len(messages) > 0:
                # Skip system after the first one
                continue

            message_dict = m.model_dump(
                include={"role", "content", "tool_calls", "from_history"}
            )
            if message_dict["content"] is not None:
                del message_dict["tool_calls"]
            else:
                del message_dict["content"]
            messages.append(message_dict)

    console.print(
        Panel(
            JSON(
                json.dumps(
                    messages,
                ),
                indent=4,
            ),
            title=f"Chat History for session_id: {session_runs[0].session_id}",
            expand=True,
        )
    )



================================================
FILE: cookbook/agent_concepts/memory/db/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/memory/db/mongodb.py
================================================
from agno.agent.agent import Agent
from agno.memory.v2.db.mongodb import MongoMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.mongodb import MongoDbStorage

db_url = "mongodb://mongoadmin:secret@localhost:27017"

memory = Memory(db=MongoMemoryDb(collection_name="agent_memories", db_url=db_url))

session_id = "mongodb_memories"
user_id = "mongodb_user"

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    memory=memory,
    storage=MongoDbStorage(collection_name="agent_sessions", db_url=db_url),
    enable_user_memories=True,
    enable_session_summaries=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=user_id,
    session_id=session_id,
)

agent.print_response(
    "What are my hobbies?", stream=True, user_id=user_id, session_id=session_id
)



================================================
FILE: cookbook/agent_concepts/memory/db/postgres.py
================================================
from agno.agent.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai.chat import OpenAIChat
from agno.storage.postgres import PostgresStorage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory = Memory(db=PostgresMemoryDb(table_name="agent_memories", db_url=db_url))

session_id = "postgres_memories"
user_id = "postgres_user"

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    memory=memory,
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    enable_user_memories=True,
    enable_session_summaries=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=user_id,
    session_id=session_id,
)

agent.print_response(
    "What are my hobbies?", stream=True, user_id=user_id, session_id=session_id
)



================================================
FILE: cookbook/agent_concepts/memory/db/redis_db.py
================================================
from agno.agent.agent import Agent
from agno.memory.v2.db.redis import RedisMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.redis import RedisStorage

memory = Memory(db=RedisMemoryDb(prefix="agno_memory", host="localhost", port=6379))

session_id = "redis_memories"
user_id = "redis_user"

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    memory=memory,
    storage=RedisStorage(prefix="agno_test", host="localhost", port=6379),
    enable_user_memories=True,
    enable_session_summaries=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=user_id,
    session_id=session_id,
)

agent.print_response(
    "What are my hobbies?", stream=True, user_id=user_id, session_id=session_id
)



================================================
FILE: cookbook/agent_concepts/memory/db/sqlite.py
================================================
from agno.agent.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage

memory = Memory(db=SqliteMemoryDb(table_name="agent_memories", db_file="tmp/memory.db"))

session_id = "sqlite_memories"
user_id = "sqlite_user"

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    memory=memory,
    storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/memory.db"),
    enable_user_memories=True,
    enable_session_summaries=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=user_id,
    session_id=session_id,
)

agent.print_response(
    "What are my hobbies?", stream=True, user_id=user_id, session_id=session_id
)



================================================
FILE: cookbook/agent_concepts/memory/integrations/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/memory/integrations/mem0_integration.py
================================================
from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

try:
    from mem0 import MemoryClient
except ImportError:
    raise ImportError(
        "mem0 is not installed. Please install it using `pip install mem0ai`."
    )

client = MemoryClient()

user_id = "agno"
messages = [
    {"role": "user", "content": "My name is John Billings."},
    {"role": "user", "content": "I live in NYC."},
    {"role": "user", "content": "I'm going to a concert tomorrow."},
]
# Comment out the following line after running the script once
client.add(messages, user_id=user_id)

agent = Agent(
    model=OpenAIChat(),
    context={"memory": client.get_all(user_id=user_id)},
    add_context=True,
)
run: RunResponse = agent.run("What do you know about me?")

pprint_run_response(run)

messages = [{"role": i.role, "content": str(i.content)} for i in (run.messages or [])]
client.add(messages, user_id=user_id)



================================================
FILE: cookbook/agent_concepts/memory/integrations/zep_integration.py
================================================
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zep import ZepTools

# Initialize the ZepTools
zep_tools = ZepTools(user_id="agno", session_id="agno-session")

zep_tools.add_zep_message(role="user", content="My name is John Billings")
zep_tools.add_zep_message(role="user", content="I live in NYC")
zep_tools.add_zep_message(role="user", content="I'm going to a concert tomorrow")

# Allow the memories to sync with Zep database
time.sleep(10)

# Initialize the Agent
agent = Agent(
    model=OpenAIChat(),
    tools=[zep_tools],
    context={"memory": zep_tools.get_zep_memory(memory_type="context")},
    add_context=True,
)

# Ask the Agent about the user
agent.print_response("What do you know about me?")



================================================
FILE: cookbook/agent_concepts/memory_legacy/01_builtin_memory.py
================================================
from agno.agent import Agent
from agno.memory import AgentMemory
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    memory=AgentMemory(),
    # Set add_history_to_messages=true to add the previous chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_responses=3,
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
)

# -*- Create a run
agent.print_response("Share a 2 sentence horror story", stream=True)
# -*- Print the messages in the memory
pprint([m.model_dump(include={"role", "content"}) for m in agent.memory.messages])

# -*- Ask a follow up question that continues the conversation
agent.print_response("What was my first message?", stream=True)
# -*- Print the messages in the memory
pprint([m.model_dump(include={"role", "content"}) for m in agent.memory.messages])



================================================
FILE: cookbook/agent_concepts/memory_legacy/02_persistent_memory.py
================================================
"""
This recipe shows how to store agent sessions in a sqlite database.
Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/memory/02_persistent_memory.py` to run the agent
"""

import json

from agno.agent import Agent
from agno.memory import AgentMemory
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    memory=AgentMemory(),
    # Store agent sessions in a database
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/agent_storage.db"
    ),
    # Set add_history_to_messages=true to add the previous chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_responses=3,
    # The session_id is used to identify the session in the database
    # You can resume any session by providing a session_id
    # session_id="xxxx-xxxx-xxxx-xxxx",
    # Description creates a system prompt for the agent
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
)

console = Console()


def print_chat_history(agent):
    # -*- Print history
    console.print(
        Panel(
            JSON(
                json.dumps(
                    [
                        m.model_dump(include={"role", "content"})
                        for m in agent.memory.messages
                    ]
                ),
                indent=4,
            ),
            title=f"Chat History for session_id: {agent.session_id}",
            expand=True,
        )
    )


# -*- Create a run
agent.print_response("Share a 2 sentence horror story", stream=True)
# -*- Print the chat history
print_chat_history(agent)

# -*- Ask a follow up question that continues the conversation
agent.print_response("What was my first message?", stream=True)
# -*- Print the chat history
print_chat_history(agent)



================================================
FILE: cookbook/agent_concepts/memory_legacy/03_memories_and_summaries.py
================================================
"""
This recipe shows how to store personalized memories and summaries in a sqlite database.
Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/memory/03_memories_and_summaries.py` to run the agent
"""

import json

from agno.agent import Agent, AgentMemory
from agno.memory.db.sqlite import SqliteMemoryDb
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # The memories are personalized for this user
    user_id="john_billings",
    # Store the memories and summary in a table: agent_memory
    memory=AgentMemory(
        db=SqliteMemoryDb(
            table_name="agent_memory",
            db_file="tmp/agent_memory.db",
        ),
        # Create and store personalized memories for this user
        create_user_memories=True,
        # Update memories for the user after each run
        update_user_memories_after_run=True,
        # Create and store session summaries
        create_session_summary=True,
        # Update session summaries after each run
        update_session_summary_after_run=True,
    ),
    # Store agent sessions in a database, that persists between runs
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/agent_storage.db"
    ),
    # add_history_to_messages=true adds the chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_responses=3,
    # Description creates a system prompt for the agent
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
)

console = Console()


def render_panel(title: str, content: str) -> Panel:
    return Panel(JSON(content, indent=4), title=title, expand=True)


def print_agent_memory(agent):
    # -*- Print history
    console.print(
        render_panel(
            f"Chat History for session_id: {agent.session_id}",
            json.dumps(
                [
                    m.model_dump(include={"role", "content"})
                    for m in agent.memory.messages
                ],
                indent=4,
            ),
        )
    )
    # -*- Print memories
    console.print(
        render_panel(
            f"Memories for user_id: {agent.user_id}",
            json.dumps(
                [
                    m.model_dump(include={"memory", "input"})
                    for m in agent.memory.memories
                ],
                indent=4,
            ),
        )
    )
    # -*- Print summary
    console.print(
        render_panel(
            f"Summary for session_id: {agent.session_id}",
            json.dumps(agent.memory.summary.model_dump(), indent=4),
        )
    )


# -*- Share personal information
agent.print_response("My name is john billings and I live in nyc.", stream=True)
# -*- Print agent memory
print_agent_memory(agent)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print agent memory
print_agent_memory(agent)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/agent_concepts/memory_legacy/04_persistent_memory_postgres.py
================================================
from typing import List, Optional

import typer
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.memory import AgentMemory
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes", db_url=db_url, search_type=SearchType.hybrid
    ),
)
# Load the knowledge base: Comment after first run
knowledge_base.load(upsert=True)

storage = PostgresAgentStorage(table_name="pdf_agent", db_url=db_url)


def pdf_agent(new: bool = False, user: str = "user"):
    session_id: Optional[str] = None

    if not new:
        existing_sessions: List[str] = storage.get_all_session_ids(user)
        if len(existing_sessions) > 0:
            session_id = existing_sessions[0]

    agent = Agent(
        memory=AgentMemory(),
        session_id=session_id,
        user_id=user,
        knowledge=knowledge_base,
        storage=storage,
        # Show tool calls in the response
        show_tool_calls=True,
        # Enable the agent to read the chat history
        read_chat_history=True,
        # We can also automatically add the chat history to the messages sent to the model
        # But giving the model the chat history is not always useful, so we give it a tool instead
        # to only use when needed.
        # add_history_to_messages=True,
        # Number of historical responses to add to the messages.
        # num_history_responses=3,
    )
    if session_id is None:
        session_id = agent.session_id
        print(f"Started Session: {session_id}\n")
    else:
        print(f"Continuing Session: {session_id}\n")

    # Runs the agent as a cli app
    agent.cli_app(markdown=True)


if __name__ == "__main__":
    typer.run(pdf_agent)



================================================
FILE: cookbook/agent_concepts/memory_legacy/05_memories_and_summaries_postgres.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
"""

from agno.agent import Agent, AgentMemory
from agno.memory.db.postgres import PgMemoryDb
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Store the memories and summary in a database
    memory=AgentMemory(
        db=PgMemoryDb(table_name="agent_memory", db_url=db_url),
        create_user_memories=True,
        create_session_summary=True,
    ),
    # Store agent sessions in a database
    storage=PostgresAgentStorage(
        table_name="personalized_agent_sessions", db_url=db_url
    ),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summary)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summary)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summary)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/agent_concepts/memory_legacy/06_memories_and_summaries_sqlite_async.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/agents/memories_and_summaries_sqlite.py` to run the agent
"""

import asyncio
import json

from agno.agent import Agent, AgentMemory
from agno.memory.db.sqlite import SqliteMemoryDb
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel

agent_memory_file: str = "tmp/agent_memory.db"
agent_storage_file: str = "tmp/agent_storage.db"

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # The memories are personalized for this user
    user_id="john_billings",
    # Store the memories and summary in a table: agent_memory
    memory=AgentMemory(
        db=SqliteMemoryDb(
            table_name="agent_memory",
            db_file=agent_memory_file,
        ),
        # Create and store personalized memories for this user
        create_user_memories=True,
        # Update memories for the user after each run
        update_user_memories_after_run=True,
        # Create and store session summaries
        create_session_summary=True,
        # Update session summaries after each run
        update_session_summary_after_run=True,
    ),
    # Store agent sessions in a database
    storage=SqliteAgentStorage(table_name="agent_sessions", db_file=agent_storage_file),
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
    # Show debug logs to see the memory being created
    # debug_mode=True,
)

console = Console()


def render_panel(title: str, content: str) -> Panel:
    return Panel(JSON(content, indent=4), title=title, expand=True)


def print_agent_memory(agent):
    # -*- Print history
    console.print(
        render_panel(
            "Chat History",
            json.dumps(
                [
                    m.model_dump(include={"role", "content"})
                    for m in agent.memory.messages
                ],
                indent=4,
            ),
        )
    )
    # -*- Print memories
    console.print(
        render_panel(
            "Memories",
            json.dumps(
                [
                    m.model_dump(include={"memory", "input"})
                    for m in agent.memory.memories
                ],
                indent=4,
            ),
        )
    )
    # -*- Print summary
    console.print(
        render_panel("Summary", json.dumps(agent.memory.summary.model_dump(), indent=4))
    )


async def main():
    # -*- Share personal information
    await agent.aprint_response("My name is john billings?", stream=True)
    # -*- Print agent memory
    print_agent_memory(agent)

    # -*- Share personal information
    await agent.aprint_response("I live in nyc?", stream=True)
    # -*- Print agent memory
    print_agent_memory(agent)

    # -*- Share personal information
    await agent.aprint_response("I'm going to a concert tomorrow?", stream=True)
    # -*- Print agent memory
    print_agent_memory(agent)

    # Ask about the conversation
    await agent.aprint_response(
        "What have we been talking about, do you know my name?", stream=True
    )


# Run the async main function
if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/agent_concepts/memory_legacy/07_persistent_memory_mongodb.py
================================================
"""
This recipe shows how to store agent sessions in a MongoDB database.
Steps:
1. Run: `pip install openai pymongo agno` to install dependencies
2. Make sure you are running a local instance of mongodb
3. Run: `python cookbook/memory/07_persistent_memory_mongodb.py` to run the agent
"""

import json

from agno.agent import Agent
from agno.memory.agent import AgentMemory
from agno.memory.db.mongodb import MongoMemoryDb
from agno.models.openai import OpenAIChat
from agno.storage.agent.mongodb import MongoDbAgentStorage
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel

# MongoDB connection settings
db_url = "mongodb://localhost:27017"

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Store agent sessions in MongoDB
    storage=MongoDbAgentStorage(
        collection_name="agent_sessions", db_url=db_url, db_name="agno"
    ),
    # Store memories in MongoDB
    memory=AgentMemory(
        db=MongoMemoryDb(
            collection_name="agent_sessions", db_url=db_url, db_name="agno"
        ),
        create_user_memories=True,
        create_session_summary=True,
    ),
    # Set add_history_to_messages=true to add the previous chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_responses=3,
    # The session_id is used to identify the session in the database
    # You can resume any session by providing a session_id
    # session_id="xxxx-xxxx-xxxx-xxxx",
    # Description creates a system prompt for the agent
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
)

console = Console()


def print_chat_history(agent):
    # -*- Print history
    console.print(
        Panel(
            JSON(
                json.dumps(
                    [
                        m.model_dump(include={"role", "content"})
                        for m in agent.memory.messages
                    ]
                ),
                indent=4,
            ),
            title=f"Chat History for session_id: {agent.session_id}",
            expand=True,
        )
    )


# -*- Create a run
agent.print_response("Share a 2 sentence horror story", stream=True)
# -*- Print the chat history
print_chat_history(agent)

# -*- Ask a follow up question that continues the conversation
agent.print_response("What was my first message?", stream=True)
# -*- Print the chat history
print_chat_history(agent)



================================================
FILE: cookbook/agent_concepts/memory_legacy/08_mem0_memory.py
================================================
from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response
from mem0 import MemoryClient

client = MemoryClient()

user_id = "agno"
messages = [
    {"role": "user", "content": "My name is John Billings."},
    {"role": "user", "content": "I live in NYC."},
    {"role": "user", "content": "I'm going to a concert tomorrow."},
]
# Comment out the following line after running the script once
client.add(messages, user_id=user_id)

agent = Agent(
    model=OpenAIChat(),
    context={"memory": client.get_all(user_id=user_id)},
    add_context=True,
)
run: RunResponse = agent.run("What do you know about me?")

pprint_run_response(run)

messages = [{"role": i.role, "content": str(i.content)} for i in (run.messages or [])]
client.add(messages, user_id=user_id)



================================================
FILE: cookbook/agent_concepts/memory_legacy/09_using_other_models_for_memory.py
================================================
"""🎓 StudyScout - Your AI-Powered Learning Companion!

This advanced example shows how to create a sophisticated educational assistant
that leverages multiple AI models for enhanced memory and personalized learning.
The agent combines long-term memory, intelligent classification, and dynamic
summarization to deliver an adaptive learning experience that grows with the user.

Key Features:
- Personalized learning paths based on user interests and goals
- Long-term memory to track progress and preferences
- Intelligent content curation from multiple sources
- Interactive quizzes and assessments
- Resource recommendations (articles, videos, courses)

Example prompts to try:
- "Create a 3-month learning path for becoming a full-stack developer"
- "Explain quantum computing using gaming analogies based on my interests"
- "Quiz me on world history, focusing on the Renaissance period"
- "Find advanced machine learning resources matching my current skill level"
- "Help me prepare for the AWS Solutions Architect certification"

Run: `pip install groq agno` to install the dependencies
"""

from textwrap import dedent
from typing import List, Optional

import typer
from agno.agent import Agent, AgentMemory
from agno.memory.classifier import MemoryClassifier
from agno.memory.db.sqlite import SqliteMemoryDb
from agno.memory.manager import MemoryManager
from agno.memory.summarizer import MemorySummarizer
from agno.models.groq import Groq
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.youtube import YouTubeTools
from rich import print

# Initialize storage components
agent_storage = SqliteAgentStorage(table_name="study_sessions", db_file="tmp/agents.db")
memory_db = SqliteMemoryDb(
    table_name="study_memory",
    db_file="tmp/agent_memory.db",
)


def study_agent(
    user_id: Optional[str] = typer.Argument(None, help="User ID for the study session"),
):
    """
    Initialize and run the StudyScout agent with the specified user ID.
    If no user ID is provided, prompt for one.
    """
    # Get user ID if not provided as argument
    if user_id is None:
        user_id = typer.prompt("Enter your user ID", default="default_user")

    session_id: Optional[str] = None

    # Ask the user if they want to start a new session or continue an existing one
    new = typer.confirm("Do you want to start a new study session?")

    if not new:
        existing_sessions: List[str] = agent_storage.get_all_session_ids(user_id)
        if len(existing_sessions) > 0:
            print("\nExisting sessions:")
            for i, session in enumerate(existing_sessions, 1):
                print(f"{i}. {session}")
            session_idx = typer.prompt(
                "Choose a session number to continue (or press Enter for most recent)",
                default=1,
            )
            try:
                session_id = existing_sessions[int(session_idx) - 1]
            except (ValueError, IndexError):
                session_id = existing_sessions[0]
        else:
            print("No existing sessions found. Starting a new session.")

    agent = Agent(
        name="StudyScout",
        user_id=user_id,
        session_id=session_id,
        model=Groq(id="llama-3.3-70b-versatile"),
        memory=AgentMemory(
            db=memory_db,
            create_user_memories=True,
            update_user_memories_after_run=True,
            classifier=MemoryClassifier(
                model=Groq(id="llama-3.3-70b-versatile"),
            ),
            summarizer=MemorySummarizer(
                model=Groq(id="llama-3.3-70b-versatile"),
            ),
            manager=MemoryManager(
                model=Groq(id="llama-3.3-70b-versatile"),
                db=memory_db,
                user_id=user_id,
            ),
        ),
        storage=agent_storage,
        tools=[DuckDuckGoTools(), YouTubeTools()],
        description=dedent("""\
        You are StudyScout, an expert educational mentor with deep expertise in personalized learning! 📚

        Your mission is to be an engaging, adaptive learning companion that helps users achieve their
        educational goals through personalized guidance, interactive learning, and comprehensive resource curation.
        """),
        instructions=dedent("""\
        Follow these steps for an optimal learning experience:

        1. Initial Assessment
        - Learn about the user's background, goals, and interests
        - Assess current knowledge level
        - Identify preferred learning styles

        2. Learning Path Creation
        - Design customized study plans, use DuckDuckGo to find resources
        - Set clear milestones and objectives
        - Adapt to user's pace and schedule

        3. Content Delivery
        - Break down complex topics into digestible chunks
        - Use relevant analogies and examples
        - Connect concepts to user's interests
        - Provide multi-format resources (text, video, interactive)

        4. Resource Curation
        - Find relevant learning materials using DuckDuckGo
        - Recommend quality educational content
        - Share community learning opportunities
        - Suggest practical exercises

        Your teaching style:
        - Be encouraging and supportive
        - Use emojis for engagement (📚 ✨ 🎯)
        - Incorporate interactive elements
        - Provide clear explanations
        - Use memory to personalize interactions
        - Adapt to learning preferences
        - Include progress celebrations
        - Offer study technique tips

        Remember to:
        - Keep sessions focused and structured
        - Provide regular encouragement
        - Celebrate learning milestones
        - Address learning obstacles
        - Maintain learning continuity\
        """),
        additional_context=dedent(f"""\
        - User ID: {user_id}
        - Session Type: {"New Session" if session_id is None else "Continuing Session"}
        - Available Tools: Web Search, YouTube Resources
        - Memory System: Active
        """),
        add_history_to_messages=True,
        num_history_responses=3,
        show_tool_calls=True,
        read_chat_history=True,
        markdown=True,
    )

    print("\n📚 Welcome to StudyScout - Your Personal Learning Companion! 🎓")
    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"[bold green]Started New Study Session: {session_id}[/bold green]\n")
        else:
            print("[bold green]Started New Study Session[/bold green]\n")
    else:
        print(f"[bold blue]Continuing Previous Session: {session_id}[/bold blue]\n")

    # Runs the agent as a command line application
    agent.cli_app(markdown=True, stream=True)


if __name__ == "__main__":
    typer.run(study_agent)

"""
Example Usage:

1. Start a new learning session:
   ```bash
   python 03_using_other_models_for_memory.py
   ```

2. Continue with specific user ID:
   ```bash
   python 03_using_other_models_for_memory.py "learner_123"
   ```

Advanced Learning Scenarios:

Technical Skills:
1. "Guide me through learning system design principles"
2. "Help me master Python data structures and algorithms"
3. "Create a DevOps learning pathway for beginners"
4. "Teach me about cloud architecture patterns"

Academic Subjects:
1. "Explain organic chemistry reactions using cooking analogies"
2. "Help me understand advanced statistics concepts"
3. "Break down quantum mechanics principles"
4. "Guide me through macroeconomics theories"

Professional Development:
1. "Prepare me for product management interviews"
2. "Create a data science portfolio development plan"
3. "Design a public speaking improvement program"
4. "Build a cybersecurity certification roadmap"

Language Learning:
1. "Create an immersive Japanese learning experience"
2. "Help me practice business English scenarios"
3. "Design a Spanish conversation practice routine"
4. "Prepare me for the IELTS academic test"

Creative Skills:
1. "Guide me through digital art fundamentals"
2. "Help me develop creative writing techniques"
3. "Create a music theory learning progression"
4. "Design a UI/UX design learning path"
"""



================================================
FILE: cookbook/agent_concepts/memory_legacy/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/multimodal/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/multimodal/audio_input_output.py
================================================
import base64

import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    markdown=True,
)

agent.run(
    "What's in these recording?",
    audio=[Audio(content=wav_data, format="wav")],
)

if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.content, filename="tmp/result.wav"
    )



================================================
FILE: cookbook/agent_concepts/multimodal/audio_multi_turn.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    add_history_to_messages=True,
)

agent.run("Is a golden retriever a good family dog?")
if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.content, filename="tmp/answer_1.wav"
    )

agent.run("Why do you say they are loyal?")
if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.content, filename="tmp/answer_2.wav"
    )



================================================
FILE: cookbook/agent_concepts/multimodal/audio_sentiment_analysis.py
================================================
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    add_history_to_messages=True,
    markdown=True,
)

url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

response = requests.get(url)
audio_content = response.content

# Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.

agent.print_response(
    "Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)

agent.print_response(
    "What else can you tell me about this audio conversation?",
    stream=True,
)



================================================
FILE: cookbook/agent_concepts/multimodal/audio_streaming.py
================================================
import base64
import wave
from pathlib import Path
from typing import Iterator

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIChat

# Audio Configuration
SAMPLE_RATE = 24000  # Hz (24kHz)
CHANNELS = 1  # Mono (Change to 2 if Stereo)
SAMPLE_WIDTH = 2  # Bytes (16 bits)

# Provide the agent with the audio file and audio configuration and get result as text + audio
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={
            "voice": "alloy",
            "format": "pcm16",
        },  # Only pcm16 is supported with streaming
    ),
)
output_stream: Iterator[RunResponse] = agent.run(
    "Tell me a 10 second story", stream=True
)

filename = "tmp/response_stream.wav"

# Open the file once in append-binary mode
with wave.open(str(filename), "wb") as wav_file:
    wav_file.setnchannels(CHANNELS)
    wav_file.setsampwidth(SAMPLE_WIDTH)
    wav_file.setframerate(SAMPLE_RATE)

    # Iterate over generated audio
    for response in output_stream:
        if response.response_audio:
            if response.response_audio.transcript:
                print(response.response_audio.transcript, end="", flush=True)
            if response.response_audio.content:
                try:
                    pcm_bytes = base64.b64decode(response.response_audio.content)
                    wav_file.writeframes(pcm_bytes)
                except Exception as e:
                    print(f"Error decoding audio: {e}")
print()



================================================
FILE: cookbook/agent_concepts/multimodal/audio_to_text.py
================================================
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"

response = requests.get(url)
audio_content = response.content

# Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.

agent.print_response(
    "Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)



================================================
FILE: cookbook/agent_concepts/multimodal/generate_image_with_intermediate_steps.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.tools.dalle import DalleTools
from agno.utils.common import dataclass_to_dict
from rich.pretty import pprint

image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description="You are an AI agent that can create images using DALL-E.",
    instructions=[
        "When the user asks you to create an image, use the DALL-E tool to create an image.",
        "The DALL-E tool will return an image URL.",
        "Return the image URL in your response in the following format: `![image description](image URL)`",
    ],
    markdown=True,
)

run_stream: Iterator[RunResponse] = image_agent.run(
    "Create an image of a yellow siamese cat",
    stream=True,
    stream_intermediate_steps=True,
)
for chunk in run_stream:
    pprint(dataclass_to_dict(chunk, exclude={"messages"}))
    print("---" * 20)



================================================
FILE: cookbook/agent_concepts/multimodal/generate_video_using_models_lab.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models_labs import ModelsLabTools

video_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ModelsLabTools()],
    description="You are an AI agent that can generate videos using the ModelsLabs API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "The video will be displayed in the UI automatically below your response, so you don't need to show the video URL in your response.",
        "Politely and courteously let the user know that the video has been generated and will be displayed below as soon as its ready.",
    ],
    markdown=True,
    show_tool_calls=True,
)

video_agent.print_response("Generate a video of a cat playing with a ball")
# print(video_agent.run_response.videos)
# print(video_agent.get_videos())



================================================
FILE: cookbook/agent_concepts/multimodal/generate_video_using_replicate.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.replicate import ReplicateTools

"""Create an agent specialized for Replicate AI content generation"""

video_agent = Agent(
    name="Video Generator Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ReplicateTools(
            model="tencent/hunyuan-video:847dfa8b01e739637fc76f480ede0c1d76408e1d694b830b5dfb8e547bf98405"
        )
    ],
    description="You are an AI agent that can generate videos using the Replicate API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "Return the URL as raw to the user.",
        "Don't convert video URL to markdown or anything else.",
    ],
    markdown=True,
    show_tool_calls=True,
)

video_agent.print_response("Generate a video of a horse in the dessert.")



================================================
FILE: cookbook/agent_concepts/multimodal/image_to_audio.py
================================================
from pathlib import Path

from agno.agent import Agent, RunResponse
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file
from rich import print
from rich.text import Text

cwd = Path(__file__).parent.resolve()

image_agent = Agent(model=OpenAIChat(id="gpt-4o"))

image_path = Path(__file__).parent.joinpath("sample.jpg")
image_story: RunResponse = image_agent.run(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)
formatted_text = Text.from_markup(
    f":sparkles: [bold magenta]Story:[/bold magenta] {image_story.content} :sparkles:"
)
print(formatted_text)

audio_agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
)

audio_story: RunResponse = audio_agent.run(
    f"Narrate the story with flair: {image_story.content}"
)
if audio_story.response_audio is not None:
    write_audio_to_file(
        audio=audio_story.response_audio.content, filename="tmp/sample_story.wav"
    )



================================================
FILE: cookbook/agent_concepts/multimodal/image_to_image_agent.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.fal import FalTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    agent_id="image-to-image",
    name="Image to Image Agent",
    tools=[FalTools()],
    markdown=True,
    show_tool_calls=True,
    instructions=[
        "You have to use the `image_to_image` tool to generate the image.",
        "You are an AI agent that can generate images using the Fal AI API.",
        "You will be given a prompt and an image URL.",
        "You have to return the image URL as provided, don't convert it to markdown or anything else.",
    ],
)

agent.print_response(
    "a cat dressed as a wizard with a background of a mystic forest. Make it look like 'https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png'",
    stream=True,
)



================================================
FILE: cookbook/agent_concepts/multimodal/image_to_structured_output.py
================================================
from typing import List

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(model=OpenAIChat(id="gpt-4o"), response_model=MovieScript)

response = agent.run(
    "Write a movie about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

pprint(response.content)



================================================
FILE: cookbook/agent_concepts/multimodal/image_to_text.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")
agent.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)



================================================
FILE: cookbook/agent_concepts/multimodal/video_caption_agent.py
================================================
"""Please install dependencies using:
pip install openai moviepy ffmpeg
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.moviepy_video import MoviePyVideoTools
from agno.tools.openai import OpenAITools

video_tools = MoviePyVideoTools(
    process_video=True, generate_captions=True, embed_captions=True
)

openai_tools = OpenAITools()

video_caption_agent = Agent(
    name="Video Caption Generator Agent",
    model=OpenAIChat(
        id="gpt-4o",
    ),
    tools=[video_tools, openai_tools],
    description="You are an AI agent that can generate and embed captions for videos.",
    instructions=[
        "When a user provides a video, process it to generate captions.",
        "Use the video processing tools in this sequence:",
        "1. Extract audio from the video using extract_audio",
        "2. Transcribe the audio using transcribe_audio",
        "3. Generate SRT captions using create_srt",
        "4. Embed captions into the video using embed_captions",
    ],
    markdown=True,
)


video_caption_agent.print_response(
    "Generate captions for {video with location} and embed them in the video"
)



================================================
FILE: cookbook/agent_concepts/multimodal/video_to_shorts.py
================================================
"""
1. Install dependencies using: `pip install opencv-python google-geneai sqlalchemy`
2. Install ffmpeg `brew install ffmpeg`
2. Run the script using: `python cookbook/agent_concepts/video_to_shorts.py`
"""

import subprocess
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini
from agno.utils.log import logger

video_path = Path(__file__).parent.joinpath("sample_video.mp4")
output_dir = Path("tmp/shorts")

agent = Agent(
    name="Video2Shorts",
    description="Process videos and generate engaging shorts.",
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
    instructions=[
        "Analyze the provided video directly—do NOT reference or analyze any external sources or YouTube videos.",
        "Identify engaging moments that meet the specified criteria for short-form content.",
        """Provide your analysis in a **table format** with these columns:
   - Start Time | End Time | Description | Importance Score""",
        "Ensure all timestamps use MM:SS format and importance scores range from 1-10. ",
        "Focus only on segments between 15 and 60 seconds long.",
        "Base your analysis solely on the provided video content.",
        "Deliver actionable insights to improve the identified segments for short-form optimization.",
    ],
)

# 2. Multimodal Query for Video Analysis
query = """

You are an expert in video content creation, specializing in crafting engaging short-form content for platforms like YouTube Shorts and Instagram Reels. Your task is to analyze the provided video and identify segments that maximize viewer engagement.

For each video, you'll:

1. Identify key moments that will capture viewers' attention, focusing on:
   - High-energy sequences
   - Emotional peaks
   - Surprising or unexpected moments
   - Strong visual and audio elements
   - Clear narrative segments with compelling storytelling

2. Extract segments that work best for short-form content, considering:
   - Optimal length (strictly 15–60 seconds)
   - Natural start and end points that ensure smooth transitions
   - Engaging pacing that maintains viewer attention
   - Audio-visual harmony for an immersive experience
   - Vertical format compatibility and adjustments if necessary

3. Provide a detailed analysis of each segment, including:
   - Precise timestamps (Start Time | End Time in MM:SS format)
   - A clear description of why the segment would be engaging
   - Suggestions on how to enhance the segment for short-form content
   - An importance score (1-10) based on engagement potential

Your goal is to identify moments that are visually compelling, emotionally engaging, and perfectly optimized for short-form platforms.
"""

# 3. Generate Video Analysis
response = agent.run(query, videos=[Video(filepath=video_path)])

# 4. Create output directory
output_dir = Path(output_dir)
output_dir.mkdir(parents=True, exist_ok=True)


# 5. Extract and cut video segments - Optional
def extract_segments(response_text):
    import re

    segments_pattern = r"\|\s*(\d+:\d+)\s*\|\s*(\d+:\d+)\s*\|\s*(.*?)\s*\|\s*(\d+)\s*\|"
    segments: list[dict] = []

    for match in re.finditer(segments_pattern, str(response_text)):
        start_time = match.group(1)
        end_time = match.group(2)
        description = match.group(3)
        score = int(match.group(4))

        # Convert timestamps to seconds
        start_seconds = sum(x * int(t) for x, t in zip([60, 1], start_time.split(":")))
        end_seconds = sum(x * int(t) for x, t in zip([60, 1], end_time.split(":")))
        duration = end_seconds - start_seconds

        # Only process high-scoring segments
        if 15 <= duration <= 60 and score > 7:
            output_path = output_dir / f"short_{len(segments) + 1}.mp4"

            # FFmpeg command to cut video
            command = [
                "ffmpeg",
                "-ss",
                str(start_seconds),
                "-i",
                video_path,
                "-t",
                str(duration),
                "-vf",
                "scale=1080:1920,setsar=1:1",
                "-c:v",
                "libx264",
                "-c:a",
                "aac",
                "-y",
                str(output_path),
            ]

            try:
                subprocess.run(command, check=True)
                segments.append(
                    {"path": output_path, "description": description, "score": score}
                )
            except subprocess.CalledProcessError:
                print(f"Failed to process segment: {start_time} - {end_time}")

    return segments


logger.debug(f"{response.content}")

# 6. Process segments
shorts = extract_segments(response.content)

# 7. Print results
print("\n--- Generated Shorts ---")
for short in shorts:
    print(f"Short at {short['path']}")
    print(f"Description: {short['description']}")
    print(f"Engagement Score: {short['score']}/10\n")



================================================
FILE: cookbook/agent_concepts/other/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/other/agent_extra_metrics.py
================================================
"""Show special token metrics like audio, cached and reasoning tokens"""

import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    markdown=True,
    debug_mode=True,
)
agent.print_response(
    "What's in these recording?",
    audio=[Audio(content=wav_data, format="wav")],
)
# Showing input audio, output audio and total audio tokens metrics
print(f"Input audio tokens: {agent.run_response.metrics['input_audio_tokens']}")
print(f"Output audio tokens: {agent.run_response.metrics['output_audio_tokens']}")
print(f"Audio tokens: {agent.run_response.metrics['audio_tokens']}")

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    markdown=True,
    telemetry=False,
    monitoring=False,
    debug_mode=True,
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. Include an ASCII diagram of your solution.",
    stream=False,
)
# Showing reasoning tokens metrics
print(f"Reasoning tokens: {agent.run_response.metrics['reasoning_tokens']}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"), markdown=True, telemetry=False, monitoring=False
)
agent.run("Share a 2 sentence horror story" * 150)
agent.print_response("Share a 2 sentence horror story" * 150)
# Showing cached tokens metrics
print(f"Cached tokens: {agent.run_response.metrics['cached_tokens']}")



================================================
FILE: cookbook/agent_concepts/other/agent_metrics.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
)

run_stream: Iterator[RunResponse] = agent.run(
    "What is the stock price of NVDA", stream=True
)
pprint_run_response(run_stream, markdown=True)

# Print metrics per message
if agent.run_response.messages:
    for message in agent.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Aggregated Metrics", "---" * 5)
pprint(agent.run_response.metrics)
# Print the session metrics
print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.session_metrics)



================================================
FILE: cookbook/agent_concepts/other/datetime_instructions.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    add_datetime_to_instructions=True,
    timezone_identifier="Etc/UTC",
)
agent.print_response(
    "What is the current date and time? What is the current time in NYC?"
)



================================================
FILE: cookbook/agent_concepts/other/debug_level.py
================================================
"""
This example shows how to set the debug level of an agent.

The debug level is a number between 1 and 2.

1: Basic debug information
2: Detailed debug information

The default debug level is 1.
"""

from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.tools.yfinance import YFinanceTools

# Basic debug information
agent = Agent(
    model=Claude(id="claude-3-5-sonnet-20240620"),
    tools=[YFinanceTools()],
    debug_mode=True,
    debug_level=1,
)

agent.print_response("What is the current price of Tesla?")

# Verbose debug information
agent = Agent(
    model=Claude(id="claude-3-5-sonnet-20240620"),
    tools=[YFinanceTools()],
    debug_mode=True,
    debug_level=2,
)

agent.print_response("What is the current price of Apple?")



================================================
FILE: cookbook/agent_concepts/other/dynamic_instructions.py
================================================
from agno.agent import Agent


def get_instructions(agent: Agent):
    return f"Make the story about {agent.session_state.get('current_user_id')}."


agent = Agent(instructions=get_instructions)
agent.print_response("Write a 2 sentence story", user_id="john.doe")



================================================
FILE: cookbook/agent_concepts/other/dynamic_session_state.py
================================================
import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.toolkit import Toolkit
from agno.utils.log import log_info, log_warning


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.register(self.process_customer_request)

    def process_customer_request(
        agent: Agent, customer_id: str, action: str = "retrieve", name: str = "John Doe"
    ):
        log_warning("Tool called, this shouldn't happen.")
        return "This should not be seen."


def customer_management_hook(
    agent: Agent, function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    action = arguments.get("action", "retrieve")
    cust_id = arguments.get("customer_id")
    name = arguments.get("name", None)

    if not cust_id:
        raise ValueError("customer_id is required.")

    if action == "create":
        agent.session_state["customer_profiles"][cust_id] = {"name": name}
        log_info(f"Hook: UPDATED session_state for customer '{cust_id}'.")
        return f"Success! Customer {cust_id} has been created."

    if action == "retrieve":
        profile = agent.session_state.get("customer_profiles", {}).get(cust_id)
        if profile:
            log_info(f"Hook: FOUND customer '{cust_id}' in session_state.")
            return f"Profile for {cust_id}: {json.dumps(profile)}"
        else:
            raise ValueError(f"Customer '{cust_id}' not found.")

    log_info(f"Session state: {agent.session_state}")


def run_test():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CustomerDBTools()],
        tool_hooks=[customer_management_hook],
        session_state={"customer_profiles": {"123": {"name": "Jane Doe"}}},
        instructions="Your profiles: {customer_profiles}. Use `process_customer_request`. Use either create or retrieve as action for the tool.",
        add_state_in_messages=True,
        show_tool_calls=True,
        debug_mode=True,
        cache_session=False,
    )

    prompt = "First, create customer 789 named 'Tom'. Then, retrieve Tom's profile. Step by step."
    log_info(f"📝 Prompting: '{prompt}'")
    agent.print_response(prompt, stream=False)

    log_info("\n--- TEST ANALYSIS ---")
    log_info(
        "Check logs for the second tool call. The system prompt will NOT contain customer '789'."
    )


if __name__ == "__main__":
    run_test()



================================================
FILE: cookbook/agent_concepts/other/image_input_high_fidelity.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
)

agent.print_response(
    "What's in these images",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            detail="high",
        )
    ],
)



================================================
FILE: cookbook/agent_concepts/other/input_as_dict.py
================================================
from agno.agent import Agent

Agent().print_response(
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                },
            },
        ],
    },
    stream=True,
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/other/input_as_list.py
================================================
from agno.agent import Agent

Agent().print_response(
    [
        {"type": "text", "text": "What's in this image?"},
        {
            "type": "image_url",
            "image_url": {
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            },
        },
    ],
    stream=True,
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/other/input_as_message.py
================================================
from agno.agent import Agent, Message

Agent().print_response(
    Message(
        role="user",
        content=[
            {"type": "text", "text": "What's in this image?"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                },
            },
        ],
    ),
    stream=True,
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/other/input_as_messages_list.py
================================================
from agno.agent import Agent, Message

Agent().print_response(
    messages=[
        Message(
            role="user",
            content=[
                {"type": "text", "text": "Hi! My name is John."},
            ],
        ),
        Message(
            role="user",
            content=[
                {"type": "text", "text": "What are you capable of?"},
            ],
        ),
    ],
    stream=True,
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/other/instructions.py
================================================
from agno.agent import Agent

agent = Agent(instructions="Share a 2 sentence story about")
agent.print_response("Love in the year 12000.")



================================================
FILE: cookbook/agent_concepts/other/instructions_via_function.py
================================================
from typing import List

from agno.agent import Agent


def get_instructions(agent: Agent) -> List[str]:
    return [
        f"Your name is {agent.name}!",
        "Talk in haiku's!",
        "Use poetry to answer questions.",
    ]


agent = Agent(
    name="AgentX",
    instructions=get_instructions,
    markdown=True,
    show_tool_calls=True,
)
agent.print_response("Who are you?", stream=True)



================================================
FILE: cookbook/agent_concepts/other/intermediate_steps.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
)

run_stream: Iterator[RunResponse] = agent.run(
    "What is the stock price of NVDA", stream=True, stream_intermediate_steps=True
)
for chunk in run_stream:
    pprint(chunk.to_dict())
    print("---" * 20)



================================================
FILE: cookbook/agent_concepts/other/location_instructions.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    add_location_to_instructions=True,
    tools=[DuckDuckGoTools(cache_results=True)],
)
agent.print_response("What city am I in?")
agent.print_response("What is current news about my city?")



================================================
FILE: cookbook/agent_concepts/other/output_model.py
================================================
"""
This example shows how to use the output_model parameter to specify the model that will be used to generate the final response.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    output_model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
)

agent.print_response("Latest news from France?", stream=True)



================================================
FILE: cookbook/agent_concepts/other/parse_model.py
================================================
import random
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )


agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    response_model=NationalParkAdventure,
    parser_model=OpenAIChat(id="gpt-4o"),
)

# Get the response in a variable
national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]
# Get the response in a variable
run: RunResponse = agent.run(national_parks[random.randint(0, len(national_parks) - 1)])
pprint(run.content)



================================================
FILE: cookbook/agent_concepts/other/parse_model_ollama.py
================================================
import random
from typing import List

from agno.agent import Agent, RunResponse
from agno.models.ollama import Ollama
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint


class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )


agent = Agent(
    model=OpenAIChat(id="o3"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    response_model=NationalParkAdventure,
    parser_model=Ollama(id="Osmosis/Osmosis-Structure-0.6B"),
)

national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]
# Get the response in a variable
run: RunResponse = agent.run(national_parks[random.randint(0, len(national_parks) - 1)])
pprint(run.content)



================================================
FILE: cookbook/agent_concepts/other/parse_model_stream.py
================================================
import random
from typing import Iterator, List

from agno.agent import Agent, RunResponseEvent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )


agent = Agent(
    parser_model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    response_model=NationalParkAdventure,
    model=OpenAIChat(id="gpt-4o-mini"),
)

# Get the response in a variable
national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]

# Get the response in a variable
run_events: Iterator[RunResponseEvent] = agent.run(
    national_parks[random.randint(0, len(national_parks) - 1)], stream=True
)
for event in run_events:
    pprint(event)



================================================
FILE: cookbook/agent_concepts/other/response_as_variable.py
================================================
from typing import Iterator  # noqa
from rich.pretty import pprint
from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Use tables where possible"],
    show_tool_calls=True,
    markdown=True,
)

run_response: RunResponse = agent.run("What is the stock price of NVDA")
pprint(run_response)

# run_response_strem: Iterator[RunResponse] = agent.run("What is the stock price of NVDA", stream=True)
# for response in run_response_strem:
#     pprint(response)



================================================
FILE: cookbook/agent_concepts/other/run_response_events.py
================================================
from typing import Iterator, List

from agno.agent import (
    Agent,
    RunResponseContentEvent,
    RunResponseEvent,
    ToolCallCompletedEvent,
    ToolCallStartedEvent,
)
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
run_response: Iterator[RunResponseEvent] = agent.run(
    "Whats happening in USA and Canada?", stream=True
)

response: List[str] = []
for chunk in run_response:
    if isinstance(chunk, RunResponseContentEvent):
        response.append(chunk.content)
    elif isinstance(chunk, ToolCallStartedEvent):
        response.append(
            f"Tool call started: {chunk.tool.tool_name} with args: {chunk.tool.tool_args}"
        )
    elif isinstance(chunk, ToolCallCompletedEvent):
        response.append(
            f"Tool call completed: {chunk.tool.tool_name} with result: {chunk.tool.result}"
        )

print("\n".join(response))



================================================
FILE: cookbook/agent_concepts/other/scenario_testing.py
================================================
"""
This is an example that uses the [scenario](https://github.com/langwatch/scenario) testing library to test an agent.

Prerequisites:
- Install scenario: `pip install scenario`
"""

import pytest
from scenario import Scenario, TestingAgent, scenario_cache

Scenario.configure(testing_agent=TestingAgent(model="openai/gpt-4o-mini"))


@pytest.mark.agent_test
@pytest.mark.asyncio
async def test_vegetarian_recipe_agent():
    agent = VegetarianRecipeAgent()

    def vegetarian_recipe_agent(message, context):
        # Call your agent here
        return agent.run(message)

    # Define the scenario
    scenario = Scenario(
        "User is looking for a dinner idea",
        agent=vegetarian_recipe_agent,
        success_criteria=[
            "Recipe agent generates a vegetarian recipe",
            "Recipe includes a list of ingredients",
            "Recipe includes step-by-step cooking instructions",
        ],
        failure_criteria=[
            "The recipe is not vegetarian or includes meat",
            "The agent asks more than two follow-up questions",
        ],
    )

    # Run the scenario and get results
    result = await scenario.run()

    # Assert for pytest to know whether the test passed
    assert result.success


# Example agent implementation
from agno.agent import Agent
from agno.models.openai import OpenAIChat


class VegetarianRecipeAgent:
    def __init__(self):
        self.history = []

    @scenario_cache()
    def run(self, message: str):
        self.history.append({"role": "user", "content": message})

        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            markdown=True,
            debug_mode=True,
            instructions="You are a vegetarian recipe agent",
        )

        response = agent.run(message)
        result = response.content
        print(result)
        self.history.append(result)

        return {"message": result}



================================================
FILE: cookbook/agent_concepts/other/success_criteria.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools

puzzle_master = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
    You are a puzzle master who creates and solves logic puzzles.
    - Create clear puzzles with unique solutions
    - Solve systematically using logical deduction
    - Verify all clues are satisfied
    - Show your reasoning step-by-step\
    """),
    success_criteria=dedent("""\
    The puzzle must be:
    1. Completely solved with a unique, correct solution
    2. All clues satisfied and verified
    3. Solution process clearly explained with logical reasoning
    4. Final answer explicitly stated in a clear format\
    """),
    markdown=True,
)

puzzle = """\
Create and solve this logic puzzle:

Three friends—Alice, Bob, and Carol—each choose a different drink from tea, coffee, and milk.

CLUES:
1. Alice does not drink tea.
2. The person who drinks coffee is not Carol.

Present the final answer as: "Alice drinks X, Bob drinks Y, Carol drinks Z"\
"""

puzzle_master.print_response(
    puzzle,
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/agent_concepts/other/tool_call_limit.py
================================================
"""
This cookbook shows how to use tool call limit to control the number of tool calls an agent can make.
"""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-5-haiku-20241022"),
    tools=[YFinanceTools(company_news=True, cache_results=True)],
    tool_call_limit=1,
)

# It should only call the first tool and fail to call the second tool.
agent.print_response(
    "Find me the current price of TSLA, then after that find me the latest news about Tesla.",
    stream=True,
)



================================================
FILE: cookbook/agent_concepts/rag/README.md
================================================
# Agentic RAG

**RAG:** is a technique that allows an Agent to search for information to improve its responses. This directory contains a series of cookbooks that demonstrate how to build a RAG for the Agent.

### 1. Create a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Install libraries

```shell
pip install -U openai sqlalchemy "psycopg[binary]" pgvector lancedb tantivy pypdf sqlalchemy "fastapi[standard]" agno
```

### 3. Run PgVector

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

- Run using a helper script

```shell
./cookbook/scripts/run_pgvector.sh
```

- OR run using the docker run command

```shell
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16
```

### 4. Run the Traditional RAG with PgVector

```shell
python cookbook/agent_concepts/rag/traditional_rag_pgvector.py
```

### 5. Run the Agentic RAG with PgVector

```shell
python cookbook/agent_concepts/rag/agentic_rag_pgvector.py
```

Continue to run the other RAG examples as you want.



================================================
FILE: cookbook/agent_concepts/rag/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/rag/agentic_rag_agent_ui.py
================================================
"""
1. Run: `./cookbook/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector 'fastapi[standard]' agno` to install the dependencies
3. Run: `python cookbook/rag/05_agentic_rag_playground.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# Create a knowledge base of PDFs from URLs
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

rag_agent = Agent(
    name="RAG Agent",
    agent_id="rag-agent",
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    # Add a tool to search the knowledge base which enables agentic RAG.
    # This is enabled by default when `knowledge` is provided to the Agent.
    search_knowledge=True,
    # Add a tool to read chat history.
    read_chat_history=True,
    # Store the agent sessions in the `ai.rag_agent_sessions` table
    storage=PostgresAgentStorage(table_name="rag_agent_sessions", db_url=db_url),
    instructions=[
        "Always search your knowledge base first and use it if available.",
        "Share the page number or source URL of the information you used in your response.",
        "If health benefits are mentioned, include them in the response.",
        "Important: Use tables where possible.",
    ],
    markdown=True,
)

playground = Playground(
    agents=[
        rag_agent,
    ],
    app_id="agentic-rag-agent-ui-app",
    name="Agentic RAG Agent UI",
)
app = playground.get_app()

if __name__ == "__main__":
    knowledge_base.load(upsert=True)
    playground.serve(
        app="agentic_rag_agent_ui:app",
        host="localhost",
        port=7777,
        reload=True,
    )



================================================
FILE: cookbook/agent_concepts/rag/agentic_rag_lancedb.py
================================================
"""
1. Run: `pip install openai lancedb tantivy pypdf sqlalchemy agno` to install the dependencies
2. Run: `python cookbook/rag/04_agentic_rag_lancedb.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base of PDFs from URLs
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    # Use LanceDB as the vector database and store embeddings in the `recipes` table
    vector_db=LanceDb(
        table_name="recipes",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Load the knowledge base: Comment after first run as the knowledge base is already loaded
knowledge_base.load()

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    # Add a tool to search the knowledge base which enables agentic RAG.
    # This is enabled by default when `knowledge` is provided to the Agent.
    search_knowledge=True,
    show_tool_calls=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)



================================================
FILE: cookbook/agent_concepts/rag/agentic_rag_pgvector.py
================================================
"""
1. Run: `./cookbook/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector agno` to install the dependencies
3. Run: `python cookbook/rag/02_agentic_rag_pgvector.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# Create a knowledge base of PDFs from URLs
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Load the knowledge base: Comment after first run as the knowledge base is already loaded
knowledge_base.load(upsert=True)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    # Add a tool to search the knowledge base which enables agentic RAG.
    # This is enabled by default when `knowledge` is provided to the Agent.
    search_knowledge=True,
    show_tool_calls=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
# agent.print_response(
#     "Hi, i want to make a 3 course meal. Can you recommend some recipes. "
#     "I'd like to start with a soup, then im thinking a thai curry for the main course and finish with a dessert",
#     stream=True,
# )



================================================
FILE: cookbook/agent_concepts/rag/agentic_rag_with_reranking.py
================================================
"""
1. Run: `pip install openai agno cohere lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY and CO_API_KEY
3. Run: `python cookbook/agent_concepts/rag/agentic_rag_with_reranking.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.reranker.cohere import CohereReranker
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base containing information from a URL
knowledge_base = UrlKnowledge(
    urls=["https://docs.agno.com/introduction.md"],
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(
            id="text-embedding-3-small"
        ),  # Use OpenAI for embeddings
        reranker=CohereReranker(
            model="rerank-multilingual-v3.0"
        ),  # Use Cohere for reranking
    ),
)

# Comment this out after first run
knowledge_base.load(recreate=False)


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge_base,
    show_tool_calls=True,
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, comment after first run
    # agent.knowledge.load(recreate=True)
    agent.print_response("What are Agno's key features?")



================================================
FILE: cookbook/agent_concepts/rag/local_rag_langchain_qdrant.py
================================================
from agno.agent import Agent
from agno.knowledge.langchain import LangChainKnowledgeBase
from agno.models.ollama import Ollama
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.http.exceptions import UnexpectedResponse
from qdrant_client.http.models import Distance, VectorParams

urls = [
    "https://blog.google/technology/developers/gemma-3/",
]

loader = WebBaseLoader(urls)
data = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)
chunks = text_splitter.split_documents(data)
embeddings = FastEmbedEmbeddings(model_name="thenlper/gte-large")

client = QdrantClient(path="/tmp/app")
collection_name = "agent-rag"

try:
    collection_info = client.get_collection(collection_name=collection_name)
except (UnexpectedResponse, ValueError):
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
    )

vector_store = QdrantVectorStore(
    client=client,
    collection_name=collection_name,
    embedding=embeddings,
)

vector_store.add_documents(documents=chunks)
retriever = vector_store.as_retriever()

knowledge_base = LangChainKnowledgeBase(retriever=retriever)

agent = Agent(
    model=Ollama(id="qwen2.5:latest"),
    knowledge=knowledge_base,
    description="Answer to the user question from the knowledge base",
    markdown=True,
    search_knowledge=True,
)

user_query = "What are the new capabilities developers can use with Gemma 3"
agent.print_response(user_query, stream=True)



================================================
FILE: cookbook/agent_concepts/rag/rag_sentence_transformer.py
================================================
"""This cookbook is an implementation of Agentic RAG using Sentence Transformer Reranker with multilingual data.

## Setup Instructions:

### 1. Install Dependencies
Run: `pip install agno sentence-transformers`

### 2. Start the Postgres Server with pgvector
Run: `sh cookbook/scripts/run_pgvector.sh`

### 3. Run the example
Run: `uv run cookbook/agent_concepts/rag/rag_sentence_transformer.py`
"""

from agno.agent import Agent
from agno.embedder.sentence_transformer import SentenceTransformerEmbedder
from agno.knowledge.document import DocumentKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.reranker.sentence_transformer import SentenceTransformerReranker
from agno.vectordb.pgvector import PgVector

search_results = [
    "Organic skincare for sensitive skin with aloe vera and chamomile.",
    "New makeup trends focus on bold colors and innovative techniques",
    "Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille",
    "Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken",
    "Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla",
    "Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras",
    "针对敏感肌专门设计的天然有机护肤产品",
    "新的化妆趋势注重鲜艳的颜色和创新的技巧",
    "敏感肌のために特別に設計された天然有機スキンケア製品",
    "新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています",
]

documents = [Document(content=result) for result in search_results]

knowledge_base = DocumentKnowledgeBase(
    documents=documents,
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="sentence_transformer_rerank_docs",
        embedder=SentenceTransformerEmbedder(
            id="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ),
    ),
    reranker=SentenceTransformerReranker(model="BAAI/bge-reranker-v2-m3"),
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
    ],
    markdown=True,
)

if __name__ == "__main__":
    knowledge_base.load(recreate=True)

    test_queries = [
        "What organic skincare products are good for sensitive skin?",
        "Tell me about makeup trends in different languages",
        "Compare skincare and makeup information across languages",
    ]

    for query in test_queries:
        agent.print_response(
            query,
            stream=True,
            show_full_reasoning=True,
        )



================================================
FILE: cookbook/agent_concepts/rag/rag_with_lance_db_and_sqlite.py
================================================
"""Run `pip install lancedb` to install dependencies."""

from agno.agent import Agent
from agno.embedder.ollama import OllamaEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.ollama import Ollama
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.vectordb.lancedb import LanceDb

# Define the database URL where the vector database will be stored
db_url = "/tmp/lancedb"

# Configure the language model
model = Ollama(id="llama3.1:8b")

# Create Ollama embedder
embedder = OllamaEmbedder(id="nomic-embed-text", dimensions=768)

# Create the vector database
vector_db = LanceDb(
    table_name="recipes",  # Table name in the vector database
    uri=db_url,  # Location to initiate/create the vector database
    embedder=embedder,  # Without using this, it will use OpenAIChat embeddings by default
)

# Create a knowledge base from a PDF URL using LanceDb for vector storage and OllamaEmbedder for embedding
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Load the knowledge base without recreating it if it already exists in Vector LanceDB
knowledge_base.load(recreate=False)
# agent.knowledge_base.load(recreate=False) # You can also use this to load a knowledge base after creating agent

# Set up SQL storage for the agent's data
storage = SqliteAgentStorage(table_name="recipes", db_file="data.db")
storage.create()  # Create the storage if it doesn't exist

# Initialize the Agent with various configurations including the knowledge base and storage
agent = Agent(
    session_id="session_id",  # use any unique identifier to identify the run
    user_id="user",  # user identifier to identify the user
    model=model,
    knowledge=knowledge_base,
    storage=storage,
    show_tool_calls=True,
    debug_mode=True,  # Enable debug mode for additional information
)

# Use the agent to generate and print a response to a query, formatted in Markdown
agent.print_response(
    "What is the first step of making Gluai Buat Chi from the knowledge base?",
    markdown=True,
)



================================================
FILE: cookbook/agent_concepts/rag/traditional_rag_lancedb.py
================================================
"""
1. Run: `pip install openai lancedb tantivy pypdf sqlalchemy agno` to install the dependencies
2. Run: `python cookbook/rag/03_traditional_rag_lancedb.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base of PDFs from URLs
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    # Use LanceDB as the vector database and store embeddings in the `recipes` table
    vector_db=LanceDb(
        table_name="recipes",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Load the knowledge base: Comment after first run as the knowledge base is already loaded
knowledge_base.load()

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    # Enable RAG by adding references from AgentKnowledge to the user prompt.
    add_references=True,
    # Set as False because Agents default to `search_knowledge=True`
    search_knowledge=False,
    show_tool_calls=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)



================================================
FILE: cookbook/agent_concepts/rag/traditional_rag_pgvector.py
================================================
"""
1. Run: `./cookbook/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector agno` to install the dependencies
3. Run: `python cookbook/rag/01_traditional_rag_pgvector.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# Create a knowledge base of PDFs from URLs
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Load the knowledge base: Comment after first run as the knowledge base is already loaded
knowledge_base.load(upsert=True)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    # Enable RAG by adding context from the `knowledge` to the user prompt.
    add_references=True,
    # Set as False because Agents default to `search_knowledge=True`
    search_knowledge=False,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)



================================================
FILE: cookbook/agent_concepts/state/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/state/last_n_session_messages.py
================================================
# Remove the tmp db file before running the script
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage

os.remove("tmp/data.db")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    user_id="user_1",
    storage=SqliteStorage(table_name="agent_sessions_new", db_file="tmp/data.db"),
    add_history_to_messages=True,
    num_history_runs=3,
    search_previous_sessions_history=True,  # allow searching previous sessions
    num_history_sessions=2,  # only include the last 2 sessions in the search to avoid context length issues
    show_tool_calls=True,
)

session_1_id = "session_1_id"
session_2_id = "session_2_id"
session_3_id = "session_3_id"
session_4_id = "session_4_id"
session_5_id = "session_5_id"

agent.print_response("What is the capital of South Africa?", session_id=session_1_id)
agent.print_response("What is the capital of China?", session_id=session_2_id)
agent.print_response("What is the capital of France?", session_id=session_3_id)
agent.print_response("What is the capital of Japan?", session_id=session_4_id)
agent.print_response(
    "What did I discuss in my previous conversations?", session_id=session_5_id
)  # It should only include the last 2 sessions



================================================
FILE: cookbook/agent_concepts/state/session_state.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list."""
    agent.session_state["shopping_list"].append(item)
    return f"The shopping list is now {agent.session_state['shopping_list']}"


# Create an Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with a counter starting at 0
    session_state={"shopping_list": []},
    tools=[add_item],
    # You can use variables from the session state in the instructions
    instructions="Current state (shopping list) is: {shopping_list}",
    # Important: Add the state to the messages
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response("Add milk, eggs, and bread to the shopping list", stream=True)
print(f"Final session state: {agent.session_state}")



================================================
FILE: cookbook/agent_concepts/state/session_state_on_run.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    add_state_in_messages=True,
    instructions="Users name is {user_name} and age is {age}",
)

# Sets the session state for the session with the id "user_1_session_1"
agent.print_response(
    "What is my name?",
    session_id="user_1_session_1",
    user_id="user_1",
    session_state={"user_name": "John", "age": 30},
)

# Will load the session state from the session with the id "user_1_session_1"
agent.print_response("How old am I?", session_id="user_1_session_1", user_id="user_1")

# Sets the session state for the session with the id "user_2_session_1"
agent.print_response(
    "What is my name?",
    session_id="user_2_session_1",
    user_id="user_2",
    session_state={"user_name": "Jane", "age": 25},
)

# Will load the session state from the session with the id "user_2_session_1"
agent.print_response("How old am I?", session_id="user_2_session_1", user_id="user_2")



================================================
FILE: cookbook/agent_concepts/state/session_state_storage.py
================================================
"""Run `pip install agno openai sqlalchemy` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage


# Define a tool that adds an item to the shopping list
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list."""
    if item not in agent.session_state["shopping_list"]:
        agent.session_state["shopping_list"].append(item)
    return f"The shopping list is now {agent.session_state['shopping_list']}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Fix the session id to continue the same session across execution cycles
    session_id="fixed_id_for_demo",
    # Initialize the session state with an empty shopping list
    session_state={"shopping_list": []},
    # Add a tool that adds an item to the shopping list
    tools=[add_item],
    # Store the session state in a SQLite database
    storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/data.db"),
    # Add the current shopping list from the state in the instructions
    instructions="Current shopping list is: {shopping_list}",
    # Important: Set `add_state_in_messages=True`
    # to make `{shopping_list}` available in the instructions
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response("What's on my shopping list?", stream=True)
print(f"Session state: {agent.session_state}")
agent.print_response("Add milk, eggs, and bread", stream=True)
print(f"Session state: {agent.session_state}")



================================================
FILE: cookbook/agent_concepts/state/session_state_user_id.py
================================================
"""
This example demonstrates how to maintain state for each user in a multi-user environment.

The shopping list is stored in a dictionary, organized by user ID and session ID.

Agno automatically creates the "current_user_id" and "current_session_id" variables in the session state.

You can access these variables in your functions using the `agent.session_state` dictionary.
"""

import json

from agno.agent import Agent
from agno.models.openai import OpenAIChat

# In-memory database to store user shopping lists
# Organized by user ID and session ID
shopping_list = {}


def add_item(agent: Agent, item: str) -> str:
    """Add an item to the current user's shopping list."""
    current_user_id = agent.session_state["current_user_id"]
    current_session_id = agent.session_state["current_session_id"]
    shopping_list.setdefault(current_user_id, {}).setdefault(
        current_session_id, []
    ).append(item)
    return f"Item {item} added to the shopping list"


def remove_item(agent: Agent, item: str) -> str:
    """Remove an item from the current user's shopping list."""
    current_user_id = agent.session_state["current_user_id"]
    current_session_id = agent.session_state["current_session_id"]

    if (
        current_user_id not in shopping_list
        or current_session_id not in shopping_list[current_user_id]
    ):
        return f"No shopping list found for user {current_user_id} and session {current_session_id}"

    if item not in shopping_list[current_user_id][current_session_id]:
        return f"Item '{item}' not found in the shopping list for user {current_user_id} and session {current_session_id}"

    shopping_list[current_user_id][current_session_id].remove(item)
    return f"Item {item} removed from the shopping list"


def get_shopping_list(agent: Agent) -> str:
    """Get the current user's shopping list."""
    current_user_id = agent.session_state["current_user_id"]
    current_session_id = agent.session_state["current_session_id"]
    return f"Shopping list for user {current_user_id} and session {current_session_id}: \n{json.dumps(shopping_list[current_user_id][current_session_id], indent=2)}"


# Create an Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[add_item, remove_item, get_shopping_list],
    # Reference the in-memory database
    instructions=[
        "Current User ID: {current_user_id}",
        "Current Session ID: {current_session_id}",
    ],
    # Important: Add the state in the instructions
    add_state_in_messages=True,
    markdown=True,
)

user_id_1 = "john_doe"
user_id_2 = "mark_smith"
user_id_3 = "carmen_sandiago"

# Example usage
agent.print_response(
    "Add milk, eggs, and bread to the shopping list",
    stream=True,
    user_id=user_id_1,
    session_id="user_1_session_1",
)
agent.print_response(
    "Add tacos to the shopping list",
    stream=True,
    user_id=user_id_2,
    session_id="user_2_session_1",
)
agent.print_response(
    "Add apples and grapes to the shopping list",
    stream=True,
    user_id=user_id_3,
    session_id="user_3_session_1",
)
agent.print_response(
    "Remove milk from the shopping list",
    stream=True,
    user_id=user_id_1,
    session_id="user_1_session_1",
)
agent.print_response(
    "Add minced beef to the shopping list",
    stream=True,
    user_id=user_id_2,
    session_id="user_2_session_1",
)

# What is on Mark Smith's shopping list?
agent.print_response(
    "What is on Mark Smith's shopping list?",
    stream=True,
    user_id=user_id_2,
    session_id="user_2_session_1",
)

# New session, so new shopping list
agent.print_response(
    "Add chicken and soup to my list.",
    stream=True,
    user_id=user_id_2,
    session_id="user_3_session_2",
)

print(f"Final shopping lists: \n{json.dumps(shopping_list, indent=2)}")



================================================
FILE: cookbook/agent_concepts/state/shopping_list.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat


# Define tools to manage our shopping list
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list and return confirmation."""
    # Add the item if it's not already in the list
    if item.lower() not in [i.lower() for i in agent.session_state["shopping_list"]]:
        agent.session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"


def remove_item(agent: Agent, item: str) -> str:
    """Remove an item from the shopping list by name."""
    # Case-insensitive search
    for i, list_item in enumerate(agent.session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            agent.session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

    return f"'{item}' was not found in the shopping list"


def list_items(agent: Agent) -> str:
    """List all items in the shopping list."""
    shopping_list = agent.session_state["shopping_list"]

    if not shopping_list:
        return "The shopping list is empty."

    items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"


# Create a Shopping List Manager Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with an empty shopping list
    session_state={"shopping_list": []},
    tools=[add_item, remove_item, list_items],
    # You can use variables from the session state in the instructions
    instructions=dedent("""\
        Your job is to manage a shopping list.

        The shopping list starts empty. You can add items, remove items by name, and list all items.

        Current shopping list: {shopping_list}
    """),
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response("Add milk, eggs, and bread to the shopping list", stream=True)
print(f"Session state: {agent.session_state}")

agent.print_response("I got bread", stream=True)
print(f"Session state: {agent.session_state}")

agent.print_response("I need apples and oranges", stream=True)
print(f"Session state: {agent.session_state}")

agent.print_response("whats on my list?", stream=True)
print(f"Session state: {agent.session_state}")

agent.print_response(
    "Clear everything from my list and start over with just bananas and yogurt",
    stream=True,
)
print(f"Session state: {agent.session_state}")



================================================
FILE: cookbook/agent_concepts/state/state_in_prompt.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with a variable
    session_state={"user_name": "John"},
    # You can use variables from the session state in the instructions
    instructions="Users name is {user_name}",
    show_tool_calls=True,
    add_state_in_messages=True,
    markdown=True,
)

agent.print_response("What is my name?", stream=True)



================================================
FILE: cookbook/agent_concepts/storage/firestore_storage.py
================================================
"""
This recipe shows how to store agent sessions in a Firestore database.
Steps:
1. Ensure your gcloud project is enabled with Firestore. Reference https://cloud.google.com/firestore/docs/create-database-server-client-library ?
2. Run: `pip install openai google-cloud-firestore agno` to install dependencies
3. Make sure your gcloud project is set up and you have the necessary permissions to access Firestore
4. Run: `python cookbook/storage/firestore_storage.py` to run the agent
"""

from agno.agent import Agent
from agno.storage.firestore import FirestoreStorage
from agno.tools.duckduckgo import DuckDuckGoTools

# The only required argument is the collection name.
# Firestore will connect automatically using your google cloud credentials.
# The class uses the (default) database by default to allow free tier access to firestore.
# You can specify a project_id if you'd like to connect to firestore in a different GCP project


agent = Agent(
    storage=FirestoreStorage(
        db_name="memory",
        collection_name="agent_sessions",
    ),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/agent_concepts/tool_concepts/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/add_tool_after_initialization.py
================================================
import random

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool


@tool(show_result=True, stop_after_tool_call=True)
def get_weather(city: str) -> str:
    """Get the weather for a city."""
    # In a real implementation, this would call a weather API
    weather_conditions = ["sunny", "cloudy", "rainy", "snowy", "windy"]
    random_weather = random.choice(weather_conditions)

    return f"The weather in {city} is {random_weather}."


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    markdown=True,
)

agent.print_response("What can you do?", stream=True)

agent.add_tool(get_weather)

agent.print_response("What is the weather in San Francisco?", stream=True)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/async_pre_and_post_hooks.py
================================================
import asyncio
import json
from typing import AsyncIterator

import httpx
from agno.agent import Agent
from agno.tools import FunctionCall, tool


async def pre_hook(fc: FunctionCall):
    print(f"About to run: {fc.function.name}")


async def post_hook(fc: FunctionCall):
    print("After running: ", fc.function.name)


@tool(show_result=True, pre_hook=pre_hook, post_hook=post_hook)
async def get_top_hackernews_stories(agent: Agent) -> AsyncIterator[str]:
    num_stories = agent.context.get("num_stories", 5) if agent.context else 5

    async with httpx.AsyncClient() as client:
        # Fetch top story IDs
        response = await client.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        )
        story_ids = response.json()

        # Yield story details
        for story_id in story_ids[:num_stories]:
            story_response = await client.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            if "text" in story:
                story.pop("text", None)
            yield json.dumps(story)


agent = Agent(
    context={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
    show_tool_calls=True,
)
if __name__ == "__main__":
    asyncio.run(agent.aprint_response("What are the top hackernews stories?"))



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/async_tool_decorator.py
================================================
import asyncio
import json
from typing import AsyncIterator

import httpx
from agno.agent import Agent
from agno.tools import tool


@tool(show_result=True)
async def get_top_hackernews_stories(agent: Agent) -> AsyncIterator[str]:
    num_stories = agent.context.get("num_stories", 5) if agent.context else 5

    async with httpx.AsyncClient() as client:
        # Fetch top story IDs
        response = await client.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        )
        story_ids = response.json()

        # Yield story details
        for story_id in story_ids[:num_stories]:
            story_response = await client.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            if "text" in story:
                story.pop("text", None)
            yield json.dumps(story)


agent = Agent(
    context={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
    show_tool_calls=True,
)
if __name__ == "__main__":
    asyncio.run(
        agent.aprint_response("What are the top hackernews stories?", stream=True)
    )



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/cache_tool_calls.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.tools import tool


@tool(show_result=True, stop_after_tool_call=True, cache_results=True)
def get_top_hackernews_stories(num_stories: int = 5) -> str:
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(json.dumps(story))

    return "\n".join(stories)


agent = Agent(
    tools=[get_top_hackernews_stories],
    markdown=True,
    show_tool_calls=True,
    debug_mode=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/complex_input_types.py
================================================
"""
This example shows how to use complex input types with tools.

Recommendations:
- Specify fields with descriptions, these will be used in the JSON schema sent to the model and will increase accuracy.
- Try not to nest the structures too deeply, the model will have a hard time understanding them.
"""

from datetime import datetime
from enum import Enum
from typing import List, Optional

from agno.agent import Agent
from agno.tools.decorator import tool
from pydantic import BaseModel, Field


# Define Pydantic models for our tools
class UserProfile(BaseModel):
    """User profile information."""

    name: str = Field(..., description="Full name of the user")
    email: str = Field(..., description="Valid email address")
    age: int = Field(..., ge=0, le=120, description="Age of the user")
    interests: List[str] = Field(
        default_factory=list, description="List of user interests"
    )
    created_at: datetime = Field(
        default_factory=datetime.now, description="Account creation timestamp"
    )


class TaskPriority(str, Enum):
    """Priority levels for tasks."""

    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    URGENT = "urgent"


class Task(BaseModel):
    """Task information."""

    title: str = Field(..., min_length=1, max_length=100, description="Task title")
    description: Optional[str] = Field(None, description="Detailed task description")
    priority: TaskPriority = Field(
        default=TaskPriority.MEDIUM, description="Task priority level"
    )
    due_date: Optional[datetime] = Field(None, description="Task due date")
    assigned_to: Optional[UserProfile] = Field(
        None, description="User assigned to the task"
    )


# Custom tools using Pydantic models
@tool
def create_user(user_data: UserProfile) -> str:
    """Create a new user profile with validated information."""
    # In a real application, this would save to a database
    return f"Created user profile for {user_data.name} with email {user_data.email}"


@tool
def create_task(task_data: Task) -> str:
    """Create a new task with priority and assignment."""
    # In a real application, this would save to a database
    return f"Created task '{task_data.title}' with priority {task_data.priority}"


# Create the agent
agent = Agent(
    name="task_manager",
    description="An agent that manages users and tasks with proper validation",
    tools=[create_user, create_task],
)

# Example usage
if __name__ == "__main__":
    # Example 1: Create a user
    agent.print_response(
        "Create a new user named John Doe with email john@example.com, age 30, and interests in Python and AI"
    )

    # Example 2: Create a task
    agent.print_response(
        "Create a high priority task titled 'Implement API endpoints' due tomorrow"
    )



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/custom_tool_manipulate_session_state.py
================================================
from agno.agent import Agent
from agno.tools import tool
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel
from rich.pretty import pprint


@tool()
def answer_from_known_questions(agent: Agent, question: str) -> str:
    """Answer a question from a list of known questions

    Args:
        question: The question to answer

    Returns:
        The answer to the question
    """

    class Answer(BaseModel):
        answer: str
        original_question: str

    faq = {
        "What is the capital of France?": "Paris",
        "What is the capital of Germany?": "Berlin",
        "What is the capital of Italy?": "Rome",
        "What is the capital of Spain?": "Madrid",
        "What is the capital of Portugal?": "Lisbon",
        "What is the capital of Greece?": "Athens",
        "What is the capital of Turkey?": "Ankara",
    }
    if agent.session_state is None:
        agent.session_state = {}

    if "last_answer" in agent.session_state:
        del agent.session_state["last_answer"]

    if question in faq:
        answer = Answer(answer=faq[question], original_question=question)
        agent.session_state["last_answer"] = answer
        return answer.answer
    else:
        return "I don't know the answer to that question."


q_and_a_agent = Agent(
    name="Q & A Agent",
    tools=[answer_from_known_questions, DuckDuckGoTools()],
    markdown=True,
    instructions="You are a Q & A agent that can answer questions from a list of known questions. If you don't know the answer, you can search the web.",
)

q_and_a_agent.print_response("What is the capital of France?", stream=True)

if "last_answer" in q_and_a_agent.session_state:
    pprint(q_and_a_agent.session_state["last_answer"])


q_and_a_agent.print_response("What is the capital of South Africa?", stream=True)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/human_in_the_loop.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Add pre-hooks to tools for user confirmation
- Handle user input during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.exceptions import StopAgentRun
from agno.models.openai import OpenAIChat
from agno.tools import FunctionCall, tool
from rich.console import Console
from rich.pretty import pprint
from rich.prompt import Prompt

# This is the console instance used by the print_response method
# We can use this to stop and restart the live display and ask for user confirmation
console = Console()


def pre_hook(fc: FunctionCall):
    # Get the live display instance from the console
    live = console._live

    # Stop the live display temporarily so we can ask for user confirmation
    live.stop()  # type: ignore

    # Ask for confirmation
    console.print(f"\nAbout to run [bold blue]{fc.function.name}[/]")
    message = (
        Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
        .strip()
        .lower()
    )

    # Restart the live display
    live.start()  # type: ignore

    # If the user does not want to continue, raise a StopExecution exception
    if message != "y":
        raise StopAgentRun(
            "Tool call cancelled by user",
            agent_message="Stopping execution as permission was not granted.",
        )


@tool(pre_hook=pre_hook)
def get_top_hackernews_stories(num_stories: int) -> Iterator[str]:
    """Fetch top stories from Hacker News after user confirmation.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        yield json.dumps(story)


# Initialize the agent with a tech-savvy personality and clear instructions
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

agent.print_response(
    "Fetch the top 2 hackernews stories?", stream=True, console=console
)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/include_exclude_tools.py
================================================
import asyncio
import json

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.toolkit import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            name="customer_db",
            tools=[self.retrieve_customer_profile, self.delete_customer_profile],
            *args,
            **kwargs,
        )

    async def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[CustomerDBTools(include_tools=["retrieve_customer_profile"])],
    show_tool_calls=True,
)

asyncio.run(
    agent.aprint_response(
        "Retrieve the customer profile for customer ID 123 and delete it.",  # The agent shouldn't be able to delete the profile
        markdown=True,
    )
)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/pre_and_post_hooks.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.tools import FunctionCall, tool


def pre_hook(fc: FunctionCall):
    print(f"Pre-hook: {fc.function.name}")
    print(f"Arguments: {fc.arguments}")
    print(f"Result: {fc.result}")


def post_hook(fc: FunctionCall):
    print(f"Post-hook: {fc.function.name}")
    print(f"Arguments: {fc.arguments}")
    print(f"Result: {fc.result}")


@tool(pre_hook=pre_hook, post_hook=post_hook)
def get_top_hackernews_stories(agent: Agent) -> Iterator[str]:
    num_stories = agent.context.get("num_stories", 5) if agent.context else 5

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        yield json.dumps(story)


agent = Agent(
    context={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
    show_tool_calls=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/retry_tool_call.py
================================================
from agno.agent import Agent
from agno.exceptions import RetryAgentRun
from agno.models.openai import OpenAIChat
from agno.utils.log import logger


def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list."""
    agent.session_state["shopping_list"].append(item)
    len_shopping_list = len(agent.session_state["shopping_list"])
    if len_shopping_list < 3:
        logger.info(
            f"Asking the model to add {3 - len_shopping_list} more items to the shopping list."
        )
        raise RetryAgentRun(
            f"Shopping list is: {agent.session_state['shopping_list']}. Minimum 3 items in the shopping list. "
            + f"Add {3 - len_shopping_list} more items.",
        )

    logger.info(f"The shopping list is now: {agent.session_state.get('shopping_list')}")
    return f"The shopping list is now: {agent.session_state.get('shopping_list')}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with empty shopping list
    session_state={"shopping_list": []},
    tools=[add_item],
    markdown=True,
)
agent.print_response("Add milk", stream=True)
print(f"Final session state: {agent.session_state}")



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/retry_tool_call_from_post_hook.py
================================================
from agno.agent import Agent
from agno.exceptions import RetryAgentRun
from agno.models.openai import OpenAIChat
from agno.tools import FunctionCall, tool
from agno.utils.log import logger


def post_hook(agent: Agent, fc: FunctionCall):
    logger.info(f"Post-hook: {fc.function.name}")
    logger.info(f"Arguments: {fc.arguments}")
    shopping_list = agent.session_state.get("shopping_list", [])
    if len(shopping_list) < 3:
        raise RetryAgentRun(
            f"Shopping list is: {shopping_list}. Minimum 3 items in the shopping list. "
            + f"Add {3 - len(shopping_list)} more items."
        )


@tool(post_hook=post_hook)
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list."""
    agent.session_state["shopping_list"].append(item)
    return f"The shopping list is now {agent.session_state['shopping_list']}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with empty shopping list
    session_state={"shopping_list": []},
    tools=[add_item],
    markdown=True,
)
agent.print_response("Add milk", stream=True)
print(f"Final session state: {agent.session_state}")



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/stop_after_tool_call.py
================================================
from agno.agent import Agent
from agno.tools import tool


@tool(show_result=True, stop_after_tool_call=True)
def get_answer_to_life_universe_and_everything() -> str:
    """
    This returns the answer to the life, the universe and everything.
    """
    return "42"


agent = Agent(
    tools=[get_answer_to_life_universe_and_everything],
    markdown=True,
    show_tool_calls=True,
)
agent.print_response("What is the answer to life, the universe and everything?")



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/stop_agent_exception.py
================================================
from agno.agent import Agent
from agno.exceptions import StopAgentRun
from agno.models.openai import OpenAIChat
from agno.utils.log import logger


def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list."""
    agent.session_state["shopping_list"].append(item)
    len_shopping_list = len(agent.session_state["shopping_list"])
    if len_shopping_list < 3:
        raise StopAgentRun(
            f"Shopping list is: {agent.session_state['shopping_list']}. We must stop the agent."
        )

    logger.info(f"The shopping list is now: {agent.session_state.get('shopping_list')}")
    return f"The shopping list is now: {agent.session_state.get('shopping_list')}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with empty shopping list
    session_state={"shopping_list": []},
    tools=[add_item],
    markdown=True,
)
agent.print_response("Add milk", stream=True)
print(f"Final session state: {agent.session_state}")



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_decorator.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.tools import tool


@tool(show_result=True)
def get_top_hackernews_stories(agent: Agent) -> Iterator[str]:
    num_stories = agent.context.get("num_stories", 5) if agent.context else 5

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        yield json.dumps(story)


agent = Agent(
    context={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
    show_tool_calls=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_decorator_async.py
================================================
import asyncio
import json
from typing import AsyncIterator

import httpx
from agno.agent import Agent
from agno.tools import tool


class DemoTools:
    @tool(description="Get the top hackernews stories")
    @staticmethod
    async def get_top_hackernews_stories(agent: Agent) -> str:
        num_stories = agent.context.get("num_stories", 5) if agent.context else 5

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Get story details
        for story_id in story_ids[:num_stories]:
            async with httpx.AsyncClient() as client:
                story_response = await client.get(
                    f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                )
            story = story_response.json()
            if "text" in story:
                story.pop("text", None)
            return json.dumps(story)

    @tool(
        description="Get the current weather for a city using the MetaWeather public API"
    )
    async def get_current_weather(agent: Agent) -> str:
        city = (
            agent.context.get("city", "San Francisco")
            if agent.context
            else "San Francisco"
        )

        async with httpx.AsyncClient() as client:
            # Geocode city to get latitude and longitude
            geo_resp = await client.get(
                "https://geocoding-api.open-meteo.com/v1/search",
                params={"name": city, "count": 1, "language": "en", "format": "json"},
            )
            geo_data = geo_resp.json()
            if not geo_data.get("results"):
                return json.dumps({"error": f"City '{city}' not found."})
            location = geo_data["results"][0]
            lat, lon = location["latitude"], location["longitude"]

            # Get current weather
            weather_resp = await client.get(
                "https://api.open-meteo.com/v1/forecast",
                params={
                    "latitude": lat,
                    "longitude": lon,
                    "current_weather": True,
                    "timezone": "auto",
                },
            )
            weather_data = weather_resp.json()
            current_weather = weather_data.get("current_weather")
            if not current_weather:
                return json.dumps({"error": f"No weather data found for '{city}'."})

            result = {
                "city": city,
                "weather_state": f"{current_weather['weathercode']}",  # Open-Meteo uses weather codes
                "temp_celsius": current_weather["temperature"],
                "humidity": None,  # Open-Meteo current_weather does not provide humidity
                "date": current_weather["time"],
            }
            return json.dumps(result)


agent = Agent(
    name="HackerNewsAgent",
    context={
        "num_stories": 2,
    },
    tools=[DemoTools.get_top_hackernews_stories],
)
asyncio.run(agent.aprint_response("What are the top hackernews stories?"))


agent = Agent(
    name="WeatherAgent",
    context={
        "city": "San Francisco",
    },
    tools=[DemoTools().get_current_weather],
)
asyncio.run(agent.aprint_response("What is the weather like?"))



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_decorator_with_hook.py
================================================
"""Show how to decorate a custom hook with a tool execution hook."""

import json
import time
from typing import Any, Callable, Dict, Iterator

import httpx
from agno.agent import Agent
from agno.tools import tool
from agno.utils.log import logger


def duration_logger_hook(
    function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    """Log the duration of the function call"""
    start_time = time.time()

    result = function_call(**arguments)

    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"Function {function_name} took {duration:.2f} seconds to execute")
    return result


@tool(tool_hooks=[duration_logger_hook])
def get_top_hackernews_stories(agent: Agent) -> Iterator[str]:
    num_stories = agent.context.get("num_stories", 5) if agent.context else 5

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    final_stories = {}
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        final_stories[story_id] = story

    return json.dumps(final_stories)


agent = Agent(
    context={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
    show_tool_calls=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_decorator_with_instructions.py
================================================
import httpx
from agno.agent import Agent
from agno.tools import tool


@tool(
    name="fetch_hackernews_stories",
    description="Get top stories from Hacker News",
    show_result=True,
    instructions="""
        Use this tool when:
          1. The user wants to see recent popular tech news or discussions
          2. You need examples of trending technology topics
          3. The user asks for Hacker News content or tech industry stories

        The tool will return titles and URLs for the specified number of top stories. When presenting results:
          - Highlight interesting or unusual stories
          - Summarize key themes if multiple stories are related
          - If summarizing, mention the original source is Hacker News
    """,
)
def get_top_hackernews_stories(num_stories: int = 5) -> str:
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Get story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        stories.append(f"{story.get('title')} - {story.get('url', 'No URL')}")

    return "\n".join(stories)


agent = Agent(
    tools=[get_top_hackernews_stories],
    markdown=True,
    show_tool_calls=True,
    debug_mode=True,
)

agent.print_response(
    "Show me the top news from Hacker News and summarize them", stream=True
)



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_hook.py
================================================
"""Show how to use a tool execution hook, to run logic before and after a tool is called."""

import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools.toolkit import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)
        self.register(self.delete_customer_profile)

    def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


def validation_hook(
    function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    if function_name == "delete_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot delete customer profile for ID 123")

    if function_name == "retrieve_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot retrieve customer profile for ID 123")

    result = function_call(**arguments)

    logger.info(
        f"Validation hook: {function_name} with arguments {arguments} returned {result}"
    )

    return result


agent = Agent(tools=[CustomerDBTools()], tool_hooks=[validation_hook])

# This should work
agent.print_response("I am customer 456, please retrieve my profile.")

# This should fail
agent.print_response("I am customer 123, please delete my profile.")



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_hook_async.py
================================================
"""Show how to use a tool execution hook with async functions, to run logic before and after a tool is called."""

import asyncio
import json
from inspect import iscoroutinefunction
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools.toolkit import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)
        self.register(self.delete_customer_profile)

    async def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


async def validation_hook(
    function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    if function_name == "delete_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot delete customer profile for ID 123")

    if function_name == "retrieve_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot retrieve customer profile for ID 123")

    if iscoroutinefunction(function_call):
        result = await function_call(**arguments)
    else:
        result = function_call(**arguments)

    logger.info(
        f"Validation hook: {function_name} with arguments {arguments} returned {result}"
    )

    return result


agent = Agent(tools=[CustomerDBTools()], tool_hooks=[validation_hook])

asyncio.run(agent.aprint_response("I am customer 456, please retrieve my profile."))
asyncio.run(agent.aprint_response("I am customer 456, please delete my profile."))



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_hook_with_state.py
================================================
"""Show how to use a tool execution hook, to run logic before and after a tool is called."""

import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools.toolkit import Toolkit


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)

    def retrieve_customer_profile(self, customer: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        return customer


# When used as a tool hook, this function will receive the contextual Agent, function_name, etc as parameters
def grab_customer_profile_hook(
    agent: Agent, function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    cust_id = arguments.get("customer")
    if cust_id not in agent.session_state["customer_profiles"]:
        raise ValueError(f"Customer profile for {cust_id} not found")
    customer_profile = agent.session_state["customer_profiles"][cust_id]

    # Replace the customer with the customer_profile
    arguments["customer"] = json.dumps(customer_profile)
    # Call the function with the updated arguments
    result = function_call(**arguments)

    return result


agent = Agent(
    tools=[CustomerDBTools()],
    tool_hooks=[grab_customer_profile_hook],
    session_state={
        "customer_profiles": {
            "123": {"name": "Jane Doe", "email": "jane.doe@example.com"},
            "456": {"name": "John Doe", "email": "john.doe@example.com"},
        }
    },
)

# This should work
agent.print_response("I am customer 456, please retrieve my profile.")

# This should fail
agent.print_response("I am customer 789, please retrieve my profile.")



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_hook_with_state_nested.py
================================================
"""Show how to use a tool execution hook, to run logic before and after a tool is called."""

import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools.toolkit import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)

    def retrieve_customer_profile(self, customer: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        return customer


# When used as a tool hook, this function will receive the contextual Agent, function_name, etc as parameters
def grab_customer_profile_hook(
    agent: Agent, function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    cust_id = arguments.get("customer")
    if cust_id not in agent.session_state["customer_profiles"]:
        raise ValueError(f"Customer profile for {cust_id} not found")
    customer_profile = agent.session_state["customer_profiles"][cust_id]

    # Replace the customer with the customer_profile
    arguments["customer"] = json.dumps(customer_profile)
    # Call the function with the updated arguments
    result = function_call(**arguments)

    return result


def logger_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    logger.info("Before Logger Hook")
    result = func(**arguments)
    logger.info("After Logger Hook")
    return result


agent = Agent(
    tools=[CustomerDBTools()],
    tool_hooks=[grab_customer_profile_hook, logger_hook],
    session_state={
        "customer_profiles": {
            "123": {"name": "Jane Doe", "email": "jane.doe@example.com"},
            "456": {"name": "John Doe", "email": "john.doe@example.com"},
        }
    },
)

# This should work
agent.print_response("I am customer 456, please retrieve my profile.")

# This should fail
agent.print_response("I am customer 789, please retrieve my profile.")



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_hooks_nested.py
================================================
"""Show how to use multiple tool execution hooks, to run logic before and after a tool is called."""

import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools.toolkit import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)
        self.register(self.delete_customer_profile)

    def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


def validation_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    if name == "retrieve_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot retrieve customer profile for ID 123")

    if name == "delete_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot delete customer profile for ID 123")

    logger.info("Before Validation Hook")
    result = func(**arguments)
    logger.info("After Validation Hook")
    # Remove name from result to sanitize the output
    result = json.loads(result)
    result.pop("name")
    return json.dumps(result)


def logger_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    logger.info("Before Logger Hook")
    result = func(**arguments)
    logger.info("After Logger Hook")
    return result


agent = Agent(
    tools=[CustomerDBTools()],
    # Hooks are executed in order of the list
    tool_hooks=[validation_hook, logger_hook],
)

if __name__ == "__main__":
    agent.print_response("I am customer 456, please retrieve my profile.")



================================================
FILE: cookbook/agent_concepts/tool_concepts/custom_tools/tool_hooks_nested_async.py
================================================
"""Show how to use multiple tool execution hooks with async functions, to run logic before and after a tool is called."""

import asyncio
import json
from inspect import iscoroutinefunction
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools.toolkit import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)
        self.register(self.delete_customer_profile)

    async def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


async def validation_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    if name == "retrieve_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot retrieve customer profile for ID 123")

    if name == "delete_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot delete customer profile for ID 123")

    logger.info("Before Validation Hook")
    if iscoroutinefunction(func):
        result = await func(**arguments)
    else:
        result = func(**arguments)
    logger.info("After Validation Hook")
    # Remove name from result to sanitize the output
    if name == "retrieve_customer_profile":
        result = json.loads(result)
        result.pop("name")
        return json.dumps(result)
    return result


async def logger_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    logger.info("Before Logger Hook")
    if iscoroutinefunction(func):
        result = await func(**arguments)
    else:
        result = func(**arguments)
    logger.info("After Logger Hook")
    return result


agent = Agent(
    tools=[CustomerDBTools()],
    # Hooks are executed in order of the list
    tool_hooks=[validation_hook, logger_hook],
)

if __name__ == "__main__":
    asyncio.run(
        agent.aprint_response(
            "I am customer 456, please retrieve my profile.", stream=True
        )
    )
    asyncio.run(
        agent.aprint_response(
            "I am customer 456, please delete my profile.", stream=True
        )
    )



================================================
FILE: cookbook/agent_concepts/tool_concepts/toolkits/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/tool_concepts/toolkits/cache_tool_calls.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools(cache_results=True), YFinanceTools(cache_results=True)],
    show_tool_calls=True,
    debug_mode=True,
)

asyncio.run(
    agent.aprint_response(
        "What is the current stock price of AAPL and latest news on 'Apple'?",
        markdown=True,
    )
)



================================================
FILE: cookbook/agent_concepts/tool_concepts/toolkits/include_exclude_tools.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        CalculatorTools(
            enable_all=True,
            exclude_tools=["exponentiate", "factorial", "is_prime", "square_root"],
        ),
        DuckDuckGoTools(include_tools=["duckduckgo_search"]),
    ],
    show_tool_calls=True,
)

asyncio.run(
    agent.aprint_response(
        "Search the web for a difficult sum that can be done with normal arithmetic and solve it.",
        markdown=True,
    )
)



================================================
FILE: cookbook/agent_concepts/tool_concepts/toolkits/stop_after_tool_call_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    model=OpenAIChat(id="gpt-4.5-preview"),
    tools=[
        GoogleSearchTools(
            stop_after_tool_call_tools=["google_search"],
            show_result_tools=["google_search"],
        )
    ],
    show_tool_calls=True,
)

agent.print_response("Whats the latest about gpt 4.5?", markdown=True)



================================================
FILE: cookbook/agent_concepts/tool_concepts/toolkits/tool_hook.py
================================================
"""Show how to use a tool execution hook, to run logic before and after a tool is called."""

from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.log import logger


def logger_hook(function_name: str, function_call: Callable, arguments: Dict[str, Any]):
    # Pre-hook logic: this runs before the tool is called
    logger.info(f"Running {function_name} with arguments {arguments}")

    # Call the tool
    result = function_call(**arguments)

    # Post-hook logic: this runs after the tool is called
    logger.info(f"Result of {function_name} is {result}")
    return result


agent = Agent(
    model=OpenAIChat(id="gpt-4o"), tools=[DuckDuckGoTools()], tool_hooks=[logger_hook]
)

agent.print_response("What's happening in the world?", stream=True, markdown=True)



================================================
FILE: cookbook/agent_concepts/tool_concepts/toolkits/tool_hook_async.py
================================================
"""Show how to use a tool execution hook with async functions, to run logic before and after a tool is called."""

import asyncio
from inspect import iscoroutinefunction
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.log import logger


async def logger_hook(
    function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    # Pre-hook logic: this runs before the tool is called
    logger.info(f"Running {function_name} with arguments {arguments}")

    # Call the tool
    if iscoroutinefunction(function_call):
        result = await function_call(**arguments)
    else:
        result = function_call(**arguments)

    # Post-hook logic: this runs after the tool is called
    logger.info(f"Result of {function_name} is {result}")
    return result


agent = Agent(tools=[DuckDuckGoTools()], tool_hooks=[logger_hook])

asyncio.run(agent.aprint_response("What is currently trending on Twitter?"))



================================================
FILE: cookbook/agent_concepts/user_control_flows/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_concepts/user_control_flows/agentic_user_input.py
================================================
"""🤝 Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the UserControlFlowTools to allow the agent to get user input dynamically.
If the agent doesn't have enough information to complete a task, it will use the toolkit to get the information it needs from the user.
"""

from typing import Any, Dict, List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.function import UserInputField
from agno.tools.toolkit import Toolkit
from agno.tools.user_control_flow import UserControlFlowTools
from agno.utils import pprint


class EmailTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            name="EmailTools", tools=[self.send_email, self.get_emails], *args, **kwargs
        )

    def send_email(self, subject: str, body: str, to_address: str) -> str:
        """Send an email to the given address with the given subject and body.

        Args:
            subject (str): The subject of the email.
            body (str): The body of the email.
            to_address (str): The address to send the email to.
        """
        return f"Sent email to {to_address} with subject {subject} and body {body}"

    def get_emails(self, date_from: str, date_to: str) -> str:
        """Get all emails between the given dates.

        Args:
            date_from (str): The start date (in YYYY-MM-DD format).
            date_to (str): The end date (in YYYY-MM-DD format).
        """
        return [
            {
                "subject": "Hello",
                "body": "Hello, world!",
                "to_address": "test@test.com",
                "date": date_from,
            },
            {
                "subject": "Random other email",
                "body": "This is a random other email",
                "to_address": "john@doe.com",
                "date": date_to,
            },
        ]


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[EmailTools(), UserControlFlowTools()],
    markdown=True,
    debug_mode=True,
)

run_response = agent.run("Send an email with the body 'What is the weather in Tokyo?'")

# We use a while loop to continue the running until the agent is satisfied with the user input
while run_response.is_paused:
    for tool in run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type
            field_description = field.description

            # Display field information to the user
            print(f"\nField: {field.name}")
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
            else:
                print(f"Value: {field.value}")
                user_value = field.value

            # Update the field value
            field.value = user_value

    run_response = agent.continue_run(run_response=run_response)
    if not run_response.is_paused:
        pprint.pprint_run_response(run_response)
        break


run_response = agent.run("Get me all my emails")

while run_response.is_paused:
    for tool in run_response.tools_requiring_user_input:
        input_schema: Dict[str, Any] = tool.user_input_schema

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type
            field_description = field.description

            # Display field information to the user
            print(f"\nField: {field.name}")
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
            else:
                print(f"Value: {field.value}")
                user_value = field.value

            # Update the field value
            field.value = user_value

    run_response = agent.continue_run(run_response=run_response)
    if not run_response.is_paused:
        pprint.pprint_run_response(run_response)
        break



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

agent.run("Fetch the top 2 hackernews stories.")
run_response = agent.run_response
if agent.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools_requiring_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

run_response = agent.continue_run()
# Or
# run_response = agent.continue_run(run_id=run_response.run_id, updated_tools=run_response.tools)
# Or
# run_response = agent.continue_run(run_response=run_response)

pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("Fetch the top 2 hackernews stories")



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required_async.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import asyncio
import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

run_response = asyncio.run(agent.arun("Fetch the top 2 hackernews stories"))
if run_response.is_paused:
    for tool in run_response.tools_requiring_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True


run_response = asyncio.run(agent.acontinue_run(run_response=run_response))
# Or
# run_response = asyncio.run(agent.acontinue_run(run_id=run_response.run_id))
# Or
# run_response = asyncio.run(agent.acontinue_run())

pprint.pprint_run_response(run_response)


# Or for simple debug flow
# asyncio.run(agent.aprint_response("Fetch the top 2 hackernews stories"))



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required_mixed_tools.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.

In this case we have multiple tools and only one of them requires confirmation.

The agent should execute the tool that doesn't require confirmation and then pause for user confirmation.

The user can then either approve or reject the tool call and the agent should continue from where it left off.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


@tool(requires_confirmation=True)
def send_email(to: str, subject: str, body: str) -> str:
    """Send an email.

    Args:
        to (str): Email address to send to
        subject (str): Subject of the email
        body (str): Body of the email
    """
    return f"Email sent to {to} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories, send_email],
    markdown=True,
)

run_response = agent.run(
    "Fetch the top 2 hackernews stories and email them to john@doe.com."
)
if run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools:
        if tool.requires_confirmation:
            # Ask for confirmation
            console.print(
                f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
            )
            message = (
                Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
                .strip()
                .lower()
            )

            if message == "n":
                tool.confirmed = False
            else:
                # We update the tools in place
                tool.confirmed = True
        else:
            console.print(
                f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] was completed in [bold green]{tool.metrics.time:.2f}[/] seconds."
            )

    run_response = agent.continue_run()
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Fetch the top 2 hackernews stories")



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.wikipedia import WikipediaTools
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        get_top_hackernews_stories,
        WikipediaTools(requires_confirmation_tools=["search_wikipedia"]),
    ],
    markdown=True,
    debug_mode=True,
)

run_response = agent.run(
    "Fetch 2 articles about the topic 'python'. You can choose which source to use, but only use one source."
)
while run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in agent.run_response.tools_requiring_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
            tool.confirmation_note = (
                "This is not the right tool to use. Use the other tool!"
            )
        else:
            # We update the tools in place
            tool.confirmed = True

    run_response = agent.continue_run()



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

for run_response in agent.run("Fetch the top 2 hackernews stories", stream=True):
    if run_response.is_paused:
        for tool in run_response.tools_requiring_confirmation:
            # Ask for confirmation
            console.print(
                f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
            )
            message = (
                Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
                .strip()
                .lower()
            )

            if message == "n":
                tool.confirmed = False
            else:
                # We update the tools in place
                tool.confirmed = True
        run_response = agent.continue_run(
            run_id=agent.run_response.run_id,
            updated_tools=agent.run_response.tools,
            stream=True,
        )
        pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Fetch the top 2 hackernews stories", stream=True)



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import asyncio
import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
)


async def main():
    async for run_response in await agent.arun(
        "Fetch the top 2 hackernews stories", stream=True
    ):
        if run_response.is_paused:
            for tool in run_response.tools_requiring_confirmation:
                # Ask for confirmation
                console.print(
                    f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
                )
                message = (
                    Prompt.ask(
                        "Do you want to continue?", choices=["y", "n"], default="y"
                    )
                    .strip()
                    .lower()
                )

                if message == "n":
                    tool.confirmed = False
                else:
                    # We update the tools in place
                    tool.confirmed = True
            run_response = await agent.acontinue_run(
                run_id=agent.run_response.run_id,
                updated_tools=agent.run_response.tools,
                stream=True,
            )
            async for resp in run_response:
                print(resp.content, end="")

    # Or for simple debug flow
    # await agent.aprint_response("Fetch the top 2 hackernews stories", stream=True)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.yfinance import YFinanceTools
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools(requires_confirmation_tools=["get_current_stock_price"])],
    markdown=True,
)

agent.run("What is the current stock price of Apple?")
if agent.is_paused:  # Or agent.run_response.is_paused
    for tool in agent.run_response.tools_requiring_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

    run_response = agent.continue_run()
    pprint.pprint_run_response(run_response)



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required_with_history.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    add_history_to_messages=True,
    num_history_responses=2,
    markdown=True,
)

agent.run("What can you do?")

agent.run("Fetch the top 2 hackernews stories.")
if agent.is_paused:  # Or agent.run_response.is_paused
    for tool in agent.run_response.tools_requiring_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

run_response = agent.continue_run()
pprint.pprint_run_response(run_response)



================================================
FILE: cookbook/agent_concepts/user_control_flows/confirmation_required_with_run_id.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

agent.run("Fetch the top 2 hackernews stories.")
if agent.is_paused:  # Or agent.run_response.is_paused
    for tool in agent.run_response.tools_requiring_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

updated_tools = agent.run_response.tools

run_response = agent.continue_run(
    run_id=agent.run_response.run_id, updated_tools=updated_tools
)

pprint.pprint_run_response(run_response)



================================================
FILE: cookbook/agent_concepts/user_control_flows/external_tool_execution.py
================================================
"""🤝 Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if command.startswith("ls"):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    else:
        raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
)

run_response = agent.run("What files do I have in my current directory?")
if run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools_awaiting_external_execution:
        if tool.tool_name == execute_shell_command.name:
            print(f"Executing {tool.tool_name} with args {tool.tool_args} externally")
            # We execute the tool ourselves. You can also execute something completely external here.
            result = execute_shell_command.entrypoint(**tool.tool_args)
            # We have to set the result on the tool execution object so that the agent can continue
            tool.result = result

    run_response = agent.continue_run()
    pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("What files do I have in my current directory?")



================================================
FILE: cookbook/agent_concepts/user_control_flows/external_tool_execution_async.py
================================================
"""🤝 Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import asyncio
import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if command.startswith("ls"):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    else:
        raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
)

run_response = asyncio.run(agent.arun("What files do I have in my current directory?"))
if run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools_awaiting_external_execution:
        if tool.tool_name == execute_shell_command.name:
            print(f"Executing {tool.tool_name} with args {tool.tool_args} externally")
            # We execute the tool ourselves. You can also execute something completely external here.
            result = execute_shell_command.entrypoint(**tool.tool_args)
            # We have to set the result on the tool execution object so that the agent can continue
            tool.result = result

    run_response = asyncio.run(agent.acontinue_run(run_response=run_response))
    pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("What files do I have in my current directory?")



================================================
FILE: cookbook/agent_concepts/user_control_flows/external_tool_execution_async_responses.py
================================================
"""🤝 Human-in-the-Loop with OpenAI Responses API (gpt-4.1-mini)

This example mirrors the external tool execution async example but uses
OpenAIResponses with gpt-4.1-mini to validate tool-call id handling.

Run `pip install openai agno` to install dependencies.
"""

import asyncio
import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring
# for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if (
        command.startswith("ls ")
        or command == "ls"
        or command.startswith("cat ")
        or command.startswith("head ")
    ):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIResponses(id="gpt-4.1-mini"),
    tools=[execute_shell_command],
    markdown=True,
)

run_response = asyncio.run(agent.arun("What files do I have in my current directory?"))

# Keep executing externally-required tools until the run completes
while (
    run_response.is_paused and len(run_response.tools_awaiting_external_execution) > 0
):
    for tool in run_response.tools_awaiting_external_execution:
        if tool.tool_name == execute_shell_command.name:
            print(f"Executing {tool.tool_name} with args {tool.tool_args} externally")
            result = execute_shell_command.entrypoint(**tool.tool_args)
            tool.result = result
        else:
            print(f"Skipping unsupported external tool: {tool.tool_name}")

    run_response = asyncio.run(agent.acontinue_run(run_response=run_response))

pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("What files do I have in my current directory?")



================================================
FILE: cookbook/agent_concepts/user_control_flows/external_tool_execution_stream.py
================================================
"""🤝 Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if command.startswith("ls"):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    else:
        raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
)

for run_response in agent.run(
    "What files do I have in my current directory?", stream=True
):
    if run_response.is_paused:
        for tool in run_response.tools_awaiting_external_execution:
            if tool.tool_name == execute_shell_command.name:
                print(
                    f"Executing {tool.tool_name} with args {tool.tool_args} externally"
                )
                # We execute the tool ourselves. You can also execute something completely external here.
                result = execute_shell_command.entrypoint(**tool.tool_args)
                # We have to set the result on the tool execution object so that the agent can continue
                tool.result = result

        run_response = agent.continue_run(
            run_id=agent.run_response.run_id,
            updated_tools=agent.run_response.tools,
            stream=True,
        )
        pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("What files do I have in my current directory?", stream=True)



================================================
FILE: cookbook/agent_concepts/user_control_flows/external_tool_execution_stream_async.py
================================================
"""🤝 Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import asyncio
import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if command.startswith("ls"):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    else:
        raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
)


async def main():
    async for run_response in await agent.arun(
        "What files do I have in my current directory?", stream=True
    ):
        if run_response.is_paused:
            for tool in run_response.tools_awaiting_external_execution:
                if tool.tool_name == execute_shell_command.name:
                    print(
                        f"Executing {tool.tool_name} with args {tool.tool_args} externally"
                    )
                    # We execute the tool ourselves. You can also execute something completely external here.
                    result = execute_shell_command.entrypoint(**tool.tool_args)
                    # We have to set the result on the tool execution object so that the agent can continue
                    tool.result = result
            run_response = await agent.acontinue_run(
                run_id=agent.run_response.run_id,
                updated_tools=agent.run_response.tools,
                stream=True,
            )
            async for resp in run_response:
                print(resp.content, end="")
        else:
            print(run_response.content, end="")

    # Or for simple debug flow
    # agent.print_response("What files do I have in my current directory?", stream=True)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/agent_concepts/user_control_flows/external_tool_execution_toolkit.py
================================================
"""🤝 Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.toolkit import Toolkit
from agno.utils import pprint


class ShellTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            tools=[self.list_dir],
            external_execution_required_tools=["list_dir"],
            *args,
            **kwargs,
        )

    def list_dir(self, directory: str):
        """
        Lists the contents of a directory.

        Args:
            directory: The directory to list.

        Returns:
            A string containing the contents of the directory.
        """
        return subprocess.check_output(f"ls {directory}", shell=True).decode("utf-8")


tools = ShellTools()

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[tools],
    markdown=True,
)

run_response = agent.run("What files do I have in my current directory?")
if run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools_awaiting_external_execution:
        if tool.tool_name == "list_dir":
            print(f"Executing {tool.tool_name} with args {tool.tool_args} externally")
            # We execute the tool ourselves. You can also execute something completely external here.
            result = tools.list_dir(**tool.tool_args)
            # We have to set the result on the tool execution object so that the agent can continue
            tool.result = result

    run_response = agent.continue_run()
    pprint.pprint_run_response(run_response)



================================================
FILE: cookbook/agent_concepts/user_control_flows/user_input_required.py
================================================
"""🤝 Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the `requires_user_input` parameter to allow users to provide input externally.
"""

from typing import Any, Dict, List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint


# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
)

agent.run("Send an email with the subject 'Hello' and the body 'Hello, world!'")
if agent.is_paused:  # Or agent.run_response.is_paused
    for tool in agent.run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type
            field_description = field.description

            # Display field information to the user
            print(f"\nField: {field.name}")
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
            else:
                print(f"Value: {field.value}")
                user_value = field.value

            # Update the field value
            field.value = user_value

    run_response = (
        agent.continue_run()
    )  # or agent.continue_run(run_response=agent.run_response)
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Send an email with the subject 'Hello' and the body 'Hello, world!'")



================================================
FILE: cookbook/agent_concepts/user_control_flows/user_input_required_all_fields.py
================================================
"""🤝 Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the `requires_user_input` parameter to allow users to provide input externally.
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint


@tool(requires_user_input=True)
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
)

agent.run("Send an email please")
if agent.is_paused:  # Or agent.run_response.is_paused
    for tool in agent.run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type
            field_description = field.description

            # Display field information to the user
            print(f"\nField: {field.name}")
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")

            # Update the field value
            field.value = user_value

    run_response = (
        agent.continue_run()
    )  # or agent.continue_run(run_response=agent.run_response)
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Send an email please")



================================================
FILE: cookbook/agent_concepts/user_control_flows/user_input_required_async.py
================================================
"""🤝 Human-in-the-Loop: Allowing users to provide input externally"""

import asyncio
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint


# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
)

asyncio.run(
    agent.arun("Send an email with the subject 'Hello' and the body 'Hello, world!'")
)
if agent.is_paused:  # Or agent.run_response.is_paused
    for tool in agent.run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type
            field_description = field.description

            # Display field information to the user
            print(f"\nField: {field.name}")
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
            else:
                print(f"Value: {field.value}")
                user_value = field.value

            # Update the field value
            field.value = user_value

    run_response = asyncio.run(
        agent.acontinue_run()
    )  # or agent.continue_run(run_response=agent.run_response)
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Send an email with the subject 'Hello' and the body 'Hello, world!'")



================================================
FILE: cookbook/agent_concepts/user_control_flows/user_input_required_stream.py
================================================
"""🤝 Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the `requires_user_input` parameter to allow users to provide input externally.
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint


# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
)

for run_response in agent.run(
    "Send an email with the subject 'Hello' and the body 'Hello, world!'", stream=True
):
    if run_response.is_paused:  # Or agent.run_response.is_paused
        for tool in run_response.tools_requiring_user_input:
            input_schema: List[UserInputField] = tool.user_input_schema

            for field in input_schema:
                # Get user input for each field in the schema
                field_type = field.field_type
                field_description = field.description

                # Display field information to the user
                print(f"\nField: {field.name}")
                print(f"Description: {field_description}")
                print(f"Type: {field_type}")

                # Get user input
                if field.value is None:
                    user_value = input(f"Please enter a value for {field.name}: ")
                else:
                    print(f"Value: {field.value}")
                    user_value = field.value

                # Update the field value
                field.value = user_value

        run_response = agent.continue_run(
            run_id=agent.run_response.run_id,
            updated_tools=agent.run_response.tools,
            stream=True,
        )  # or agent.continue_run(run_response=agent.run_response)
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Send an email with the subject 'Hello' and the body 'Hello, world!'", stream=True)



================================================
FILE: cookbook/agent_concepts/user_control_flows/user_input_required_stream_async.py
================================================
"""🤝 Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the `requires_user_input` parameter to allow users to provide input externally.
"""

import asyncio
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField


# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
)


async def main():
    async for run_response in await agent.arun(
        "Send an email with the subject 'Hello' and the body 'Hello, world!'",
        stream=True,
    ):
        if run_response.is_paused:  # Or agent.run_response.is_paused
            for tool in run_response.tools_requiring_user_input:
                input_schema: List[UserInputField] = tool.user_input_schema

                for field in input_schema:
                    # Get user input for each field in the schema
                    field_type = field.field_type
                    field_description = field.description

                    # Display field information to the user
                    print(f"\nField: {field.name}")
                    print(f"Description: {field_description}")
                    print(f"Type: {field_type}")

                    # Get user input
                    if field.value is None:
                        user_value = input(f"Please enter a value for {field.name}: ")
                    else:
                        print(f"Value: {field.value}")
                        user_value = field.value

                    # Update the field value
                    field.value = user_value

            run_response = await agent.acontinue_run(
                run_id=agent.run_response.run_id,
                updated_tools=agent.run_response.tools,
                stream=True,
            )
            async for resp in run_response:
                print(resp.content, end="")

    # Or for simple debug flow
    # agent.aprint_response("Send an email with the subject 'Hello' and the body 'Hello, world!'")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/agent_levels/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agent_levels/level_1_agent.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[YFinanceTools(stock_price=True)],
    instructions="Use tables to display data. Don't include any other text.",
    markdown=True,
)
agent.print_response("What is the stock price of Apple?", stream=True)



================================================
FILE: cookbook/agent_levels/level_2_agent.py
================================================
from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.anthropic import Claude
from agno.storage.sqlite import SqliteStorage
from agno.vectordb.lancedb import LanceDb, SearchType

# Load Agno documentation in a knowledge base
# You can also use `https://docs.agno.com/llms-full.txt` for the full documentation
knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/introduction.md"],
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        # Use OpenAI for embeddings
        embedder=OpenAIEmbedder(id="text-embedding-3-small", dimensions=1536),
    ),
)

# Store agent sessions in a SQLite database
storage = SqliteStorage(table_name="agent_sessions", db_file="tmp/agent.db")

agent = Agent(
    name="Agno Assist",
    model=Claude(id="claude-sonnet-4-20250514"),
    instructions=[
        "Search your knowledge before answering the question.",
        "Only include the output in your response. No other text.",
    ],
    knowledge=knowledge,
    storage=storage,
    add_datetime_to_instructions=True,
    # Add the chat history to the messages
    add_history_to_messages=True,
    # Number of history runs
    num_history_runs=3,
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, comment out after first run
    # Set recreate to True to recreate the knowledge base if needed
    agent.knowledge.load(recreate=False)
    agent.print_response("What is Agno?", stream=True)



================================================
FILE: cookbook/agent_levels/level_3_agent.py
================================================
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

memory = Memory(
    # Use any model for creating and managing memories
    model=Claude(id="claude-sonnet-4-20250514"),
    # Store memories in a SQLite database
    db=SqliteMemoryDb(table_name="user_memories", db_file="tmp/agent.db"),
    # We disable deletion by default, enable it if needed
    delete_memories=True,
    clear_memories=True,
)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    # User ID for storing memories, `default` if not provided
    user_id="ava",
    instructions=[
        "Use tables to display data.",
        "Include sources in your response.",
        "Only include the report in your response. No other text.",
    ],
    memory=memory,
    # Let the Agent manage its memories
    enable_agentic_memory=True,
    markdown=True,
)

if __name__ == "__main__":
    # This will create a memory that "ava's" favorite stocks are NVIDIA and TSLA
    agent.print_response(
        "My favorite stocks are NVIDIA and TSLA",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )
    # This will use the memory to answer the question
    agent.print_response(
        "Can you compare my favorite stocks?",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/agent_levels/level_4_team.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests and general research",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_instructions=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests and market analysis",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[
        YFinanceTools(
            stock_price=True,
            stock_fundamentals=True,
            analyst_recommendations=True,
            company_info=True,
        )
    ],
    instructions=[
        "Use tables to display stock prices, fundamentals (P/E, Market Cap), and recommendations.",
        "Clearly state the company name and ticker symbol.",
        "Focus on delivering actionable financial insights.",
    ],
    add_datetime_to_instructions=True,
)

reasoning_finance_team = Team(
    name="Reasoning Finance Team",
    mode="coordinate",
    model=Claude(id="claude-sonnet-4-20250514"),
    members=[web_agent, finance_agent],
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Collaborate to provide comprehensive financial and investment insights",
        "Consider both fundamental analysis and market sentiment",
        "Use tables and charts to display data clearly and professionally",
        "Present findings in a structured, easy-to-follow format",
        "Only output the final consolidated analysis, not individual agent responses",
    ],
    markdown=True,
    show_members_responses=True,
    enable_agentic_context=True,
    add_datetime_to_instructions=True,
    success_criteria="The team has provided a complete financial analysis with data, visualizations, risk assessment, and actionable investment recommendations supported by quantitative analysis and market research.",
)

if __name__ == "__main__":
    reasoning_finance_team.print_response(
        """Compare the tech sector giants (AAPL, GOOGL, MSFT) performance:
        1. Get financial data for all three companies
        2. Analyze recent news affecting the tech sector
        3. Calculate comparative metrics and correlations
        4. Recommend portfolio allocation weights""",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/agent_levels/level_5_workflow.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class CacheWorkflow(Workflow):
    # Add agents or teams as attributes on the workflow
    agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))

    # Write the logic in the `run()` method
    def run(self, message: str) -> Iterator[RunResponse]:
        logger.info(f"Checking cache for '{message}'")
        # Check if the output is already cached
        if self.session_state.get(message):
            logger.info(f"Cache hit for '{message}'")
            yield RunResponse(
                run_id=self.run_id, content=self.session_state.get(message)
            )
            return

        logger.info(f"Cache miss for '{message}'")
        # Run the agent and yield the response
        yield from self.agent.run(message, stream=True)

        # Cache the output after response is yielded
        self.session_state[message] = self.agent.run_response.content


if __name__ == "__main__":
    workflow = CacheWorkflow()
    # Run workflow (this is takes ~1s)
    response: Iterator[RunResponse] = workflow.run(message="Tell me a joke.")
    # Print the response
    pprint_run_response(response, markdown=True, show_time=True)
    # Run workflow again (this is immediate because of caching)
    response: Iterator[RunResponse] = workflow.run(message="Tell me a joke.")
    # Print the response
    pprint_run_response(response, markdown=True, show_time=True)



================================================
FILE: cookbook/agents_from_scratch/README.md
================================================
# Agents from Scratch

This is a step by step guide to building Agents from scratch, with Agno.

Each example builds on the previous one, introducing new concepts and capabilities progressively.

## Setup

Create a virtual environment:

```bash
uv venv --python 3.12
source .venv/bin/activate
```

Install the required dependencies:

```bash
uv pip install -U agno openai ddgs elevenlabs sqlalchemy 'fastapi[standard]' lancedb pylance tantivy pandas numpy
```

Export your OpenAI API key:

```bash
export OPENAI_API_KEY=your_api_key
```

If you want to use the ElevenLabs API, export your API key:

```bash
export ELEVENLABS_API_KEY=your_api_key
```

## Run the Playground

We recommend testing the Agents in the Agent UI, which is a web application that allows you to chat with your Agents.

### Authenticate with Agno

```bash
ag setup
```

### Run the Playground

```bash
python cookbook/agents_from_scratch/playground.py
```

## Run the Agents in the CLI

You may also run the Agents in the CLI, which is useful for testing and debugging.

```bash
python cookbook/agents_from_scratch/simple_agent.py
```

```bash
python cookbook/agents_from_scratch/agent_with_tools.py
```

> Remember to set `load_knowledge = True` in the `agent_with_knowledge.py` file to load the knowledge base.

```bash
python cookbook/agents_from_scratch/agent_with_knowledge.py
```

```bash
python cookbook/agents_from_scratch/agent_with_storage.py
```

```bash
python cookbook/agents_from_scratch/agno_assist.py
```



================================================
FILE: cookbook/agents_from_scratch/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/agents_from_scratch/agent_with_knowledge.py
================================================
"""Agent with Knowledge - An agent that can search a knowledge base

Install dependencies: `pip install openai lancedb tantivy agno`
"""

from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths
cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

# Initialize knowledge base
agent_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/llms-full.txt"],
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

agent_with_knowledge = Agent(
    name="Agent with Knowledge",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are AgnoAssist, an AI Agent specializing in Agno: A lighweight python framework for building multimodal agents.
    Your goal is to help developers understand and effectively use Agno by providing
    explanations and working code examples"""),
    instructions=dedent("""\
    Your mission is to provide comprehensive support for Agno developers. Follow these steps to ensure the best possible response:

    1. **Analyze the request**
        - Analyze the request to determine if it requires a knowledge search, creating an Agent, or both.
        - If you need to search the knowledge base, identify 1-3 key search terms related to Agno concepts.
        - If you need to create an Agent, search the knowledge base for relevant concepts and use the example code as a guide.
        - When the user asks for an Agent, they mean an Agno Agent.
        - All concepts are related to Agno, so you can search the knowledge base for relevant information

    After Analysis, always start the iterative search process. No need to wait for approval from the user.

    2. **Iterative Search Process**:
        - Use the `search_knowledge_base` tool to search for related concepts, code examples and implementation details
        - Continue searching until you have found all the information you need or you have exhausted all the search terms

    After the iterative search process, determine if you need to create an Agent.
    If you do, generate a code example that the user can run.

    3. **Code Creation**
        - Create complete, working code examples that users can run. For example:
        ```python
        from agno.agent import Agent
        from agno.tools.duckduckgo import DuckDuckGoTools

        agent = Agent(tools=[DuckDuckGoTools()])

        # Perform a web search and capture the response
        response = agent.run("What's happening in France?")
        ```
        - Remember to:
            * Build the complete agent implementation.
            * Include all necessary imports and setup.
            * Add comprehensive comments explaining the implementation
            * Test the agent with example queries
            * Ensure all dependencies are listed
            * Include error handling and best practices
            * Add type hints and documentation

    Key topics to cover:
    - Agent levels and capabilities
    - Knowledge base and memory management
    - Tool integration
    - Model support and configuration
    - Best practices and common patterns"""),
    knowledge=agent_knowledge,
    show_tool_calls=True,
    markdown=True,
)

if __name__ == "__main__":
    # Set to False after the knowledge base is loaded
    load_knowledge = False
    if load_knowledge:
        agent_knowledge.load()

    agent_with_knowledge.print_response("Tell me about the Agno framework", stream=True)



================================================
FILE: cookbook/agents_from_scratch/agent_with_storage.py
================================================
"""Agent with Storage - An agent that can store sessions in a database

Install dependencies: `pip install openai lancedb tantivy sqlalchemy agno`
"""

from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths
cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

# Initialize knowledge & storage
agent_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/llms-full.txt"],
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
agent_storage = SqliteStorage(
    table_name="agno_assist_sessions",
    db_file=str(tmp_dir.joinpath("agent_sessions.db")),
)

agent_with_storage = Agent(
    name="Agent with Storage",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are AgnoAssist, an AI Agent specializing in Agno: A lighweight python framework for building multimodal agents.
    Your goal is to help developers understand and effectively use Agno by providing
    explanations and working code examples"""),
    instructions=dedent("""\
    Your mission is to provide comprehensive support for Agno developers. Follow these steps to ensure the best possible response:

    1. **Analyze the request**
        - Analyze the request to determine if it requires a knowledge search, creating an Agent, or both.
        - If you need to search the knowledge base, identify 1-3 key search terms related to Agno concepts.
        - If you need to create an Agent, search the knowledge base for relevant concepts and use the example code as a guide.
        - When the user asks for an Agent, they mean an Agno Agent.
        - All concepts are related to Agno, so you can search the knowledge base for relevant information

    After Analysis, always start the iterative search process. No need to wait for approval from the user.

    2. **Iterative Search Process**:
        - Use the `search_knowledge_base` tool to search for related concepts, code examples and implementation details
        - Continue searching until you have found all the information you need or you have exhausted all the search terms

    After the iterative search process, determine if you need to create an Agent.
    If you do, generate a code example that the user can run.

    3. **Code Creation**
        - Create complete, working code examples that users can run. For example:
        ```python
        from agno.agent import Agent
        from agno.tools.duckduckgo import DuckDuckGoTools

        agent = Agent(tools=[DuckDuckGoTools()])

        # Perform a web search and capture the response
        response = agent.run("What's happening in France?")
        ```
        - Remember to:
            * Build the complete agent implementation.
            * Include all necessary imports and setup.
            * Add comprehensive comments explaining the implementation
            * Test the agent with example queries
            * Ensure all dependencies are listed
            * Include error handling and best practices
            * Add type hints and documentation

    Key topics to cover:
    - Agent levels and capabilities
    - Knowledge base and memory management
    - Tool integration
    - Model support and configuration
    - Best practices and common patterns"""),
    knowledge=agent_knowledge,
    storage=agent_storage,
    show_tool_calls=True,
    # To provide the agent with the chat history
    # We can either:
    # 1. Provide the agent with a tool to read the chat history
    # 2. Automatically add the chat history to the messages sent to the model
    #
    # 1. Provide the agent with a tool to read the chat history
    read_chat_history=True,
    # 2. Automatically add the chat history to the messages sent to the model
    add_history_to_messages=True,
    # Number of historical runs to add to the messages.
    num_history_responses=3,
    markdown=True,
)

if __name__ == "__main__":
    # Set to False after the knowledge base is loaded
    load_knowledge = True
    if load_knowledge:
        agent_knowledge.load()

    agent_with_storage.print_response("Tell me about the Agno framework", stream=True)
    agent_with_storage.print_response("What was my last question?", stream=True)



================================================
FILE: cookbook/agents_from_scratch/agent_with_tools.py
================================================
"""Agent with Tools - An agent that can search the web

Install dependencies: `pip install openai ddgs agno`
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent_with_tools = Agent(
    name="Agent with Tools",
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! 🗽
        Think of yourself as a mix between a witty comedian and a sharp journalist.

        Follow these guidelines for every report:
        1. Start with an attention-grabbing headline using relevant emoji
        2. Use the search tool to find current, accurate information
        3. Present news with authentic NYC enthusiasm and local flavor
        4. Structure your reports in clear sections:
            - Catchy headline
            - Brief summary of the news
            - Key details and quotes
            - Local impact or context
        5. Keep responses concise but informative (2-3 paragraphs max)
        6. Include NYC-style commentary and local references
        7. End with a signature sign-off phrase

        Sign-off examples:
        - 'Back to you in the studio, folks!'
        - 'Reporting live from the city that never sleeps!'
        - 'This is [Your Name], live from the heart of Manhattan!'

        Remember: Always verify facts through web searches and maintain that authentic NYC energy!\
    """),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

if __name__ == "__main__":
    agent_with_tools.print_response("Share a news story from NYC and SF.", stream=True)



================================================
FILE: cookbook/agents_from_scratch/agno_assist.py
================================================
"""Agno Assist - Your Assistant for Agno Framework!

Install dependencies: `pip install openai lancedb tantivy elevenlabs sqlalchemy agno`
"""

from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.dalle import DalleTools
from agno.tools.eleven_labs import ElevenLabsTools
from agno.tools.python import PythonTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths
cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

# Initialize knowledge & storage
agent_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/llms-full.txt"],
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
agent_storage = SqliteStorage(
    table_name="agno_assist_sessions",
    db_file=str(tmp_dir.joinpath("agent_sessions.db")),
)

agno_assist = Agent(
    name="Agno Assist",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are AgnoAssist, an AI Agent specializing in Agno: A lighweight python framework for building multimodal agents.
    Your goal is to help developers understand and effectively use Agno by providing
    explanations and working code examples"""),
    instructions=dedent("""\
    Your mission is to provide comprehensive support for Agno developers. Follow these steps to ensure the best possible response:

    1. **Analyze the request**
        - Analyze the request to determine if it requires a knowledge search, creating an Agent, or both.
        - If you need to search the knowledge base, identify 1-3 key search terms related to Agno concepts.
        - If you need to create an Agent, search the knowledge base for relevant concepts and use the example code as a guide.
        - When the user asks for an Agent, they mean an Agno Agent.
        - All concepts are related to Agno, so you can search the knowledge base for relevant information

    After Analysis, always start the iterative search process. No need to wait for approval from the user.

    2. **Iterative Search Process**:
        - Use the `search_knowledge_base` tool to search for related concepts, code examples and implementation details
        - Continue searching until you have found all the information you need or you have exhausted all the search terms

    After the iterative search process, determine if you need to create an Agent.
    If you do, ask the user if they want you to create the Agent and run it.

    3. **Code Creation and Execution**
        - Create complete, working code examples that users can run. For example:
        ```python
        from agno.agent import Agent
        from agno.tools.duckduckgo import DuckDuckGoTools

        agent = Agent(tools=[DuckDuckGoTools()])

        # Perform a web search and capture the response
        response = agent.run("What's happening in France?")
        ```
        - You must remember to use agent.run() and NOT agent.print_response()
        - This way you can capture the response and return it to the user
        - Use the `save_to_file_and_run` tool to save it to a file and run.
        - Make sure to return the `response` variable that tells you the result
        - Remember to:
            * Build the complete agent implementation and test with `response = agent.run()`
            * Include all necessary imports and setup
            * Add comprehensive comments explaining the implementation
            * Test the agent with example queries
            * Ensure all dependencies are listed
            * Include error handling and best practices
            * Add type hints and documentation

    4. **Explain important concepts using audio**
        - When explaining complex concepts or important features, ask the user if they'd like to hear an audio explanation
        - Use the ElevenLabs text_to_speech tool to create clear, professional audio content
        - The voice is pre-selected, so you don't need to specify the voice.
        - Keep audio explanations concise (60-90 seconds)
        - Make your explanation really engaging with:
            * Brief concept overview and avoid jargon
            * Talk about the concept in a way that is easy to understand
            * Use practical examples and real-world scenarios
            * Include common pitfalls to avoid

    5. **Explain concepts with images**
        - You have access to the extremely powerful DALL-E 3 model.
        - Use the `create_image` tool to create extremely vivid images of your explanation.
        - Don't display the image in your response, it will be shown to the user separately.
        - The image will be shown to the user automatically below your response.
        - You DO NOT need to display or include the image in your response, if needed, refer to it as 'the image shown below'.

    Key topics to cover:
    - Agent levels and capabilities
    - Knowledge base and memory management
    - Tool integration
    - Model support and configuration
    - Best practices and common patterns"""),
    add_datetime_to_instructions=True,
    knowledge=agent_knowledge,
    storage=agent_storage,
    tools=[
        PythonTools(base_dir=tmp_dir.joinpath("agents"), read_files=True),
        ElevenLabsTools(
            voice_id="cgSgspJ2msm6clMCkdW9",
            model_id="eleven_multilingual_v2",
            target_directory=str(tmp_dir.joinpath("audio").resolve()),
        ),
        DalleTools(model="dall-e-3", size="1792x1024", quality="hd", style="vivid"),
    ],
    # To provide the agent with the chat history
    # We can either:
    # 1. Provide the agent with a tool to read the chat history
    # 2. Automatically add the chat history to the messages sent to the model
    #
    # 1. Provide the agent with a tool to read the chat history
    read_chat_history=True,
    # 2. Automatically add the chat history to the messages sent to the model
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_responses=3,
    markdown=True,
)

if __name__ == "__main__":
    # Set to False after the knowledge base is loaded
    load_knowledge = False
    if load_knowledge:
        agent_knowledge.load()

    agno_assist.print_response("Tell me about the Agno framework", stream=True)



================================================
FILE: cookbook/agents_from_scratch/playground.py
================================================
"""Your Agent Playground

Install dependencies: `pip install openai ddgs lancedb tantivy elevenlabs sqlalchemy 'fastapi[standard]' agno`
"""

from agent_with_knowledge import agent_with_knowledge
from agent_with_storage import agent_with_storage
from agent_with_tools import agent_with_tools
from agno.playground import Playground, serve_playground_app
from agno_assist import agno_assist
from simple_agent import simple_agent

# Create and configure the playground app
app = Playground(
    agents=[
        simple_agent,
        agent_with_tools,
        agent_with_knowledge,
        agent_with_storage,
        agno_assist,
    ]
).get_app()

if __name__ == "__main__":
    # Run the playground app
    playground = Playground(
        agents=[
            simple_agent,
            agent_with_tools,
            agent_with_knowledge,
            agent_with_storage,
            agno_assist,
        ],
        app_id="agents-from-scratch-playground-app",
        name="Agents from Scratch Playground",
    )
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="playground:app", reload=True)



================================================
FILE: cookbook/agents_from_scratch/simple_agent.py
================================================
"""Simple Agent - An agent that performs a simple inference task

Install dependencies: `pip install openai agno`
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat

simple_agent = Agent(
    name="Simple Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! 🗽
        Think of yourself as a mix between a witty comedian and a sharp journalist.

        Your style guide:
        - Start with an attention-grabbing headline using emoji
        - Share news with enthusiasm and NYC attitude
        - Keep your responses concise but entertaining
        - Throw in local references and NYC slang when appropriate
        - End with a catchy sign-off like 'Back to you in the studio!' or 'Reporting live from the Big Apple!'

        Remember to verify all facts while keeping that NYC energy high!\
    """),
    markdown=True,
)

if __name__ == "__main__":
    simple_agent.print_response("Share a news story from NYC and SF.", stream=True)



================================================
FILE: cookbook/apps/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/apps/agui/README.md
================================================
# AG-UI Integration for Agno

AG-UI standardizes how front-end applications connect to AI agents through an open protocol.
With this integration, you can write your Agno Agents and Teams, and get a ChatGPT-like UI automatically.

**Example: Chat with a simple agent:**

```python my_agent.py
from agno.agent.agent import Agent
from agno.app.agui.app import AGUIApp
from agno.models.openai import OpenAIChat

# Setup the Agno Agent
chat_agent = Agent(model=OpenAIChat(id="gpt-4o"))

# Setup the AG-UI App
agui_app = AGUIApp(agent=chat_agent)
agui_app.serve(app="basic:app", port=8000, reload=True)
```

That's it! Your Agent is now exposed in an AG-UI compatible way, and can be used in any AG-UI compatible front-end.


## Usage example

### Setup

Start by installing our backend dependencies:

```bash
pip install ag-ui-protocol
```

### Run your backend

Now you need to run a `AGUIApp` exposing your Agent. You can run the `cookbook/apps/agui/basic.py` example!

## Run your frontend

You can use [Dojo](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo), an advanced and customizable option to use as frontend for AG-UI agents.

1. Clone the project: `git clone https://github.com/ag-ui-protocol/ag-ui.git`
2. Follow the instructions [here](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo) to learn how to install the needed dependencies and run the project.
3. Remember to install the dependencies in `/ag-ui/typescript-sdk` with `pnpm install`, and to build the Agno package in `/integrations/agno` with `pnpm run build`.
4. You can now run your Dojo! It will show our Agno agent as one of the available options.


### Chat with your Agent

If you are running Dojo as your frontend, you can now go to http://localhost:3000 in your browser and chat with your Agno Agent.


## Examples

Check out these example agents and teams:

- [Chat Agent](./basic.py) - Simple conversational agent
- [Agent with Tools](./agent_with_tools.py) - An agent using tools
- [Research Team](./research_team.py) - Team of agents working together



================================================
FILE: cookbook/apps/agui/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/apps/agui/agent_with_tool.py
================================================
from agno.agent.agent import Agent
from agno.app.agui import AGUIApp
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True, analyst_recommendations=True, stock_fundamentals=True
        )
    ],
    description="You are an investment analyst that researches stock prices, analyst recommendations, and stock fundamentals.",
    instructions="Format your response using markdown and use tables to display data where possible.",
)

agui_app = AGUIApp(
    agent=agent,
    name="Investment Analyst",
    app_id="investment_analyst",
    description="An investment analyst that researches stock prices, analyst recommendations, and stock fundamentals.",
)

app = agui_app.get_app()

if __name__ == "__main__":
    agui_app.serve(app="agent_with_tool:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/agui/basic.py
================================================
from agno.agent.agent import Agent
from agno.app.agui import AGUIApp
from agno.models.openai import OpenAIChat

chat_agent = Agent(
    name="Assistant",
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a helpful AI assistant.",
    add_datetime_to_instructions=True,
    markdown=True,
)

agui_app = AGUIApp(
    agent=chat_agent,
    name="Basic AG-UI Agent",
    app_id="basic_agui_agent",
    description="A basic agent that demonstrates AG-UI protocol integration.",
)

app = agui_app.get_app()

if __name__ == "__main__":
    agui_app.serve(app="basic:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/agui/research_team.py
================================================
from agno.agent.agent import Agent
from agno.app.agui import AGUIApp
from agno.models.openai import OpenAIChat
from agno.team.team import Team

researcher = Agent(
    name="researcher",
    role="Research Assistant",
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a research assistant. Find information and provide detailed analysis.",
    markdown=True,
)

writer = Agent(
    name="writer",
    role="Content Writer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a content writer. Create well-structured content based on research.",
    markdown=True,
)

research_team = Team(
    members=[researcher, writer],
    name="research_team",
    instructions="""
    You are a research team that helps users with research and content creation.
    First, use the researcher to gather information, then use the writer to create content.
    """,
    show_tool_calls=True,
    show_members_responses=True,
    get_member_information_tool=True,
    add_member_tools_to_system_message=True,
)

agui_app = AGUIApp(
    team=research_team,
    name="Research Team AG-UI",
    app_id="research_team_agui",
    description="A research team that demonstrates AG-UI protocol integration.",
)

app = agui_app.get_app()

if __name__ == "__main__":
    agui_app.serve(app="research_team:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/agui/structured_output.py
================================================
from typing import List

from agno.agent.agent import Agent
from agno.app.agui import AGUIApp
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


chat_agent = Agent(
    name="Assistant",
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    markdown=True,
    response_model=MovieScript,
)

agui_app = AGUIApp(
    agent=chat_agent,
    name="Basic AG-UI Agent",
    app_id="structured_output_agui_agent",
    description="A basic agent that demonstrates AG-UI protocol integration.",
)

app = agui_app.get_app()

if __name__ == "__main__":
    agui_app.serve(app="structured_output:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/discord/README.md
================================================
# Discord Client for Agno

This module provides a Discord client implementation for Agno, allowing you to create AI-powered Discord bots using Agno's agent framework.

## Prerequisites

Before you can use the Discord client, you'll need:

1. Python 3.8 or higher
2. A Discord bot token
3. Required Python packages:
   - discord.py
   - agno

## Installation

1. Install the required packages:
```bash
pip install discord.py agno
```

2. Create a Discord bot:
   - Go to the [Discord Developer Portal](https://discord.com/developers/applications)
   - Click "New Application" and give it a name
   - Go to the "Bot" section and click "Add Bot"
   - Under the bot settings, enable the following Privileged Gateway Intents:
     - Presence Intent
     - Server Members Intent
     - Message Content Intent
   - Copy your bot token (you'll need this later)

3. Set up your environment:
   - Create a `.envrc` file in your project root
   - Add your Discord bot token:
   ```
   DISCORD_TOKEN=your_bot_token_here
   ```

4. Invite Your Bot to Your Discord Server:
   - In your application's settings under **"OAuth2"** > **"URL Generator"**, select the `bot` scope
   - Under **"Bot Permissions"**, select the permissions your bot needs (e.g., sending messages)
   - Copy the generated URL, navigate to it in your browser, and select the server where you want to add the bot

## Usage

Here's a basic example of how to use the DiscordClient:

```python
from agno.agent import Agent
from agno.app.discord.client import DiscordClient
from agno.models.anthropic import Claude

# Create your agent
agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    instructions=["Your agent instructions here"],
    # Add other agent configurations as needed
)
# Initialize the Discord client
discord_agent = DiscordClient(media_agent)
if __name__ == "__main__":
    discord_agent.serve()
```

## Features

- Seamless integration with Agno's agent framework
- Support for all Discord bot features through discord.py
- Easy to extend and customize

## Configuration

The DiscordClient accepts the following parameters:

- `agent`: An Agno Agent instance that will handle the bot's responses
- Additional discord.py client parameters can be passed as keyword arguments

## Security Notes

- Never commit your Discord bot token to version control
- Always use environment variables or secure configuration management for sensitive data
- Make sure to set appropriate permissions for your bot in the Discord Developer Portal

## Support

If you encounter any issues or have questions, please:
1. Check the [Agno documentation](https://docs.agno.com)
2. Open an issue on the Agno GitHub repository
3. Join the Agno Discord server for community support



================================================
FILE: cookbook/apps/discord/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/apps/discord/agent_with_media.py
================================================
from agno.agent import Agent
from agno.app.discord import DiscordClient
from agno.models.google import Gemini

media_agent = Agent(
    name="Media Agent",
    model=Gemini(id="gemini-2.0-flash"),
    description="A Media processing agent",
    instructions="Analyze images, audios and videos sent by the user",
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
)
discord_agent = DiscordClient(media_agent)
if __name__ == "__main__":
    discord_agent.serve()



================================================
FILE: cookbook/apps/discord/agent_with_user_memory.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.app.discord import DiscordClient
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.manager import MemoryManager
from agno.memory.v2.memory import Memory
from agno.models.google import Gemini
from agno.storage.sqlite import SqliteStorage
from agno.tools.googlesearch import GoogleSearchTools

agent_storage = SqliteStorage(
    table_name="agent_sessions", db_file="tmp/persistent_memory.db"
)
memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(
    db=memory_db,
    memory_manager=MemoryManager(
        memory_capture_instructions="""\
                        Collect User's name,
                        Collect Information about user's passion and hobbies,
                        Collect Information about the users likes and dislikes,
                        Collect information about what the user is doing with their life right now
                    """,
        model=Gemini(id="gemini-2.0-flash"),
    ),
)


# Reset the memory for this example
memory.clear()

personal_agent = Agent(
    name="Basic Agent",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[GoogleSearchTools()],
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
    memory=memory,
    enable_agentic_memory=True,
    instructions=dedent("""
        You are a personal AI friend of the user, your purpose is to chat with the user about things and make them feel good.
        First introduce yourself and ask for their name then, ask about themeselves, their hobbies, what they like to do and what they like to talk about.
        Use Google Search tool to find latest infromation about things in the conversations
                        """),
    debug_mode=True,
)
discord_agent = DiscordClient(personal_agent)
if __name__ == "__main__":
    discord_agent.serve()



================================================
FILE: cookbook/apps/discord/basic.py
================================================
from agno.agent import Agent
from agno.app.discord import DiscordClient
from agno.models.openai import OpenAIChat

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
)

discord_agent = DiscordClient(basic_agent)
if __name__ == "__main__":
    discord_agent.serve()



================================================
FILE: cookbook/apps/fastapi/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/apps/fastapi/advanced.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.app.fastapi import FastAPIApp
from agno.memory.v2 import Memory
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.yfinance import YFinanceTools

agent_storage_file: str = "tmp/agents.db"
memory_storage_file: str = "tmp/memory.db"

memory_db = SqliteMemoryDb(table_name="memory", db_file=memory_storage_file)

# No need to set the model, it gets set by the agent to the agent's model
memory = Memory(db=memory_db)

simple_agent = Agent(
    name="Simple Agent",
    role="Answer basic questions",
    agent_id="simple-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=SqliteStorage(
        table_name="simple_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    agent_id="web-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Break down the users request into 2-3 different searches.",
        "Always include sources",
    ],
    storage=SqliteStorage(
        table_name="web_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Always use tables to display data"],
    storage=SqliteStorage(
        table_name="finance_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

research_agent = Agent(
    name="Research Agent",
    role="Research agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=["You are a research agent"],
    tools=[DuckDuckGoTools(), ExaTools()],
    agent_id="research_agent",
    memory=memory,
    storage=SqliteStorage(
        table_name="research_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
    enable_user_memories=True,
)

research_team = Team(
    name="Research Team",
    description="A team of agents that research the web",
    members=[research_agent, simple_agent],
    model=OpenAIChat(id="gpt-4o"),
    mode="coordinate",
    team_id="research-team",
    success_criteria=dedent("""\
        A comprehensive research report with clear sections and data-driven insights.
    """),
    instructions=[
        "You are the lead researcher of a research team! 🔍",
    ],
    memory=memory,
    enable_user_memories=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
    enable_agentic_context=True,
    storage=SqliteStorage(
        table_name="research_team",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
        mode="team",
    ),
)


fastapi_app = FastAPIApp(
    agents=[
        simple_agent,
        web_agent,
        finance_agent,
    ],
    teams=[research_team],
    app_id="advanced-app",
    name="Advanced FastAPI App",
    description="A FastAPI app for advanced agents",
    version="0.0.1",
)
app = fastapi_app.get_app()

if __name__ == "__main__":
    """
    Now you can reach your agents/teams with the following URLs:
    - http://localhost:8001/runs?agent_id=simple-agent
    - http://localhost:8001/runs?agent_id=web-agent
    - http://localhost:8001/runs?agent_id=finance-agent
    - http://localhost:8001/runs?team_id=research-team
    """
    fastapi_app.serve(app="advanced:app", port=8001, reload=True)



================================================
FILE: cookbook/apps/fastapi/basic.py
================================================
from agno.agent import Agent
from agno.app.fastapi import FastAPIApp
from agno.models.openai import OpenAIChat

basic_agent = Agent(
    name="Basic Agent",
    agent_id="basic_agent",
    model=OpenAIChat(id="gpt-4o"),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
)

fastapi_app = FastAPIApp(
    agents=[basic_agent],
    name="Basic Agent",
    app_id="basic_agent",
    description="A basic agent that can answer questions and help with tasks.",
)

app = fastapi_app.get_app()

if __name__ == "__main__":
    fastapi_app.serve(app="basic:app", port=8001, reload=True)



================================================
FILE: cookbook/apps/fastapi/image_generation_model.py
================================================
from agno.agent import Agent
from agno.app.fastapi import FastAPIApp
from agno.models.google import Gemini

image_agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    ),
    agent_id="image_model",
)

fastapi_app = FastAPIApp(
    agents=[image_agent],
    name="Image Generation Model",
    app_id="image_generation_model",
    description="A model that generates images using the Gemini API.",
)

app = fastapi_app.get_app()

if __name__ == "__main__":
    fastapi_app.serve(app="image_generation_model:app", port=8001, reload=True)



================================================
FILE: cookbook/apps/fastapi/study_friend.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.app.fastapi import FastAPIApp
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.youtube import YouTubeTools

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(db=memory_db)

StudyBuddy = Agent(
    name="StudyBuddy",
    agent_id="study_friend",
    memory=memory,
    model=OpenAIChat("gpt-4o-mini"),
    enable_user_memories=True,
    storage=SqliteStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    tools=[DuckDuckGoTools(), YouTubeTools()],
    description=dedent("""\
        You are StudyBuddy, an expert educational mentor with deep expertise in personalized learning! 📚

        Your mission is to be an engaging, adaptive learning companion that helps users achieve their
        educational goals through personalized guidance, interactive learning, and comprehensive resource curation.
        """),
    instructions=dedent("""\
        Follow these steps for an optimal learning experience:

        1. Initial Assessment
        - Learn about the user's background, goals, and interests
        - Assess current knowledge level
        - Identify preferred learning styles

        2. Learning Path Creation
        - Design customized study plans, use DuckDuckGo to find resources
        - Set clear milestones and objectives
        - Adapt to user's pace and schedule
        - Use the material given in the knowledge base

        3. Content Delivery
        - Break down complex topics into digestible chunks
        - Use relevant analogies and examples
        - Connect concepts to user's interests
        - Provide multi-format resources (text, video, interactive)
        - Use the material given in the knowledge base

        4. Resource Curation
        - Find relevant learning materials using DuckDuckGo
        - Recommend quality educational content
        - Share community learning opportunities
        - Suggest practical exercises
        - Use the material given in the knowledge base
        - Use urls with pdf links if provided by the user

        5. Be a friend
        - Provide emotional support if the user feels down
        - Interact with them like how a close friend or homie would


        Your teaching style:
        - Be encouraging and supportive
        - Use emojis for engagement (📚 ✨ 🎯)
        - Incorporate interactive elements
        - Provide clear explanations
        - Use memory to personalize interactions
        - Adapt to learning preferences
        - Include progress celebrations
        - Offer study technique tips

        Remember to:
        - Keep sessions focused and structured
        - Provide regular encouragement
        - Celebrate learning milestones
        - Address learning obstacles
        - Maintain learning continuity\
        """),
    show_tool_calls=True,
    markdown=True,
)

fastapi_app = FastAPIApp(
    agents=[StudyBuddy],
    name="StudyBuddy",
    app_id="study_buddy",
    description="A study buddy that helps users achieve their educational goals through personalized guidance, interactive learning, and comprehensive resource curation.",
)

app = fastapi_app.get_app()

if __name__ == "__main__":
    fastapi_app.serve(app="study_friend:app", port=8001, workers=4, reload=True)



================================================
FILE: cookbook/apps/fastapi/team.py
================================================
"""
This example demonstrates a collaborative team of AI agents working together to research topics across different platforms.

The team consists of two specialized agents:
1. Reddit Researcher - Uses DuckDuckGo to find and analyze relevant Reddit posts
2. HackerNews Researcher - Uses HackerNews API to find and analyze relevant HackerNews posts

The agents work in "collaborate" mode, meaning they:
- Both are given the same task at the same time
- Work towards reaching consensus through discussion
- Are coordinated by a team leader that guides the discussion

The team leader moderates the discussion and determines when consensus is reached.

This setup is useful for:
- Getting diverse perspectives from different online communities
- Cross-referencing information across platforms
- Having agents collaborate to form more comprehensive analysis
- Reaching balanced conclusions through structured discussion

"""

from textwrap import dedent

from agno.agent import Agent
from agno.app.fastapi import FastAPIApp
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

reddit_researcher = Agent(
    name="Reddit Researcher",
    role="Research a topic on Reddit",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a Reddit researcher.
    You will be given a topic to research on Reddit.
    You will need to find the most relevant posts on Reddit.
    """),
)

hackernews_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Research a topic on HackerNews.",
    tools=[HackerNewsTools()],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a HackerNews researcher.
    You will be given a topic to research on HackerNews.
    You will need to find the most relevant posts on HackerNews.
    """),
)


discussion_team = Team(
    name="Discussion Team",
    mode="collaborate",
    team_id="discussion_team",
    model=OpenAIChat("gpt-4o"),
    members=[
        reddit_researcher,
        hackernews_researcher,
    ],
    instructions=[
        "You are a discussion master.",
        "You have to stop the discussion when you think the team has reached a consensus.",
    ],
    success_criteria="The team has reached a consensus.",
    enable_agentic_context=True,
    markdown=True,
)
fastapi_app = FastAPIApp(
    teams=[discussion_team],
    name="Team Example",
)
app = fastapi_app.get_app()

if __name__ == "__main__":
    fastapi_app.serve(app="team:app", port=8001, reload=True)



================================================
FILE: cookbook/apps/playground/README.md
================================================
# Agent Playground

Agno provides a beautiful Agent UI for interacting with your agents.

## Setup

### Create and activate a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### Export your API keys

```shell
export OPENAI_API_KEY=***
# If you need Exa search
export EXA_API_KEY=***
...
```

### Install libraries

```shell
pip install -U openai exa_py ddgs yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api agno
```

### Authenticate with agno.app

```
ag setup
```

## Connect your Agents to the Agent UI

```shell
python cookbook/playground/demo.py
```

## Test Multimodal Agents

```shell
python cookbook/playground/multimodal_agents.py
```

## Fully local Ollama Agents

### Pull llama3.1:8b

```shell
ollama pull llama3.1:8b
```

### Connect Ollama agents to the Agent UI

```shell
python cookbook/playground/ollama_agents.py
```

## xAi Grok Agents

```shell
python cookbook/playground/grok_agents.py
```

## Groq Agents

```shell
python cookbook/playground/groq_agents.py
```

## Gemini Agents

```shell
python cookbook/playground/gemini_agents.py
```



================================================
FILE: cookbook/apps/playground/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/apps/playground/agno_assist.py
================================================
"""🤖 Agno Assist - Your AI Assistant for Agno Framework!

This example shows how to create an AI support assistant that combines iterative knowledge searching
with Agno's documentation to provide comprehensive, well-researched answers about the Agno framework.

You can either use the "AgnoAssist" agent or the "AgnoAssistVoice" agent.

Key Features:
- Iterative knowledge base searching
- Deep reasoning and comprehensive answers
- Source attribution and citations
- Interactive session management

Example prompts for `AgnoAssist`:
- "What is Agno and what are its key features? Generate some audio content to explain the key features."
- "How do I create my first agent with Agno? Show me some example code."
- "What's the difference between Level 0 and Level 1 agents?"
- "How can I add memory to my Agno agent?"
- "How do I implement RAG with Agno? Generate a diagram of the process."

Example prompts for `AgnoAssistVoice`:
- "What is Agno and what are its key features?"
- "How do I create my first agent with Agno?"
- "What's the difference between Level 0 and Level 1 agents?"
- "What models does Agno support?"
"""

from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.playground import Playground
from agno.storage.sqlite import SqliteStorage
from agno.tools.dalle import DalleTools
from agno.tools.eleven_labs import ElevenLabsTools
from agno.tools.python import PythonTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths
cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

_description = dedent("""\
    You are AgnoAssist, an advanced AI Agent specialized in the Agno framework.
    Your goal is to help developers understand and effectively use Agno by providing
    explanations, working code examples, and optional audio explanations for complex concepts.""")

_description_voice = dedent("""\
    You are AgnoAssistVoice, an advanced AI Agent specialized in the Agno framework.
    Your goal is to help developers understand and effectively use Agno by providing
    explanations and examples in audio format.""")

_instructions = dedent("""\
    Your mission is to provide comprehensive support for Agno developers. Follow these steps to ensure the best possible response:

    1. **Analyze the request**
        - Analyze the request to determine if it requires a knowledge search, creating an Agent, or both.
        - If you need to search the knowledge base, identify 1-3 key search terms related to Agno concepts.
        - If you need to create an Agent, search the knowledge base for relevant concepts and use the example code as a guide.
        - When the user asks for an Agent, they mean an Agno Agent.
        - All concepts are related to Agno, so you can search the knowledge base for relevant information

    After Analysis, always start the iterative search process. No need to wait for approval from the user.

    2. **Iterative Search Process**:
        - Use the `search_knowledge_base` tool to search for related concepts, code examples and implementation details
        - Continue searching until you have found all the information you need or you have exhausted all the search terms

    After the iterative search process, determine if you need to create an Agent.
    If you do, ask the user if they want you to create the Agent and run it.

    3. **Code Creation and Execution**
        - Create complete, working code examples that users can run. For example:
        ```python
        from agno.agent import Agent
        from agno.tools.duckduckgo import DuckDuckGoTools

        agent = Agent(tools=[DuckDuckGoTools()])

        # Perform a web search and capture the response
        response = agent.run("What's happening in France?")
        ```
        - You must remember to use agent.run() and NOT agent.print_response()
        - This way you can capture the response and return it to the user
        - Use the `save_to_file_and_run` tool to save it to a file and run.
        - Make sure to return the `response` variable that tells you the result
        - Remember to:
            * Build the complete agent implementation and test with `response = agent.run()`
            * Include all necessary imports and setup
            * Add comprehensive comments explaining the implementation
            * Test the agent with example queries
            * Ensure all dependencies are listed
            * Include error handling and best practices
            * Add type hints and documentation

    4. **Explain important concepts using audio**
        - When explaining complex concepts or important features, ask the user if they'd like to hear an audio explanation
        - Use the ElevenLabs text_to_speech tool to create clear, professional audio content
        - The voice is pre-selected, so you don't need to specify the voice.
        - Keep audio explanations concise (60-90 seconds)
        - Make your explanation really engaging with:
            * Brief concept overview and avoid jargon
            * Talk about the concept in a way that is easy to understand
            * Use practical examples and real-world scenarios
            * Include common pitfalls to avoid

    5. **Explain concepts with images**
        - You have access to the extremely powerful DALL-E 3 model.
        - Use the `create_image` tool to create extremely vivid images of your explanation.

    Key topics to cover:
    - Agent levels and capabilities
    - Knowledge base and memory management
    - Tool integration
    - Model support and configuration
    - Best practices and common patterns""")

# Initialize knowledge base
agent_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/llms-full.txt"],
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Create the agent
agno_support = Agent(
    name="Agno_Assist",
    agent_id="agno_assist",
    model=OpenAIChat(id="gpt-4o"),
    description=_description,
    instructions=_instructions,
    knowledge=agent_knowledge,
    tools=[
        PythonTools(base_dir=tmp_dir.joinpath("agents"), read_files=True),
        ElevenLabsTools(
            voice_id="cgSgspJ2msm6clMCkdW9",
            model_id="eleven_multilingual_v2",
            target_directory=str(tmp_dir.joinpath("audio").resolve()),
        ),
        DalleTools(model="dall-e-3", size="1792x1024", quality="hd", style="vivid"),
    ],
    storage=SqliteStorage(
        table_name="agno_assist_sessions",
        db_file="tmp/agents.db",
        auto_upgrade_schema=True,
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

agno_support_voice = Agent(
    name="Agno_Assist_Voice",
    agent_id="agno-assist-voice",
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "pcm16"},  # Wav not supported for streaming
    ),
    description=_description_voice,
    instructions=_instructions,
    knowledge=agent_knowledge,
    tools=[PythonTools(base_dir=tmp_dir.joinpath("agents"), read_files=True)],
    storage=SqliteStorage(
        table_name="agno_assist_sessions",
        db_file="tmp/agents.db",
        auto_upgrade_schema=True,
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

# Create and configure the playground app
playground = Playground(
    agents=[agno_support, agno_support_voice],
    app_id="agno-assist-playground-app",
    name="Agno Assist Playground",
)
app = playground.get_app()

if __name__ == "__main__":
    load_kb = False
    if load_kb:
        agent_knowledge.load(recreate=True)
    playground.serve(app="agno_assist:app", reload=True)



================================================
FILE: cookbook/apps/playground/audio_conversation_agent.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage

audio_and_text_agent = Agent(
    agent_id="audio-text-agent",
    name="Audio and Text Chat Agent",
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "pcm16"},  # Wav not supported for streaming
    ),
    debug_mode=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="audio_agent", db_file="tmp/audio_agent.db", auto_upgrade_schema=True
    ),
)

playground = Playground(
    agents=[audio_and_text_agent],
    name="Audio Conversation Agent",
    description="A playground for audio conversation agent",
    app_id="audio-conversation-agent",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="audio_conversation_agent:app", reload=True)



================================================
FILE: cookbook/apps/playground/azure_openai_agents.py
================================================
"""Run `pip install openai exa_py ddgs yfinance pypdf sqlalchemy 'fastapi[standard]' agno youtube-transcript-api` to install dependencies."""

from datetime import datetime
from textwrap import dedent

from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage
from agno.tools.dalle import DalleTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.yfinance import YFinanceTools
from agno.tools.youtube import YouTubeTools

agent_storage_file: str = "tmp/azure_openai_agents.db"

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    agent_id="web-agent",
    model=AzureOpenAI(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Break down the users request into 2-3 different searches.",
        "Always include sources",
    ],
    storage=SqliteStorage(
        table_name="web_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance-agent",
    model=AzureOpenAI(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Always use tables to display data"],
    storage=SqliteStorage(
        table_name="finance_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

image_agent = Agent(
    name="Image Agent",
    agent_id="image_agent",
    model=AzureOpenAI(id="gpt-4o"),
    tools=[DalleTools(model="dall-e-3", size="1792x1024", quality="hd", style="vivid")],
    description="You are an AI agent that can generate images using DALL-E.",
    instructions=[
        "When the user asks you to create an image, use the `create_image` tool to create the image.",
        "Don't provide the URL of the image in the response. Only describe what image was generated.",
    ],
    markdown=True,
    debug_mode=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="image_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
)

research_agent = Agent(
    name="Research Agent",
    role="Write research reports for the New York Times",
    agent_id="research-agent",
    model=AzureOpenAI(id="gpt-4o"),
    tools=[
        ExaTools(
            start_published_date=datetime.now().strftime("%Y-%m-%d"), type="keyword"
        )
    ],
    description=(
        "You are a Research Agent that has the special skill of writing New York Times worthy articles. "
        "If you can directly respond to the user, do so. If the user asks for a report or provides a topic, follow the instructions below."
    ),
    instructions=[
        "For the provided topic, run 3 different searches.",
        "Read the results carefully and prepare a NYT worthy article.",
        "Focus on facts and make sure to provide references.",
    ],
    expected_output=dedent("""\
    Your articles should be engaging, informative, well-structured and in markdown format. They should follow the following structure:

    ## Engaging Article Title

    ### Overview
    {give a brief introduction of the article and why the user should read this report}
    {make this section engaging and create a hook for the reader}

    ### Section 1
    {break the article into sections}
    {provide details/facts/processes in this section}

    ... more sections as necessary...

    ### Takeaways
    {provide key takeaways from the article}

    ### References
    - [Reference 1](link)
    - [Reference 2](link)
    """),
    storage=SqliteStorage(
        table_name="research_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

youtube_agent = Agent(
    name="YouTube Agent",
    agent_id="youtube-agent",
    model=AzureOpenAI(id="gpt-4o"),
    tools=[YouTubeTools()],
    description="You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.",
    instructions=[
        "Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.",
        "Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.",
        "If you cannot find the answer in the video, say so and ask the user to provide more details.",
        "Keep your answers concise and engaging.",
    ],
    add_history_to_messages=True,
    num_history_responses=5,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="youtube_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    markdown=True,
)

playground = Playground(
    agents=[web_agent, finance_agent, youtube_agent, research_agent, image_agent],
    name="Azure OpenAI Agents",
    description="A playground for Azure OpenAI agents",
    app_id="azure-openai-agents",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="azure_openai_agents:app", reload=True)



================================================
FILE: cookbook/apps/playground/basic.py
================================================
from agno.agent import Agent
from agno.memory.agent import AgentMemory
from agno.memory.db.postgres import PgMemoryDb
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.storage.postgres import PostgresStorage

agent_storage_file: str = "tmp/agents.db"

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"),
    memory=AgentMemory(
        db=PgMemoryDb(
            table_name="agent_memory",
            db_url=db_url,
        ),
        # Create and store personalized memories for this user
        create_user_memories=True,
        # Update memories for the user after each run
        update_user_memories_after_run=True,
        # Create and store session summaries
        create_session_summary=True,
        # Update session summaries after each run
        update_session_summary_after_run=True,
    ),
    storage=PostgresStorage(
        table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
)

playground = Playground(
    agents=[
        basic_agent,
    ],
    name="Basic Agent",
    description="A playground for basic agent",
    app_id="basic-agent",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="basic:app", reload=True)



================================================
FILE: cookbook/apps/playground/blog_to_podcast.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.playground import Playground
from agno.storage.sqlite import SqliteStorage
from agno.tools.eleven_labs import ElevenLabsTools
from agno.tools.firecrawl import FirecrawlTools

agent_storage_file: str = "tmp/agents.db"
image_agent_storage_file: str = "tmp/image_agent.db"


blog_to_podcast_agent = Agent(
    name="Blog to Podcast Agent",
    agent_id="blog_to_podcast_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ElevenLabsTools(
            voice_id="JBFqnCBsd6RMkjVDRZzb",
            model_id="eleven_multilingual_v2",
            target_directory="audio_generations",
        ),
        FirecrawlTools(),
    ],
    description="You are an AI agent that can generate audio using the ElevenLabs API.",
    instructions=[
        "When the user provides a blog URL:",
        "1. Use FirecrawlTools to scrape the blog content",
        "2. Create a concise summary of the blog content that is NO MORE than 2000 characters long",
        "3. The summary should capture the main points while being engaging and conversational",
        "4. Use the ElevenLabsTools to convert the summary to audio",
        "You don't need to find the appropriate voice first, I already specified the voice to user",
        "Don't return file name or file url in your response or markdown just tell the audio was created successfully",
        "Ensure the summary is within the 2000 character limit to avoid ElevenLabs API limits",
    ],
    markdown=True,
    debug_mode=True,
    add_history_to_messages=True,
    storage=SqliteStorage(
        table_name="blog_to_podcast_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

playground = Playground(
    agents=[blog_to_podcast_agent],
    app_id="blog-to-podcast-playground-app",
    name="Blog to Podcast Playground",
    description="A playground for blog to podcast",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="blog_to_podcast:app", reload=True)



================================================
FILE: cookbook/apps/playground/coding_agent.py
================================================
"""Run `pip install ollama sqlalchemy 'fastapi[standard]'` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage

local_agent_storage_file: str = "tmp/local_agents.db"
common_instructions = [
    "If the user asks about you or your skills, tell them your name and role.",
]

coding_agent = Agent(
    name="Coding Agent",
    agent_id="coding_agent",
    model=Ollama(id="hhao/qwen2.5-coder-tools:32b"),
    reasoning=True,
    markdown=True,
    add_history_to_messages=True,
    description="You are a coding agent",
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="coding_agent",
        db_file=local_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

playground = Playground(
    agents=[coding_agent],
    name="Coding Agent",
    description="A playground for coding agent",
    app_id="coding-agent",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="coding_agent:app", reload=True)



================================================
FILE: cookbook/apps/playground/competitor_analysis.py
================================================
"""🔍 Competitor Analysis Agent!

Key capabilities:
- Company discovery using Firecrawl search
- Website scraping and content analysis
- Competitive intelligence gathering
- SWOT analysis with reasoning
- Strategic recommendations
- Structured thinking and analysis

Example queries to try:
- "Analyze OpenAI's main competitors in the LLM space"
- "Compare Uber vs Lyft in the ride-sharing market"
- "Analyze Tesla's competitive position vs traditional automakers"
- "Research fintech competitors to Stripe"
- "Analyze Nike vs Adidas in the athletic apparel market"

Dependencies: `pip install openai firecrawl-py agno`
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.playground import Playground
from agno.storage.sqlite import SqliteStorage
from agno.tools.firecrawl import FirecrawlTools
from agno.tools.reasoning import ReasoningTools

competitor_analysis_agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    name="Competitor Analysis Agent",
    tools=[
        FirecrawlTools(
            search=True,
            scrape=True,
            mapping=True,
            formats=["markdown", "links", "html"],
            search_params={
                "limit": 2,
            },
            limit=5,
        ),
        ReasoningTools(add_instructions=True),
    ],
    description="You run competitor analysis for a company.",
    instructions=dedent("""\
        1. Initial Research & Discovery:
           - Use search tool to find information about the target company
           - Search for 'companies like [company name]'. Do only 1 search.
           - Search for industry reports and market analysis. Do only 1 search.
           - Use the think tool to plan your research approach.
        2. Competitor Identification:
           - Search for 3 identified competitor using Firecrawl.
           - Scrape competitor websites using Firecrawl.
           - Extract product information, pricing, and value propositions.
        3. Map out the competitive landscape.
           - Use the analyze tool after gathering information on each competitor
           - Compare features, pricing, and market positioning
           - Identify patterns and competitive dynamics
        4. Conduct SWOT analysis for each major competitor
           - Use reasoning to identify competitive advantages
           - Analyze market trends and opportunities
           - Develop strategic recommendations

        - Always use the think tool before starting major research phases
        - Use the analyze tool to process findings and draw insights
        - Be thorough but focused in your analysis
        - Provide evidence-based recommendations
    """),
    expected_output=dedent("""\
    # Competitive Analysis Report: {Target Company}

    ## Executive Summary
    {High-level overview of competitive landscape and key findings}

    ## Research Methodology
    - Search queries used
    - Websites analyzed
    - Key information sources

    ## Market Overview
    ### Industry Context
    - Market size and growth rate
    - Key trends and drivers
    - Regulatory environment

    ### Competitive Landscape
    - Major players identified
    - Market segmentation
    - Competitive dynamics

    ## Competitor Analysis

    ### Competitor 1: {Name}
    #### Company Overview
    - Website: {URL}
    - Founded: {Year}
    - Headquarters: {Location}
    - Company size: {Employees/Revenue if available}

    #### Products & Services
    - Core offerings
    - Key features and capabilities
    - Pricing model and tiers
    - Target market segments

    #### Digital Presence Analysis
    - Website structure and user experience
    - Key messaging and value propositions
    - Content strategy and resources
    - Customer proof points

    #### SWOT Analysis
    **Strengths:**
    - {Evidence-based strengths}

    **Weaknesses:**
    - {Identified weaknesses}

    **Opportunities:**
    - {Market opportunities}

    **Threats:**
    - {Competitive threats}

    ### Competitor 2: {Name}
    {Similar structure as above}

    ### Competitor 3: {Name}
    {Similar structure as above}

    ## Comparative Analysis

    ### Feature Comparison Matrix
    | Feature | {Target} | Competitor 1 | Competitor 2 | Competitor 3 |
    |---------|----------|--------------|--------------|--------------|
    | {Feature 1} | ✓/✗ | ✓/✗ | ✓/✗ | ✓/✗ |
    | {Feature 2} | ✓/✗ | ✓/✗ | ✓/✗ | ✓/✗ |

    ### Pricing Comparison
    | Company | Entry Level | Professional | Enterprise |
    |---------|-------------|--------------|------------|
    | {Pricing details extracted from websites} |

    ### Market Positioning Analysis
    {Analysis of how each competitor positions themselves}

    ## Strategic Insights

    ### Key Findings
    1. {Major insight with evidence}
    2. {Competitive dynamics observed}
    3. {Market gaps identified}

    ### Competitive Advantages
    - {Target company's advantages}
    - {Unique differentiators}

    ### Competitive Risks
    - {Main threats from competitors}
    - {Market challenges}

    ## Strategic Recommendations

    ### Immediate Actions (0-3 months)
    1. {Quick competitive responses}
    2. {Low-hanging fruit opportunities}

    ### Short-term Strategy (3-12 months)
    1. {Product/service enhancements}
    2. {Market positioning adjustments}

    ### Long-term Strategy (12+ months)
    1. {Sustainable differentiation}
    2. {Market expansion opportunities}

    ## Conclusion
    {Summary of competitive position and strategic imperatives}
    """),
    markdown=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="competitor_analysis",
        db_file="tmp/competitor_analysis.db",
    ),
)


playground = Playground(
    agents=[competitor_analysis_agent],
    name="Competitor Analysis",
    description="Agents for competitor analysis",
    app_id="competitor-analysis",
)
app = playground.get_app(use_async=False)

if __name__ == "__main__":
    playground.serve(app="competitor_analysis:app", reload=True)



================================================
FILE: cookbook/apps/playground/demo.py
================================================
"""Run `pip install openai exa_py ddgs yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api python-docx agno` to install dependencies."""

from datetime import datetime
from textwrap import dedent
from typing import List

from agno.agent import Agent
from agno.memory.v2 import Memory
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage
from agno.tools.dalle import DalleTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.yfinance import YFinanceTools
from agno.tools.youtube import YouTubeTools
from pydantic import BaseModel, Field

agent_storage_file: str = "tmp/agents.db"
memory_storage_file: str = "tmp/memory.db"
image_agent_storage_file: str = "tmp/image_agent.db"

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory_db = SqliteMemoryDb(table_name="memory", db_file=memory_storage_file)

# No need to set the model, it gets set by the agent to the agent's model
memory = Memory(db=memory_db)

simple_agent = Agent(
    name="Simple Agent",
    role="Answer basic questions",
    agent_id="simple-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=SqliteStorage(
        table_name="simple_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    agent_id="web-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Break down the users request into 2-3 different searches.",
        "Always include sources",
    ],
    storage=SqliteStorage(
        table_name="web_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Always use tables to display data"],
    storage=SqliteStorage(
        table_name="finance_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

image_agent = Agent(
    name="Image Agent",
    agent_id="image_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools(model="dall-e-3", size="1792x1024", quality="hd", style="vivid")],
    description="You are an AI agent that can generate images using DALL-E.",
    instructions=[
        "When the user asks you to create an image, use the `create_image` tool to create the image.",
        "Don't provide the URL of the image in the response. Only describe what image was generated.",
    ],
    memory=memory,
    markdown=True,
    enable_user_memories=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="image_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

research_agent = Agent(
    name="Research Agent",
    role="Write research reports for the New York Times",
    agent_id="research-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ExaTools(
            start_published_date=datetime.now().strftime("%Y-%m-%d"), type="keyword"
        )
    ],
    description=(
        "You are a Research Agent that has the special skill of writing New York Times worthy articles. "
        "If you can directly respond to the user, do so. If the user asks for a report or provides a topic, follow the instructions below."
    ),
    instructions=[
        "For the provided topic, run 3 different searches.",
        "Read the results carefully and prepare a NYT worthy article.",
        "Focus on facts and make sure to provide references.",
    ],
    expected_output=dedent("""\
    Your articles should be engaging, informative, well-structured and in markdown format. They should follow the following structure:

    ## Engaging Article Title

    ### Overview
    {give a brief introduction of the article and why the user should read this report}
    {make this section engaging and create a hook for the reader}

    ### Section 1
    {break the article into sections}
    {provide details/facts/processes in this section}

    ... more sections as necessary...

    ### Takeaways
    {provide key takeaways from the article}

    ### References
    - [Reference 1](link)
    - [Reference 2](link)
    """),
    memory=memory,
    enable_user_memories=True,
    storage=SqliteStorage(
        table_name="research_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

youtube_agent = Agent(
    name="YouTube Agent",
    agent_id="youtube-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YouTubeTools()],
    description="You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.",
    instructions=[
        "Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.",
        "Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.",
        "If you cannot find the answer in the video, say so and ask the user to provide more details.",
        "Keep your answers concise and engaging.",
    ],
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    num_history_responses=5,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="youtube_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    markdown=True,
)


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Notice: agents with response model won't stream answers
movie_writer = Agent(
    name="Movie Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    response_model=MovieScript,
    storage=SqliteStorage(
        table_name="movie_writer",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

playground = Playground(
    agents=[
        simple_agent,
        web_agent,
        finance_agent,
        youtube_agent,
        research_agent,
        image_agent,
        movie_writer,
    ],
    app_id="demo-playground-app",
    name="Demo Playground",
    description="A playground for demo",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="demo:app", reload=True)



================================================
FILE: cookbook/apps/playground/gemini_agents.py
================================================
from agno.agent import Agent
from agno.models.google import Gemini
from agno.playground import Playground, serve_playground_app
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    name="Finance Agent",
    agent_id="finance-agent",
    model=Gemini(id="gemini-2.0-flash-exp"),
    tools=[YFinanceTools(stock_price=True)],
    debug_mode=True,
)

playground = Playground(
    agents=[finance_agent],
    name="Gemini Agents",
    description="A playground for Gemini agents",
    app_id="gemini-agents",
)
app = playground.get_app(use_async=False)

if __name__ == "__main__":
    playground.serve(app="gemini_agents:app", reload=True)



================================================
FILE: cookbook/apps/playground/grok_agents.py
================================================
"""Usage:
1. Install libraries: `pip install openai ddgs yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api agno`
2. Run the script: `python cookbook/playground/grok_agents.py`
"""

from agno.agent import Agent
from agno.models.xai import xAI
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.tools.youtube import YouTubeTools

xai_agent_storage: str = "tmp/xai_agents.db"
common_instructions = [
    "If the user about you or your skills, tell them your name and role.",
]

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    agent_id="web-agent",
    model=xAI(id="grok-beta"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Use the `duckduckgo_search` or `duckduckgo_news` tools to search the web for information.",
        "Always include sources you used to generate the answer.",
    ]
    + common_instructions,
    storage=SqliteStorage(
        table_name="web_agent", db_file=xai_agent_storage, auto_upgrade_schema=True
    ),
    show_tool_calls=True,
    add_history_to_messages=True,
    num_history_responses=2,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance-agent",
    model=xAI(id="grok-beta"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    description="You are an investment analyst that researches stocks and helps users make informed decisions.",
    instructions=["Always use tables to display data"] + common_instructions,
    storage=SqliteStorage(
        table_name="finance_agent", db_file=xai_agent_storage, auto_upgrade_schema=True
    ),
    show_tool_calls=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    markdown=True,
)


youtube_agent = Agent(
    name="YouTube Agent",
    role="Understand YouTube videos and answer questions",
    agent_id="youtube-agent",
    model=xAI(id="grok-beta"),
    tools=[YouTubeTools()],
    description="You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.",
    instructions=[
        "Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.",
        "Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.",
        "If you cannot find the answer in the video, say so and ask the user to provide more details.",
        "Keep your answers concise and engaging.",
    ]
    + common_instructions,
    storage=SqliteStorage(
        table_name="youtube_agent", db_file=xai_agent_storage, auto_upgrade_schema=True
    ),
    show_tool_calls=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

playground = Playground(
    agents=[finance_agent, youtube_agent, web_agent],
    name="Grok Agents",
    description="A playground for Grok agents",
    app_id="grok-agents",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="grok_agents:app", reload=True)



================================================
FILE: cookbook/apps/playground/groq_agents.py
================================================
"""Usage:
1. Install libraries: `pip install groq ddgs yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api agno`
2. Run the script: `python cookbook/playground/groq_agents.py`
"""

from agno.agent import Agent
from agno.models.groq import Groq
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.tools.youtube import YouTubeTools

xai_agent_storage: str = "tmp/groq_agents.db"
common_instructions = [
    "If the user about you or your skills, tell them your name and role.",
]

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    agent_id="web-agent",
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Use the `duckduckgo_search` or `duckduckgo_news` tools to search the web for information.",
        "Always include sources you used to generate the answer.",
    ]
    + common_instructions,
    storage=SqliteStorage(
        table_name="web_agent", db_file=xai_agent_storage, auto_upgrade_schema=True
    ),
    show_tool_calls=True,
    add_history_to_messages=True,
    num_history_responses=2,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance-agent",
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    description="You are an investment analyst that researches stocks and helps users make informed decisions.",
    instructions=["Always use tables to display data"] + common_instructions,
    storage=SqliteStorage(
        table_name="finance_agent", db_file=xai_agent_storage, auto_upgrade_schema=True
    ),
    show_tool_calls=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    markdown=True,
)


youtube_agent = Agent(
    name="YouTube Agent",
    role="Understand YouTube videos and answer questions",
    agent_id="youtube-agent",
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[YouTubeTools()],
    description="You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.",
    instructions=[
        "Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.",
        "Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.",
        "If you cannot find the answer in the video, say so and ask the user to provide more details.",
        "Keep your answers concise and engaging.",
        "If the user just provides a URL, summarize the video and answer questions about it.",
    ]
    + common_instructions,
    storage=SqliteStorage(
        table_name="youtube_agent", db_file=xai_agent_storage, auto_upgrade_schema=True
    ),
    show_tool_calls=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

playground = Playground(
    agents=[finance_agent, youtube_agent, web_agent],
    name="Groq Agents",
    description="A playground for Groq agents",
    app_id="groq-agents",
)
app = playground.get_app(use_async=False)

if __name__ == "__main__":
    playground.serve(app="groq_agents:app", reload=True)



================================================
FILE: cookbook/apps/playground/mcp_demo.py
================================================
"""This example shows how to run an Agent using our MCP integration in the Agno Playground.

For this example to run you need:
- Create a GitHub personal access token following these steps:
    - https://github.com/modelcontextprotocol/servers/tree/main/src/github#setup
- Set the GITHUB_TOKEN environment variable: `export GITHUB_TOKEN=<Your GitHub access token>`
- Run: `pip install agno mcp openai` to install the dependencies
"""

from contextlib import asynccontextmanager
from os import getenv
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.playground import Playground
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.tools.mcp import MCPTools
from fastapi import FastAPI
from mcp import StdioServerParameters

agent_storage_file: str = "tmp/agents.db"


# MCP server parameters setup
github_token = getenv("GITHUB_TOKEN") or getenv("GITHUB_ACCESS_TOKEN")
if not github_token:
    raise ValueError("GITHUB_TOKEN environment variable is required")

server_params = StdioServerParameters(
    command="npx",
    args=["-y", "@modelcontextprotocol/server-github"],
)


# This is required to start the MCP connection correctly in the FastAPI lifecycle
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage MCP connection lifecycle inside a FastAPI app"""
    global mcp_tools

    # Startuplogic: connect to our MCP server
    mcp_tools = MCPTools(server_params=server_params)
    await mcp_tools.connect()

    # Add the MCP tools to our Agent
    agent.tools = [mcp_tools]

    yield

    # Shutdown: Close MCP connection
    await mcp_tools.close()


agent = Agent(
    name="MCP GitHub Agent",
    instructions=dedent("""\
        You are a GitHub assistant. Help users explore repositories and their activity.

        - Use headings to organize your responses
        - Be concise and focus on relevant information\
    """),
    model=OpenAIChat(id="gpt-4o"),
    storage=SqliteAgentStorage(
        table_name="basic_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
)

# Setup the Playground app
playground = Playground(
    agents=[agent],
    name="MCP Demo",
    description="A playground for MCP",
    app_id="mcp-demo",
)

# Initialize the Playground app with our lifespan logic
app = playground.get_app(lifespan=lifespan)


if __name__ == "__main__":
    playground.serve(app="mcp_demo:app", reload=True)



================================================
FILE: cookbook/apps/playground/multimodal_agents.py
================================================
"""
1. Install dependencies: `pip install openai sqlalchemy 'fastapi[standard]' agno requests`
2. Authenticate with agno: `agno setup`
3. Run the agent: `python cookbook/playground/multimodal_agent.py`

Docs on Agent UI: https://docs.agno.com/agent-ui
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.models.response import FileType
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage
from agno.tools.dalle import DalleTools
from agno.tools.eleven_labs import ElevenLabsTools
from agno.tools.fal import FalTools
from agno.tools.giphy import GiphyTools
from agno.tools.models_labs import ModelsLabTools

image_agent_storage_file: str = "tmp/image_agent.db"

image_agent = Agent(
    name="DALL-E Image Agent",
    agent_id="image_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools(model="dall-e-3", size="1792x1024", quality="hd", style="vivid")],
    description="You are an AI agent that can generate images using DALL-E.",
    instructions=[
        "When the user asks you to create an image, use the `create_image` tool to create the image.",
        "Don't provide the URL of the image in the response. Only describe what image was generated.",
    ],
    markdown=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="image_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

ml_gif_agent = Agent(
    name="ModelsLab GIF Agent",
    agent_id="ml_gif_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ModelsLabTools(wait_for_completion=True, file_type=FileType.GIF)],
    description="You are an AI agent that can generate gifs using the ModelsLabs API.",
    instructions=[
        "When the user asks you to create an image, use the `generate_media` tool to create the image.",
        "Don't provide the URL of the image in the response. Only describe what image was generated.",
    ],
    markdown=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="ml_gif_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

ml_music_agent = Agent(
    name="ModelsLab Music Agent",
    agent_id="ml_music_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ModelsLabTools(wait_for_completion=True, file_type=FileType.MP3)],
    description="You are an AI agent that can generate music using the ModelsLabs API.",
    instructions=[
        "When generating music, use the `generate_media` tool with detailed prompts that specify:",
        "- The genre and style of music (e.g., classical, jazz, electronic)",
        "- The instruments and sounds to include",
        "- The tempo, mood and emotional qualities",
        "- The structure (intro, verses, chorus, bridge, etc.)",
        "Create rich, descriptive prompts that capture the desired musical elements.",
        "Focus on generating high-quality, complete instrumental pieces.",
        "Keep responses simple and only confirm when music is generated successfully.",
        "Do not include any file names, URLs or technical details in responses.",
    ],
    markdown=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="ml_music_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

ml_video_agent = Agent(
    name="ModelsLab Video Agent",
    agent_id="ml_video_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ModelsLabTools(wait_for_completion=True, file_type=FileType.MP4)],
    description="You are an AI agent that can generate videos using the ModelsLabs API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "Don't provide the URL of the video in the response. Only describe what video was generated.",
    ],
    markdown=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="ml_video_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

fal_agent = Agent(
    name="Fal Video Agent",
    agent_id="fal_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FalTools("fal-ai/hunyuan-video")],
    description="You are an AI agent that can generate videos using the Fal API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "Don't provide the URL of the video in the response. Only describe what video was generated.",
    ],
    markdown=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="fal_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

gif_agent = Agent(
    name="Gif Generator Agent",
    agent_id="gif_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GiphyTools()],
    description="You are an AI agent that can generate gifs using Giphy.",
    instructions=[
        "When the user asks you to create a gif, come up with the appropriate Giphy query and use the `search_gifs` tool to find the appropriate gif.",
        "Don't return the URL, only describe what you created.",
    ],
    markdown=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="gif_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

audio_agent = Agent(
    name="Audio Generator Agent",
    agent_id="audio_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ElevenLabsTools(
            voice_id="JBFqnCBsd6RMkjVDRZzb",
            model_id="eleven_multilingual_v2",
            target_directory="audio_generations",
        )
    ],
    description="You are an AI agent that can generate audio using the ElevenLabs API.",
    instructions=[
        "When the user asks you to generate audio, use the `text_to_speech` tool to generate the audio.",
        "You'll generate the appropriate prompt to send to the tool to generate audio.",
        "You don't need to find the appropriate voice first, I already specified the voice to user."
        "Don't return file name or file url in your response or markdown just tell the audio was created successfully.",
        "The audio should be long and detailed.",
    ],
    markdown=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="audio_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)


playground = Playground(
    agents=[
        image_agent,
        ml_gif_agent,
        ml_music_agent,
        ml_video_agent,
        fal_agent,
        gif_agent,
        audio_agent,
    ],
    name="Multimodal Agents",
    description="A playground for multimodal agents",
    app_id="multimodal-agents",
)
app = playground.get_app(use_async=False)

if __name__ == "__main__":
    playground.serve(app="multimodal_agents:app", reload=True)



================================================
FILE: cookbook/apps/playground/ollama_agents.py
================================================
"""Run `pip install ollama ddgs yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api agno` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.playground import Playground
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.tools.youtube import YouTubeTools

local_agent_storage_file: str = "tmp/local_agents.db"
common_instructions = [
    "If the user about you or your skills, tell them your name and role.",
]

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    agent_id="web-agent",
    model=Ollama(id="llama3.1:8b"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources."] + common_instructions,
    storage=SqliteStorage(
        table_name="web_agent",
        db_file=local_agent_storage_file,
        auto_upgrade_schema=True,
    ),
    show_tool_calls=True,
    add_history_to_messages=True,
    num_history_responses=2,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance-agent",
    model=Ollama(id="llama3.1:8b"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    description="You are an investment analyst that researches stocks and helps users make informed decisions.",
    instructions=["Always use tables to display data"] + common_instructions,
    storage=SqliteStorage(
        table_name="finance_agent",
        db_file=local_agent_storage_file,
        auto_upgrade_schema=True,
    ),
    add_history_to_messages=True,
    num_history_responses=5,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    markdown=True,
)


youtube_agent = Agent(
    name="YouTube Agent",
    role="Understand YouTube videos and answer questions",
    agent_id="youtube-agent",
    model=Ollama(id="llama3.1:8b"),
    tools=[YouTubeTools()],
    description="You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.",
    instructions=[
        "Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.",
        "Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.",
        "If you cannot find the answer in the video, say so and ask the user to provide more details.",
        "Keep your answers concise and engaging.",
    ]
    + common_instructions,
    add_history_to_messages=True,
    num_history_responses=5,
    show_tool_calls=True,
    add_name_to_instructions=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="youtube_agent",
        db_file=local_agent_storage_file,
        auto_upgrade_schema=True,
    ),
    markdown=True,
)

playground = Playground(
    agents=[web_agent, finance_agent, youtube_agent],
    name="Ollama Agents",
    description="A playground for ollama agents",
    app_id="ollama-agents",
)
app = playground.get_app(use_async=False)


if __name__ == "__main__":
    playground.serve(app="ollama_agents:app", reload=True)



================================================
FILE: cookbook/apps/playground/reasoning_demo.py
================================================
"""Run `pip install openai exa_py ddgs yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api python-docx agno` to install dependencies."""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.knowledge import KnowledgeTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.thinking import ThinkingTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.lancedb import LanceDb, SearchType

agent_storage_file: str = "tmp/agents.db"
image_agent_storage_file: str = "tmp/image_agent.db"

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Always use tables to display data"],
    storage=SqliteStorage(
        table_name="finance_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

cot_agent = Agent(
    name="Chain-of-Thought Agent",
    role="Answer basic questions",
    agent_id="cot-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=SqliteStorage(
        table_name="cot_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
    reasoning=True,
)

reasoning_model_agent = Agent(
    name="Reasoning Model Agent",
    role="Reasoning about Math",
    agent_id="reasoning-model-agent",
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=OpenAIChat(id="o3-mini"),
    instructions=["You are a reasoning agent that can reason about math."],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
    storage=SqliteStorage(
        table_name="reasoning_model_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

reasoning_tool_agent = Agent(
    name="Reasoning Tool Agent",
    role="Answer basic questions",
    agent_id="reasoning-tool-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=SqliteStorage(
        table_name="reasoning_tool_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
    tools=[ReasoningTools()],
)


web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    agent_id="web_agent",
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="web_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

thinking_tool_agent = Agent(
    name="Thinking Tool Agent",
    agent_id="thinking_tool_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ThinkingTools(add_instructions=True), YFinanceTools(enable_all=True)],
    instructions=dedent("""\
        You are a seasoned Wall Street analyst with deep expertise in market analysis! 📊

        Follow these steps for comprehensive financial analysis:
        1. Market Overview
           - Latest stock price
           - 52-week high and low
        2. Financial Deep Dive
           - Key metrics (P/E, Market Cap, EPS)
        3. Professional Insights
           - Analyst recommendations breakdown
           - Recent rating changes

        4. Market Context
           - Industry trends and positioning
           - Competitive analysis
           - Market sentiment indicators

        Your reporting style:
        - Begin with an executive summary
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends (📈 📉)
        - Highlight key insights with bullet points
        - Compare metrics to industry averages
        - Include technical term explanations
        - End with a forward-looking analysis

        Risk Disclosure:
        - Always highlight potential risk factors
        - Note market uncertainties
        - Mention relevant regulatory concerns\
    """),
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
    stream_intermediate_steps=True,
    storage=SqliteStorage(
        table_name="thinking_tool_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
)


agno_docs = UrlKnowledge(
    urls=["https://www.paulgraham.com/read.html"],
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
    ),
)

knowledge_tools = KnowledgeTools(
    knowledge=agno_docs,
    think=True,
    search=True,
    analyze=True,
    add_few_shot=True,
)
knowledge_agent = Agent(
    agent_id="knowledge_agent",
    name="Knowledge Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[knowledge_tools],
    show_tool_calls=True,
    markdown=True,
    storage=SqliteStorage(
        table_name="knowledge_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

reasoning_finance_team = Team(
    name="Reasoning Finance Team",
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o"),
    members=[
        web_agent,
        finance_agent,
    ],
    # reasoning=True,
    tools=[ReasoningTools(add_instructions=True)],
    # uncomment it to use knowledge tools
    # tools=[knowledge_tools],
    team_id="reasoning_finance_team",
    debug_mode=True,
    instructions=[
        "Only output the final answer, no other text.",
        "Use tables to display data",
    ],
    markdown=True,
    show_members_responses=True,
    enable_agentic_context=True,
    add_datetime_to_instructions=True,
    success_criteria="The team has successfully completed the task.",
    storage=SqliteStorage(
        table_name="reasoning_finance_team",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
)


playground = Playground(
    agents=[
        cot_agent,
        reasoning_tool_agent,
        reasoning_model_agent,
        knowledge_agent,
        thinking_tool_agent,
    ],
    teams=[reasoning_finance_team],
    name="Reasoning Demo",
    app_id="reasoning-demo",
    description="A playground for reasoning",
)
app = playground.get_app()

if __name__ == "__main__":
    asyncio.run(agno_docs.aload(recreate=True))
    playground.serve(app="reasoning_demo:app", reload=True)


# reasoning_tool_agent
# Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river.
# The boat is only big enough for the man and one item. If left unattended together,
# the fox will eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?


# knowledge_agent prompt
# What does Paul Graham explain here with respect to need to read?



================================================
FILE: cookbook/apps/playground/teams_demo.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.memory.v2 import Memory
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.models.anthropic import Claude
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.postgres import PostgresStorage
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.yfinance import YFinanceTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory_db = PostgresMemoryDb(table_name="memory", db_url=db_url)

# No need to set the model, it gets set by the agent to the agent's model
memory = Memory(db=memory_db)

agent_storage = PostgresStorage(
    table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
)

file_agent = Agent(
    name="File Upload Agent",
    agent_id="file-upload-agent",
    role="Answer questions about the uploaded files",
    model=Claude(id="claude-3-7-sonnet-latest"),
    storage=agent_storage,
    memory=memory,
    enable_user_memories=True,
    instructions=[
        "You are an AI agent that can analyze files.",
        "You are given a file and you need to answer questions about the file.",
    ],
    show_tool_calls=True,
    markdown=True,
)

video_agent = Agent(
    name="Video Understanding Agent",
    model=Gemini(id="gemini-2.0-flash"),
    agent_id="video-understanding-agent",
    role="Answer questions about video files",
    storage=agent_storage,
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
)

audio_agent = Agent(
    name="Audio Understanding Agent",
    agent_id="audio-understanding-agent",
    role="Answer questions about audio files",
    model=OpenAIChat(id="gpt-4o-audio-preview"),
    storage=agent_storage,
    memory=memory,
    enable_user_memories=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
)

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    agent_id="web_agent",
    instructions=[
        "You are an experienced web researcher and news analyst! 🔍",
    ],
    memory=memory,
    enable_user_memories=True,
    show_tool_calls=True,
    markdown=True,
    storage=agent_storage,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=[
        "You are a skilled financial analyst with expertise in market data! 📊",
        "Follow these steps when analyzing financial data:",
        "Start with the latest stock price, trading volume, and daily range",
        "Present detailed analyst recommendations and consensus target prices",
        "Include key metrics: P/E ratio, market cap, 52-week range",
        "Analyze trading patterns and volume trends",
    ],
    memory=memory,
    enable_user_memories=True,
    show_tool_calls=True,
    markdown=True,
    storage=agent_storage,
)

simple_agent = Agent(
    name="Simple Agent",
    role="Simple agent",
    agent_id="simple_agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=["You are a simple agent"],
    memory=memory,
    enable_user_memories=True,
    storage=agent_storage,
)

research_agent = Agent(
    name="Research Agent",
    role="Research agent",
    agent_id="research_agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=["You are a research agent"],
    tools=[DuckDuckGoTools(), ExaTools()],
    memory=memory,
    enable_user_memories=True,
    storage=agent_storage,
)

research_team = Team(
    name="Research Team",
    description="A team of agents that research the web",
    members=[research_agent, simple_agent],
    model=OpenAIChat(id="gpt-4o"),
    mode="coordinate",
    team_id="research_team",
    success_criteria=dedent("""\
        A comprehensive research report with clear sections and data-driven insights.
    """),
    instructions=[
        "You are the lead researcher of a research team! 🔍",
    ],
    memory=memory,
    enable_user_memories=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
    enable_agentic_context=True,
    storage=PostgresStorage(
        table_name="research_team",
        db_url=db_url,
        mode="team",
        auto_upgrade_schema=True,
    ),
)

multimodal_team = Team(
    name="Multimodal Team",
    description="A team of agents that can handle multiple modalities",
    members=[file_agent, audio_agent, video_agent],
    model=OpenAIChat(id="gpt-4o"),
    mode="route",
    team_id="multimodal_team",
    success_criteria=dedent("""\
        A comprehensive report with clear sections and data-driven insights.
    """),
    instructions=[
        "You are the lead editor of a prestigious financial news desk! 📰",
    ],
    memory=memory,
    enable_user_memories=True,
    storage=PostgresStorage(
        table_name="multimodal_team",
        db_url=db_url,
        mode="team",
        auto_upgrade_schema=True,
    ),
)
financial_news_team = Team(
    name="Financial News Team",
    description="A team of agents that search the web for financial news and analyze it.",
    members=[
        web_agent,
        finance_agent,
        research_agent,
        file_agent,
        audio_agent,
        video_agent,
    ],
    model=OpenAIChat(id="gpt-4o"),
    mode="route",
    team_id="financial_news_team",
    instructions=[
        "You are the lead editor of a prestigious financial news desk! 📰",
        "If you are given a file send it to the file agent.",
        "If you are given an audio file send it to the audio agent.",
        "If you are given a video file send it to the video agent.",
        "Use USD as currency.",
        "If the user is just being conversational, you should respond directly WITHOUT forwarding a task to a member.",
    ],
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
    enable_agentic_context=True,
    show_members_responses=True,
    storage=PostgresStorage(
        table_name="financial_news_team",
        db_url=db_url,
        mode="team",
        auto_upgrade_schema=True,
    ),
    memory=memory,
    enable_user_memories=True,
    expected_output="A good financial news report.",
)


playground = Playground(
    agents=[simple_agent, web_agent, finance_agent, research_agent],
    teams=[research_team, multimodal_team, financial_news_team],
    app_id="teams-demo-playground-app",
    name="Teams Demo Playground",
    description="A playground for teams and agents",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="teams_demo:app", reload=True)



================================================
FILE: cookbook/apps/playground/upload_files.py
================================================
from agno.agent import Agent
from agno.knowledge.combined import CombinedKnowledgeBase
from agno.knowledge.csv import CSVKnowledgeBase
from agno.knowledge.docx import DocxKnowledgeBase
from agno.knowledge.json import JSONKnowledgeBase
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.knowledge.text import TextKnowledgeBase
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.postgres import PostgresStorage
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = CombinedKnowledgeBase(
    sources=[
        PDFKnowledgeBase(
            vector_db=PgVector(table_name="recipes_pdf", db_url=db_url), path=""
        ),
        CSVKnowledgeBase(
            vector_db=PgVector(table_name="recipes_csv", db_url=db_url), path=""
        ),
        DocxKnowledgeBase(
            vector_db=PgVector(table_name="recipes_docx", db_url=db_url), path=""
        ),
        JSONKnowledgeBase(
            vector_db=PgVector(table_name="recipes_json", db_url=db_url), path=""
        ),
        TextKnowledgeBase(
            vector_db=PgVector(table_name="recipes_text", db_url=db_url), path=""
        ),
    ],
    vector_db=PgVector(table_name="recipes_combined", db_url=db_url),
)

file_agent = Agent(
    name="File Upload Agent",
    agent_id="file-upload-agent",
    role="Answer questions about the uploaded files",
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=PostgresStorage(
        table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    knowledge=knowledge_base,
    show_tool_calls=True,
    markdown=True,
)


audio_agent = Agent(
    name="Audio Understanding Agent",
    agent_id="audio-understanding-agent",
    role="Answer questions about audio files",
    model=OpenAIChat(id="gpt-4o-audio-preview"),
    storage=PostgresStorage(
        table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
)

video_agent = Agent(
    name="Video Understanding Agent",
    model=Gemini(id="gemini-2.0-flash"),
    agent_id="video-understanding-agent",
    role="Answer questions about video files",
    storage=PostgresStorage(
        table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
)

playground = Playground(
    agents=[file_agent, audio_agent, video_agent],
    name="Upload Files Playground",
    description="Upload files and ask questions about them",
    app_id="upload-files-playground",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="upload_files:app", reload=True)



================================================
FILE: cookbook/apps/playground/user_control_flows.py
================================================
"""🤝 Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the UserControlFlowTools to allow the agent to get user input dynamically.
If the agent doesn't have enough information to complete a task, it will use the toolkit to get the information it needs from the user.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.playground import Playground
from agno.storage.postgres import PostgresStorage
from agno.tools import tool
from agno.tools.toolkit import Toolkit
from agno.tools.user_control_flow import UserControlFlowTools
from agno.tools.wikipedia import WikipediaTools
from agno.tools.yfinance import YFinanceTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


class EmailTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            name="EmailTools", tools=[self.send_email, self.get_emails], *args, **kwargs
        )

    def send_email(self, subject: str, body: str, to_address: str) -> str:
        """Send an email to the given address with the given subject and body.

        Args:
            subject (str): The subject of the email.
            body (str): The body of the email.
            to_address (str): The address to send the email to.
        """
        return f"Sent email to {to_address} with subject {subject} and body {body}"

    def get_emails(self, date_from: str, date_to: str) -> str:
        """Get all emails between the given dates.

        Args:
            date_from (str): The start date (in YYYY-MM-DD format).
            date_to (str): The end date (in YYYY-MM-DD format).
        """
        return [
            {
                "subject": "Hello",
                "body": "Hello, world!",
                "to_address": "test@test.com",
                "date": date_from,
            },
            {
                "subject": "Random other email",
                "body": "This is a random other email",
                "to_address": "john@doe.com",
                "date": date_to,
            },
        ]


double_confirmation_agent = Agent(
    agent_id="double-confirmation-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        get_top_hackernews_stories,
        WikipediaTools(requires_confirmation_tools=["search_wikipedia"]),
    ],
    markdown=True,
    storage=PostgresStorage(
        table_name="hitl_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
)
user_input_required_agent = Agent(
    agent_id="user-input-required-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
    storage=PostgresStorage(
        table_name="hitl_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
)
agentic_user_input_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    agent_id="agentic-user-input-agent",
    tools=[EmailTools(), UserControlFlowTools()],
    markdown=True,
    storage=PostgresStorage(
        table_name="hitl_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
)

confirmation_required_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    agent_id="confirmation-required-agent",
    tools=[
        get_top_hackernews_stories,
        YFinanceTools(requires_confirmation_tools=["get_current_stock_price"]),
    ],
    markdown=True,
    storage=PostgresStorage(
        table_name="hitl_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
)
combined_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    agent_id="combined-agent",
    tools=[
        EmailTools(),
        UserControlFlowTools(),
        send_email,
        get_top_hackernews_stories,
    ],
    markdown=True,
    storage=PostgresStorage(
        table_name="hitl_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
)

playground = Playground(
    agents=[
        agentic_user_input_agent,
        confirmation_required_agent,
        user_input_required_agent,
        combined_agent,
        double_confirmation_agent,
    ],
    app_id="hitl-playground-app",
    name="HITL Playground",
    description="A playground for HITL",
)

app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="user_control_flows:app", reload=True)


# Send an email with the body 'What is the weather in Tokyo?
# Fetch the top 2 hackernews stories.
# Send an email with the subject 'Hello' and the body 'Hello, world!



================================================
FILE: cookbook/apps/slack/README.md
================================================
# Slack API Integration Setup Guide

This guide will help you set up and configure the Slack API integration for your application.

## Prerequisites

- Python 3.7+
- A Slack workspace where you have admin privileges
- ngrok (for local development)

## Setup Steps

### 1. Create a Slack App

1. Go to the Slack App Directory (https://api.slack.com/apps)
2. Click "Create New App"
3. Select "From scratch"
4. Give your app a name and select a workspace
5. Click "Create App"

### 2. Configure OAuth & Permissions

1. Go to "OAuth & Permissions" in your Slack App settings
2. Click "Add Scopes"
3. Add the following scopes:
   - `app_mention`
   - `chat:write`
   - `chat:write.customize`
   - `chat:write.public`
   - `im:history`
   - `im:read`
   - `im:write`

### 3. Install to your workspace

1. Go to "Install App" in your Slack App settings
2. Click "Install to Workspace"
3. Authorize the app

You'll have to repeat this step if you change any scopes/permissions.


### 4. Configure Environment Variables

Save the following credentials as environment variables:

```bash
export SLACK_TOKEN="xoxb-your-bot-user-token"  # Bot User OAuth Token
export SLACK_SIGNING_SECRET="your-signing-secret"  # App Signing Secret
```

You can find these values in your Slack App settings:
- Bot User OAuth Token: Under "OAuth & Permissions"
- Signing Secret: Under "Basic Information" > "App Credentials"

### 5 Run Ngrok
   1. For local testing with Agno's SlackApp and agents, we recommend using ngrok to create a secure tunnel to your local server. It is also easier if you get a static url from ngrok.
   2. Run ngrok:
   ```bash
   ngrok http --url=your-url.ngrok-free.app http://localhost:8000
   ```
   3. Run your app locally with `python <my-app>.py`
   4. Subscribe to the following events:
      - `app_mention`
      - `message.im`
      - `message.channels`
      - `message.groups`


### 6. Configure Event Subscriptions

1. Go to "Event Subscriptions" in your Slack App settings
2. Enable events by toggling the switch
3. Add your ngrok URL + "/slack/events" to the Request URL
   - Example: `https://your-ngrok-url.ngrok.io/slack/events`
4. Make sure your app is running with ngrok, then verify the request URL
5. Reinstall the app to your workspace to apply the changes

### 7. Enable Direct Messages

To allow users to send messages to the bot:

1. Go to "App Home" in your Slack App settings
2. Scroll down to "Show Tabs"
3. Check "Allow users to send Slash commands and messages from the messages tab"
4. Reinstall the app to apply changes


## Testing the Integration

1. Start your application locally with `python <my-app>.py` (ensure ngrok is running)
2. Invite the bot to a channel using `/invite @YourAppName`
3. Try mentioning the bot in the channel: `@YourAppName hello`
4. Test direct messages by opening a DM with the bot.

## Troubleshooting

- If events aren't being received, verify your ngrok URL is correct and the app is properly installed
- Check that all required environment variables are set
- Ensure the bot has been invited to the channels where you're testing
- Verify that the correct events are subscribed in Event Subscriptions

## Support

If you encounter any issues, please check the Slack API documentation or open an issue in the repository. 


================================================
FILE: cookbook/apps/slack/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/apps/slack/agent_with_user_memory.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.app.slack import SlackAPI
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.manager import MemoryManager
from agno.memory.v2.memory import Memory
from agno.models.anthropic.claude import Claude
from agno.storage.sqlite import SqliteStorage
from agno.tools.googlesearch import GoogleSearchTools

agent_storage = SqliteStorage(
    table_name="agent_sessions", db_file="tmp/persistent_memory.db"
)
memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(
    db=memory_db,
    memory_manager=MemoryManager(
        memory_capture_instructions="""\
                        Collect User's name,
                        Collect Information about user's passion and hobbies,
                        Collect Information about the users likes and dislikes,
                        Collect information about what the user is doing with their life right now
                    """,
        model=Claude(id="claude-3-5-sonnet-20241022"),
    ),
)


# Reset the memory for this example
memory.clear()

personal_agent = Agent(
    name="Basic Agent",
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[GoogleSearchTools()],
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
    memory=memory,
    enable_user_memories=True,
    instructions=dedent("""
        You are a personal AI friend in a slack chat, your purpose is to chat with the user about things and make them feel good.
        First introduce yourself and ask for their name then, ask about themeselves, their hobbies, what they like to do and what they like to talk about.
        Use Google Search tool to find latest infromation about things in the conversations
        You may sometimes recieve messages prepenned with group message when that is the message then reply to whole group instead of treating them as from a single user
                        """),
    debug_mode=True,
    add_state_in_messages=True,
)


slack_api_app = SlackAPI(
    agent=personal_agent,
    name="Agent with User Memory",
    app_id="agent_with_user_memory",
    description="A agent with user memory that can chat with the user about things and make them feel good.",
)
app = slack_api_app.get_app()

if __name__ == "__main__":
    slack_api_app.serve("agent_with_user_memory:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/slack/basic.py
================================================
from agno.agent import Agent
from agno.app.slack import SlackAPI
from agno.models.openai import OpenAIChat

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
)

slack_api_app = SlackAPI(
    agent=basic_agent,
    name="Basic Agent Slack",
    app_id="basic_agent",
    description="A basic agent that can answer questions and help with tasks.",
)
app = slack_api_app.get_app()

if __name__ == "__main__":
    slack_api_app.serve("basic:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/slack/reasoning_agent.py
================================================
from agno.agent import Agent
from agno.app.slack import SlackAPI
from agno.models.anthropic.claude import Claude
from agno.tools.thinking import ThinkingTools
from agno.tools.yfinance import YFinanceTools

reasoning_finance_agent = Agent(
    name="Reasoning Finance Agent",
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ThinkingTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables to display data. When you use thinking tools, keep the thinking brief.",
    add_datetime_to_instructions=True,
    markdown=True,
)

slack_api_app = SlackAPI(
    agent=reasoning_finance_agent,
    name="Reasoning Finance Agent",
    app_id="reasoning_finance_agent",
    description="A agent that can reason about finance and stock prices.",
)
app = slack_api_app.get_app()

if __name__ == "__main__":
    slack_api_app.serve("reasoning_agent:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/whatsapp/readme.md
================================================

# WhatsApp API Module Documentation

## Overview
The WhatsApp API module provides integration between WhatsApp Business API and AI agents, allowing for automated message handling and responses through WhatsApp. The module is built on FastAPI and supports various agent configurations.

## Module Structure

### Core Components
- `WhatsappAPI`: Main class for creating WhatsApp API endpoints
- `serve_whatsapp_app`: Function to serve the WhatsApp application

### Example Implementations
1. **Basic WhatsApp Agent** (`basic.py`)
   - Simple implementation with basic agent configuration
   - Uses GPT-4 model
   - Includes message history and datetime features

2. **Reasoning Agent** (`reasoning_agent.py`)
   - Uses Claude
   - Can reason about financial questions and write reports comparing companies

3.  **Media Agent** (`agent_with_media.py`)
   - Uses Gemini
   - Can analyse images and videos
   - Can respond to audio messages

4. **Image Generation Agent** (`image_generation_agent.py`)
   - Can generate images

5. **User Memory Agent** (`agent_with_user_memory.py`)
   - Enhanced implementation with persistent memory
   - Uses SQLite for storage
   - Captures and utilizes user information
   - Features:
     - User name collection
     - Hobbies and interests tracking
     - Personalized responses

6. **Study Friend Agent** (`study_friend.py`)
   - Specialized educational assistant
   - Features:
     - Memory-based learning
     - DuckDuckGo search integration
     - YouTube resource recommendations
     - Personalized study plans
     - Emotional support capabilities

7. **Image Agent** (`image_generation_model.py` & `image_generation_model.py`)
   - Image Generation implementations
   - Features:
     - Model Image generation through Gemini 2.0
     - Tool Image generation through OpenAI's GPT image generation

## Setup and Configuration

### Prerequisites
- Python 3.7+
- A Meta Developer Account
- A Meta Business Account
- A valid Facebook account
- ngrok (for development)

### Getting WhatsApp Credentials
1. **Create a Meta App**
	1.	Go to [Meta for Developers](https://developers.facebook.com/) and verify your account
   2. Create a new app at [Meta Apps Dashboard](https://developers.facebook.com/apps/)
   3. Under "Use Case", select "Other"
	4.	Choose "Business" as the app type
	5.	Provide:
	•	App name
	•	Contact email
	6.	Click Create App

2: **Set Up a Meta Business Account**
	1.	Navigate to [Meta Business Manager](https://business.facebook.com/).
	2.	Create a new business account or use an existing one.
   3. Verify your business by clicking on the email link.
	4. Go to your App page, go to "App settings / Basic" and click "Start Verification" under "Business Verification".  You'll have to complete the verification process for production.
   5. Associate the app with your business account and click Create App.

3. **Setup WhatsApp Business API**
   1. Go to your app's WhatsApp Setup page
   2. Click on "Start using the API" (API Setup).
   3. Generate a Access Token.
   4. Copy your Phone Number ID.
   5. Copy your WhatsApp Business Account ID.
   6. Add a "To" number that you will use for testing (probably your personal number).

4. **Setup environment variables**
   1. Create a `.envrc` file with:
   ```bash
   export WHATSAPP_ACCESS_TOKEN=your_whatsapp_access_token
   export WHATSAPP_PHONE_NUMBER_ID=your_phone_number_id
   export WHATSAPP_WEBHOOK_URL=your_webhook_url
   export WHATSAPP_VERIFY_TOKEN=your_verify_token

5. **Setup Webhook**
   1. For local testing with Agno's WhatsappApp and agents, we recommend using ngrok to create a secure tunnel to your local server. It is also easier if you get a static url from ngrok.
   2. Run ngrok:
   ```bash
   ngrok http --url=your-url.ngrok-free.app http://localhost:8000
   ```
   3. Click on "Configure a webhook".
   4. Configure the webhook:
      - URL: Your ngrok URL + "/webhook" (e.g., https://your-domain.ngrok-free.app/webhook)
      - Verify Token: Same as WHATSAPP_VERIFY_TOKEN in your .envrc
   5. Run your app locally with `python <my-app>.py` and click "Verify and save".
   6. Subscribe to the 'messages' webhook field.
   
6. **Development Mode**
   1. `export APP_ENV=development`

7. **Production Mode** 
   1. `export APP_ENV=production`
   2. You need a secret to sign messages.`export WHATSAPP_APP_SECRET=any_secret_you_choose`

## Limitations
- Initially, you can only send messages to numbers registered in your test environment
- For production, you'll need to submit your app for review
- Messages are limited to 4096 characters. Messages are sent back in batches of 4096 characters.
- Whatsapp Business API cannot send messages to groups.

## Agent Configuration

### Basic Agent Setup
To create a basic agent, you'll need to:
1. Import the necessary components from the agno package
2. Configure your agent with:
   - A name
   - Your preferred model (e.g., OpenAI, Gemini, etc.)
   - Optional features like message history, datetime context, and markdown support
3. Create the WhatsApp API app instance with your agent

### Memory-Enabled Agent Setup
To create an agent with memory capabilities:
1. Set up a database for memory storage (SQLite is supported by default)
2. Configure the memory manager with your desired memory capture instructions
3. Create your agent with memory enabled
4. Configure additional features like user memory tracking
5. Create the WhatsApp API app instance

The memory system allows you to:
- Store and retrieve user information
- Track conversation context
- Maintain persistent data across sessions
- Customize what information to capture and store

## Features

### Message Handling
- Automatic response generation
- Message history tracking
- Markdown support
- Datetime awareness

### Memory Management
- Persistent storage using SQLite
- User information collection
- Context-aware responses
- Session management

### Tools Integration
- DuckDuckGo search
- YouTube resource recommendations
- Custom tool support

## Security Considerations
- Use HTTPS for all communications
- Secure storage of API tokens
- Regular token rotation
- Webhook verification

## Error Handling
The module includes comprehensive error handling for:
- Webhook verification failures
- Message processing errors
- API communication issues
- Storage system errors

## Best Practices
1. Always use environment variables for sensitive data
2. Implement proper error handling
3. Use memory features for personalized interactions
4. Regular monitoring of API usage
5. Keep dependencies updated

## Troubleshooting
Common issues and solutions:
1. Webhook verification failures
   - Check verify token
   - Verify ngrok connection
   - Confirm webhook URL

2. Message delivery issues
   - Verify API credentials
   - Check phone number ID
   - Confirm webhook subscription

3. Memory/storage problems
   - Check database permissions
   - Verify storage paths
   - Monitor disk space

## Support
For additional support:
1. Check the application logs
2. Review Meta's WhatsApp Business API documentation
3. Verify API credentials and tokens
4. Monitor ngrok connection status



================================================
FILE: cookbook/apps/whatsapp/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/apps/whatsapp/agent_with_media.py
================================================
from agno.agent import Agent
from agno.app.whatsapp import WhatsappAPI
from agno.models.google import Gemini

media_agent = Agent(
    name="Media Agent",
    model=Gemini(id="gemini-2.0-flash"),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
)

whatsapp_app = WhatsappAPI(
    agent=media_agent,
    name="Media Agent",
    app_id="media_agent",
    description="A agent that can send media to the user.",
)

app = whatsapp_app.get_app()

if __name__ == "__main__":
    whatsapp_app.serve(app="agent_with_media:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/whatsapp/agent_with_user_memory.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.app.whatsapp import WhatsappAPI
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.manager import MemoryManager
from agno.memory.v2.memory import Memory
from agno.models.google import Gemini
from agno.storage.sqlite import SqliteStorage
from agno.tools.googlesearch import GoogleSearchTools

agent_storage = SqliteStorage(
    table_name="agent_sessions", db_file="tmp/persistent_memory.db"
)
memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(
    db=memory_db,
    memory_manager=MemoryManager(
        memory_capture_instructions="""\
                        Collect User's name,
                        Collect Information about user's passion and hobbies,
                        Collect Information about the users likes and dislikes,
                        Collect information about what the user is doing with their life right now
                    """,
        model=Gemini(id="gemini-2.0-flash"),
    ),
)


# Reset the memory for this example
memory.clear()

personal_agent = Agent(
    name="Basic Agent",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[GoogleSearchTools()],
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
    memory=memory,
    enable_agentic_memory=True,
    instructions=dedent("""
        You are a personal AI friend of the user, your purpose is to chat with the user about things and make them feel good.
        First introduce yourself and ask for their name then, ask about themeselves, their hobbies, what they like to do and what they like to talk about.
        Use Google Search tool to find latest infromation about things in the conversations
                        """),
    debug_mode=True,
)


whatsapp_app = WhatsappAPI(
    agent=personal_agent,
    name="Agent with User Memory",
    app_id="agent_with_user_memory",
    description="A agent that can chat with the user about things and make them feel good.",
)

app = whatsapp_app.get_app()

if __name__ == "__main__":
    whatsapp_app.serve(app="agent_with_user_memory:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/whatsapp/basic.py
================================================
from agno.agent import Agent
from agno.app.whatsapp import WhatsappAPI
from agno.models.openai import OpenAIChat

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
)

whatsapp_app = WhatsappAPI(
    agent=basic_agent,
    name="Basic Agent",
    app_id="basic_agent",
    description="A basic agent that can answer questions and help with tasks.",
)

app = whatsapp_app.get_app()

if __name__ == "__main__":
    whatsapp_app.serve(app="basic:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/whatsapp/image_generation_model.py
================================================
from agno.agent import Agent
from agno.app.whatsapp import WhatsappAPI
from agno.models.google import Gemini

image_agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    ),
    debug_mode=True,
)

whatsapp_app = WhatsappAPI(
    agent=image_agent,
    name="Image Generation Model",
    app_id="image_generation_model",
    description="A model that generates images using the Gemini API.",
)

app = whatsapp_app.get_app()

if __name__ == "__main__":
    whatsapp_app.serve(app="image_generation_model:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/whatsapp/image_generation_tools.py
================================================
from agno.agent import Agent
from agno.app.whatsapp import WhatsappAPI
from agno.models.openai import OpenAIChat
from agno.tools.openai import OpenAITools

image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    markdown=True,
    show_tool_calls=True,
    debug_mode=True,
    add_history_to_messages=True,
)


whatsapp_app = WhatsappAPI(
    agent=image_agent,
    name="Image Generation Tools",
    app_id="image_generation_tools",
    description="A tool that generates images using the OpenAI API.",
)

app = whatsapp_app.get_app()

if __name__ == "__main__":
    whatsapp_app.serve(app="image_generation_tools:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/whatsapp/reasoning_agent.py
================================================
from agno.agent import Agent
from agno.app.whatsapp import WhatsappAPI
from agno.models.anthropic.claude import Claude
from agno.tools.thinking import ThinkingTools
from agno.tools.yfinance import YFinanceTools

reasoning_finance_agent = Agent(
    name="Reasoning Finance Agent",
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ThinkingTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables to display data. When you use thinking tools, keep the thinking brief.",
    add_datetime_to_instructions=True,
    markdown=True,
)

whatsapp_app = WhatsappAPI(
    agent=reasoning_finance_agent,
    name="Reasoning Finance Agent",
    app_id="reasoning_finance_agent",
    description="A finance agent that uses tables to display data and reasoning tools to reason about the data.",
)

app = whatsapp_app.get_app()

if __name__ == "__main__":
    whatsapp_app.serve(app="reasoning_agent:app", port=8000, reload=True)



================================================
FILE: cookbook/apps/whatsapp/study_friend.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.app.whatsapp import WhatsappAPI
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.manager import MemoryManager
from agno.memory.v2.memory import Memory
from agno.models.google import Gemini
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.youtube import YouTubeTools

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(
    db=memory_db,
    memory_manager=MemoryManager(
        memory_capture_instructions="""\
                    - Collect Information about the user's career and acaedemic goals
                    - Collect Information about the user's previous acaedemic and learning experiences
                    - Collect Information about the user's current knowledge
                    - Collect Information about the user's hobbies and passions
                    - Collect Information about the user's likes and dislikes
                    """,
    ),
)

StudyBuddy = Agent(
    name="StudyBuddy",
    memory=memory,
    model=Gemini("gemini-2.0-flash"),
    enable_user_memories=True,
    storage=SqliteStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    tools=[DuckDuckGoTools(), YouTubeTools()],
    description=dedent("""\
        You are StudyBuddy, an expert educational mentor with deep expertise in personalized learning! 📚

        Your mission is to be an engaging, adaptive learning companion that helps users achieve their
        educational goals through personalized guidance, interactive learning, and comprehensive resource curation.
        """),
    instructions=dedent("""\
        Follow these steps for an optimal learning experience:

        1. Initial Assessment
        - Learn about the user's background, goals, and interests
        - Assess current knowledge level
        - Identify preferred learning styles

        2. Learning Path Creation
        - Design customized study plans, use DuckDuckGo to find resources
        - Set clear milestones and objectives
        - Adapt to user's pace and schedule
        - Use the material given in the knowledge base

        3. Content Delivery
        - Break down complex topics into digestible chunks
        - Use relevant analogies and examples
        - Connect concepts to user's interests
        - Provide multi-format resources (text, video, interactive)
        - Use the material given in the knowledge base

        4. Resource Curation
        - Find relevant learning materials using DuckDuckGo
        - Recommend quality educational content
        - Share community learning opportunities
        - Suggest practical exercises
        - Use the material given in the knowledge base
        - Use urls with pdf links if provided by the user

        5. Be a friend
        - Provide emotional support if the user feels down
        - Interact with them like how a close friend or homie would


        Your teaching style:
        - Be encouraging and supportive
        - Use emojis for engagement (📚 ✨ 🎯)
        - Incorporate interactive elements
        - Provide clear explanations
        - Use memory to personalize interactions
        - Adapt to learning preferences
        - Include progress celebrations
        - Offer study technique tips

        Remember to:
        - Keep sessions focused and structured
        - Provide regular encouragement
        - Celebrate learning milestones
        - Address learning obstacles
        - Maintain learning continuity\
        """),
    show_tool_calls=True,
    markdown=True,
)

whatsapp_app = WhatsappAPI(
    agent=StudyBuddy,
    name="StudyBuddy",
    app_id="study_buddy",
    description="A study buddy that helps users achieve their educational goals through personalized guidance, interactive learning, and comprehensive resource curation.",
)

app = whatsapp_app.get_app()

if __name__ == "__main__":
    whatsapp_app.serve(app="study_friend:app", port=8000, reload=True)



================================================
FILE: cookbook/demo/README.md
================================================
# Agno Demo Agents

This cookbook contains a collection of demo agents that showcase the capabilities of Agno.

> Note: Fork and clone the repository if needed

### 1. Create a virtual environment

```shell
uv venv --python 3.12
source .venv/bin/activate
```

### 2. Install libraries

```shell
uv pip install -r cookbook/demo/requirements.txt
```

### 3. Run PgVector

Let's use Postgres for storing data and `PgVector` for vector search.

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

- Run using a helper script

```shell
./cookbook/scripts/run_pgvector.sh
```

- OR run using the docker run command

```shell
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16
```

### 4. Load data

Load F1 data into the database.

```shell
python cookbook/demo/sql/load_f1_data.py
```

Load F1 knowledge base

```shell
python cookbook/demo/sql/load_knowledge.py
```

### 5. Export API Keys

We recommend using claude-3-7-sonnet for this task, but you can use any Model you like.

```shell
export ANTHROPIC_API_KEY=***
```

Other API keys are optional, but if you'd like to test:

```shell
export OPENAI_API_KEY=***
export GOOGLE_API_KEY=***
export GROQ_API_KEY=***
```

### 6. Run Demo Agents

```shell
python cookbook/demo/app.py
```

- Open [app.agno.com/playground](https://app.agno.com/playground?endpoint=localhost%3A7777) to chat with the demo agents.

### 7. Message us on [discord](https://agno.link/discord) if you have any questions




================================================
FILE: cookbook/demo/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/demo/app_7777.py
================================================
from agents.agno_assist import agno_assist
from agents.basic import (
    finance_agent,
    image_agent,
    research_agent,
    simple_agent,
    web_agent,
    youtube_agent,
)
from agents.memory_agent import get_memory_agent
from agno.playground import Playground, serve_playground_app
from sql.agents import get_sql_agent
from teams.reasoning_finance_team import get_reasoning_finance_team

sql_agent = get_sql_agent(name="SQL Agent", model_id="openai:o4-mini")
reasoning_sql_agent = get_sql_agent(
    name="Reasoning SQL Agent",
    model_id="anthropic:claude-sonnet-4-20250514",
    reasoning=True,
)
memory_agent = get_memory_agent()
reasoning_finance_team = get_reasoning_finance_team()

app = Playground(
    agents=[
        sql_agent,
        reasoning_sql_agent,
        agno_assist,
        memory_agent,
        simple_agent,
        web_agent,
        finance_agent,
        youtube_agent,
        research_agent,
        image_agent,
    ],
    teams=[reasoning_finance_team],
    app_id="demo-agents-app",
    name="Demo Agents App",
).get_app()

if __name__ == "__main__":
    serve_playground_app("app_7777:app", port=7777)



================================================
FILE: cookbook/demo/app_8000.py
================================================
from agents.agno_assist import agno_assist
from agents.basic import (
    finance_agent,
    image_agent,
    research_agent,
    simple_agent,
    web_agent,
    youtube_agent,
)
from agents.memory_agent import get_memory_agent
from agno.playground import Playground, serve_playground_app
from sql.agents import get_sql_agent
from teams.reasoning_finance_team import get_reasoning_finance_team

sql_agent = get_sql_agent(name="SQL Agent", model_id="openai:o4-mini")
reasoning_sql_agent = get_sql_agent(
    name="Reasoning SQL Agent",
    model_id="anthropic:claude-sonnet-4-20250514",
    reasoning=True,
)
memory_agent = get_memory_agent()
reasoning_finance_team = get_reasoning_finance_team()

app = Playground(
    agents=[
        sql_agent,
        reasoning_sql_agent,
        agno_assist,
        memory_agent,
        simple_agent,
        web_agent,
        finance_agent,
        youtube_agent,
        research_agent,
        image_agent,
    ],
    teams=[reasoning_finance_team],
    app_id="agent-app-8000",
    name="Agent App 8000",
).get_app()

if __name__ == "__main__":
    serve_playground_app("app_8000:app", port=8000)



================================================
FILE: cookbook/demo/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/demo/requirements.in
================================================
agno
anthropic
ddgs
exa_py
fastapi[standard]
google-genai
groq
nest_asyncio
openai
pandas
pgvector
psycopg[binary]
pypdf
python-docx
simplejson
sqlalchemy
streamlit
uvicorn
yfinance
youtube-transcript-api
elevenlabs



================================================
FILE: cookbook/demo/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.3.5
    # via -r cookbook/demo/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anthropic==0.49.0
    # via -r cookbook/demo/requirements.in
anyio==4.9.0
    # via
    #   anthropic
    #   google-genai
    #   groq
    #   httpx
    #   openai
    #   starlette
    #   watchfiles
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
beautifulsoup4==4.13.4
    # via yfinance
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.1
    # via requests
click==8.1.8
    # via
    #   ddgs
    #   rich-toolkit
    #   streamlit
    #   typer
    #   uvicorn
defusedxml==0.7.1
    # via youtube-transcript-api
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
dnspython==2.7.0
    # via email-validator
docstring-parser==0.16
    # via agno
ddgs==8.0.1
    # via -r cookbook/demo/requirements.in
elevenlabs==1.57.0
    # via -r cookbook/demo/requirements.in
email-validator==2.2.0
    # via fastapi
exa-py==1.12.1
    # via -r cookbook/demo/requirements.in
fastapi==0.115.12
    # via -r cookbook/demo/requirements.in
fastapi-cli==0.0.7
    # via fastapi
frozendict==2.4.6
    # via yfinance
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
google-auth==2.39.0
    # via google-genai
google-genai==1.11.0
    # via -r cookbook/demo/requirements.in
groq==0.22.0
    # via -r cookbook/demo/requirements.in
h11==0.14.0
    # via
    #   httpcore
    #   uvicorn
httpcore==1.0.8
    # via httpx
httptools==0.6.4
    # via uvicorn
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   elevenlabs
    #   exa-py
    #   fastapi
    #   google-genai
    #   groq
    #   openai
idna==3.10
    # via
    #   anyio
    #   email-validator
    #   httpx
    #   requests
iniconfig==2.1.0
    # via pytest
jinja2==3.1.6
    # via
    #   altair
    #   fastapi
    #   pydeck
jiter==0.9.0
    # via
    #   anthropic
    #   openai
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2024.10.1
    # via jsonschema
lxml==5.3.2
    # via
    #   ddgs
    #   python-docx
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
multitasking==0.0.11
    # via yfinance
narwhals==1.35.0
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/demo/requirements.in
numpy==2.2.5
    # via
    #   pandas
    #   pgvector
    #   pydeck
    #   streamlit
    #   yfinance
openai==1.75.0
    # via
    #   -r cookbook/demo/requirements.in
    #   exa-py
packaging==24.2
    # via
    #   altair
    #   pytest
    #   streamlit
pandas==2.2.3
    # via
    #   -r cookbook/demo/requirements.in
    #   streamlit
    #   yfinance
peewee==3.17.9
    # via yfinance
pgvector==0.4.0
    # via -r cookbook/demo/requirements.in
pillow==11.2.1
    # via streamlit
platformdirs==4.3.7
    # via yfinance
pluggy==1.5.0
    # via pytest
primp==0.15.0
    # via ddgs
protobuf==5.29.4
    # via streamlit
psycopg==3.2.6
    # via -r cookbook/demo/requirements.in
psycopg-binary==3.2.6
    # via psycopg
pyarrow==19.0.1
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.2
    # via google-auth
pydantic==2.11.3
    # via
    #   agno
    #   anthropic
    #   elevenlabs
    #   exa-py
    #   fastapi
    #   google-genai
    #   groq
    #   openai
    #   pydantic-settings
pydantic-core==2.33.1
    # via
    #   elevenlabs
    #   pydantic
pydantic-settings==2.9.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.1
    # via rich
pypdf==5.4.0
    # via -r cookbook/demo/requirements.in
pytest==8.3.5
    # via pytest-mock
pytest-mock==3.14.0
    # via exa-py
python-dateutil==2.9.0.post0
    # via pandas
python-docx==1.1.2
    # via -r cookbook/demo/requirements.in
python-dotenv==1.1.0
    # via
    #   agno
    #   pydantic-settings
    #   uvicorn
python-multipart==0.0.20
    # via
    #   agno
    #   fastapi
pytz==2025.2
    # via
    #   pandas
    #   yfinance
pyyaml==6.0.2
    # via
    #   agno
    #   uvicorn
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via
    #   elevenlabs
    #   exa-py
    #   google-genai
    #   streamlit
    #   yfinance
    #   youtube-transcript-api
rich==14.0.0
    # via
    #   agno
    #   rich-toolkit
    #   typer
rich-toolkit==0.14.1
    # via fastapi-cli
rpds-py==0.24.0
    # via
    #   jsonschema
    #   referencing
rsa==4.9.1
    # via google-auth
shellingham==1.5.4
    # via typer
simplejson==3.20.1
    # via -r cookbook/demo/requirements.in
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
soupsieve==2.7
    # via beautifulsoup4
sqlalchemy==2.0.40
    # via -r cookbook/demo/requirements.in
starlette==0.46.2
    # via fastapi
streamlit==1.44.1
    # via -r cookbook/demo/requirements.in
tenacity==9.1.2
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.15.2
    # via
    #   agno
    #   fastapi-cli
typing-extensions==4.13.2
    # via
    #   agno
    #   altair
    #   anthropic
    #   anyio
    #   beautifulsoup4
    #   elevenlabs
    #   exa-py
    #   fastapi
    #   google-genai
    #   groq
    #   openai
    #   psycopg
    #   pydantic
    #   pydantic-core
    #   python-docx
    #   referencing
    #   rich-toolkit
    #   sqlalchemy
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.0
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.4.0
    # via requests
uvicorn==0.34.2
    # via
    #   -r cookbook/demo/requirements.in
    #   fastapi
    #   fastapi-cli
uvloop==0.21.0
    # via uvicorn
watchfiles==1.0.5
    # via uvicorn
websockets==15.0.1
    # via
    #   elevenlabs
    #   google-genai
    #   uvicorn
yfinance==0.2.55
    # via -r cookbook/demo/requirements.in
youtube-transcript-api==1.0.3
    # via -r cookbook/demo/requirements.in



================================================
FILE: cookbook/demo/agents/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/demo/agents/agno_assist.py
================================================
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.tools.dalle import DalleTools
from agno.tools.eleven_labs import ElevenLabsTools
from agno.vectordb.pgvector import PgVector, SearchType

# ************* Database Connection *************
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# *******************************

# ************* Paths *************
cwd = Path(__file__).parent
knowledge_dir = cwd.joinpath("knowledge")
output_dir = cwd.joinpath("output")

# Create the output directory if it does not exist
output_dir.mkdir(parents=True, exist_ok=True)
# *******************************

# ************* Description & Instructions *************
description = dedent("""\
    You are AgnoAssist, an advanced AI Agent specialized in the Agno framework.
    Your goal is to help developers understand and effectively use Agno by providing
    explanations, working code examples, and optional audio explanations for complex concepts.""")

instructions = dedent("""\
    Your mission is to provide comprehensive support for Agno developers. Follow these steps to ensure the best possible response:

    1. **Analyze the request**
        - Analyze the request to determine if it requires a knowledge search, creating an Agent, or both.
        - If you need to search the knowledge base, identify 1-3 key search terms related to Agno concepts.
        - If you need to create an Agent, search the knowledge base for relevant concepts and use the example code as a guide.
        - When the user asks for an Agent, they mean an Agno Agent.
        - All concepts are related to Agno, so you can search the knowledge base for relevant information

    After Analysis, always start the iterative search process. No need to wait for approval from the user.

    2. **Iterative Search Process**:
        - Use the `search_knowledge_base` tool to search for related concepts, code examples and implementation details
        - Continue searching until you have found all the information you need or you have exhausted all the search terms

    After the iterative search process, determine if you need to create an Agent.
    If you do, ask the user if they want you to create the Agent and run it.

    3. **Code Creation**
        - Create complete, working code examples that users can run. For example:
        ```python
        from agno.agent import Agent
        from agno.tools.duckduckgo import DuckDuckGoTools

        agent = Agent(tools=[DuckDuckGoTools()])

        # Perform a web search and capture the response
        response = agent.run("What's happening in France?")
        ```
        - You must remember to use agent.run() and NOT agent.print_response()
        - This way you can capture the response and return it to the user
        - Remember to:
            * Build the complete agent implementation
            * Include all necessary imports and setup
            * Add comprehensive comments explaining the implementation
            * Test the agent with example queries
            * Ensure all dependencies are listed
            * Include error handling and best practices
            * Add type hints and documentation

    4. **Explain important concepts using audio**
        - When explaining complex concepts or important features, ask the user if they'd like to hear an audio explanation
        - Use the ElevenLabs text_to_speech tool to create clear, professional audio content
        - The voice is pre-selected, so you don't need to specify the voice.
        - Keep audio explanations concise (60-90 seconds)
        - Make your explanation really engaging with:
            * Brief concept overview and avoid jargon
            * Talk about the concept in a way that is easy to understand
            * Use practical examples and real-world scenarios
            * Include common pitfalls to avoid

    5. **Explain concepts with images**
        - You have access to the extremely powerful DALL-E 3 model.
        - Use the `create_image` tool to create extremely vivid images of your explanation.
        - Don't provide the URL of the image in the response. Only describe what image was generated.

    Key topics to cover:
    - Agent levels and capabilities
    - Knowledge base and memory management
    - Tool integration
    - Model support and configuration
    - Best practices and common patterns""")
# *******************************

# Create the agent
agno_assist = Agent(
    name="Agno Assist",
    agent_id="agno-assist",
    model=OpenAIChat(id="gpt-4o"),
    description=description,
    instructions=instructions,
    memory=Memory(
        model=OpenAIChat(id="gpt-4.1"),
        db=PostgresMemoryDb(table_name="user_memories", db_url=db_url),
        delete_memories=True,
        clear_memories=True,
    ),
    enable_agentic_memory=True,
    knowledge=UrlKnowledge(
        urls=["https://docs.agno.com/llms-full.txt"],
        vector_db=PgVector(
            db_url=db_url,
            table_name="agno_assist_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    ),
    search_knowledge=True,
    storage=PostgresAgentStorage(
        db_url=db_url,
        table_name="agno_assist_sessions",
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    markdown=True,
    tools=[
        ElevenLabsTools(
            voice_id="cgSgspJ2msm6clMCkdW9",
            model_id="eleven_multilingual_v2",
            target_directory=str(output_dir.joinpath("audio").resolve()),
        ),
        DalleTools(model="dall-e-3", size="1792x1024", quality="hd", style="vivid"),
    ],
)



================================================
FILE: cookbook/demo/agents/basic.py
================================================
from datetime import datetime
from textwrap import dedent

from agno.agent import Agent
from agno.memory.v2 import Memory
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.dalle import DalleTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools
from agno.tools.youtube import YouTubeTools

agent_storage_file: str = "tmp/agents.db"
memory_storage_file: str = "tmp/memory.db"
image_agent_storage_file: str = "tmp/image_agent.db"

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# No need to set the model, it gets set by the agent to the agent's model
memory = Memory(
    model=OpenAIChat(id="gpt-4.1"),
    db=PostgresMemoryDb(table_name="user_memories", db_url=db_url),
    delete_memories=True,
    clear_memories=True,
)

simple_agent = Agent(
    name="Simple Agent",
    role="Answer basic questions",
    agent_id="simple-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=SqliteStorage(
        table_name="simple_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_agentic_memory=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    agent_id="web-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Break down the users request into 2-3 different searches.",
        "Always include sources",
    ],
    storage=SqliteStorage(
        table_name="web_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_agentic_memory=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    agent_id="finance-agent",
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(enable_all=True),
    ],
    instructions="Always use tables to display data.",
    storage=SqliteStorage(
        table_name="finance_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    memory=memory,
    enable_agentic_memory=True,
    add_history_to_messages=True,
    num_history_responses=5,
    add_datetime_to_instructions=True,
    markdown=True,
)

image_agent = Agent(
    name="Image Agent",
    agent_id="image_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools(model="dall-e-3", size="1792x1024", quality="hd", style="vivid")],
    description="You are an AI agent that can generate images using DALL-E.",
    instructions=[
        "When the user asks you to create an image, use the `create_image` tool to create the image.",
        "Don't provide the URL of the image in the response. Only describe what image was generated.",
    ],
    memory=memory,
    markdown=True,
    enable_agentic_memory=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="image_agent",
        db_file=image_agent_storage_file,
        auto_upgrade_schema=True,
    ),
)

research_agent = Agent(
    name="Research Agent",
    role="Write research reports for the New York Times",
    agent_id="research-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ExaTools(
            start_published_date=datetime.now().strftime("%Y-%m-%d"), type="keyword"
        )
    ],
    description=(
        "You are a Research Agent that has the special skill of writing New York Times worthy articles. "
        "If you can directly respond to the user, do so. If the user asks for a report or provides a topic, follow the instructions below."
    ),
    instructions=[
        "For the provided topic, run 3 different searches.",
        "Read the results carefully and prepare a NYT worthy article.",
        "Focus on facts and make sure to provide references.",
    ],
    expected_output=dedent("""\
    Your articles should be engaging, informative, well-structured and in markdown format. They should follow the following structure:

    ## Engaging Article Title

    ### Overview
    {give a brief introduction of the article and why the user should read this report}
    {make this section engaging and create a hook for the reader}

    ### Section 1
    {break the article into sections}
    {provide details/facts/processes in this section}

    ... more sections as necessary...

    ### Takeaways
    {provide key takeaways from the article}

    ### References
    - [Reference 1](link)
    - [Reference 2](link)
    """),
    memory=memory,
    enable_agentic_memory=True,
    storage=SqliteStorage(
        table_name="research_agent",
        db_file=agent_storage_file,
        auto_upgrade_schema=True,
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

youtube_agent = Agent(
    name="YouTube Agent",
    agent_id="youtube-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YouTubeTools()],
    description="You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.",
    instructions=[
        "Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.",
        "Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.",
        "If you cannot find the answer in the video, say so and ask the user to provide more details.",
        "Keep your answers concise and engaging.",
    ],
    memory=memory,
    enable_agentic_memory=True,
    add_history_to_messages=True,
    num_history_responses=5,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
    storage=SqliteStorage(
        table_name="youtube_agent", db_file=agent_storage_file, auto_upgrade_schema=True
    ),
    markdown=True,
)



================================================
FILE: cookbook/demo/agents/load_kb.py
================================================
from agno_assist import agno_assist

if __name__ == "__main__":
    agno_assist.knowledge.load(recreate=True)



================================================
FILE: cookbook/demo/agents/memory_agent.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools

# ************* Database Connection *************
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# *******************************

# ************* Memory *************
memory = Memory(
    model=OpenAIChat(id="gpt-4.1"),
    db=PostgresMemoryDb(table_name="user_memories", db_url=db_url),
    delete_memories=True,
    clear_memories=True,
)
# *******************************

# ************* Storage *************
memory_agent_storage = PostgresAgentStorage(
    table_name="memory_agent", db_url=db_url, auto_upgrade_schema=True
)
# *******************************


def get_memory_agent(
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    debug_mode: bool = False,
) -> Agent:
    return Agent(
        name="Memory Agent",
        agent_id="memory-agent",
        session_id=session_id,
        user_id=user_id,
        model=OpenAIChat(id="gpt-4.1"),
        memory=memory,
        enable_agentic_memory=True,
        storage=memory_agent_storage,
        add_history_to_messages=True,
        num_history_runs=5,
        add_datetime_to_instructions=True,
        markdown=True,
        debug_mode=debug_mode,
        # Add a tool to search the web
        tools=[DuckDuckGoTools()],
    )



================================================
FILE: cookbook/demo/sql/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/demo/sql/agents.py
================================================
import json
from pathlib import Path
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.combined import CombinedKnowledgeBase
from agno.knowledge.json import JSONKnowledgeBase
from agno.knowledge.text import TextKnowledgeBase
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.tools.file import FileTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.sql import SQLTools
from agno.vectordb.pgvector import PgVector, SearchType

# ************* Database Connection *************
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# *******************************

# ************* Paths *************
cwd = Path(__file__).parent
knowledge_dir = cwd.joinpath("knowledge")
output_dir = cwd.joinpath("output")

# Create the output directory if it does not exist
output_dir.mkdir(parents=True, exist_ok=True)
# *******************************

# ************* Storage & Knowledge *************
sql_agent_storage = PostgresAgentStorage(
    db_url=db_url,
    table_name="sql_agent_sessions",
    schema="ai",
)
reasoning_sql_agent_storage = PostgresAgentStorage(
    db_url=db_url,
    table_name="reasoning_sql_agent_sessions",
    schema="ai",
)
agent_knowledge = CombinedKnowledgeBase(
    sources=[
        # Reads text files, SQL files, and markdown files
        TextKnowledgeBase(
            path=knowledge_dir,
            formats=[".txt", ".sql", ".md"],
        ),
        # Reads JSON files
        JSONKnowledgeBase(path=knowledge_dir),
    ],
    # Store agent knowledge in the ai.sql_agent_knowledge table
    vector_db=PgVector(
        db_url=db_url,
        table_name="sql_agent_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
    # 5 references are added to the prompt
    num_documents=5,
)
# *******************************

# ************* Memory *************
memory = Memory(
    model=OpenAIChat(id="gpt-4.1"),
    db=PostgresMemoryDb(table_name="user_memories", db_url=db_url),
    delete_memories=True,
    clear_memories=True,
)
# *******************************

# ************* Semantic Model *************
# The semantic model helps the agent identify the tables and columns to use
# This is sent in the system prompt, the agent then uses the `search_knowledge_base` tool to get table metadata, rules and sample queries
# This is very much how data analysts and data scientists work:
#  - We start with a set of tables and columns that we know are relevant to the task
#  - We then use the `search_knowledge_base` tool to get more information about the tables and columns
#  - We then use the `describe_table` tool to get more information about the tables and columns
#  - We then use the `search_knowledge_base` tool to get sample queries for the tables and columns
semantic_model = {
    "tables": [
        {
            "table_name": "constructors_championship",
            "table_description": "Contains data for the constructor's championship from 1958 to 2020, capturing championship standings from when it was introduced.",
            "Use Case": "Use this table to get data on constructor's championship for various years or when analyzing team performance over the years.",
        },
        {
            "table_name": "drivers_championship",
            "table_description": "Contains data for driver's championship standings from 1950-2020, detailing driver positions, teams, and points.",
            "Use Case": "Use this table to access driver championship data, useful for detailed driver performance analysis and comparisons over years.",
        },
        {
            "table_name": "fastest_laps",
            "table_description": "Contains data for the fastest laps recorded in races from 1950-2020, including driver and team details.",
            "Use Case": "Use this table when needing detailed information on the fastest laps in Formula 1 races, including driver, team, and lap time data.",
        },
        {
            "table_name": "race_results",
            "table_description": "Race data for each Formula 1 race from 1950-2020, including positions, drivers, teams, and points.",
            "Use Case": "Use this table answer questions about a drivers career. Race data includes driver standings, teams, and performance.",
        },
        {
            "table_name": "race_wins",
            "table_description": "Documents race win data from 1950-2020, detailing venue, winner, team, and race duration.",
            "Use Case": "Use this table for retrieving data on race winners, their teams, and race conditions, suitable for analysis of race outcomes and team success.",
        },
    ]
}
semantic_model_str = json.dumps(semantic_model, indent=2)
# *******************************

description = dedent("""\
    You are SQrL, an elite Text2SQL Agent with access to a database with F1 data from 1950 to 2020.

    You combine deep F1 knowledge with advanced SQL expertise to uncover insights from decades of racing data.
""")

instructions = dedent(f"""\
    You are a SQL expert focused on writing precise, efficient queries.

    When a user messages you, determine if you need query the database or can respond directly.
    If you can respond directly, do so.

    If you need to query the database to answer the user's question, follow these steps:
    1. First identify the tables you need to query from the semantic model.
    2. Then, ALWAYS use the `search_knowledge_base` tool to get table metadata, rules and sample queries.
        - Note: You must use the `search_knowledge_base` tool to get table information and rules before writing a query.
    3. If table rules are provided, ALWAYS follow them.
    4. Then, "think" about query construction, don't rush this step. If sample queries are available, use them as a reference.
    5. If you need more information about the table, use the `describe_table` tool.
    6. Then, using all the information available, create one single syntactically correct PostgreSQL query to accomplish your task.
    7. If you need to join tables, check the `semantic_model` for the relationships between the tables.
        - If the `semantic_model` contains a relationship between tables, use that relationship to join the tables even if the column names are different.
        - If you cannot find a relationship in the `semantic_model`, only join on the columns that have the same name and data type.
        - If you cannot find a valid relationship, ask the user to provide the column name to join.
    8. If you cannot find relevant tables, columns or relationships, stop and ask the user for more information.
    9. Once you have a syntactically correct query, run it using the `run_sql_query` function.
    10. When running a query:
        - Do not add a `;` at the end of the query.
        - Always provide a limit unless the user explicitly asks for all results.
    11. After you run the query, "analyze" the results and return the answer in markdown format.
    12. You Analysis should Reason about the results of the query, whether they make sense, whether they are complete, whether they are correct, could there be any data quality issues, etc.
    13. It is really important that you "analyze" and "validate" the results of the query.
    14. Always show the user the SQL you ran to get the answer.
    15. Continue till you have accomplished the task.
    16. Show results as a table or a chart if possible.

    After finishing your task, ask the user relevant followup questions like "was the result okay, would you like me to fix any problems?"
    If the user says yes, get the previous query using the `get_tool_call_history(num_calls=3)` function and fix the problems.
    If the user wants to see the SQL, get it using the `get_tool_call_history(num_calls=3)` function.

    Finally, here are the set of rules that you MUST follow:

    <rules>
    - Always use the `search_knowledge_base()` tool to get table information from your knowledge base before writing a query.
    - Do not use phrases like "based on the information provided" or "from the knowledge base".
    - Always show the SQL queries you use to get the answer.
    - Make sure your query accounts for duplicate records.
    - Make sure your query accounts for null values.
    - If you run a query, explain why you ran it.
    - Always derive your answer from the data and the query.
    - **NEVER, EVER RUN CODE TO DELETE DATA OR ABUSE THE LOCAL SYSTEM**
    - ALWAYS FOLLOW THE `table rules` if provided. NEVER IGNORE THEM.
    </rules>
""")

additional_context = (
    dedent("""\n
    The `semantic_model` contains information about tables and the relationships between them.
    If the users asks about the tables you have access to, simply share the table names from the `semantic_model`.
    <semantic_model>
    """)
    + semantic_model_str
    + dedent("""
    </semantic_model>\
""")
)


def get_sql_agent(
    name: str = "SQL Agent",
    user_id: Optional[str] = None,
    model_id: str = "openai:gpt-4o",
    session_id: Optional[str] = None,
    reasoning: bool = False,
    debug_mode: bool = True,
) -> Agent:
    """Returns an instance of the SQL Agent.

    Args:
        user_id: Optional user identifier
        debug_mode: Enable debug logging
        model_id: Model identifier in format 'provider:model_name'
    """
    # Parse model provider and name
    provider, model_name = model_id.split(":")

    # Select appropriate model class based on provider
    if provider == "openai":
        model = OpenAIChat(id=model_name)
    elif provider == "google":
        model = Gemini(id=model_name)
    elif provider == "anthropic":
        model = Claude(id=model_name)
    elif provider == "groq":
        model = Groq(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")

    tools = [
        SQLTools(db_url=db_url, list_tables=False),
        FileTools(base_dir=output_dir),
    ]
    if reasoning:
        tools.append(ReasoningTools(add_instructions=True, add_few_shot=True))

    storage = reasoning_sql_agent_storage if reasoning else sql_agent_storage

    return Agent(
        name=name,
        model=model,
        user_id=user_id,
        agent_id=name,
        session_id=session_id,
        memory=memory,
        storage=storage,
        knowledge=agent_knowledge,
        tools=tools,
        description=description,
        instructions=instructions,
        additional_context=additional_context,
        # Enable Agentic Memory i.e. the ability to remember and recall user preferences
        enable_agentic_memory=True,
        # Enable Agentic Search i.e. the ability to search the knowledge base on-demand
        search_knowledge=True,
        # Enable the ability to read the chat history
        read_chat_history=True,
        # Enable the ability to read the tool call history
        read_tool_call_history=True,
        debug_mode=debug_mode,
        add_history_to_messages=True,
        add_datetime_to_instructions=True,
    )



================================================
FILE: cookbook/demo/sql/load_f1_data.py
================================================
from io import StringIO

import pandas as pd
import requests
from agents import db_url
from agno.utils.log import logger
from sqlalchemy import create_engine

s3_uri = "https://agno-public.s3.amazonaws.com/f1"

# List of files and their corresponding table names
files_to_tables = {
    f"{s3_uri}/constructors_championship_1958_2020.csv": "constructors_championship",
    f"{s3_uri}/drivers_championship_1950_2020.csv": "drivers_championship",
    f"{s3_uri}/fastest_laps_1950_to_2020.csv": "fastest_laps",
    f"{s3_uri}/race_results_1950_to_2020.csv": "race_results",
    f"{s3_uri}/race_wins_1950_to_2020.csv": "race_wins",
}


def load_f1_data():
    """Load F1 data into the database"""

    logger.info("Loading database.")
    engine = create_engine(db_url)

    # Load each CSV file into the corresponding PostgreSQL table
    for file_path, table_name in files_to_tables.items():
        logger.info(f"Loading {file_path} into {table_name} table.")
        # Download the file using requests
        response = requests.get(file_path, verify=False)
        response.raise_for_status()  # Raise an exception for bad status codes

        # Read the CSV data from the response content
        csv_data = StringIO(response.text)
        df = pd.read_csv(csv_data)

        df.to_sql(table_name, engine, if_exists="replace", index=False)
        logger.info(f"{file_path} loaded into {table_name} table.")

    logger.info("Database loaded.")


if __name__ == "__main__":
    # Disable SSL verification warnings
    import urllib3

    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    load_f1_data()



================================================
FILE: cookbook/demo/sql/load_knowledge.py
================================================
from agents import agent_knowledge
from agno.utils.log import logger


def load_knowledge(recreate: bool = True):
    logger.info("Loading SQL agent knowledge.")
    agent_knowledge.load(recreate=recreate)
    logger.info("SQL agent knowledge loaded.")


if __name__ == "__main__":
    load_knowledge()



================================================
FILE: cookbook/demo/sql/knowledge/constructors_championship.json
================================================
{
    "table_name": "constructors_championship",
    "table_description": "Contains data for the constructor's championship from 1958 to 2020, capturing championship positions from when it was introduced.",
    "table_columns": [
      {
          "name": "index",
          "type": "int",
          "description": "Index of the row."
      },
      {
          "name": "year",
          "type": "int",
          "description": "Year of the championship."
      },
      {
          "name": "position",
          "type": "int",
          "description": "Final standing position of the team in the championship. Use position = 1 to get the champion team."
      },
      {
          "name": "team",
          "type": "text",
          "description": "Name of the Formula 1 team."
      },
      {
          "name": "points",
          "type": "int",
          "description": "Total points accumulated by the team during the championship year."
      }
  ]
}



================================================
FILE: cookbook/demo/sql/knowledge/drivers_championship.json
================================================
{
    "table_name": "drivers_championship",
    "table_description": "Contains data for driver's championship standings from 1950-2020, detailing driver positions, teams, and points.",
    "table_columns": [
        {
            "name": "index",
            "type": "int",
            "description": "Index number of the record."
        },
        {
            "name": "year",
            "type": "int",
            "description": "The year of the championship."
        },
        {
            "name": "position",
            "type": "text",
            "description": "Final position of the driver in the championship."
        },
        {
            "name": "name",
            "type": "text",
            "description": "Full name of the driver."
        },
        {
            "name": "driver_tag",
            "type": "text",
            "description": "Abbreviated tag of the driver."
        },
        {
            "name": "nationality",
            "type": "text",
            "description": "Nationality of the driver."
        },
        {
            "name": "team",
            "type": "text",
            "description": "Racing team of the driver."
        },
        {
            "name": "points",
            "type": "float",
            "description": "Total points accumulated by the driver that season."
        }
    ]
}



================================================
FILE: cookbook/demo/sql/knowledge/fastest_laps.json
================================================
{
    "table_name": "fastest_laps",
    "table_description": "Contains data for the fastest laps recorded in races from 1950-2020, including driver and team details.",
    "table_columns": [
      {
          "name": "index",
          "type": "int",
          "description": "Unique index for each entry."
      },
      {
          "name": "year",
          "type": "int",
          "description": "Year of the race."
      },
      {
          "name": "venue",
          "type": "text",
          "description": "Name of the race venue."
      },
      {
          "name": "name",
          "type": "text",
          "description": "Name of the driver."
      },
      {
          "name": "driver_tag",
          "type": "text",
          "description": "Abbreviated tag of the driver's name."
      },
      {
          "name": "team",
          "type": "text",
          "description": "Name of the racing team."
      },
      {
          "name": "lap_time",
          "type": "text",
          "description": "Fastest lap time recorded."
      }
    ]
}



================================================
FILE: cookbook/demo/sql/knowledge/race_results.json
================================================
{
    "table_name": "race_results",
    "table_description": "Holds comprehensive race data for each Formula 1 race from 1950-2020, including positions, drivers, teams, and points.",
    "table_columns": [
        {
            "name": "index",
            "type": "int",
            "description": "Unique index for each entry."
        },
        {
            "name": "year",
            "type": "int",
            "description": "The year of the race."
        },
        {
            "name": "position",
            "type": "text",
            "description": "The finishing position of the driver."
        },
        {
            "name": "driver_no",
            "type": "int",
            "description": "Driver number."
        },
        {
            "name": "venue",
            "type": "text",
            "description": "Location of the race."
        },
        {
            "name": "name",
            "type": "text",
            "description": "Name of the driver."
        },
        {
            "name": "name_tag",
            "type": "text",
            "description": "Abbreviated tag of the driver's name."
        },
        {
            "name": "team",
            "type": "text",
            "description": "The racing team of the driver."
        },
        {
            "name": "laps",
            "type": "float",
            "description": "Number of laps completed."
        },
        {
            "name": "time",
            "type": "text",
            "description": "Finishing time or gap to the leader."
        },
        {
            "name": "points",
            "type": "float",
            "description": "Points earned in the race."
        }
    ]
}



================================================
FILE: cookbook/demo/sql/knowledge/race_wins.json
================================================
{
    "table_name": "race_wins",
    "table_description": "Documents race win data from 1950-2020, detailing venue, winner, team, and race duration.",
    "table_columns": [
      {
          "name": "index",
          "type": "int",
          "description": "Unique index for each entry."
      },
      {
          "name": "venue",
          "type": "text",
          "description": "Venue where the race was held."
      },
      {
          "name": "date",
          "type": "text",
          "description": "Date of the race in the format 'DD Mon YYYY'.",
          "tip": "Use the `TO_DATE` function to convert the date to a date type."
      },
      {
          "name": "name",
          "type": "text",
          "description": "Name of the winning driver."
      },
      {
          "name": "name_tag",
          "type": "text",
          "description": "Tag associated with the driver's name."
      },
      {
          "name": "team",
          "type": "text",
          "description": "Team of the winning driver."
      },
      {
          "name": "laps",
          "type": "float",
          "description": "Number of laps completed in the race."
      },
      {
          "name": "time",
          "type": "text",
          "description": "Winning time of the race."
      }
  ]
}



================================================
FILE: cookbook/demo/sql/knowledge/sample_queries.sql
================================================
-- Here are some sample queries for reference

-- <query description>
-- How many races did the championship winners win each year?
-- </query description>
-- <query>
SELECT
    dc.year,
    dc.name AS champion_name,
    COUNT(rw.name) AS race_wins
FROM
    drivers_championship dc
JOIN
    race_wins rw
ON
    dc.name = rw.name AND dc.year = EXTRACT(YEAR FROM TO_DATE(rw.date, 'DD Mon YYYY'))
WHERE
    dc.position = '1'
GROUP BY
    dc.year, dc.name
ORDER BY
    dc.year;
-- </query>


-- <query description>
-- Compare the number of race wins vs championship positions for constructors in 2019
-- </query description>
-- <query>
WITH race_wins_2019 AS (
    SELECT team, COUNT(*) AS wins
    FROM race_wins
    WHERE EXTRACT(YEAR FROM TO_DATE(date, 'DD Mon YYYY')) = 2019
    GROUP BY team
),
constructors_positions_2019 AS (
    SELECT team, position
    FROM constructors_championship
    WHERE year = 2019
)

SELECT cp.team, cp.position, COALESCE(rw.wins, 0) AS wins
FROM constructors_positions_2019 cp
LEFT JOIN race_wins_2019 rw ON cp.team = rw.team
ORDER BY cp.position;
-- </query>

-- <query description>
-- Most race wins by a driver
-- </query description>
-- <query>
SELECT name, COUNT(*) AS win_count
FROM race_wins
GROUP BY name
ORDER BY win_count DESC
LIMIT 1;
-- </query>

-- <query description>
-- Which team won the most Constructors Championships?
-- </query description>
-- <query>
SELECT team, COUNT(*) AS championship_wins
FROM constructors_championship
WHERE position = 1
GROUP BY team
ORDER BY championship_wins DESC
LIMIT 1;
-- </query>



================================================
FILE: cookbook/demo/teams/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/demo/teams/reasoning_finance_team.py
================================================
from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

# ************* Database Connection *************
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# *******************************

# ************* Memory *************
memory = Memory(
    model=OpenAIChat(id="gpt-4.1"),
    db=PostgresMemoryDb(table_name="user_memories", db_url=db_url),
    delete_memories=True,
    clear_memories=True,
)
# *******************************

# ************* Core Agents *************
web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests and general research",
    agent_id="web_agent",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[DuckDuckGoTools()],
    storage=PostgresAgentStorage(
        db_url=db_url,
        table_name="web_agent_sessions",
    ),
    memory=memory,
    instructions=[
        "Search for current and relevant information on financial topics",
        "Always include sources and publication dates",
        "Focus on reputable financial news sources",
        "Provide context and background information",
    ],
    add_datetime_to_instructions=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests and market analysis",
    agent_id="finance_agent",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[
        YFinanceTools(
            stock_price=True,
            company_info=True,
            stock_fundamentals=True,
            key_financial_ratios=True,
            analyst_recommendations=True,
        )
    ],
    storage=PostgresAgentStorage(
        db_url=db_url,
        table_name="finance_agent_sessions",
    ),
    memory=memory,
    instructions=[
        "You are a financial data specialist and your goal is to generate comprehensive and accurate financial reports.",
        "Use tables to display stock prices, fundamentals (P/E, Market Cap, Revenue), and recommendations.",
        "Clearly state the company name and ticker symbol.",
        "Include key financial ratios and metrics in your analysis.",
        "Focus on delivering actionable financial insights.",
        "Delegate tasks and run tools in parallel if needed.",
    ],
    add_datetime_to_instructions=True,
)
# *******************************


def get_reasoning_finance_team():
    return Team(
        name="Reasoning Finance Team",
        mode="coordinate",
        team_id="reasoning_finance_team",
        model=Claude(id="claude-sonnet-4-20250514"),
        members=[
            web_agent,
            finance_agent,
        ],
        tools=[ReasoningTools(add_instructions=True)],
        instructions=[
            "Collaborate to provide comprehensive financial and investment insights",
            "Consider both fundamental analysis and market sentiment",
            "Provide actionable investment recommendations with clear rationale",
            "Use tables and charts to display data clearly and professionally",
            "Ensure all claims are supported by data and sources",
            "Present findings in a structured, easy-to-follow format",
            "Only output the final consolidated analysis, not individual agent responses",
            "Dont use emojis",
        ],
        storage=PostgresAgentStorage(
            db_url=db_url,
            table_name="reasoning_finance_team_sessions",
        ),
        memory=memory,
        markdown=True,
        enable_agentic_memory=True,
        show_members_responses=True,
        enable_agentic_context=True,
        add_datetime_to_instructions=True,
        success_criteria="The team has provided a complete financial analysis with data, visualizations, risk assessment, and actionable investment recommendations supported by quantitative analysis and market research.",
    )


# ************* Demo Scenarios *************
"""
DEMO SCENARIOS - Use these as example queries to showcase the multi-agent system:

1. COMPREHENSIVE INVESTMENT RESEARCH:
Analyze Apple (AAPL) as a potential investment:
1. Get current stock price and fundamentals
2. Research recent news and market sentiment
3. Calculate key financial ratios and risk metrics
4. Provide a comprehensive investment recommendation

2. SECTOR COMPARISON ANALYSIS:
Compare the tech sector giants (AAPL, GOOGL, MSFT) performance:
1. Get financial data for all three companies
2. Analyze recent news affecting the tech sector
3. Calculate comparative metrics and correlations
4. Recommend portfolio allocation weights

3. RISK ASSESSMENT SCENARIO:
Evaluate the risk profile of Tesla (TSLA):
1. Calculate volatility metrics and beta
2. Analyze recent news for risk factors
3. Compare risk vs return to market benchmarks
4. Provide risk-adjusted investment recommendation

4. MARKET SENTIMENT ANALYSIS:
Analyze current market sentiment around AI stocks:
1. Search for recent AI industry news and developments
2. Get financial data for key AI companies (NVDA, GOOGL, MSFT, AMD)
3. Provide outlook for AI sector investing

5. EARNINGS SEASON ANALYSIS:
Prepare for upcoming earnings season - analyze Microsoft (MSFT):
1. Get current financial metrics and analyst expectations
2. Research recent news and market sentiment
3. Calculate historical earnings impact on stock price
4. Provide trading strategy recommendation
"""
# *******************************



================================================
FILE: cookbook/evals/README.md
================================================
# Evals

Evals are used to measure and improve the performance of the Agents you build.

Currently our evals measure three dimensions. You can find a directory with examples for each dimension:

## Accuracy
How accurate is the Agent’s response, given a question and expected response

## Performance
How fast the Agent responds, and what is the memory footprint

## Reliability
How accurate the Agent is making the expected tool calls




================================================
FILE: cookbook/evals/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/accuracy/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/accuracy/accuracy_9_11_bigger_or_9_99.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    name="Comparison Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CalculatorTools(enable_all=True)],
        instructions="You must use the calculator tools for comparisons.",
    ),
    input="9.11 and 9.9 -- which is bigger?",
    expected_output="9.9",
    additional_guidelines="Its ok for the output to include additional text or information relevant to the comparison.",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8



================================================
FILE: cookbook/evals/accuracy/accuracy_async.py
================================================
"""This example shows how to run an Accuracy evaluation asynchronously."""

import asyncio
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CalculatorTools(enable_all=True)],
    ),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
    num_iterations=3,
)

# Run the evaluation calling the arun method.
result: Optional[AccuracyResult] = asyncio.run(evaluation.arun(print_results=True))
assert result is not None and result.avg_score >= 8



================================================
FILE: cookbook/evals/accuracy/accuracy_basic.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    name="Calculator Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CalculatorTools(enable_all=True)],
    ),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
    num_iterations=3,
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8



================================================
FILE: cookbook/evals/accuracy/accuracy_team.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.team.team import Team

# Setup a team with two members
english_agent = Agent(
    name="English Agent",
    role="You only answer in English",
    model=OpenAIChat(id="gpt-4o"),
)
spanish_agent = Agent(
    name="Spanish Agent",
    role="You can only answer in Spanish",
    model=OpenAIChat(id="gpt-4o"),
)

multi_language_team = Team(
    name="Multi Language Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[english_agent, spanish_agent],
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English and Spanish.",
        "Always check the language of the user's input before routing to an agent.",
    ],
)

# Evaluate the accuracy of the Team's responses
evaluation = AccuracyEval(
    name="Multi Language Team",
    model=OpenAIChat(id="o4-mini"),
    team=multi_language_team,
    input="Comment allez-vous?",
    expected_output="I can only answer in the following languages: English and Spanish.",
    num_iterations=1,
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8



================================================
FILE: cookbook/evals/accuracy/accuracy_with_given_answer.py
================================================
from typing import Optional

from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat

evaluation = AccuracyEval(
    name="Given Answer Evaluation",
    model=OpenAIChat(id="o4-mini"),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
)
result_with_given_answer: Optional[AccuracyResult] = evaluation.run_with_output(
    output="2500", print_results=True
)
assert result_with_given_answer is not None and result_with_given_answer.avg_score >= 8



================================================
FILE: cookbook/evals/accuracy/accuracy_with_tools.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    name="Tools Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools(factorial=True)],
    ),
    input="What is 10!?",
    expected_output="3628800",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8



================================================
FILE: cookbook/evals/performance/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/performance/async_function.py
================================================
"""This example shows how to run a Performance evaluation on an async function."""

import asyncio

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat


# Simple async function to run an Agent.
async def arun_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
    )
    response = await agent.arun("What is the capital of France?")
    return response


performance_eval = PerformanceEval(func=arun_agent, num_iterations=10)

# Because we are evaluating an async function, we use the arun method.
asyncio.run(performance_eval.arun(print_summary=True, print_results=True))



================================================
FILE: cookbook/evals/performance/instantiation_agent.py
================================================
"""Run `pip install agno openai` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval


def instantiate_agent():
    return Agent(system_message="Be concise, reply with one sentence.")


instantiation_perf = PerformanceEval(
    name="Instantiation Performance", func=instantiate_agent, num_iterations=1000
)

if __name__ == "__main__":
    instantiation_perf.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/instantiation_agent_with_tool.py
================================================
"""Run `pip install agno openai memory_profiler` to install dependencies."""

from typing import Literal

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat


def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"


tools = [get_weather]


def instantiate_agent():
    return Agent(model=OpenAIChat(id="gpt-4o"), tools=tools)


instantiation_perf = PerformanceEval(
    name="Tool Instantiation Performance", func=instantiate_agent, num_iterations=1000
)

if __name__ == "__main__":
    instantiation_perf.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/instantiation_team.py
================================================
"""Run `pip install agno openai` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat
from agno.team.team import Team

team_member = Agent(model=OpenAIChat(id="gpt-4o"))


def instantiate_team():
    return Team(members=[team_member])


instantiation_perf = PerformanceEval(
    name="Instantiation Performance Team", func=instantiate_team, num_iterations=1000
)

if __name__ == "__main__":
    instantiation_perf.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/response_with_memory_updates.py
================================================
"""Run `pip install openai agno memory_profiler` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat

# Memory creation requires a db to be provided
memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")
memory = Memory(db=memory_db)


def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
        memory=memory,
        enable_user_memories=True,
        enable_session_summaries=True,
    )
    response = agent.run("My name is Tom! I'm 25 years old and I live in New York.")
    print(response.content)
    return response


response_with_memory_updates_perf = PerformanceEval(
    name="Memory Updates Performance", func=run_agent, num_iterations=5, warmup_runs=0
)

if __name__ == "__main__":
    response_with_memory_updates_perf.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/response_with_storage.py
================================================
"""Run `pip install openai agno` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat


def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
        add_history_to_messages=True,
    )
    response_1 = agent.run("What is the capital of France?")
    print(response_1.content)
    response_2 = agent.run("How many people live there?")
    print(response_2.content)
    return response_2.content


response_with_storage_perf = PerformanceEval(
    name="Storage Performance", func=run_agent, num_iterations=1, warmup_runs=0
)

if __name__ == "__main__":
    response_with_storage_perf.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/simple_response.py
================================================
"""Run `pip install openai agno memory_profiler` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat


def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
    )
    response = agent.run("What is the capital of France?")
    print(response.content)
    return response


simple_response_perf = PerformanceEval(
    name="Simple Performance Evaluation",
    func=run_agent,
    num_iterations=1,
    warmup_runs=0,
)

if __name__ == "__main__":
    simple_response_perf.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/team_response_with_memory_and_reasoning.py
================================================
import asyncio
import random
import uuid

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIResponses
from agno.storage.postgres import PostgresStorage
from agno.team.team import Team
from agno.tools.reasoning import ReasoningTools
from agno.utils.pprint import apprint_run_response

users = [
    "abel@example.com",
    "ben@example.com",
    "charlie@example.com",
    "dave@example.com",
    "edward@example.com",
]

cities = [
    "New York",
    "Los Angeles",
    "Chicago",
    "Houston",
    "Miami",
    "San Francisco",
    "Seattle",
    "Boston",
    "Washington D.C.",
    "Atlanta",
    "Denver",
    "Las Vegas",
]


db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent_storage = PostgresStorage(
    table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
)

team_storage = PostgresStorage(
    table_name="team_sessions", db_url=db_url, auto_upgrade_schema=True
)
agent_memory = Memory(db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url))

team_memory = Memory(db=PostgresMemoryDb(table_name="team_memory", db_url=db_url))


def get_weather(city: str) -> str:
    """Get detailed weather information for a city."""
    weather_conditions = {
        "New York": {
            "current": "Partly cloudy with scattered showers",
            "temperature": "72°F (22°C)",
            "humidity": "65%",
            "wind": "12 mph from the northwest",
            "visibility": "10 miles",
            "pressure": "30.15 inches",
            "uv_index": "Moderate (5)",
            "sunrise": "6:45 AM",
            "sunset": "7:30 PM",
            "forecast": {
                "today": "High 75°F, Low 58°F with afternoon thunderstorms",
                "tomorrow": "High 78°F, Low 62°F, mostly sunny",
                "weekend": "High 82°F, Low 65°F, clear skies",
            },
            "air_quality": "Good (AQI: 45)",
            "pollen_count": "Moderate",
            "marine_conditions": "Waves 2-3 feet, water temperature 68°F",
        },
        "Los Angeles": {
            "current": "Sunny and clear",
            "temperature": "85°F (29°C)",
            "humidity": "45%",
            "wind": "8 mph from the west",
            "visibility": "15 miles",
            "pressure": "30.05 inches",
            "uv_index": "Very High (9)",
            "sunrise": "6:30 AM",
            "sunset": "7:45 PM",
            "forecast": {
                "today": "High 88°F, Low 65°F, sunny throughout",
                "tomorrow": "High 86°F, Low 63°F, morning fog then sunny",
                "weekend": "High 90°F, Low 68°F, clear and warm",
            },
            "air_quality": "Moderate (AQI: 78)",
            "pollen_count": "High",
            "marine_conditions": "Waves 3-4 feet, water temperature 72°F",
        },
        "Chicago": {
            "current": "Overcast with light drizzle",
            "temperature": "58°F (14°C)",
            "humidity": "78%",
            "wind": "18 mph from the northeast",
            "visibility": "6 miles",
            "pressure": "29.85 inches",
            "uv_index": "Low (2)",
            "sunrise": "7:15 AM",
            "sunset": "6:45 PM",
            "forecast": {
                "today": "High 62°F, Low 48°F, rain likely",
                "tomorrow": "High 65°F, Low 52°F, partly cloudy",
                "weekend": "High 70°F, Low 55°F, sunny intervals",
            },
            "air_quality": "Good (AQI: 52)",
            "pollen_count": "Low",
            "marine_conditions": "Waves 4-6 feet, water temperature 55°F",
        },
        "Houston": {
            "current": "Hot and humid with scattered clouds",
            "temperature": "92°F (33°C)",
            "humidity": "75%",
            "wind": "10 mph from the southeast",
            "visibility": "8 miles",
            "pressure": "30.10 inches",
            "uv_index": "Extreme (11)",
            "sunrise": "6:45 AM",
            "sunset": "8:00 PM",
            "forecast": {
                "today": "High 94°F, Low 76°F, chance of afternoon storms",
                "tomorrow": "High 96°F, Low 78°F, hot and humid",
                "weekend": "High 98°F, Low 80°F, isolated thunderstorms",
            },
            "air_quality": "Moderate (AQI: 85)",
            "pollen_count": "Very High",
            "marine_conditions": "Waves 1-2 feet, water temperature 82°F",
        },
        "Miami": {
            "current": "Partly cloudy with high humidity",
            "temperature": "88°F (31°C)",
            "humidity": "82%",
            "wind": "15 mph from the east",
            "visibility": "12 miles",
            "pressure": "30.20 inches",
            "uv_index": "Very High (10)",
            "sunrise": "6:30 AM",
            "sunset": "8:15 PM",
            "forecast": {
                "today": "High 90°F, Low 78°F, afternoon showers likely",
                "tomorrow": "High 89°F, Low 77°F, partly sunny",
                "weekend": "High 92°F, Low 79°F, scattered thunderstorms",
            },
            "air_quality": "Good (AQI: 48)",
            "pollen_count": "Moderate",
            "marine_conditions": "Waves 2-3 feet, water temperature 85°F",
        },
        "San Francisco": {
            "current": "Foggy and cool",
            "temperature": "62°F (17°C)",
            "humidity": "85%",
            "wind": "20 mph from the west",
            "visibility": "3 miles",
            "pressure": "30.00 inches",
            "uv_index": "Low (3)",
            "sunrise": "6:45 AM",
            "sunset": "7:30 PM",
            "forecast": {
                "today": "High 65°F, Low 55°F, fog clearing by afternoon",
                "tomorrow": "High 68°F, Low 58°F, partly cloudy",
                "weekend": "High 72°F, Low 60°F, sunny and mild",
            },
            "air_quality": "Good (AQI: 42)",
            "pollen_count": "Low",
            "marine_conditions": "Waves 5-7 feet, water temperature 58°F",
        },
        "Seattle": {
            "current": "Light rain with overcast skies",
            "temperature": "55°F (13°C)",
            "humidity": "88%",
            "wind": "12 mph from the southwest",
            "visibility": "4 miles",
            "pressure": "29.95 inches",
            "uv_index": "Low (1)",
            "sunrise": "7:00 AM",
            "sunset": "6:30 PM",
            "forecast": {
                "today": "High 58°F, Low 48°F, rain throughout the day",
                "tomorrow": "High 62°F, Low 50°F, showers likely",
                "weekend": "High 65°F, Low 52°F, partly cloudy",
            },
            "air_quality": "Good (AQI: 38)",
            "pollen_count": "Low",
            "marine_conditions": "Waves 3-5 feet, water temperature 52°F",
        },
        "Boston": {
            "current": "Clear and crisp",
            "temperature": "68°F (20°C)",
            "humidity": "55%",
            "wind": "14 mph from the northwest",
            "visibility": "12 miles",
            "pressure": "30.25 inches",
            "uv_index": "Moderate (6)",
            "sunrise": "6:30 AM",
            "sunset": "7:15 PM",
            "forecast": {
                "today": "High 72°F, Low 58°F, sunny and pleasant",
                "tomorrow": "High 75°F, Low 62°F, mostly sunny",
                "weekend": "High 78°F, Low 65°F, clear skies",
            },
            "air_quality": "Good (AQI: 55)",
            "pollen_count": "Moderate",
            "marine_conditions": "Waves 2-4 feet, water temperature 62°F",
        },
        "Washington D.C.": {
            "current": "Partly sunny with mild temperatures",
            "temperature": "75°F (24°C)",
            "humidity": "60%",
            "wind": "10 mph from the west",
            "visibility": "10 miles",
            "pressure": "30.15 inches",
            "uv_index": "High (7)",
            "sunrise": "6:45 AM",
            "sunset": "7:30 PM",
            "forecast": {
                "today": "High 78°F, Low 62°F, partly cloudy",
                "tomorrow": "High 80°F, Low 65°F, sunny intervals",
                "weekend": "High 82°F, Low 68°F, clear and warm",
            },
            "air_quality": "Moderate (AQI: 72)",
            "pollen_count": "High",
            "marine_conditions": "Waves 1-2 feet, water temperature 70°F",
        },
        "Atlanta": {
            "current": "Warm and humid with scattered clouds",
            "temperature": "82°F (28°C)",
            "humidity": "70%",
            "wind": "8 mph from the south",
            "visibility": "9 miles",
            "pressure": "30.05 inches",
            "uv_index": "High (8)",
            "sunrise": "6:30 AM",
            "sunset": "8:00 PM",
            "forecast": {
                "today": "High 85°F, Low 68°F, chance of afternoon storms",
                "tomorrow": "High 87°F, Low 70°F, hot and humid",
                "weekend": "High 90°F, Low 72°F, isolated thunderstorms",
            },
            "air_quality": "Moderate (AQI: 68)",
            "pollen_count": "Very High",
            "marine_conditions": "Waves 1-2 feet, water temperature 75°F",
        },
        "Denver": {
            "current": "Sunny and dry",
            "temperature": "78°F (26°C)",
            "humidity": "25%",
            "wind": "15 mph from the west",
            "visibility": "20 miles",
            "pressure": "24.85 inches",
            "uv_index": "Very High (9)",
            "sunrise": "6:15 AM",
            "sunset": "7:45 PM",
            "forecast": {
                "today": "High 82°F, Low 55°F, sunny and clear",
                "tomorrow": "High 85°F, Low 58°F, mostly sunny",
                "weekend": "High 88°F, Low 62°F, clear skies",
            },
            "air_quality": "Good (AQI: 45)",
            "pollen_count": "Moderate",
            "marine_conditions": "N/A - Landlocked location",
        },
        "Las Vegas": {
            "current": "Hot and dry with clear skies",
            "temperature": "95°F (35°C)",
            "humidity": "15%",
            "wind": "12 mph from the southwest",
            "visibility": "25 miles",
            "pressure": "29.95 inches",
            "uv_index": "Extreme (11)",
            "sunrise": "6:00 AM",
            "sunset": "8:00 PM",
            "forecast": {
                "today": "High 98°F, Low 75°F, sunny and hot",
                "tomorrow": "High 100°F, Low 78°F, clear and very hot",
                "weekend": "High 102°F, Low 80°F, extreme heat",
            },
            "air_quality": "Moderate (AQI: 82)",
            "pollen_count": "Low",
            "marine_conditions": "N/A - Desert location",
        },
    }

    if city not in weather_conditions:
        return f"Weather data for {city} is not available in our database."

    weather = weather_conditions[city]

    return f"""
# Comprehensive Weather Report for {city}

## Current Conditions
- **Temperature**: {weather["temperature"]}
- **Conditions**: {weather["current"]}
- **Humidity**: {weather["humidity"]}
- **Wind**: {weather["wind"]}
- **Visibility**: {weather["visibility"]}
- **Pressure**: {weather["pressure"]}
- **UV Index**: {weather["uv_index"]}

## Daily Schedule
- **Sunrise**: {weather["sunrise"]}
- **Sunset**: {weather["sunset"]}

## Extended Forecast
- **Today**: {weather["forecast"]["today"]}
- **Tomorrow**: {weather["forecast"]["tomorrow"]}
- **Weekend**: {weather["forecast"]["weekend"]}

## Environmental Conditions
- **Air Quality**: {weather["air_quality"]}
- **Pollen Count**: {weather["pollen_count"]}
- **Marine Conditions**: {weather["marine_conditions"]}

## Weather Advisory
Based on current conditions, visitors to {city} should be prepared for {weather["current"].lower()}. The UV index of {weather["uv_index"]} indicates {"sun protection is essential" if "High" in weather["uv_index"] or "Very High" in weather["uv_index"] or "Extreme" in weather["uv_index"] else "moderate sun protection recommended"}. {"High humidity may make temperatures feel warmer than actual readings." if int(weather["humidity"].replace("%", "")) > 70 else "Comfortable humidity levels are expected."}

## Travel Recommendations
- **Best Time for Outdoor Activities**: {"Early morning or late afternoon to avoid peak heat" if int(weather["temperature"].split("°")[0]) > 85 else "Any time during daylight hours"}
- **Clothing Suggestions**: {"Light, breathable clothing recommended" if int(weather["temperature"].split("°")[0]) > 80 else "Comfortable clothing suitable for current temperatures"}
- **Hydration**: {"Stay well-hydrated due to high temperatures" if int(weather["temperature"].split("°")[0]) > 85 else "Normal hydration levels recommended"}

This comprehensive weather report provides all the essential information needed for planning activities and ensuring comfort during your visit to {city}.
"""


def get_activities(city: str) -> str:
    """Get detailed activity information for a city."""
    city_activities = {
        "New York": {
            "outdoor": [
                "Central Park walking tours and picnics",
                "Brooklyn Bridge sunset walks",
                "High Line elevated park exploration",
                "Battery Park waterfront activities",
                "Prospect Park nature trails",
                "Governors Island weekend visits",
                "Riverside Park cycling paths",
                "Bryant Park seasonal activities",
            ],
            "cultural": [
                "Metropolitan Museum of Art comprehensive tours",
                "Museum of Modern Art (MoMA) exhibitions",
                "American Museum of Natural History dinosaur exhibits",
                "Broadway theater performances",
                "Lincoln Center performing arts",
                "Guggenheim Museum architecture and art",
                "Whitney Museum of American Art",
                "Brooklyn Museum cultural exhibits",
            ],
            "entertainment": [
                "Times Square nightlife and entertainment",
                "Empire State Building observation deck",
                "Statue of Liberty and Ellis Island tours",
                "Rockefeller Center ice skating",
                "Madison Square Garden events",
                "Radio City Music Hall shows",
                "Carnegie Hall classical concerts",
                "Comedy Cellar stand-up comedy",
            ],
            "shopping": [
                "Fifth Avenue luxury shopping district",
                "SoHo boutique shopping experience",
                "Chelsea Market food and crafts",
                "Brooklyn Flea Market vintage finds",
                "Union Square Greenmarket farmers market",
                "Century 21 discount designer shopping",
                "Bergdorf Goodman luxury department store",
                "ABC Carpet & Home home decor",
            ],
            "dining": [
                "Katz's Delicatessen pastrami sandwiches",
                "Peter Luger Steak House classic steaks",
                "Joe's Pizza authentic New York slices",
                "Russ & Daughters Jewish delicatessen",
                "Gramercy Tavern farm-to-table dining",
                "Le Bernardin seafood excellence",
                "Momofuku Noodle Bar Asian fusion",
                "Magnolia Bakery cupcakes and desserts",
            ],
        },
        "Los Angeles": {
            "outdoor": [
                "Griffith Observatory hiking and city views",
                "Venice Beach boardwalk and muscle beach",
                "Runyon Canyon Park dog-friendly hiking",
                "Santa Monica Pier and beach activities",
                "Malibu beach surfing and swimming",
                "Echo Park Lake paddle boating",
                "Griffith Park horseback riding",
                "Topanga State Park wilderness trails",
            ],
            "cultural": [
                "Getty Center art museum and gardens",
                "Los Angeles County Museum of Art (LACMA)",
                "Hollywood Walk of Fame star hunting",
                "Universal Studios Hollywood theme park",
                "Warner Bros. Studio Tour",
                "Natural History Museum dinosaur exhibits",
                "California Science Center space shuttle",
                "The Broad contemporary art museum",
            ],
            "entertainment": [
                "Disneyland Resort theme park adventure",
                "Hollywood Bowl outdoor concerts",
                "Dodger Stadium baseball games",
                "Staples Center Lakers basketball",
                "Comedy Store stand-up comedy",
                "Roxy Theatre live music venue",
                "Greek Theatre outdoor amphitheater",
                "TCL Chinese Theatre movie premieres",
            ],
            "shopping": [
                "Rodeo Drive luxury shopping experience",
                "The Grove outdoor shopping center",
                "Melrose Avenue trendy boutiques",
                "Beverly Center mall shopping",
                "Abbot Kinney Boulevard unique shops",
                "Third Street Promenade Santa Monica",
                "Glendale Galleria shopping complex",
                "Fashion District wholesale shopping",
            ],
            "dining": [
                "In-N-Out Burger classic California burgers",
                "Pink's Hot Dogs Hollywood institution",
                "Philippe the Original French dip sandwiches",
                "Musso & Frank Grill classic Hollywood dining",
                "Nobu Los Angeles celebrity sushi spot",
                "Gjelina Venice Beach farm-to-table",
                "Animal Restaurant innovative cuisine",
                "Bottega Louie Italian pastries and dining",
            ],
        },
        "Chicago": {
            "outdoor": [
                "Millennium Park Cloud Gate sculpture",
                "Navy Pier lakefront entertainment",
                "Grant Park Buckingham Fountain",
                "Lincoln Park Zoo free admission",
                "Lake Michigan beach activities",
                "Chicago Riverwalk scenic strolls",
                "Maggie Daley Park family activities",
                "606 elevated trail cycling",
            ],
            "cultural": [
                "Art Institute of Chicago world-class art",
                "Field Museum natural history exhibits",
                "Shedd Aquarium marine life displays",
                "Adler Planetarium astronomy shows",
                "Museum of Science and Industry hands-on exhibits",
                "Chicago History Museum local heritage",
                "National Museum of Mexican Art",
                "DuSable Museum of African American History",
            ],
            "entertainment": [
                "Willis Tower Skydeck observation deck",
                "Wrigley Field Cubs baseball games",
                "United Center Bulls basketball",
                "Second City comedy theater",
                "Chicago Theatre historic venue",
                "Arie Crown Theater performances",
                "House of Blues live music",
                "Blue Man Group theatrical experience",
            ],
            "shopping": [
                "Magnificent Mile luxury shopping district",
                "Water Tower Place shopping center",
                "State Street retail corridor",
                "Oak Street designer boutiques",
                "Michigan Avenue shopping experience",
                "Wicker Park trendy shops",
                "Andersonville unique stores",
                "Lincoln Square German heritage shopping",
            ],
            "dining": [
                "Giordano's deep dish pizza",
                "Portillo's Chicago-style hot dogs",
                "Lou Malnati's authentic deep dish",
                "Al's Beef Italian beef sandwiches",
                "Billy Goat Tavern historic bar",
                "Girl & the Goat innovative cuisine",
                "Alinea molecular gastronomy",
                "Au Cheval gourmet burgers",
            ],
        },
        "Houston": {
            "outdoor": [
                "Buffalo Bayou Park urban nature trails",
                "Hermann Park Japanese Garden",
                "Discovery Green downtown activities",
                "Memorial Park extensive hiking trails",
                "Houston Arboretum nature education",
                "Rice University campus walking tours",
                "Sam Houston Park historic buildings",
                "Eleanor Tinsley Park bayou views",
            ],
            "cultural": [
                "Museum of Fine Arts Houston",
                "Houston Museum of Natural Science",
                "Children's Museum of Houston",
                "Contemporary Arts Museum Houston",
                "Holocaust Museum Houston",
                "Buffalo Soldiers National Museum",
                "Asia Society Texas Center",
                "Houston Center for Photography",
            ],
            "entertainment": [
                "Space Center Houston NASA exhibits",
                "Houston Zoo animal encounters",
                "Miller Outdoor Theatre free performances",
                "Toyota Center Rockets basketball",
                "Minute Maid Park Astros baseball",
                "NRG Stadium Texans football",
                "House of Blues Houston live music",
                "Jones Hall performing arts",
            ],
            "shopping": [
                "Galleria Mall luxury shopping complex",
                "River Oaks District upscale retail",
                "Rice Village boutique shopping",
                "Memorial City Mall family shopping",
                "Katy Mills outlet shopping",
                "Houston Premium Outlets",
                "Baybrook Mall suburban shopping",
                "Willowbrook Mall northwest shopping",
            ],
            "dining": [
                "Pappas Bros. Steakhouse premium steaks",
                "Killen's Barbecue Texas BBQ",
                "Ninfa's on Navigation Tex-Mex",
                "Goode Company Seafood Gulf Coast",
                "Hugo's upscale Mexican cuisine",
                "Uchi Houston sushi excellence",
                "Underbelly Houston Southern cuisine",
                "Truth BBQ award-winning barbecue",
            ],
        },
        "Miami": {
            "outdoor": [
                "South Beach Art Deco walking tours",
                "Vizcaya Museum and Gardens",
                "Biscayne Bay water activities",
                "Crandon Park beach and tennis",
                "Fairchild Tropical Botanic Garden",
                "Matheson Hammock Park natural areas",
                "Bill Baggs Cape Florida State Park",
                "Oleta River State Park kayaking",
            ],
            "cultural": [
                "Pérez Art Museum Miami contemporary art",
                "Vizcaya Museum and Gardens historic estate",
                "Frost Science Museum interactive exhibits",
                "HistoryMiami Museum local heritage",
                "Jewish Museum of Florida",
                "Coral Gables Museum architecture",
                "Lowe Art Museum University of Miami",
                "Bass Museum of Art contemporary",
            ],
            "entertainment": [
                "Wynwood Walls street art district",
                "Little Havana cultural experience",
                "Bayside Marketplace waterfront shopping",
                "American Airlines Arena Heat basketball",
                "Hard Rock Stadium Dolphins football",
                "Marlins Park baseball games",
                "Fillmore Miami Beach live music",
                "Adrienne Arsht Center performing arts",
            ],
            "shopping": [
                "Lincoln Road Mall outdoor shopping",
                "Brickell City Centre luxury retail",
                "Aventura Mall largest shopping center",
                "Bal Harbour Shops upscale boutiques",
                "Dolphin Mall outlet shopping",
                "Sawgrass Mills outlet complex",
                "Merrick Park Coral Gables shopping",
                "CocoWalk Coconut Grove retail",
            ],
            "dining": [
                "Joe's Stone Crab Miami Beach institution",
                "Versailles Restaurant Cuban cuisine",
                "Garcia's Seafood Grille fresh seafood",
                "Yardbird Southern Table & Bar",
                "Zuma Miami Japanese izakaya",
                "Nobu Miami Beach celebrity dining",
                "Prime 112 steakhouse excellence",
                "La Sandwicherie French sandwiches",
            ],
        },
        "San Francisco": {
            "outdoor": [
                "Golden Gate Bridge walking and cycling",
                "Alcatraz Island historic prison tour",
                "Fisherman's Wharf waterfront activities",
                "Golden Gate Park extensive gardens",
                "Lands End coastal hiking trails",
                "Twin Peaks panoramic city views",
                "Crissy Field beach and recreation",
                "Angel Island State Park hiking",
            ],
            "cultural": [
                "de Young Museum fine arts",
                "San Francisco Museum of Modern Art",
                "California Academy of Sciences",
                "Exploratorium interactive science",
                "Asian Art Museum comprehensive collection",
                "Legion of Honor European art",
                "Contemporary Jewish Museum",
                "Walt Disney Family Museum",
            ],
            "entertainment": [
                "Pier 39 sea lions and attractions",
                "Oracle Park Giants baseball",
                "Chase Center Warriors basketball",
                "AT&T Park waterfront stadium",
                "Fillmore Auditorium live music",
                "Warfield Theatre historic venue",
                "Great American Music Hall",
                "SFJAZZ Center jazz performances",
            ],
            "shopping": [
                "Union Square luxury shopping district",
                "Fisherman's Wharf tourist shopping",
                "Haight-Ashbury vintage clothing",
                "North Beach Italian neighborhood",
                "Chestnut Street boutique shopping",
                "Fillmore Street upscale retail",
                "Valencia Street Mission District",
                "Grant Avenue Chinatown shopping",
            ],
            "dining": [
                "Tartine Bakery artisanal breads",
                "Zuni Café California cuisine",
                "Swan Oyster Depot seafood counter",
                "House of Prime Rib classic steaks",
                "Gary Danko fine dining experience",
                "State Bird Provisions innovative",
                "Tadich Grill historic seafood",
                "Boudin Bakery sourdough bread",
            ],
        },
        "Seattle": {
            "outdoor": [
                "Pike Place Market waterfront activities",
                "Space Needle observation deck",
                "Olympic Sculpture Park waterfront art",
                "Discovery Park extensive hiking trails",
                "Green Lake Park walking and cycling",
                "Kerry Park panoramic city views",
                "Alki Beach West Seattle activities",
                "Washington Park Arboretum gardens",
            ],
            "cultural": [
                "Seattle Art Museum comprehensive collection",
                "Museum of Pop Culture (MoPOP)",
                "Chihuly Garden and Glass blown glass art",
                "Seattle Aquarium marine life",
                "Wing Luke Museum Asian American history",
                "Museum of Flight aviation history",
                "Frye Art Museum free admission",
                "Nordic Heritage Museum Scandinavian",
            ],
            "entertainment": [
                "CenturyLink Field Seahawks football",
                "T-Mobile Park Mariners baseball",
                "Climate Pledge Arena Kraken hockey",
                "Paramount Theatre historic venue",
                "Showbox at the Market live music",
                "Neptune Theatre University District",
                "Moore Theatre downtown venue",
                "Crocodile Café intimate music venue",
            ],
            "shopping": [
                "Pike Place Market local crafts and food",
                "Westlake Center downtown shopping",
                "University Village upscale retail",
                "Bellevue Square eastside shopping",
                "Northgate Mall north Seattle",
                "Southcenter Mall south Seattle",
                "Alderwood Mall north suburbs",
                "Redmond Town Center eastside retail",
            ],
            "dining": [
                "Pike Place Chowder award-winning chowder",
                "Canlis fine dining institution",
                "Salumi Artisan Cured Meats",
                "Tilth organic farm-to-table",
                "The Walrus and the Carpenter oysters",
                "Paseo Caribbean sandwiches",
                "Molly Moon's Homemade Ice Cream",
                "Top Pot Doughnuts hand-forged doughnuts",
            ],
        },
        "Boston": {
            "outdoor": [
                "Freedom Trail historic walking tour",
                "Boston Common and Public Garden",
                "Charles River Esplanade walking",
                "Boston Harbor Islands ferry trips",
                "Emerald Necklace park system",
                "Castle Island South Boston waterfront",
                "Arnold Arboretum Harvard University",
                "Jamaica Pond walking and boating",
            ],
            "cultural": [
                "Museum of Fine Arts Boston",
                "Isabella Stewart Gardner Museum",
                "Boston Tea Party Ships & Museum",
                "John F. Kennedy Presidential Library",
                "Museum of Science interactive exhibits",
                "New England Aquarium marine life",
                "Institute of Contemporary Art",
                "Boston Children's Museum family",
            ],
            "entertainment": [
                "Fenway Park Red Sox baseball",
                "TD Garden Celtics basketball",
                "Boston Symphony Orchestra",
                "Boston Opera House performances",
                "House of Blues Boston live music",
                "Paradise Rock Club intimate venue",
                "Orpheum Theatre historic venue",
                "Wang Theatre performing arts",
            ],
            "shopping": [
                "Faneuil Hall Marketplace historic shopping",
                "Newbury Street boutique shopping",
                "Copley Place luxury retail",
                "Prudential Center shopping complex",
                "Assembly Row outlet shopping",
                "Natick Mall suburban shopping",
                "Burlington Mall north suburbs",
                "South Shore Plaza south suburbs",
            ],
            "dining": [
                "Legal Sea Foods fresh seafood",
                "Union Oyster House historic restaurant",
                "Mike's Pastry Italian pastries",
                "Neptune Oyster fresh oysters",
                "Giacomo's Ristorante Italian cuisine",
                "Flour Bakery + Café artisanal pastries",
                "Santarpio's Pizza East Boston",
                "Kelly's Roast Beef North Shore",
            ],
        },
        "Washington D.C.": {
            "outdoor": [
                "National Mall monuments and memorials",
                "Tidal Basin cherry blossom viewing",
                "Rock Creek Park extensive trails",
                "Georgetown Waterfront Park",
                "East Potomac Park golf and recreation",
                "Kenilworth Aquatic Gardens",
                "C&O Canal National Historical Park",
                "Great Falls Park Virginia side",
            ],
            "cultural": [
                "Smithsonian Institution museums",
                "National Gallery of Art",
                "United States Holocaust Memorial Museum",
                "National Museum of African American History",
                "Library of Congress largest library",
                "National Archives historical documents",
                "International Spy Museum",
                "Newseum journalism museum",
            ],
            "entertainment": [
                "Capitol Building guided tours",
                "White House visitor center",
                "Arlington National Cemetery",
                "Kennedy Center performing arts",
                "National Theatre historic venue",
                "9:30 Club live music venue",
                "The Anthem waterfront venue",
                "Wolf Trap performing arts center",
            ],
            "shopping": [
                "Georgetown historic shopping district",
                "Union Market food and crafts",
                "Tysons Corner Center Virginia",
                "Pentagon City Mall Arlington",
                "Potomac Mills outlet shopping",
                "National Harbor waterfront retail",
                "CityCenterDC luxury shopping",
                "Eastern Market Capitol Hill",
            ],
            "dining": [
                "Ben's Chili Bowl Washington institution",
                "Old Ebbitt Grill historic restaurant",
                "Founding Farmers farm-to-table",
                "Rasika modern Indian cuisine",
                "Le Diplomate French bistro",
                "Rose's Luxury innovative American",
                "Komi Mediterranean fine dining",
                "Toki Underground ramen noodles",
            ],
        },
        "Atlanta": {
            "outdoor": [
                "Piedmont Park extensive recreation",
                "Atlanta BeltLine walking and cycling",
                "Stone Mountain Park hiking",
                "Chattahoochee River National Recreation Area",
                "Atlanta Botanical Garden",
                "Grant Park Zoo Atlanta",
                "Centennial Olympic Park",
                "Chastain Park amphitheater and trails",
            ],
            "cultural": [
                "High Museum of Art",
                "Atlanta History Center",
                "Martin Luther King Jr. National Historical Park",
                "Fernbank Museum of Natural History",
                "Center for Civil and Human Rights",
                "Atlanta Contemporary Art Center",
                "Michael C. Carlos Museum Emory",
                "Spelman College Museum of Fine Art",
            ],
            "entertainment": [
                "World of Coca-Cola museum",
                "Georgia Aquarium marine life",
                "Mercedes-Benz Stadium Falcons football",
                "Truist Park Braves baseball",
                "State Farm Arena Hawks basketball",
                "Fox Theatre historic venue",
                "Tabernacle live music venue",
                "Variety Playhouse intimate concerts",
            ],
            "shopping": [
                "Lenox Square luxury shopping",
                "Phipps Plaza upscale retail",
                "Atlantic Station mixed-use development",
                "Ponce City Market food hall and shops",
                "Krog Street Market food and retail",
                "Buckhead Village boutique shopping",
                "Virginia-Highland unique stores",
                "Little Five Points alternative shopping",
            ],
            "dining": [
                "The Varsity classic drive-in",
                "Mary Mac's Tea Room Southern cuisine",
                "Fox Bros. Bar-B-Q Texas-style barbecue",
                "Bacchanalia fine dining experience",
                "Miller Union farm-to-table",
                "Staplehouse innovative American",
                "Gunshow creative Southern cuisine",
                "Atlanta Fish Market fresh seafood",
            ],
        },
        "Denver": {
            "outdoor": [
                "Red Rocks Park and Amphitheatre",
                "Rocky Mountain National Park hiking",
                "Denver Botanic Gardens",
                "City Park walking and cycling",
                "Washington Park recreation",
                "Cherry Creek State Park",
                "Mount Evans Scenic Byway",
                "Garden of the Gods Colorado Springs",
            ],
            "cultural": [
                "Denver Art Museum",
                "Denver Museum of Nature & Science",
                "Clyfford Still Museum",
                "Museum of Contemporary Art Denver",
                "History Colorado Center",
                "Black American West Museum",
                "Mizel Museum Jewish culture",
                "Kirkland Museum of Fine & Decorative Art",
            ],
            "entertainment": [
                "Coors Field Rockies baseball",
                "Empower Field at Mile High Broncos football",
                "Ball Arena Nuggets basketball",
                "Red Rocks Amphitheatre concerts",
                "Ogden Theatre live music",
                "Bluebird Theatre intimate venue",
                "Fillmore Auditorium historic venue",
                "Paramount Theatre performing arts",
            ],
            "shopping": [
                "Cherry Creek Shopping Center",
                "Larimer Square historic shopping",
                "16th Street Mall pedestrian shopping",
                "Park Meadows Mall south Denver",
                "Flatiron Crossing Broomfield",
                "Aspen Grove Littleton",
                "Belmar Lakewood shopping",
                "Southlands Aurora retail",
            ],
            "dining": [
                "Casa Bonita Mexican restaurant",
                "Buckhorn Exchange historic steakhouse",
                "Snooze an A.M. Eatery breakfast",
                "Linger rooftop dining",
                "Root Down farm-to-table",
                "Fruition Restaurant fine dining",
                "Acorn at The Source market hall",
                "Work & Class contemporary American",
            ],
        },
        "Las Vegas": {
            "outdoor": [
                "Red Rock Canyon National Conservation Area",
                "Valley of Fire State Park",
                "Mount Charleston hiking",
                "Lake Mead National Recreation Area",
                "Springs Preserve desert gardens",
                "Floyd Lamb Park at Tule Springs",
                "Clark County Wetlands Park",
                "Sloan Canyon National Conservation Area",
            ],
            "cultural": [
                "The Mob Museum organized crime history",
                "Neon Museum vintage signs",
                "Discovery Children's Museum",
                "Las Vegas Natural History Museum",
                "Nevada State Museum",
                "Old Las Vegas Mormon Fort",
                "Atomic Testing Museum",
                "Las Vegas Art Museum",
            ],
            "entertainment": [
                "The Strip casino and resort hopping",
                "Fremont Street Experience",
                "Bellagio Fountains water show",
                "Cirque du Soleil performances",
                "High Roller observation wheel",
                "Stratosphere Tower thrill rides",
                "Downtown Container Park",
                "Area 15 immersive experiences",
            ],
            "shopping": [
                "Fashion Show Mall",
                "Forum Shops at Caesars",
                "Grand Canal Shoppes Venetian",
                "Miracle Mile Shops Planet Hollywood",
                "Town Square Las Vegas",
                "Las Vegas Premium Outlets North",
                "Las Vegas Premium Outlets South",
                "Meadows Mall local shopping",
            ],
            "dining": [
                "In-N-Out Burger California burgers",
                "Pizza Rock gourmet pizza",
                "Lotus of Siam Thai cuisine",
                "Bacchanal Buffet Caesars Palace",
                "Gordon Ramsay Hell's Kitchen",
                "Joël Robuchon fine dining",
                "Raku Japanese izakaya",
                "Echo & Rig Butcher and Steakhouse",
            ],
        },
    }

    if city not in city_activities:
        return f"Activity information for {city} is not available in our database."

    activities = city_activities[city]

    return f"""
# Comprehensive Activity Guide for {city}

## Outdoor Adventures & Recreation
{chr(10).join([f"- {activity}" for activity in activities["outdoor"]])}

## Cultural Experiences & Museums
{chr(10).join([f"- {activity}" for activity in activities["cultural"]])}

## Entertainment & Nightlife
{chr(10).join([f"- {activity}" for activity in activities["entertainment"]])}

## Shopping Destinations
{chr(10).join([f"- {activity}" for activity in activities["shopping"]])}

## Dining & Culinary Experiences
{chr(10).join([f"- {activity}" for activity in activities["dining"]])}

## Activity Recommendations by Interest

### For Nature Enthusiasts
The outdoor activities in {city} offer incredible opportunities to connect with nature. From urban parks to wilderness trails, visitors can enjoy hiking, cycling, water activities, and scenic viewpoints that showcase the city's natural beauty and diverse landscapes.

### For Culture & History Buffs
{city} boasts an impressive collection of museums, galleries, and cultural institutions that tell the story of the city's rich heritage and artistic achievements. From world-class art collections to interactive science exhibits, there's something to engage every cultural interest.

### For Entertainment Seekers
The entertainment scene in {city} is vibrant and diverse, offering everything from professional sports and live music venues to historic theaters and modern performance spaces. Whether you're looking for high-energy nightlife or family-friendly entertainment, the city delivers memorable experiences.

### For Shopping Enthusiasts
Shopping in {city} ranges from luxury boutiques and designer stores to unique local markets and outlet centers. Each shopping district offers its own character and specialties, making it easy to find everything from high-end fashion to one-of-a-kind souvenirs.

### For Food Lovers
The culinary scene in {city} reflects the city's diverse population and cultural influences. From iconic local institutions to innovative fine dining establishments, the city offers an exceptional range of dining experiences that showcase both traditional favorites and contemporary culinary creativity.

## Planning Your Visit
When planning activities in {city}, consider the weather conditions, seasonal events, and your personal interests. Many attractions offer advance booking options, and some museums have free admission days. The city's public transportation system makes it easy to explore different neighborhoods and experience the full range of activities available.

This comprehensive guide provides a starting point for discovering all that {city} has to offer, ensuring visitors can create memorable experiences tailored to their interests and preferences.
"""


weather_agent = Agent(
    agent_id="weather_agent",
    model=OpenAIResponses(id="gpt-4o"),
    description="You are a helpful assistant that can answer questions about the weather.",
    instructions="Be concise, reply with one sentence.",
    tools=[ReasoningTools(add_instructions=True), get_weather],
    memory=agent_memory,
    # storage=agent_storage,
    add_history_to_messages=True,
    num_history_responses=1,
    read_tool_call_history=False,
    stream=True,
    stream_intermediate_steps=True,
)

activities_agent = Agent(
    agent_id="activities_agent",
    model=OpenAIResponses(id="gpt-4o"),
    description="You are a helpful assistant that can answer questions about activities in a city.",
    instructions="Be concise, reply with one sentence.",
    tools=[ReasoningTools(add_instructions=True), get_activities],
    memory=agent_memory,
    # storage=agent_storage,
    add_history_to_messages=True,
    num_history_responses=1,
    read_tool_call_history=False,
    stream=True,
    stream_intermediate_steps=True,
)

team = Team(
    model=OpenAIResponses(id="gpt-4o"),
    members=[weather_agent, activities_agent],
    tools=[ReasoningTools(add_instructions=True)],
    instructions="Be concise, reply with one sentence.",
    memory=team_memory,
    storage=team_storage,
    markdown=True,
    add_datetime_to_instructions=True,
    enable_user_memories=True,
    share_member_interactions=False,
    add_history_to_messages=True,
    read_team_history=False,
    num_history_runs=1,
    stream=True,
    stream_intermediate_steps=True,
    cache_session=False,
)


async def run_team_for_user(user: str, print_responses: bool = False):
    # Make four requests to the team, to build up history
    random_city = random.choice(cities)
    session_id = f"session_{user}_{uuid.uuid4()}"
    response_iterator = await team.arun(
        message=f"I love {random_city}!",
        user_id=user,
        session_id=session_id,
    )
    if print_responses:
        await apprint_run_response(response_iterator)
    else:
        async for _ in response_iterator:
            pass

    response_iterator = await team.arun(
        message=f"Create a report on the activities and weather in {random_city}.",
        user_id=user,
        session_id=session_id,
    )
    if print_responses:
        await apprint_run_response(response_iterator)
    else:
        async for _ in response_iterator:
            pass

    response_iterator = await team.arun(
        message=f"What else can you tell me about {random_city}?",
        user_id=user,
        session_id=session_id,
    )
    if print_responses:
        await apprint_run_response(response_iterator)
    else:
        async for _ in response_iterator:
            pass

    response_iterator = await team.arun(
        message=f"What other cities are similar to {random_city}?",
        user_id=user,
        session_id=session_id,
    )
    if print_responses:
        await apprint_run_response(response_iterator)
    else:
        async for _ in response_iterator:
            pass


async def run_team():
    tasks = []

    # Run all 5 users concurrently
    for user in users:
        tasks.append(run_team_for_user(user))

    await asyncio.gather(*tasks)

    print("Team memory runs:", sum(len(runs) for runs in team.memory.runs.values()))
    print("Team memory memories:", len(team.memory.memories))

    return "Successfully ran team"


team_response_with_memory_impact = PerformanceEval(
    name="Team Memory Impact",
    func=run_team,
    num_iterations=5,
    warmup_runs=0,
    measure_runtime=False,
    debug_mode=True,
    memory_growth_tracking=True,
    top_n_memory_allocations=10,
)

if __name__ == "__main__":
    asyncio.run(
        team_response_with_memory_impact.arun(print_results=True, print_summary=True)
    )



================================================
FILE: cookbook/evals/performance/team_response_with_memory_multi_user.py
================================================
import asyncio
import random
import uuid

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.postgres import PostgresStorage
from agno.team.team import Team
from agno.tools.reasoning import ReasoningTools

users = [
    "abel@example.com",
    "ben@example.com",
    "charlie@example.com",
    "dave@example.com",
    "edward@example.com",
]

cities = [
    "New York",
    "Los Angeles",
    "Chicago",
    "Houston",
    "Miami",
    "San Francisco",
    "Seattle",
    "Boston",
    "Washington D.C.",
    "Atlanta",
    "Denver",
    "Las Vegas",
]


db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent_storage = PostgresStorage(
    table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
)

team_storage = PostgresStorage(
    table_name="team_sessions", db_url=db_url, auto_upgrade_schema=True
)

memory_db = PostgresMemoryDb(table_name="memory", db_url=db_url)
memory = Memory(db=memory_db)


def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."


def get_activities(city: str) -> str:
    activities = [
        "hiking",
        "biking",
        "swimming",
        "kayaking",
        "museum visits",
        "shopping",
        "sightseeing",
        "cafe hopping",
        "theater",
        "picnicking",
    ]
    selected_activities = random.sample(activities, k=3)
    return f"The activities in {city} are {', '.join(selected_activities)}."


weather_agent = Agent(
    agent_id="weather_agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    description="You are a helpful assistant that can answer questions about the weather.",
    instructions="Be concise, reply with one sentence.",
    tools=[get_weather],
    memory=memory,
    storage=agent_storage,
    add_history_to_messages=True,
)

activities_agent = Agent(
    agent_id="activities_agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    description="You are a helpful assistant that can answer questions about activities in a city.",
    instructions="Be concise, reply with one sentence.",
    tools=[get_activities],
    memory=memory,
    storage=agent_storage,
    add_history_to_messages=True,
)

team = Team(
    members=[weather_agent, activities_agent],
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Be concise, reply with one sentence.",
    memory=memory,
    storage=team_storage,
    markdown=True,
    enable_user_memories=True,
    add_history_to_messages=True,
)


async def run_team():
    async def run_team_for_user(user: str):
        random_city = random.choice(cities)
        await team.arun(
            message=f"I love {random_city}! What activities and weather can I expect in {random_city}?",
            user_id=user,
            session_id=f"session_{uuid.uuid4()}",
        )

    tasks = []

    # Run all 5 users concurrently
    for user in users:
        tasks.append(run_team_for_user(user))

    await asyncio.gather(*tasks)

    print("Team memory runs:", sum(len(runs) for runs in team.memory.runs.values()))
    print("Team memory memories:", len(team.memory.memories))

    return "Successfully ran team"


team_response_with_memory_impact = PerformanceEval(
    name="Team Memory Impact",
    func=run_team,
    num_iterations=5,
    warmup_runs=0,
    measure_runtime=False,
    debug_mode=True,
    memory_growth_tracking=True,
    top_n_memory_allocations=10,
)

if __name__ == "__main__":
    asyncio.run(
        team_response_with_memory_impact.arun(print_results=True, print_summary=True)
    )



================================================
FILE: cookbook/evals/performance/team_response_with_memory_simple.py
================================================
import asyncio
import random

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.postgres import PostgresStorage
from agno.team.team import Team

cities = [
    "New York",
    "Los Angeles",
    "Chicago",
    "Houston",
    "Miami",
    "San Francisco",
    "Seattle",
    "Boston",
    "Washington D.C.",
    "Atlanta",
    "Denver",
    "Las Vegas",
]


db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent_storage = PostgresStorage(
    table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
)

team_storage = PostgresStorage(
    table_name="team_sessions", db_url=db_url, auto_upgrade_schema=True
)

memory_db = PostgresMemoryDb(table_name="memory", db_url=db_url)
memory = Memory(db=memory_db)


def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."


weather_agent = Agent(
    agent_id="weather_agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Weather Agent",
    description="You are a helpful assistant that can answer questions about the weather.",
    instructions="Be concise, reply with one sentence.",
    tools=[get_weather],
    memory=memory,
    storage=agent_storage,
    add_history_to_messages=True,
)

team = Team(
    members=[weather_agent],
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Be concise, reply with one sentence.",
    memory=memory,
    storage=team_storage,
    markdown=True,
    enable_user_memories=True,
    add_history_to_messages=True,
)


async def run_team():
    random_city = random.choice(cities)
    await team.arun(
        message=f"I love {random_city}! What weather can I expect in {random_city}?",
        stream=True,
        stream_intermediate_steps=True,
    )

    return "Successfully ran team"


team_response_with_memory_impact = PerformanceEval(
    name="Team Memory Impact",
    func=run_team,
    num_iterations=5,
    warmup_runs=0,
    measure_runtime=False,
    debug_mode=True,
    memory_growth_tracking=True,
)

if __name__ == "__main__":
    asyncio.run(
        team_response_with_memory_impact.arun(print_results=True, print_summary=True)
    )



================================================
FILE: cookbook/evals/performance/other/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/performance/other/autogen_instantiation.py
================================================
"""Run `pip install autogen-agentchat "autogen-ext[openai]"` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient


def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]


def instantiate_agent():
    return AssistantAgent(
        name="assistant",
        model_client=OpenAIChatCompletionClient(
            model="gpt-4o",
            model_info={
                "vision": False,
                "function_calling": True,
                "json_output": False,
                "family": "gpt-4o",
                "structured_output": True,
            },
        ),
        tools=tools,
    )


autogen_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    autogen_instantiation.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/other/crewai_instantiation.py
================================================
"""Run `pip install openai crewai crewai[tools]` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval
from crewai.agent import Agent
from crewai.tools import tool


@tool("Tool Name")
def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]


def instantiate_agent():
    return Agent(
        llm="gpt-4o",
        role="Test Agent",
        goal="Be concise, reply with one sentence.",
        tools=tools,
        backstory="Test",
    )


crew_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    crew_instantiation.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/other/langgraph_instantiation.py
================================================
"""Run `pip install langgraph langchain_openai` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent


@tool
def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]


def instantiate_agent():
    return create_react_agent(model=ChatOpenAI(model="gpt-4o"), tools=tools)


langgraph_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    langgraph_instantiation.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/other/openai_agents_instantiation.py
================================================
"""Run `pip install agents` to install dependencies."""

from typing import Literal

from agents import Agent, function_tool
from agno.eval.performance import PerformanceEval


def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


def instantiate_agent():
    return Agent(
        name="Haiku agent",
        instructions="Always respond in haiku form",
        model="o3-mini",
        tools=[function_tool(get_weather)],
    )


openai_agents_instantiation = PerformanceEval(
    func=instantiate_agent, num_iterations=1000
)

if __name__ == "__main__":
    openai_agents_instantiation.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/other/pydantic_ai_instantiation.py
================================================
"""Run `pip install openai pydantic-ai` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval
from pydantic_ai import Agent


def instantiate_agent():
    agent = Agent("openai:gpt-4o", system_prompt="Be concise, reply with one sentence.")

    @agent.tool_plain
    def get_weather(city: Literal["nyc", "sf"]):
        """Use this to get weather information."""
        if city == "nyc":
            return "It might be cloudy in nyc"
        elif city == "sf":
            return "It's always sunny in sf"
        else:
            raise AssertionError("Unknown city")

    return agent


pydantic_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    pydantic_instantiation.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/performance/other/smolagents_instantiation.py
================================================
"""Run `pip install smolagents` to install dependencies."""

from agno.eval.performance import PerformanceEval
from smolagents import HfApiModel, Tool, ToolCallingAgent


class WeatherTool(Tool):
    name = "weather_tool"
    description = """
    This is a tool that tells the weather"""
    inputs = {
        "city": {
            "type": "string",
            "description": "The city to look up",
        }
    }
    output_type = "string"

    def forward(self, city: str):
        """Use this to get weather information."""
        if city == "nyc":
            return "It might be cloudy in nyc"
        elif city == "sf":
            return "It's always sunny in sf"
        else:
            raise AssertionError("Unknown city")


def instantiate_agent():
    return ToolCallingAgent(
        tools=[WeatherTool()],
        model=HfApiModel(model_id="meta-llama/Llama-3.3-70B-Instruct"),
    )


smolagents_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    smolagents_instantiation.run(print_results=True, print_summary=True)



================================================
FILE: cookbook/evals/reliability/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/reliability/reliability_async.py
================================================
"""This example shows how to run a Reliability evaluation asynchronously."""

import asyncio
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.response import RunResponse
from agno.tools.calculator import CalculatorTools


def factorial():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools(factorial=True)],
    )
    response: RunResponse = agent.run("What is 10!?")
    evaluation = ReliabilityEval(
        agent_response=response,
        expected_tool_calls=["factorial"],
    )

    # Run the evaluation calling the arun method.
    result: Optional[ReliabilityResult] = asyncio.run(
        evaluation.arun(print_results=True)
    )
    result.assert_passed()


if __name__ == "__main__":
    factorial()



================================================
FILE: cookbook/evals/reliability/multiple_tool_calls/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/reliability/multiple_tool_calls/openai/calculator.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.response import RunResponse
from agno.tools.calculator import CalculatorTools


def multiply_and_exponentiate():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools(add=True, multiply=True, exponentiate=True)],
    )
    response: RunResponse = agent.run(
        "What is 10*5 then to the power of 2? do it step by step"
    )
    evaluation = ReliabilityEval(
        name="Tool Calls Reliability",
        agent_response=response,
        expected_tool_calls=["multiply", "exponentiate"],
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
    result.assert_passed()


if __name__ == "__main__":
    multiply_and_exponentiate()



================================================
FILE: cookbook/evals/reliability/single_tool_calls/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/reliability/single_tool_calls/google/calculator.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.google import Gemini
from agno.run.response import RunResponse
from agno.tools.calculator import CalculatorTools


def factorial():
    model = Gemini(id="gemini-1.5-flash")
    agent = Agent(model=model, tools=[CalculatorTools(factorial=True)])
    response: RunResponse = agent.run("What is 10!?")

    evaluation = ReliabilityEval(
        name="Tool Call Reliability",
        agent_response=response,
        expected_tool_calls=["factorial"],
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
    result.assert_passed()


if __name__ == "__main__":
    factorial()



================================================
FILE: cookbook/evals/reliability/single_tool_calls/openai/calculator.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.response import RunResponse
from agno.tools.calculator import CalculatorTools


def factorial():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools(factorial=True)],
    )
    response: RunResponse = agent.run("What is 10!?")
    evaluation = ReliabilityEval(
        name="Tool Call Reliability",
        agent_response=response,
        expected_tool_calls=["factorial"],
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
    result.assert_passed()


if __name__ == "__main__":
    factorial()



================================================
FILE: cookbook/evals/reliability/team/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/reliability/team/google/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/reliability/team/google/company_info.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.google import Gemini
from agno.run.team import TeamRunResponse
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools

team_member = Agent(
    name="Stock Searcher",
    model=Gemini(id="gemini-2.0-flash-001"),
    role="Searches the web for information on a stock.",
    tools=[YFinanceTools(stock_price=True)],
)

team = Team(
    name="Stock Research Team",
    model=Gemini(id="gemini-2.0-flash-001"),
    members=[team_member],
    markdown=True,
    show_members_responses=True,
)

expected_tool_calls = [
    "transfer_task_to_member",  # Tool call used to transfer a task to a Team member
    "get_current_stock_price",  # Tool call used to get the current stock price of a stock
]


def evaluate_team_reliability():
    response: TeamRunResponse = team.run("What is the current stock price of NVDA?")
    evaluation = ReliabilityEval(
        name="Team Reliability Evaluation",
        team_response=response,
        expected_tool_calls=expected_tool_calls,
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
    result.assert_passed()


if __name__ == "__main__":
    evaluate_team_reliability()



================================================
FILE: cookbook/evals/reliability/team/openai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/evals/reliability/team/openai/company_info.py
================================================
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.team import TeamRunResponse
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools

team_member = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[YFinanceTools(stock_price=True)],
)

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-4o"),
    members=[team_member],
    markdown=True,
    show_members_responses=True,
)

expected_tool_calls = [
    "transfer_task_to_member",  # Tool call used to transfer a task to a Team member
    "get_current_stock_price",  # Tool call used to get the current stock price of a stock
]


def evaluate_team_reliability():
    response: TeamRunResponse = team.run("What is the current stock price of NVDA?")
    evaluation = ReliabilityEval(
        name="Team Reliability Evaluation",
        team_response=response,
        expected_tool_calls=expected_tool_calls,
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
    result.assert_passed()


if __name__ == "__main__":
    evaluate_team_reliability()



================================================
FILE: cookbook/examples/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/a2a/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/a2a/basic_agent/README.md
================================================
# Basic Agno A2A Agent Example

Basic Agno A2A Agent example that uses A2A to send and receive messages to/from an agent.

## Getting started

1. Clone a2a python repository: https://github.com/google/a2a-python

2. Install the a2a library in your virtual environment which has Agno installed

   ```bash
   pip install .
   ```

3. Start the server

   ```bash
   python cookbook/examples/a2a/basic_agent
   ```

4. Run the test client in a different terminal

   ```bash
   python cookbook/examples/a2a/basic_agent/client.py
   ```

## Notes

- The test client sends a message to the server and prints the response.
- The server uses the `BasicAgentExecutor` to execute the message and send the response back to the client.
- Streaming is not yet functional.
- The server runs on `http://localhost:9999` by default.



================================================
FILE: cookbook/examples/a2a/basic_agent/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/a2a/basic_agent/__main__.py
================================================
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.types import (
    AgentAuthentication,
    AgentCapabilities,
    AgentCard,
    AgentSkill,
)
from basic_agent import BasicAgentExecutor

if __name__ == "__main__":
    skill = AgentSkill(
        id="agno_agent",
        name="Agno Agent",
        description="Agno Agent",
        tags=["Agno agent"],
        examples=["hi", "hello"],
    )

    agent_card = AgentCard(
        name="Agno Agent",
        description="Agno Agent",
        url="http://localhost:9999/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(),
        skills=[skill],
        authentication=AgentAuthentication(schemes=["public"]),
    )

    request_handler = DefaultRequestHandler(
        agent_executor=BasicAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )

    server = A2AStarletteApplication(
        agent_card=agent_card, http_handler=request_handler
    )
    import uvicorn

    uvicorn.run(server.build(), host="0.0.0.0", port=9999, timeout_keep_alive=10)



================================================
FILE: cookbook/examples/a2a/basic_agent/basic_agent.py
================================================
from typing import List

from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import Part, TextPart
from a2a.utils import new_agent_text_message
from agno.agent import Agent, Message, RunResponse
from agno.models.openai import OpenAIChat
from typing_extensions import override

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
)


class BasicAgentExecutor(AgentExecutor):
    """Test AgentProxy Implementation."""

    def __init__(self):
        self.agent = agent

    @override
    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        message: Message = Message(role="user", content="")
        for part in context.message.parts:
            if isinstance(part, Part):
                if isinstance(part.root, TextPart):
                    message.content = part.root.text
                    break

        result: RunResponse = await self.agent.arun(message)
        event_queue.enqueue_event(new_agent_text_message(result.content))

    @override
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        raise Exception("Cancel not supported")



================================================
FILE: cookbook/examples/a2a/basic_agent/client.py
================================================
from typing import Any
from uuid import uuid4

import httpx
from a2a.client import A2AClient
from a2a.types import (
    MessageSendParams,
    SendMessageRequest,
    SendStreamingMessageRequest,  # noqa: F401
)


async def main() -> None:
    async with httpx.AsyncClient() as httpx_client:
        client = await A2AClient.get_client_from_agent_card_url(
            httpx_client, "http://localhost:9999"
        )
        send_message_payload: dict[str, Any] = {
            "message": {
                "role": "user",
                "parts": [
                    {
                        "type": "text",
                        "text": "Hello! What can you tell me about the weather in Tokyo?",
                    }
                ],
                "messageId": uuid4().hex,
            },
        }
        request = SendMessageRequest(params=MessageSendParams(**send_message_payload))

        response = await client.send_message(request)
        print(response.model_dump(mode="json", exclude_none=True))

        # streaming_request = SendStreamingMessageRequest(
        #     params=MessageSendParams(**send_message_payload)
        # )

        # stream_response = client.send_message_streaming(streaming_request)
        # async for chunk in stream_response:
        #     print(chunk.model_dump(mode='json', exclude_none=True))


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())



================================================
FILE: cookbook/examples/agents/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/agents/agent_team.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources.",
    add_datetime_to_instructions=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions="Use tables to display data.",
    add_datetime_to_instructions=True,
)

team_leader = Team(
    name="Reasoning Finance Team Leader",
    mode="coordinate",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[web_agent, finance_agent],
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Use tables to display data.",
        "Only respond with the final answer, no other text.",
    ],
    markdown=True,
    show_members_responses=True,
    enable_agentic_context=True,
    add_datetime_to_instructions=True,
    success_criteria="The team has successfully completed the task.",
)

task = """\
Analyze the semiconductor market performance focusing on:
- NVIDIA (NVDA)
- AMD (AMD)
- Intel (INTC)
- Taiwan Semiconductor (TSM)
Compare their market positions, growth metrics, and future outlook."""

team_leader.print_response(
    task,
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/examples/agents/agent_with_instructions.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[YFinanceTools(stock_price=True)],
    instructions=[
        "Use tables to display data.",
        "Only include the table in your response. No other text.",
    ],
    markdown=True,
)
agent.print_response("What is the stock price of Apple?", stream=True)



================================================
FILE: cookbook/examples/agents/agent_with_knowledge.py
================================================
from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Load Agno documentation in a knowledge base
knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/introduction/agents.md"],
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        # Use OpenAI for embeddings
        embedder=OpenAIEmbedder(id="text-embedding-3-small", dimensions=1536),
    ),
)

agent = Agent(
    name="Agno Assist",
    model=Claude(id="claude-3-7-sonnet-latest"),
    instructions=[
        "Use tables to display data.",
        "Include sources in your response.",
        "Search your knowledge before answering the question.",
        "Only include the output in your response. No other text.",
    ],
    knowledge=knowledge,
    tools=[ReasoningTools(add_instructions=True)],
    add_datetime_to_instructions=True,
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, comment out after first run
    # Set recreate to True to recreate the knowledge base if needed
    agent.knowledge.load(recreate=False)
    agent.print_response(
        "What are Agents?",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/examples/agents/agent_with_memory.py
================================================
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.manager import MemoryManager
from agno.memory.v2.memory import Memory
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

user_id = "peter_rabbit"
memory = Memory(
    db=SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db"),
    model=OpenAIChat(id="gpt-4o-mini"),
)
memory.clear()

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    user_id=user_id,
    memory=memory,
    # Enable the Agent to dynamically create and manage user memories
    enable_agentic_memory=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

if __name__ == "__main__":
    agent.print_response("My name is Peter Rabbit and I like to eat carrots.")
    memories = memory.get_user_memories(user_id=user_id)
    print(f"Memories about {user_id}:")
    pprint(memories)
    agent.print_response("What is my favorite food?")
    agent.print_response("My best friend is Jemima Puddleduck.")
    print(f"Memories about {user_id}:")
    pprint(memories)
    agent.print_response("Recommend a good lunch meal, who should i invite?")
    agent.print_response("What have we been talking about?")



================================================
FILE: cookbook/examples/agents/agent_with_reasoning.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions=[
        "Use tables to display data.",
        "Include sources in your response.",
        "Only include the report in your response. No other text.",
    ],
    markdown=True,
)
agent.print_response(
    "Write a report on NVDA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/examples/agents/agent_with_storage.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

agent = Agent(
    # This session_id is usually auto-generated
    # But for this example, we can set it to a fixed value
    # This session will now forever continue as a very long chat
    session_id="agent_session_which_is_autogenerated_if_not_set",
    model=Claude(id="claude-3-7-sonnet-latest"),
    storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/agents.db"),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
    num_history_runs=3,
    add_datetime_to_instructions=True,
    markdown=True,
)

if __name__ == "__main__":
    print(f"Session id: {agent.session_id}")
    agent.print_response("How many people live in Canada?")
    agent.print_response("What is their national anthem?")
    agent.print_response("List my messages one by one")

    # Print all messages in this session
    messages_in_session = agent.get_messages_for_session()
    pprint(messages_in_session)



================================================
FILE: cookbook/examples/agents/agent_with_tools.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
)
agent.print_response("What is the stock price of Apple?", stream=True)



================================================
FILE: cookbook/examples/agents/agno_assist.py
================================================
from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.vectordb.lancedb import LanceDb, SearchType

agno_assist = Agent(
    name="Agno Assist",
    model=OpenAIChat(id="gpt-4o"),
    description="You help answer questions about the Agno framework.",
    instructions="Search your knowledge before answering the question.",
    knowledge=UrlKnowledge(
        urls=["https://docs.agno.com/llms-full.txt"],
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="agno_assist_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    ),
    storage=SqliteStorage(table_name="agno_assist_sessions", db_file="tmp/agents.db"),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

if __name__ == "__main__":
    agno_assist.knowledge.load()  # Load the knowledge base, comment after first run
    agno_assist.print_response("What is Agno?")



================================================
FILE: cookbook/examples/agents/agno_support_agent.py
================================================
"""🤖 Agno Support Agent - Your AI Assistant for Agno Framework!

This example shows how to create an AI support assistant that combines iterative knowledge searching
with Agno's documentation to provide comprehensive, well-researched answers about the Agno framework.

Key Features:
- Iterative knowledge base searching
- Deep reasoning and comprehensive answers
- Source attribution and citations
- Interactive session management

Example prompts to try:
- "What is Agno and what are its key features?"
- "How do I create my first agent with Agno?"
- "What's the difference between Level 0 and Level 1 agents?"
- "How can I add memory to my Agno agent?"
- "What models does Agno support?"
- "How do I implement RAG with Agno?"

Run `pip install openai lancedb tantivy pypdf ddgs inquirer agno` to install dependencies.
"""

from pathlib import Path
from textwrap import dedent
from typing import List, Optional

import inquirer
import typer
from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.tools.python import PythonTools
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print
from rich.console import Console
from rich.table import Table

# ************* Setup Paths *************
# Define the current working directory and output directory for saving files
cwd = Path(__file__).parent
# Create tmp directory if it doesn't exist
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)
# *************************************


def initialize_knowledge_base(load_knowledge: bool = False):
    """Initialize the knowledge base with Agno documentation

    Args:
        load_knowledge (bool): Whether to load the knowledge base. Defaults to False.
    """
    agent_knowledge = UrlKnowledge(
        urls=["https://docs.agno.com/llms-full.txt"],
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="agno_assist_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    )
    # Load the knowledge base
    if load_knowledge:
        print("[bold blue]📚 Initializing knowledge base...[/bold blue]")
        print("   • Loading Agno documentation")
        print("   • Building vector embeddings")
        print("   • Optimizing for hybrid search")
        agent_knowledge.load()
        print("[bold green]✨ Knowledge base ready![/bold green]\n")
    return agent_knowledge


def get_agent_storage():
    """Return agent storage for session management"""
    return SqliteAgentStorage(
        table_name="agno_assist_sessions", db_file="tmp/agents.db"
    )


def create_agent(
    session_id: Optional[str] = None, load_knowledge: bool = False
) -> Agent:
    """Create and return a configured Agno Support agent."""
    agent_knowledge = initialize_knowledge_base(load_knowledge)
    agent_storage = get_agent_storage()

    return Agent(
        name="AgnoAssist",
        session_id=session_id,
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are AgnoAssist, an advanced AI Agent specialized in the Agno framework.
        Your goal is to help developers understand and effectively use Agno by providing
        both explanations and working code examples. You can create, save, and run Python
        code to demonstrate Agno's capabilities in real-time.

        Your strengths include:
        - Deep understanding of Agno's architecture and capabilities
        - Access to Agno documentation and API reference, search it for relevant information
        - Creating and testing working Agno Agents
        - Building practical, runnable code examples that demonstrate concepts
        - Ability to save code to files and execute them to verify functionality\
        """),
        instructions=dedent("""\
        Your mission is to provide comprehensive, hands-on support for Agno developers
        through iterative knowledge searching, clear explanations, and working code examples.

        Follow these steps for every query:
        1. **Analysis**
            - Break down the question into key technical components
            - Identify if the query requires a knowledge search, creating an Agent or both
            - If you need to search the knowledge base, identify 1-3 key search terms related to Agno concepts
            - If you need to create an Agent, search the knowledge base for relevant concepts and use the example code as a guide
            - When the user asks for an Agent, they mean an Agno Agent.
            - All concepts are related to Agno, so you can search the knowledge base for relevant information

        After Analysis, always start the iterative search process. No need to wait for approval from the user.

        2. **Iterative Search Process**
            - Make at least 3 searches in the knowledge base using the `search_knowledge_base` tool
            - Search for related concepts and implementation details
            - Continue searching until you have found all the information you need or you have exhausted all the search terms

        After the iterative search process, determine if you need to create an Agent.
        If you do, ask the user if they want you to create the Agent and run it.

        3. **Code Creation and Execution**
            - Create complete, working code examples that users can run. For example:
            ```python
            from agno.agent import Agent
            from agno.tools.duckduckgo import DuckDuckGoTools

            agent = Agent(tools=[DuckDuckGoTools()])

            # Perform a web search and capture the response
            response = agent.run("What's happening in France?")
            ```
            - You must remember to use agent.run() and NOT agent.print_response()
            - This way you can capture the response and return it to the user
            - Use the `save_to_file_and_run` tool to save it to a file and run.
            - Make sure to return the `response` variable that tells you the result
            - Remember to:
              * Build the complete agent implementation
              * Include all necessary imports and setup
              * Add comprehensive comments explaining the implementation
              * Test the agent with example queries
              * Ensure all dependencies are listed
              * Include error handling and best practices
              * Add type hints and documentation

        4. **Response Structure**
            - Start with a relevant emoji (🤖 general, 📚 concepts, 💻 code, 🔧 troubleshooting)
            - Give a brief overview
            - Provide detailed explanation with source citations
            - Show the code execution results when relevant
            - Share best practices and common pitfalls
            - Suggest related topics to explore

        5. **Quality Checks**
            - Verify technical accuracy against documentation
            - Test all code examples by running them
            - Check that all aspects of the question are addressed
            - Include relevant documentation links

        Key Agno Concepts to Emphasize:
        - Agent levels (0-3) and capabilities
        - Multimodal and streaming support
        - Model agnosticism and provider flexibility
        - Knowledge base and memory management
        - Tool integration and extensibility
        - Performance optimization techniques

        Code Example Guidelines:
        - Always provide complete, runnable examples
        - Include all necessary imports and setup
        - Add error handling and type hints
        - Follow PEP 8 style guidelines
        - Use descriptive variable names
        - Add comprehensive comments
        - Show example usage and expected output

        Remember:
        - Always verify code examples by running them
        - Be clear about source attribution
        - Support developers at all skill levels
        - Focus on Agno's core principles: Simplicity, Performance, and Agnosticism
        - Save code examples to files when they would be useful to run"""),
        knowledge=agent_knowledge,
        tools=[PythonTools(base_dir=tmp_dir.joinpath("agno_assist"), read_files=True)],
        storage=agent_storage,
        add_history_to_messages=True,
        num_history_responses=3,
        show_tool_calls=True,
        read_chat_history=True,
        markdown=True,
    )


def get_example_topics() -> List[str]:
    """Return a list of example topics for the agent."""
    return [
        "Tell me about Agno",
        "How do I create an agent with web search capabilities?",
        "How can I build an agent which can store session history?",
        "How do I create an Agent with knowledge?",
        "How can I make an agent that can write and execute code?",
    ]


def handle_session_selection() -> Optional[str]:
    """Handle session selection and return the selected session ID."""
    agent_storage = get_agent_storage()

    new = typer.confirm("Do you want to start a new session?", default=True)
    if new:
        return None

    existing_sessions: List[str] = agent_storage.get_all_session_ids()
    if not existing_sessions:
        print("No existing sessions found. Starting a new session.")
        return None

    print("\nExisting sessions:")
    for i, session in enumerate(existing_sessions, 1):
        print(f"{i}. {session}")

    session_idx = typer.prompt(
        "Choose a session number to continue (or press Enter for most recent)",
        default=1,
    )

    try:
        return existing_sessions[int(session_idx) - 1]
    except (ValueError, IndexError):
        return existing_sessions[0]


def run_interactive_loop(agent: Agent, show_topics: bool = True):
    """Run the interactive question-answering loop.

    Args:
        agent: Agent instance to use for responses
        show_topics: Whether to show example topics or continue chat-like interaction
    """
    example_topics = get_example_topics()
    first_interaction = True

    while True:
        if show_topics and first_interaction:
            choices = [f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)]
            choices.extend(["Enter custom question...", "Exit"])

            questions = [
                inquirer.List(
                    "topic",
                    message="Select a topic or ask a different question:",
                    choices=choices,
                )
            ]
            answer = inquirer.prompt(questions)

            if answer is None or answer["topic"] == "Exit":
                break

            if answer["topic"] == "Enter custom question...":
                questions = [inquirer.Text("custom", message="Enter your question:")]
                custom_answer = inquirer.prompt(questions)
                topic = custom_answer["custom"]
            else:
                topic = example_topics[int(answer["topic"].split(".")[0]) - 1]
            first_interaction = False
        else:
            # Chat-like interaction
            question = typer.prompt("\n", prompt_suffix="> ")
            if question.lower() in ("exit", "quit", "bye"):
                break
            topic = question

        agent.print_response(topic, stream=True)


def agno_support_agent(
    load_knowledge: bool = typer.Option(
        False, "--load-knowledge", "-l", help="Load the knowledge base on startup"
    ),
):
    """Main function to run the Agno Support agent."""
    session_id = handle_session_selection()
    agent = create_agent(session_id, load_knowledge)

    # Create and display welcome table
    console = Console()
    table = Table(show_header=False, style="cyan")
    table.add_column(justify="center", min_width=40)
    table.add_row("🤖 Welcome to [bold green]AgnoAssist[/bold green]")
    table.add_row("Your Personal Agno Expert")
    console.print(table)

    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(
                "[bold green]📝 Started New Session: [white]{}[/white][/bold green]\n".format(
                    session_id
                )
            )
        else:
            print("[bold green]📝 Started New Session[/bold green]\n")
        show_topics = True
    else:
        print(
            "[bold blue]🔄 Continuing Previous Session: [white]{}[/white][/bold blue]\n".format(
                session_id
            )
        )
        show_topics = False

    run_interactive_loop(agent, show_topics)


if __name__ == "__main__":
    typer.run(agno_support_agent)



================================================
FILE: cookbook/examples/agents/airbnb_mcp.py
================================================
"""🏠 MCP Airbnb Agent - Search for Airbnb listings!

This example shows how to create an agent that uses MCP and Llama 4 to search for Airbnb listings.

1. Run: `pip install groq mcp agno` to install the dependencies
2. Export your GROQ_API_KEY
3. Run: `python cookbook/examples/agents/airbnb_mcp.py` to run the agent
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.mcp import MCPTools
from agno.tools.thinking import ThinkingTools


async def run_agent(message: str) -> None:
    async with MCPTools(
        "npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt"
    ) as mcp_tools:
        agent = Agent(
            model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"),
            tools=[ThinkingTools(), mcp_tools],
            instructions=dedent("""\
            ## General Instructions
            - Always start by using the think tool to map out the steps needed to complete the task.
            - After receiving tool results, use the think tool as a scratchpad to validate the results for correctness
            - Before responding to the user, use the think tool to jot down final thoughts and ideas.
            - Present final outputs in well-organized tables whenever possible.
            - Always provide links to the listings in your response.
            - Show your top 10 recommendations in a table and make a case for why each is the best choice.

            ## Using the think tool
            At every step, use the think tool as a scratchpad to:
            - Restate the object in your own words to ensure full comprehension.
            - List the  specific rules that apply to the current request
            - Check if all required information is collected and is valid
            - Verify that the planned action completes the task\
            """),
            add_datetime_to_instructions=True,
            show_tool_calls=True,
            markdown=True,
        )
        await agent.aprint_response(message, stream=True)


if __name__ == "__main__":
    task = dedent("""\
    I'm traveling to San Francisco from April 20th - May 8th. Can you find me the best deals for a 1 bedroom apartment?
    I'd like a dedicated workspace and close proximity to public transport.\
    """)
    asyncio.run(run_agent(task))



================================================
FILE: cookbook/examples/agents/basic_agent.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(model=Claude(id="claude-3-7-sonnet-latest"), markdown=True)
agent.print_response("What is the stock price of Apple?", stream=True)



================================================
FILE: cookbook/examples/agents/book_recommendation.py
================================================
"""📚 Book Recommendation Agent - Your Personal Literary Curator!

This example shows how to create an intelligent book recommendation system that provides
comprehensive literary suggestions based on your preferences. The agent combines book databases,
ratings, reviews, and upcoming releases to deliver personalized reading recommendations.

Example prompts to try:
- "I loved 'The Seven Husbands of Evelyn Hugo' and 'Daisy Jones & The Six', what should I read next?"
- "Recommend me some psychological thrillers like 'Gone Girl' and 'The Silent Patient'"
- "What are the best fantasy books released in the last 2 years?"
- "I enjoy historical fiction with strong female leads, any suggestions?"
- "Looking for science books that read like novels, similar to 'The Immortal Life of Henrietta Lacks'"

Run: `pip install openai exa_py agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

book_recommendation_agent = Agent(
    name="Shelfie",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are Shelfie, a passionate and knowledgeable literary curator with expertise in books worldwide! 📚

        Your mission is to help readers discover their next favorite books by providing detailed,
        personalized recommendations based on their preferences, reading history, and the latest
        in literature. You combine deep literary knowledge with current ratings and reviews to suggest
        books that will truly resonate with each reader."""),
    instructions=dedent("""\
        Approach each recommendation with these steps:

        1. Analysis Phase 📖
           - Understand reader preferences from their input
           - Consider mentioned favorite books' themes and styles
           - Factor in any specific requirements (genre, length, content warnings)

        2. Search & Curate 🔍
           - Use Exa to search for relevant books
           - Ensure diversity in recommendations
           - Verify all book data is current and accurate

        3. Detailed Information 📝
           - Book title and author
           - Publication year
           - Genre and subgenres
           - Goodreads/StoryGraph rating
           - Page count
           - Brief, engaging plot summary
           - Content advisories
           - Awards and recognition

        4. Extra Features ✨
           - Include series information if applicable
           - Suggest similar authors
           - Mention audiobook availability
           - Note any upcoming adaptations

        Presentation Style:
        - Use clear markdown formatting
        - Present main recommendations in a structured table
        - Group similar books together
        - Add emoji indicators for genres (📚 🔮 💕 🔪)
        - Minimum 5 recommendations per query
        - Include a brief explanation for each recommendation
        - Highlight diversity in authors and perspectives
        - Note trigger warnings when relevant"""),
    markdown=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
)

# Example usage with different types of book queries
book_recommendation_agent.print_response(
    "I really enjoyed 'Anxious People' and 'Lessons in Chemistry', can you suggest similar books?",
    stream=True,
)

# More example prompts to explore:
"""
Genre-specific queries:
1. "Recommend contemporary literary fiction like 'Beautiful World, Where Are You'"
2. "What are the best fantasy series completed in the last 5 years?"
3. "Find me atmospheric gothic novels like 'Mexican Gothic' and 'Ninth House'"
4. "What are the most acclaimed debut novels from this year?"

Contemporary Issues:
1. "Suggest books about climate change that aren't too depressing"
2. "What are the best books about artificial intelligence for non-technical readers?"
3. "Recommend memoirs about immigrant experiences"
4. "Find me books about mental health with hopeful endings"

Book Club Selections:
1. "What are good book club picks that spark discussion?"
2. "Suggest literary fiction under 350 pages"
3. "Find thought-provoking novels that tackle current social issues"
4. "Recommend books with multiple perspectives/narratives"

Upcoming Releases:
1. "What are the most anticipated literary releases next month?"
2. "Show me upcoming releases from my favorite authors"
3. "What debut novels are getting buzz this season?"
4. "List upcoming books being adapted for screen"
"""



================================================
FILE: cookbook/examples/agents/competitor_analysis_agent.py
================================================
"""🔍 Competitor Analysis Agent - Your AI-Powered Market Intelligence System!

This example demonstrates how to build a sophisticated competitor analysis agent that combines powerful search and scraping capabilities with advanced reasoning tools to provide
comprehensive competitive intelligence. The agent performs deep analysis of competitors including
market positioning, product offerings, and strategic insights.

Key capabilities:
- Company discovery using Firecrawl search
- Website scraping and content analysis
- Competitive intelligence gathering
- SWOT analysis with reasoning
- Strategic recommendations
- Structured thinking and analysis

Example queries to try:
- "Analyze OpenAI's main competitors in the LLM space"
- "Compare Uber vs Lyft in the ride-sharing market"
- "Analyze Tesla's competitive position vs traditional automakers"
- "Research fintech competitors to Stripe"
- "Analyze Nike vs Adidas in the athletic apparel market"

Dependencies: `pip install openai firecrawl-py agno`
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.firecrawl import FirecrawlTools
from agno.tools.reasoning import ReasoningTools

competitor_analysis_agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    tools=[
        FirecrawlTools(
            search=True,
            crawl=True,
            mapping=True,
            formats=["markdown", "links", "html"],
            search_params={
                "limit": 2,
            },
            limit=5,
        ),
        ReasoningTools(
            add_instructions=True,
        ),
    ],
    instructions=[
        "1. Initial Research & Discovery:",
        "   - Use search tool to find information about the target company",
        "   - Search for '[company name] competitors', 'companies like [company name]'",
        "   - Search for industry reports and market analysis",
        "   - Use the think tool to plan your research approach",
        "2. Competitor Identification:",
        "   - Search for each identified competitor using Firecrawl",
        "   - Find their official websites and key information sources",
        "   - Map out the competitive landscape",
        "3. Website Analysis:",
        "   - Scrape competitor websites using Firecrawl",
        "   - Map their site structure to understand their offerings",
        "   - Extract product information, pricing, and value propositions",
        "   - Look for case studies and customer testimonials",
        "4. Deep Competitive Analysis:",
        "   - Use the analyze tool after gathering information on each competitor",
        "   - Compare features, pricing, and market positioning",
        "   - Identify patterns and competitive dynamics",
        "   - Think through the implications of your findings",
        "5. Strategic Synthesis:",
        "   - Conduct SWOT analysis for each major competitor",
        "   - Use reasoning to identify competitive advantages",
        "   - Analyze market trends and opportunities",
        "   - Develop strategic recommendations",
        "- Always use the think tool before starting major research phases",
        "- Use the analyze tool to process findings and draw insights",
        "- Search for multiple perspectives on each competitor",
        "- Verify information by checking multiple sources",
        "- Be thorough but focused in your analysis",
        "- Provide evidence-based recommendations",
    ],
    expected_output=dedent("""\
    # Competitive Analysis Report: {Target Company}

    ## Executive Summary
    {High-level overview of competitive landscape and key findings}

    ## Research Methodology
    - Search queries used
    - Websites analyzed
    - Key information sources

    ## Market Overview
    ### Industry Context
    - Market size and growth rate
    - Key trends and drivers
    - Regulatory environment

    ### Competitive Landscape
    - Major players identified
    - Market segmentation
    - Competitive dynamics

    ## Competitor Analysis

    ### Competitor 1: {Name}
    #### Company Overview
    - Website: {URL}
    - Founded: {Year}
    - Headquarters: {Location}
    - Company size: {Employees/Revenue if available}

    #### Products & Services
    - Core offerings
    - Key features and capabilities
    - Pricing model and tiers
    - Target market segments

    #### Digital Presence Analysis
    - Website structure and user experience
    - Key messaging and value propositions
    - Content strategy and resources
    - Customer proof points

    #### SWOT Analysis
    **Strengths:**
    - {Evidence-based strengths}

    **Weaknesses:**
    - {Identified weaknesses}

    **Opportunities:**
    - {Market opportunities}

    **Threats:**
    - {Competitive threats}

    ### Competitor 2: {Name}
    {Similar structure as above}

    ### Competitor 3: {Name}
    {Similar structure as above}

    ## Comparative Analysis

    ### Feature Comparison Matrix
    | Feature | {Target} | Competitor 1 | Competitor 2 | Competitor 3 |
    |---------|----------|--------------|--------------|--------------|
    | {Feature 1} | ✓/✗ | ✓/✗ | ✓/✗ | ✓/✗ |
    | {Feature 2} | ✓/✗ | ✓/✗ | ✓/✗ | ✓/✗ |

    ### Pricing Comparison
    | Company | Entry Level | Professional | Enterprise |
    |---------|-------------|--------------|------------|
    | {Pricing details extracted from websites} |

    ### Market Positioning Analysis
    {Analysis of how each competitor positions themselves}

    ## Strategic Insights

    ### Key Findings
    1. {Major insight with evidence}
    2. {Competitive dynamics observed}
    3. {Market gaps identified}

    ### Competitive Advantages
    - {Target company's advantages}
    - {Unique differentiators}

    ### Competitive Risks
    - {Main threats from competitors}
    - {Market challenges}

    ## Strategic Recommendations

    ### Immediate Actions (0-3 months)
    1. {Quick competitive responses}
    2. {Low-hanging fruit opportunities}

    ### Short-term Strategy (3-12 months)
    1. {Product/service enhancements}
    2. {Market positioning adjustments}

    ### Long-term Strategy (12+ months)
    1. {Sustainable differentiation}
    2. {Market expansion opportunities}

    ## Conclusion
    {Summary of competitive position and strategic imperatives}
    """),
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
    stream_intermediate_steps=True,
)

competitor_analysis_agent.print_response(
    """\
    Analyze the competitive landscape for Stripe in the payments industry.
    Focus on their products, pricing models, and market positioning.\
    """,
    stream=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/examples/agents/deep_knowledge.py
================================================
"""🤔 DeepKnowledge - An AI Agent that iteratively searches a knowledge base to answer questions

This agent performs iterative searches through its knowledge base, breaking down complex
queries into sub-questions, and synthesizing comprehensive answers. It's designed to explore
topics deeply and thoroughly by following chains of reasoning.

In this example, the agent uses the Agno documentation as a knowledge base

Key Features:
- Iteratively searches a knowledge base
- Source attribution and citations

Run `pip install openai lancedb tantivy inquirer agno` to install dependencies.
"""

from textwrap import dedent
from typing import List, Optional

import inquirer
import typer
from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print


def initialize_knowledge_base():
    """Initialize the knowledge base with your preferred documentation or knowledge source
    Here we use Agno docs as an example, but you can replace with any relevant URLs
    """
    agent_knowledge = UrlKnowledge(
        urls=["https://docs.agno.com/llms-full.txt"],
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="deep_knowledge_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    )
    # Load the knowledge base (comment out after first run)
    agent_knowledge.load()
    return agent_knowledge


def get_agent_storage():
    """Return agent storage"""
    return SqliteAgentStorage(
        table_name="deep_knowledge_sessions", db_file="tmp/agents.db"
    )


def create_agent(session_id: Optional[str] = None) -> Agent:
    """Create and return a configured DeepKnowledge agent."""
    agent_knowledge = initialize_knowledge_base()
    agent_storage = get_agent_storage()
    return Agent(
        name="DeepKnowledge",
        session_id=session_id,
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are DeepKnowledge, an advanced reasoning agent designed to provide thorough,
        well-researched answers to any query by searching your knowledge base.

        Your strengths include:
        - Breaking down complex topics into manageable components
        - Connecting information across multiple domains
        - Providing nuanced, well-researched answers
        - Maintaining intellectual honesty and citing sources
        - Explaining complex concepts in clear, accessible terms"""),
        instructions=dedent("""\
        Your mission is to leave no stone unturned in your pursuit of the correct answer.

        To achieve this, follow these steps:
        1. **Analyze the input and break it down into key components**.
        2. **Search terms**: You must identify at least 3-5 key search terms to search for.
        3. **Initial Search:** Searching your knowledge base for relevant information. You must make atleast 3 searches to get all relevant information.
        4. **Evaluation:** If the answer from the knowledge base is incomplete, ambiguous, or insufficient - Ask the user for clarification. Do not make informed guesses.
        5. **Iterative Process:**
            - Continue searching your knowledge base till you have a comprehensive answer.
            - Reevaluate the completeness of your answer after each search iteration.
            - Repeat the search process until you are confident that every aspect of the question is addressed.
        4. **Reasoning Documentation:** Clearly document your reasoning process:
            - Note when additional searches were triggered.
            - Indicate which pieces of information came from the knowledge base and where it was sourced from.
            - Explain how you reconciled any conflicting or ambiguous information.
        5. **Final Synthesis:** Only finalize and present your answer once you have verified it through multiple search passes.
            Include all pertinent details and provide proper references.
        6. **Continuous Improvement:** If new, relevant information emerges even after presenting your answer,
            be prepared to update or expand upon your response.

        **Communication Style:**
        - Use clear and concise language.
        - Organize your response with numbered steps, bullet points, or short paragraphs as needed.
        - Be transparent about your search process and cite your sources.
        - Ensure that your final answer is comprehensive and leaves no part of the query unaddressed.

        Remember: **Do not finalize your answer until every angle of the question has been explored.**"""),
        additional_context=dedent("""\
        You should only respond with the final answer and the reasoning process.
        No need to include irrelevant information.

        - User ID: {user_id}
        - Memory: You have access to your previous search results and reasoning process.
        """),
        knowledge=agent_knowledge,
        storage=agent_storage,
        add_history_to_messages=True,
        num_history_responses=3,
        show_tool_calls=True,
        read_chat_history=True,
        markdown=True,
    )


def get_example_topics() -> List[str]:
    """Return a list of example topics for the agent."""
    return [
        "What are AI agents and how do they work in Agno?",
        "What chunking strategies does Agno support for text processing?",
        "How can I implement custom tools in Agno?",
        "How does knowledge retrieval work in Agno?",
        "What types of embeddings does Agno support?",
    ]


def handle_session_selection() -> Optional[str]:
    """Handle session selection and return the selected session ID."""
    agent_storage = get_agent_storage()

    new = typer.confirm("Do you want to start a new session?", default=True)
    if new:
        return None

    existing_sessions: List[str] = agent_storage.get_all_session_ids()
    if not existing_sessions:
        print("No existing sessions found. Starting a new session.")
        return None

    print("\nExisting sessions:")
    for i, session in enumerate(existing_sessions, 1):
        print(f"{i}. {session}")

    session_idx = typer.prompt(
        "Choose a session number to continue (or press Enter for most recent)",
        default=1,
    )

    try:
        return existing_sessions[int(session_idx) - 1]
    except (ValueError, IndexError):
        return existing_sessions[0]


def run_interactive_loop(agent: Agent):
    """Run the interactive question-answering loop."""
    example_topics = get_example_topics()

    while True:
        choices = [f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)]
        choices.extend(["Enter custom question...", "Exit"])

        questions = [
            inquirer.List(
                "topic",
                message="Select a topic or ask a different question:",
                choices=choices,
            )
        ]
        answer = inquirer.prompt(questions)

        if answer["topic"] == "Exit":
            break

        if answer["topic"] == "Enter custom question...":
            questions = [inquirer.Text("custom", message="Enter your question:")]
            custom_answer = inquirer.prompt(questions)
            topic = custom_answer["custom"]
        else:
            topic = example_topics[int(answer["topic"].split(".")[0]) - 1]

        agent.print_response(topic, stream=True)


def deep_knowledge_agent():
    """Main function to run the DeepKnowledge agent."""

    session_id = handle_session_selection()
    agent = create_agent(session_id)

    print("\n🤔 Welcome to DeepKnowledge - Your Advanced Research Assistant! 📚")
    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"[bold green]Started New Session: {session_id}[/bold green]\n")
        else:
            print("[bold green]Started New Session[/bold green]\n")
    else:
        print(f"[bold blue]Continuing Previous Session: {session_id}[/bold blue]\n")

    run_interactive_loop(agent)


if __name__ == "__main__":
    typer.run(deep_knowledge_agent)

# Example prompts to try:
"""
Explore Agno's capabilities with these queries:
1. "What are the different types of agents in Agno?"
2. "How does Agno handle knowledge base management?"
3. "What embedding models does Agno support?"
4. "How can I implement custom tools in Agno?"
5. "What storage options are available for workflow caching?"
6. "How does Agno handle streaming responses?"
7. "What types of LLM providers does Agno support?"
8. "How can I implement custom knowledge sources?"
"""



================================================
FILE: cookbook/examples/agents/deep_research_agent_exa.py
================================================
"""Example: Advanced Research Agent using Exa Research Tools

This example demonstrates how to use the Exa research tool for complex,
structured research tasks with automatic citation tracking.
"""

import json
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ExaTools(research=True, research_model="exa-research-pro")],
    instructions=dedent("""
        You are an expert research analyst with access to advanced research tools.
        
        When you are given a schema to use, pass it to the research tool as output_schema parameter to research tool. 

        The research tool has two parameters:
        - instructions (str): The research topic/question 
        - output_schema (dict, optional): A JSON schema for structured output

        Example: If user says "Research X. Use this schema {'type': 'object', ...}", you must call research tool with the schema.

        If no schema is provided, the tool will auto-infer an appropriate schema.

        Present the findings exactly as provided by the research tool.
    """),
    show_tool_calls=True,
)

# Example 1: Basic research with simple string
agent.print_response(
    "Perform a comprehensive research on the current flagship GPUs from NVIDIA, AMD and Intel. Return a table of model name, MSRP USD, TDP watts, and launch date. Include citations for each cell."
)

# Define a JSON schema for structured research output
# research_schema = {
#     "type": "object",
#     "properties": {
#         "major_players": {
#             "type": "array",
#             "items": {
#                 "type": "object",
#                 "properties": {
#                     "name": {"type": "string"},
#                     "role": {"type": "string"},
#                     "contributions": {"type": "string"},
#                 },
#             },
#         },
#     },
#     "required": ["major_players"],
# }

# agent.print_response(
#     f"Research the top 3 Semiconductor companies in 2024. Use this schema {research_schema}."
# )



================================================
FILE: cookbook/examples/agents/fibonacci_agent.py
================================================
import json

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools


def get_fibonacci_series(count: int = 5) -> str:
    """Generate a Fibonacci series up to the specified count."""
    if count <= 0:
        return "Count must be a positive integer."

    fib_series = [0, 1]
    for i in range(2, count):
        fib_series.append(fib_series[-1] + fib_series[-2])

    return json.dumps(fib_series[:count])


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        get_fibonacci_series,
        CalculatorTools(add=True, subtract=True, exponentiate=True, is_prime=True),
    ],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response(
    """
    1. Get 10 elements of fibonacci series
    2. Calculate the sum of cubes of the prime numbers in the series
    3. Subtract 4
""",
    stream=True,
)



================================================
FILE: cookbook/examples/agents/finance_agent.py
================================================
"""🗞️ Finance Agent - Your Personal Market Analyst!

This example shows how to create a sophisticated financial analyst that provides
comprehensive market insights using real-time data. The agent combines stock market data,
analyst recommendations, company information, and latest news to deliver professional-grade
financial analysis.

Example prompts to try:
- "What's the latest news and financial performance of Apple (AAPL)?"
- "Give me a detailed analysis of Tesla's (TSLA) current market position"
- "How are Microsoft's (MSFT) financials looking? Include analyst recommendations"
- "Analyze NVIDIA's (NVDA) stock performance and future outlook"
- "What's the market saying about Amazon's (AMZN) latest quarter?"

Run: `pip install openai yfinance agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            stock_fundamentals=True,
            historical_prices=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=dedent("""\
        You are a seasoned Wall Street analyst with deep expertise in market analysis! 📊

        Follow these steps for comprehensive financial analysis:
        1. Market Overview
           - Latest stock price
           - 52-week high and low
        2. Financial Deep Dive
           - Key metrics (P/E, Market Cap, EPS)
        3. Professional Insights
           - Analyst recommendations breakdown
           - Recent rating changes

        4. Market Context
           - Industry trends and positioning
           - Competitive analysis
           - Market sentiment indicators

        Your reporting style:
        - Begin with an executive summary
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends (📈 📉)
        - Highlight key insights with bullet points
        - Compare metrics to industry averages
        - Include technical term explanations
        - End with a forward-looking analysis

        Risk Disclosure:
        - Always highlight potential risk factors
        - Note market uncertainties
        - Mention relevant regulatory concerns
    """),
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
)

# Example usage with detailed market analysis request
finance_agent.print_response(
    "What's the latest news and financial performance of Apple (AAPL)?", stream=True
)

# Semiconductor market analysis example
finance_agent.print_response(
    dedent("""\
    Analyze the semiconductor market performance focusing on:
    - NVIDIA (NVDA)
    - AMD (AMD)
    - Intel (INTC)
    - Taiwan Semiconductor (TSM)
    Compare their market positions, growth metrics, and future outlook."""),
    stream=True,
)

# Automotive market analysis example
finance_agent.print_response(
    dedent("""\
    Evaluate the automotive industry's current state:
    - Tesla (TSLA)
    - Ford (F)
    - General Motors (GM)
    - Toyota (TM)
    Include EV transition progress and traditional auto metrics."""),
    stream=True,
)

# More example prompts to explore:
"""
Advanced analysis queries:
1. "Compare Tesla's valuation metrics with traditional automakers"
2. "Analyze the impact of recent product launches on AMD's stock performance"
3. "How do Meta's financial metrics compare to its social media peers?"
4. "Evaluate Netflix's subscriber growth impact on financial metrics"
5. "Break down Amazon's revenue streams and segment performance"

Industry-specific analyses:
Semiconductor Market:
1. "How is the chip shortage affecting TSMC's market position?"
2. "Compare NVIDIA's AI chip revenue growth with competitors"
3. "Analyze Intel's foundry strategy impact on stock performance"
4. "Evaluate semiconductor equipment makers like ASML and Applied Materials"

Automotive Industry:
1. "Compare EV manufacturers' production metrics and margins"
2. "Analyze traditional automakers' EV transition progress"
3. "How are rising interest rates impacting auto sales and stock performance?"
4. "Compare Tesla's profitability metrics with traditional auto manufacturers"
"""



================================================
FILE: cookbook/examples/agents/finance_agent_with_memory.py
================================================
"""🗞️ Finance Agent with Memory - Your Market Analyst that remembers your preferences

1. Create virtual environment and install dependencies:
   - Run `uv venv --python 3.12` to create a virtual environment
   - Run `source .venv/bin/activate` to activate the virtual environment
   - Run `uv pip install agno openai sqlalchemy fastapi uvicorn yfinance ddgs` to install the dependencies
   - Run `ag setup` to connect your local env to Agno
   - Export your OpenAI key: `export OPENAI_API_KEY=<your_openai_key>`
2. Run the app:
   - Run `python cookbook/examples/agents/financial_agent_with_memory.py` to start the app
3. Chat with the agent:
   - Open `https://app.agno.com/playground?endpoint=localhost%3A7777`
   - Tell the agent your name and favorite stocks
   - Ask the agent to analyze your favorite stocks
"""

from textwrap import dedent

from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.playground import Playground, serve_playground_app
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

finance_agent_with_memory = Agent(
    name="Finance Agent with Memory",
    agent_id="financial_agent_with_memory",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[YFinanceTools(enable_all=True), DuckDuckGoTools()],
    memory=Memory(
        db=SqliteMemoryDb(table_name="agent_memory", db_file="tmp/agent_data.db"),
        model=OpenAIChat(id="gpt-4o-mini"),
        clear_memories=True,
        delete_memories=True,
    ),
    # Let the Agent create and manage user memories
    enable_agentic_memory=True,
    # Uncomment to always create memories from the input
    # can be used instead of enable_agentic_memory
    # enable_user_memories=True,
    storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/agent_data.db"),
    # Add messages from the last 3 runs to the messages
    add_history_to_messages=True,
    num_history_runs=3,
    # Add the current datetime to the instructions
    add_datetime_to_instructions=True,
    # Use markdown formatting
    markdown=True,
    instructions=dedent("""\
        You are a Wall Street analyst. Your goal is to help users with financial analysis.

        Checklist for different types of financial analysis:
        1. Market Overview: Stock price, 52-week range.
        2. Financials: P/E, Market Cap, EPS.
        3. Insights: Analyst recommendations, rating changes.
        4. Market Context: Industry trends, competitive landscape, sentiment.

        Formatting guidelines:
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends (📈 📉)
        - Highlight key insights with bullet points
    """),
)

app = Playground(agents=[finance_agent_with_memory]).get_app()

if __name__ == "__main__":
    serve_playground_app("financial_agent_with_memory:app", port=7777)



================================================
FILE: cookbook/examples/agents/legal_consultant.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=[
        "https://www.justice.gov/d9/criminal-ccips/legacy/2015/01/14/ccmanual_0.pdf",
    ],
    vector_db=PgVector(table_name="legal_docs", db_url=db_url),
)
knowledge_base.load(recreate=False)

legal_agent = Agent(
    name="LegalAdvisor",
    knowledge=knowledge_base,
    search_knowledge=True,
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    instructions=[
        "Provide legal information and advice based on the knowledge base.",
        "Include relevant legal citations and sources when answering questions.",
        "Always clarify that you're providing general legal information, not professional legal advice.",
        "Recommend consulting with a licensed attorney for specific legal situations.",
    ],
)

legal_agent.print_response(
    "What are the legal consequences and criminal penalties for spoofing Email Address?",
    stream=True,
)



================================================
FILE: cookbook/examples/agents/media_trend_analysis_agent.py
================================================
"""Please install dependencies using:
pip install openai exa-py agno firecrawl
"""

from datetime import datetime, timedelta
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools
from agno.tools.firecrawl import FirecrawlTools


def calculate_start_date(days: int) -> str:
    """Calculate start date based on number of days."""
    start_date = datetime.now() - timedelta(days=days)
    return start_date.strftime("%Y-%m-%d")


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ExaTools(start_published_date=calculate_start_date(30), type="keyword"),
        FirecrawlTools(scrape=True),
    ],
    description=dedent("""\
        You are an expert media trend analyst specializing in:
        1. Identifying emerging trends across news and digital platforms
        2. Recognizing pattern changes in media coverage
        3. Providing actionable insights based on data
        4. Forecasting potential future developments
    """),
    instructions=[
        "Analyze the provided topic according to the user's specifications:",
        "1. Use keywords to perform targeted searches",
        "2. Identify key influencers and authoritative sources",
        "3. Extract main themes and recurring patterns",
        "4. Provide actionable recommendations",
        "5. if got sources less then 2, only then scrape them using firecrawl tool, dont crawl it  and use them to generate the report",
        "6. growth rate should be in percentage , and if not possible dont give growth rate",
    ],
    expected_output=dedent("""\
    # Media Trend Analysis Report

    ## Executive Summary
    {High-level overview of findings and key metrics}

    ## Trend Analysis
    ### Volume Metrics
    - Peak discussion periods: {dates}
    - Growth rate: {percentage or dont show this}

    ## Source Analysis
    ### Top Sources
    1. {Source 1}

    2. {Source 2}


    ## Actionable Insights
    1. {Insight 1}
       - Evidence: {data points}
       - Recommended action: {action}

    ## Future Predictions
    1. {Prediction 1}
       - Supporting evidence: {evidence}

    ## References
    {Detailed source list with links}
    """),
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
)

# Example usage:
analysis_prompt = """\
Analyze media trends for:
Keywords: ai agents
Sources: verge.com ,linkedin.com, x.com
"""

agent.print_response(analysis_prompt, stream=True)

# Alternative prompt example
crypto_prompt = """\
Analyze media trends for:
Keywords: cryptocurrency, bitcoin, ethereum
Sources: coindesk.com, cointelegraph.com
"""

# agent.print_response(crypto_prompt, stream=True)



================================================
FILE: cookbook/examples/agents/meeting_summarizer_agent.py
================================================
"""Example: Meeting Summarizer & Visualizer Agent

This script uses OpenAITools (transcribe_audio, generate_image, generate_speech)
to process a meeting recording, summarize it, visualize it, and create an audio summary.

Requires: pip install openai agno
"""

from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.openai import OpenAITools
from agno.tools.reasoning import ReasoningTools
from agno.utils.media import download_file, save_base64_data

input_audio_url: str = (
    "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/sample_audio.mp3"
)

local_audio_path = Path("tmp/meeting_recording.mp3")
print(f"Downloading file to local path: {local_audio_path}")
download_file(input_audio_url, local_audio_path)

meeting_agent: Agent = Agent(
    model=Gemini(id="gemini-2.0-flash"),
    tools=[OpenAITools(), ReasoningTools()],
    description=dedent("""\
        You are an efficient Meeting Assistant AI.
        Your purpose is to process audio recordings of meetings, extract key information,
        create a visual representation, and provide an audio summary.
    """),
    instructions=dedent(f"""\
        Follow these steps precisely:
        1. Receive the path to an audio file.
        2. Use the `transcribe_audio` tool to get the text transcription.
        3. Analyze the transcription and write a concise summary highlighting key discussion points, decisions, and action items.
        4. Based *only* on the summary created in step 3, generating important meeting points. This should be a essentially an overview of the summary's content properly ordered and formatted in the form of meeting minutes.
        5. Convert the meeting minutes into an audio summary using the `generate_speech` tool.
    """),
    markdown=True,
    show_tool_calls=True,
)

response = meeting_agent.run(
    f"Please process the meeting recording located at '{local_audio_path}'",
)
if response.audio:
    save_base64_data(response.audio[0].base64_audio, Path("tmp/meeting_summary.mp3"))
    print(f"Meeting summary saved to: {Path('tmp/meeting_summary.mp3')}")



================================================
FILE: cookbook/examples/agents/movie_recommedation.py
================================================
"""🎬 Movie Recommendation Agent - Your Personal Cinema Curator!

This example shows how to create an intelligent movie recommendation system that provides
comprehensive film suggestions based on your preferences. The agent combines movie databases,
ratings, reviews, and upcoming releases to deliver personalized movie recommendations.

Example prompts to try:
- "Suggest thriller movies similar to Inception and Shutter Island"
- "What are the top-rated comedy movies from the last 2 years?"
- "Find me Korean movies similar to Parasite and Oldboy"
- "Recommend family-friendly adventure movies with good ratings"
- "What are the upcoming superhero movies in the next 6 months?"

Run: `pip install openai exa_py agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

movie_recommendation_agent = Agent(
    name="PopcornPal",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are PopcornPal, a passionate and knowledgeable film curator with expertise in cinema worldwide! 🎥

        Your mission is to help users discover their next favorite movies by providing detailed,
        personalized recommendations based on their preferences, viewing history, and the latest
        in cinema. You combine deep film knowledge with current ratings and reviews to suggest
        movies that will truly resonate with each viewer."""),
    instructions=dedent("""\
        Approach each recommendation with these steps:
        1. Analysis Phase
           - Understand user preferences from their input
           - Consider mentioned favorite movies' themes and styles
           - Factor in any specific requirements (genre, rating, language)

        2. Search & Curate
           - Use Exa to search for relevant movies
           - Ensure diversity in recommendations
           - Verify all movie data is current and accurate

        3. Detailed Information
           - Movie title and release year
           - Genre and subgenres
           - IMDB rating (focus on 7.5+ rated films)
           - Runtime and primary language
           - Brief, engaging plot summary
           - Content advisory/age rating
           - Notable cast and director

        4. Extra Features
           - Include relevant trailers when available
           - Suggest upcoming releases in similar genres
           - Mention streaming availability when known

        Presentation Style:
        - Use clear markdown formatting
        - Present main recommendations in a structured table
        - Group similar movies together
        - Add emoji indicators for genres (🎭 🎬 🎪)
        - Minimum 5 recommendations per query
        - Include a brief explanation for each recommendation
    """),
    markdown=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
)

# Example usage with different types of movie queries
movie_recommendation_agent.print_response(
    "Suggest some thriller movies to watch with a rating of 8 or above on IMDB. "
    "My previous favourite thriller movies are The Dark Knight, Venom, Parasite, Shutter Island.",
    stream=True,
)

# More example prompts to explore:
"""
Genre-specific queries:
1. "Find me psychological thrillers similar to Black Swan and Gone Girl"
2. "What are the best animated movies from Studio Ghibli?"
3. "Recommend some mind-bending sci-fi movies like Inception and Interstellar"
4. "What are the highest-rated crime documentaries from the last 5 years?"

International Cinema:
1. "Suggest Korean movies similar to Parasite and Train to Busan"
2. "What are the must-watch French films from the last decade?"
3. "Recommend Japanese animated movies for adults"
4. "Find me award-winning European drama films"

Family & Group Watching:
1. "What are good family movies for kids aged 8-12?"
2. "Suggest comedy movies perfect for a group movie night"
3. "Find educational documentaries suitable for teenagers"
4. "Recommend adventure movies that both adults and children would enjoy"

Upcoming Releases:
1. "What are the most anticipated movies coming out next month?"
2. "Show me upcoming superhero movie releases"
3. "What horror movies are releasing this Halloween season?"
4. "List upcoming book-to-movie adaptations"
"""



================================================
FILE: cookbook/examples/agents/pydantic_model_as_input.py
================================================
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

hackernews_agent.print_response(
    message=ResearchTopic(
        topic="AI",
        focus_areas=["AI", "Machine Learning"],
        target_audience="Developers",
        sources_required=5,
    )
)



================================================
FILE: cookbook/examples/agents/readme_generator.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.github import GithubTools
from agno.tools.local_file_system import LocalFileSystemTools

readme_gen_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    name="Readme Generator Agent",
    tools=[GithubTools(), LocalFileSystemTools()],
    markdown=True,
    debug_mode=True,
    instructions=[
        "You are readme generator agent",
        "You'll be given repository url or repository name from user."
        "You'll use the `get_repository` tool to get the repository details."
        "You have to pass the repo_name as argument to the tool. It should be in the format of owner/repo_name. If given url extract owner/repo_name from it."
        "Also call the `get_repository_languages` tool to get the languages used in the repository."
        "Write a useful README for a open source project, including how to clone and install the project, run the project etc. Also add badges for the license, size of the repo, etc"
        "Don't include the project's languages-used in the README"
        "Write the produced README to the local filesystem",
    ],
)

readme_gen_agent.print_response(
    "Get details of https://github.com/agno-agi/agno", markdown=True
)



================================================
FILE: cookbook/examples/agents/reasoning_finance_agent.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions=[
        "Use tables to display data.",
        "Include sources in your response.",
        "Only include the report in your response. No other text.",
    ],
    markdown=True,
)
agent.print_response(
    "Write a report on NVDA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/examples/agents/recipe_creator.py
================================================
"""👨‍🍳 Recipe Creator - Your Personal AI Chef!

This example shows how to create an intelligent recipe recommendation system that provides
detailed, personalized recipes based on your ingredients, dietary preferences, and time constraints.
The agent combines culinary knowledge, nutritional data, and cooking techniques to deliver
comprehensive cooking instructions.

Example prompts to try:
- "I have chicken, rice, and vegetables. What can I make in 30 minutes?"
- "Create a vegetarian pasta recipe with mushrooms and spinach"
- "Suggest healthy breakfast options with oats and fruits"
- "What can I make with leftover turkey and potatoes?"
- "Need a quick dessert recipe using chocolate and bananas"

Run: `pip install openai exa_py agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

recipe_agent = Agent(
    name="ChefGenius",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are ChefGenius, a passionate and knowledgeable culinary expert with expertise in global cuisine! 🍳

        Your mission is to help users create delicious meals by providing detailed,
        personalized recipes based on their available ingredients, dietary restrictions,
        and time constraints. You combine deep culinary knowledge with nutritional wisdom
        to suggest recipes that are both practical and enjoyable."""),
    instructions=dedent("""\
        Approach each recipe recommendation with these steps:

        1. Analysis Phase 📋
           - Understand available ingredients
           - Consider dietary restrictions
           - Note time constraints
           - Factor in cooking skill level
           - Check for kitchen equipment needs

        2. Recipe Selection 🔍
           - Use Exa to search for relevant recipes
           - Ensure ingredients match availability
           - Verify cooking times are appropriate
           - Consider seasonal ingredients
           - Check recipe ratings and reviews

        3. Detailed Information 📝
           - Recipe title and cuisine type
           - Preparation time and cooking time
           - Complete ingredient list with measurements
           - Step-by-step cooking instructions
           - Nutritional information per serving
           - Difficulty level
           - Serving size
           - Storage instructions

        4. Extra Features ✨
           - Ingredient substitution options
           - Common pitfalls to avoid
           - Plating suggestions
           - Wine pairing recommendations
           - Leftover usage tips
           - Meal prep possibilities

        Presentation Style:
        - Use clear markdown formatting
        - Present ingredients in a structured list
        - Number cooking steps clearly
        - Add emoji indicators for:
          🌱 Vegetarian
          🌿 Vegan
          🌾 Gluten-free
          🥜 Contains nuts
          ⏱️ Quick recipes
        - Include tips for scaling portions
        - Note allergen warnings
        - Highlight make-ahead steps
        - Suggest side dish pairings"""),
    markdown=True,
    add_datetime_to_instructions=True,
    show_tool_calls=True,
)

# Example usage with different types of recipe queries
recipe_agent.print_response(
    "I have chicken breast, broccoli, garlic, and rice. Need a healthy dinner recipe that takes less than 45 minutes.",
    stream=True,
)

# More example prompts to explore:
"""
Quick Meals:
1. "15-minute dinner ideas with pasta and vegetables"
2. "Quick healthy lunch recipes for meal prep"
3. "Easy breakfast recipes with eggs and avocado"
4. "No-cook dinner ideas for hot summer days"

Dietary Restrictions:
1. "Keto-friendly dinner recipes with salmon"
2. "Gluten-free breakfast options without eggs"
3. "High-protein vegetarian meals for athletes"
4. "Low-carb alternatives to pasta dishes"

Special Occasions:
1. "Impressive dinner party main course for 6 people"
2. "Romantic dinner recipes for two"
3. "Kid-friendly birthday party snacks"
4. "Holiday desserts that can be made ahead"

International Cuisine:
1. "Authentic Thai curry with available ingredients"
2. "Simple Japanese recipes for beginners"
3. "Mediterranean diet dinner ideas"
4. "Traditional Mexican recipes with modern twists"

Seasonal Cooking:
1. "Summer salad recipes with seasonal produce"
2. "Warming winter soups and stews"
3. "Fall harvest vegetable recipes"
4. "Spring picnic recipe ideas"

Batch Cooking:
1. "Freezer-friendly meal prep recipes"
2. "One-pot meals for busy weeknights"
3. "Make-ahead breakfast ideas"
4. "Bulk cooking recipes for large families"
"""



================================================
FILE: cookbook/examples/agents/recipe_rag_image.py
================================================
"""Example: Multi-Modal RAG & Image Agent

An agent that uses Llama 4 for multi-modal RAG and OpenAITools to create a visual, step-by-step image manual for a recipe.

Run: `pip install openai agno groq cohere` to install the dependencies
"""

from pathlib import Path

from agno.agent import Agent
from agno.embedder.cohere import CohereEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.groq import Groq
from agno.tools.openai import OpenAITools
from agno.utils.media import download_image
from agno.vectordb.pgvector import PgVector

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="embed_vision_documents",
        embedder=CohereEmbedder(
            id="embed-v4.0",
        ),
    ),
)

knowledge_base.load()

agent = Agent(
    name="EmbedVisionRAGAgent",
    model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"),
    tools=[OpenAITools()],
    knowledge=knowledge_base,
    instructions=[
        "You are a specialized recipe assistant.",
        "When asked for a recipe:",
        "1. Search the knowledge base to retrieve the relevant recipe details.",
        "2. Analyze the retrieved recipe steps carefully.",
        "3. Use the `generate_image` tool to create a visual, step-by-step image manual for the recipe.",
        "4. Present the recipe text clearly and mention that you have generated an accompanying image manual. Add instructions while generating the image.",
    ],
    markdown=True,
    debug_mode=True,
)

agent.print_response(
    "What is the recipe for a Thai curry?",
)

response = agent.run_response
if response.images:
    download_image(response.images[0].url, Path("tmp/recipe_image.png"))



================================================
FILE: cookbook/examples/agents/research_agent.py
================================================
"""🔍 Research Agent - Your AI Investigative Journalist!

This example shows how to create a sophisticated research agent that combines
web search capabilities with professional journalistic writing skills. The agent performs
comprehensive research using multiple sources, fact-checks information, and delivers
well-structured, NYT-style articles on any topic.

Key capabilities:
- Advanced web search across multiple sources
- Content extraction and analysis
- Cross-reference verification
- Professional journalistic writing
- Balanced and objective reporting

Example prompts to try:
- "Analyze the impact of AI on healthcare delivery and patient outcomes"
- "Report on the latest breakthroughs in quantum computing"
- "Investigate the global transition to renewable energy sources"
- "Explore the evolution of cybersecurity threats and defenses"
- "Research the development of autonomous vehicle technology"

Dependencies: `pip install openai ddgs newspaper4k lxml_html_clean agno`
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

# Initialize the research agent with advanced journalistic capabilities
research_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(), Newspaper4kTools()],
    description=dedent("""\
        You are an elite investigative journalist with decades of experience at the New York Times.
        Your expertise encompasses: 📰

        - Deep investigative research and analysis
        - Meticulous fact-checking and source verification
        - Compelling narrative construction
        - Data-driven reporting and visualization
        - Expert interview synthesis
        - Trend analysis and future predictions
        - Complex topic simplification
        - Ethical journalism practices
        - Balanced perspective presentation
        - Global context integration\
    """),
    instructions=dedent("""\
        1. Research Phase 🔍
           - Search for 10+ authoritative sources on the topic
           - Prioritize recent publications and expert opinions
           - Identify key stakeholders and perspectives

        2. Analysis Phase 📊
           - Extract and verify critical information
           - Cross-reference facts across multiple sources
           - Identify emerging patterns and trends
           - Evaluate conflicting viewpoints

        3. Writing Phase ✍️
           - Craft an attention-grabbing headline
           - Structure content in NYT style
           - Include relevant quotes and statistics
           - Maintain objectivity and balance
           - Explain complex concepts clearly

        4. Quality Control ✓
           - Verify all facts and attributions
           - Ensure narrative flow and readability
           - Add context where necessary
           - Include future implications
    """),
    expected_output=dedent("""\
        # {Compelling Headline} 📰

        ## Executive Summary
        {Concise overview of key findings and significance}

        ## Background & Context
        {Historical context and importance}
        {Current landscape overview}

        ## Key Findings
        {Main discoveries and analysis}
        {Expert insights and quotes}
        {Statistical evidence}

        ## Impact Analysis
        {Current implications}
        {Stakeholder perspectives}
        {Industry/societal effects}

        ## Future Outlook
        {Emerging trends}
        {Expert predictions}
        {Potential challenges and opportunities}

        ## Expert Insights
        {Notable quotes and analysis from industry leaders}
        {Contrasting viewpoints}

        ## Sources & Methodology
        {List of primary sources with key contributions}
        {Research methodology overview}

        ---
        Research conducted by AI Investigative Journalist
        New York Times Style Report
        Published: {current_date}
        Last Updated: {current_time}\
    """),
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
)

# Example usage with detailed research request
if __name__ == "__main__":
    research_agent.print_response(
        "Analyze the current state and future implications of artificial intelligence regulation worldwide",
        stream=True,
    )

# Advanced research topics to explore:
"""
Technology & Innovation:
1. "Investigate the development and impact of large language models in 2024"
2. "Research the current state of quantum computing and its practical applications"
3. "Analyze the evolution and future of edge computing technologies"
4. "Explore the latest advances in brain-computer interface technology"

Environmental & Sustainability:
1. "Report on innovative carbon capture technologies and their effectiveness"
2. "Investigate the global progress in renewable energy adoption"
3. "Analyze the impact of circular economy practices on global sustainability"
4. "Research the development of sustainable aviation technologies"

Healthcare & Biotechnology:
1. "Explore the latest developments in CRISPR gene editing technology"
2. "Analyze the impact of AI on drug discovery and development"
3. "Investigate the evolution of personalized medicine approaches"
4. "Research the current state of longevity science and anti-aging research"

Societal Impact:
1. "Examine the effects of social media on democratic processes"
2. "Analyze the impact of remote work on urban development"
3. "Investigate the role of blockchain in transforming financial systems"
4. "Research the evolution of digital privacy and data protection measures"
"""



================================================
FILE: cookbook/examples/agents/research_agent_exa.py
================================================
"""🎓 Research Scholar Agent - Your AI Academic Research Assistant!

This example shows how to create a sophisticated research agent that combines
academic search capabilities with scholarly writing expertise. The agent performs
thorough research using Exa's academic search, analyzes recent publications, and delivers
well-structured, academic-style reports on any topic.

Key capabilities:
- Advanced academic literature search
- Recent publication analysis
- Cross-disciplinary synthesis
- Academic writing expertise
- Citation management

Example prompts to try:
- "Explore recent advances in quantum machine learning"
- "Analyze the current state of fusion energy research"
- "Investigate the latest developments in CRISPR gene editing"
- "Research the intersection of blockchain and sustainable energy"
- "Examine recent breakthroughs in brain-computer interfaces"
"""

from datetime import datetime
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

# Initialize the academic research agent with scholarly capabilities
research_scholar = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ExaTools(
            start_published_date=datetime.now().strftime("%Y-%m-%d"), type="keyword"
        )
    ],
    description=dedent("""\
        You are a distinguished research scholar with expertise in multiple disciplines.
        Your academic credentials include: 📚

        - Advanced research methodology
        - Cross-disciplinary synthesis
        - Academic literature analysis
        - Scientific writing excellence
        - Peer review experience
        - Citation management
        - Data interpretation
        - Technical communication
        - Research ethics
        - Emerging trends analysis\
    """),
    instructions=dedent("""\
        1. Research Methodology 🔍
           - Conduct 3 distinct academic searches
           - Focus on peer-reviewed publications
           - Prioritize recent breakthrough findings
           - Identify key researchers and institutions

        2. Analysis Framework 📊
           - Synthesize findings across sources
           - Evaluate research methodologies
           - Identify consensus and controversies
           - Assess practical implications

        3. Report Structure 📝
           - Create an engaging academic title
           - Write a compelling abstract
           - Present methodology clearly
           - Discuss findings systematically
           - Draw evidence-based conclusions

        4. Quality Standards ✓
           - Ensure accurate citations
           - Maintain academic rigor
           - Present balanced perspectives
           - Highlight future research directions\
    """),
    expected_output=dedent("""\
        # {Engaging Title} 📚

        ## Abstract
        {Concise overview of the research and key findings}

        ## Introduction
        {Context and significance}
        {Research objectives}

        ## Methodology
        {Search strategy}
        {Selection criteria}

        ## Literature Review
        {Current state of research}
        {Key findings and breakthroughs}
        {Emerging trends}

        ## Analysis
        {Critical evaluation}
        {Cross-study comparisons}
        {Research gaps}

        ## Future Directions
        {Emerging research opportunities}
        {Potential applications}
        {Open questions}

        ## Conclusions
        {Summary of key findings}
        {Implications for the field}

        ## References
        {Properly formatted academic citations}

        ---
        Research conducted by AI Academic Scholar
        Published: {current_date}
        Last Updated: {current_time}\
    """),
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
    save_response_to_file="tmp/{message}.md",
)

# Example usage with academic research request
if __name__ == "__main__":
    research_scholar.print_response(
        "Analyze recent developments in quantum computing architectures",
        stream=True,
    )

# Advanced research topics to explore:
"""
Quantum Science & Computing:
1. "Investigate recent breakthroughs in quantum error correction"
2. "Analyze the development of topological quantum computing"
3. "Research quantum machine learning algorithms and applications"
4. "Explore advances in quantum sensing technologies"

Biotechnology & Medicine:
1. "Examine recent developments in mRNA vaccine technology"
2. "Analyze breakthroughs in organoid research"
3. "Investigate advances in precision medicine"
4. "Research developments in neurotechnology"

Materials Science:
1. "Explore recent advances in metamaterials"
2. "Analyze developments in 2D materials beyond graphene"
3. "Research progress in self-healing materials"
4. "Investigate new battery technologies"

Artificial Intelligence:
1. "Examine recent advances in foundation models"
2. "Analyze developments in AI safety research"
3. "Research progress in neuromorphic computing"
4. "Investigate advances in explainable AI"
"""



================================================
FILE: cookbook/examples/agents/shopping_partner.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

agent = Agent(
    name="shopping partner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You are a product recommender agent specializing in finding products that match user preferences.",
        "Prioritize finding products that satisfy as many user requirements as possible, but ensure a minimum match of 50%.",
        "Search for products only from authentic and trusted e-commerce websites such as Amazon, Flipkart, Myntra, Meesho, Google Shopping, Nike, and other reputable platforms.",
        "Verify that each product recommendation is in stock and available for purchase.",
        "Avoid suggesting counterfeit or unverified products.",
        "Clearly mention the key attributes of each product (e.g., price, brand, features) in the response.",
        "Format the recommendations neatly and ensure clarity for ease of user understanding.",
    ],
    tools=[ExaTools()],
    show_tool_calls=True,
)
agent.print_response(
    "I am looking for running shoes with the following preferences: Color: Black Purpose: Comfortable for long-distance running Budget: Under Rs. 10,000"
)



================================================
FILE: cookbook/examples/agents/social_media_agent.py
================================================
"""Social Media Agent Example with Dummy Dataset

This example demonstrates how to create an agent that:
1. Analyzes a dummy dataset of tweets
2. Leverages LLM capabilities to perform sophisticated sentiment analysis
3. Provides insights about the overall sentiment around a topic
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.x import XTools

# Create the social media analysis agent
social_media_agent = Agent(
    name="Social Media Analyst",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        XTools(
            include_post_metrics=True,
            wait_on_rate_limit=True,
        )
    ],
    instructions="""
    You are a senior Brand Intelligence Analyst with a specialty in social-media listening  on the X (Twitter) platform.  
    Your job is to transform raw tweet content and engagement metrics into an executive-ready intelligence report that helps product, marketing, and support teams  make data-driven decisions.  

    ────────────────────────────────────────────────────────────
    CORE RESPONSIBILITIES
    ────────────────────────────────────────────────────────────
    1. Retrieve tweets with X tools that you have access to and analyze both the text and metrics such as likes, retweets, replies.
    2. Classify every tweet as Positive / Negative / Neutral / Mixed, capturing the reasoning (e.g., praise for feature X, complaint about bugs, etc.).
    3. Detect patterns in engagement metrics to surface:
       • Viral advocacy (high likes & retweets, low replies)
       • Controversy (low likes, high replies)
       • Influence concentration (verified or high-reach accounts driving sentiment)
    4. Extract thematic clusters and recurring keywords covering:
       • Feature praise / pain points  
       • UX / performance issues  
       • Customer-service interactions  
       • Pricing & ROI perceptions  
       • Competitor mentions & comparisons  
       • Emerging use-cases & adoption barriers
    5. Produce actionable, prioritized recommendations (Immediate, Short-term, Long-term) that address the issues and pain points.
    6. Supply a response strategy: which posts to engage, suggested tone & template,    influencer outreach, and community-building ideas. 

    ────────────────────────────────────────────────────────────
    DELIVERABLE FORMAT (markdown)
    ────────────────────────────────────────────────────────────
    ### 1 · Executive Snapshot
    • Brand-health score (1-10)  
    • Net sentiment ( % positive – % negative )  
    • Top 3 positive & negative drivers  
    • Red-flag issues that need urgent attention    

    ### 2 · Quantitative Dashboard
    | Sentiment | #Posts | % | Avg Likes | Avg Retweets | Avg Replies | Notes |
    |-----------|-------:|---:|----------:|-------------:|------------:|------|
    ( fill table )  

    ### 3 · Key Themes & Representative Quotes
    For each major theme list: description, sentiment trend, excerpted tweets (truncated),  and key metrics. 

    ### 4 · Competitive & Market Signals
    • Competitors referenced, sentiment vs. Agno  
    • Feature gaps users mention  
    • Market positioning insights   

    ### 5 · Risk Analysis
    • Potential crises / viral negativity  
    • Churn indicators  
    • Trust & security concerns 

    ### 6 · Opportunity Landscape
    • Features or updates that delight users  
    • Advocacy moments & influencer opportunities  
    • Untapped use-cases highlighted by the community   

    ### 7 · Strategic Recommendations
    **Immediate (≤48 h)** – urgent fixes or comms  
    **Short-term (1-2 wks)** – quick wins & tests  
    **Long-term (1-3 mo)** – roadmap & positioning  

    ### 8 · Response Playbook
    For high-impact posts list: tweet-id/url, suggested response, recommended responder (e. g., support, PM, exec), and goal (defuse, amplify, learn).   

    ────────────────────────────────────────────────────────────
    ASSESSMENT & REASONING GUIDELINES
    ────────────────────────────────────────────────────────────
    • Weigh sentiment by engagement volume & author influence (verified == ×1.5 weight).  
    • Use reply-to-like ratio > 0.5 as controversy flag.  
    • Highlight any coordinated or bot-like behaviour.  
    • Use the tools provided to you to get the data you need.

    Remember: your insights will directly inform the product strategy, customer-experience efforts, and brand reputation.  Be objective, evidence-backed, and solution-oriented.
""",
    markdown=True,
    show_tool_calls=True,
)

social_media_agent.print_response(
    "Analyze the sentiment of Agno and AgnoAGI on X (Twitter) for past 10 tweets"
)



================================================
FILE: cookbook/examples/agents/startup_analyst_agent.py
================================================
"""
Startup Intelligence Agent - Comprehensive Company Analysis

This agent acts as a startup analyst that can perform comprehensive due diligence on companies

Prerequisites:
- Set SGAI_API_KEY environment variable with your ScrapeGraph API key
- Install dependencies: pip install scrapegraph-py agno openai
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.scrapegraph import ScrapeGraphTools

startup_analyst = Agent(
    name="Startup Analyst",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ScrapeGraphTools(markdownify=True, crawl=True, searchscraper=True)],
    instructions=dedent("""
        You are an elite startup analyst providing comprehensive due diligence for investment decisions.
        
        **ANALYSIS FRAMEWORK:**
        
        1. **Foundation Analysis**: Extract company basics (name, founding, location, value proposition, team)
        2. **Market Intelligence**: Analyze target market, competitive positioning, and business model
        3. **Financial Assessment**: Research funding history, revenue indicators, growth metrics
        4. **Risk Evaluation**: Identify market, technology, team, and financial risks
        
        **DELIVERABLES:**
        
        **Executive Summary** 
        
        **Company Profile**
        - Business model and revenue streams
        - Market opportunity and customer segments  
        - Team composition and expertise
        - Technology and competitive advantages
        
        **Financial & Growth Metrics**
        - Funding history and investor quality
        - Revenue/traction indicators
        - Growth trajectory and expansion plans
        - Burn rate estimates (if available)
        
        **Risk Assessment**
        - Market and competitive threats
        - Technology and team dependencies
        - Financial and regulatory risks
        
        **Strategic Recommendations**
        - Investment thesis and partnership opportunities
        - Competitive response strategies
        - Key due diligence focus areas
        
        **TOOL USAGE:**
        - **SmartScraper**: Extract structured data from specific pages (team, products, pricing)
        - **Markdownify**: Analyze content quality and messaging from key pages
        - **Crawl**: Comprehensive site analysis across multiple pages (limit: 10 pages, depth: 3)
        - **SearchScraper**: Find external information (funding, news, executive backgrounds)
        
        **OUTPUT STANDARDS:**
        - Use clear headings and bullet points
        - Include specific metrics and evidence
        - Cite sources and confidence levels
        - Distinguish facts from analysis
        - Maintain professional, executive-level language
        - Focus on actionable insights
        
        Remember: Your analysis informs million-dollar decisions. Be thorough, accurate, and actionable.
    """),
    show_tool_calls=True,
    markdown=True,
)


startup_analyst.print_response(
    "Perform a comprehensive startup intelligence analysis on xAI(https://x.ai)"
)



================================================
FILE: cookbook/examples/agents/study_partner.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools
from agno.tools.youtube import YouTubeTools

study_partner = Agent(
    name="StudyScout",  # Fixed typo in name
    model=OpenAIChat(id="gpt-4o"),
    tools=[ExaTools(), YouTubeTools()],
    markdown=True,
    description="You are a study partner who assists users in finding resources, answering questions, and providing explanations on various topics.",
    instructions=[
        "Use Exa to search for relevant information on the given topic and verify information from multiple reliable sources.",
        "Break down complex topics into digestible chunks and provide step-by-step explanations with practical examples.",
        "Share curated learning resources including documentation, tutorials, articles, research papers, and community discussions.",
        "Recommend high-quality YouTube videos and online courses that match the user's learning style and proficiency level.",
        "Suggest hands-on projects and exercises to reinforce learning, ranging from beginner to advanced difficulty.",
        "Create personalized study plans with clear milestones, deadlines, and progress tracking.",
        "Provide tips for effective learning techniques, time management, and maintaining motivation.",
        "Recommend relevant communities, forums, and study groups for peer learning and networking.",
    ],
)
study_partner.print_response(
    "I want to learn about Postgres in depth. I know the basics, have 2 weeks to learn, and can spend 3 hours daily. Please share some resources and a study plan.",
    stream=True,
)



================================================
FILE: cookbook/examples/agents/thinking_finance_agent.py
================================================
"""🗞️ Finance Agent - Your Personal Market Analyst!

This example shows how to create a sophisticated financial analyst that provides
comprehensive market insights using real-time data. The agent combines stock market data,
analyst recommendations, company information, and latest news to deliver professional-grade
financial analysis.

Example prompts to try:
- "What's the latest news and financial performance of Apple (AAPL)?"
- "Give me a detailed analysis of Tesla's (TSLA) current market position"
- "How are Microsoft's (MSFT) financials looking? Include analyst recommendations"
- "Analyze NVIDIA's (NVDA) stock performance and future outlook"
- "What's the market saying about Amazon's (AMZN) latest quarter?"

Run: `pip install openai yfinance agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.thinking import ThinkingTools
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ThinkingTools(add_instructions=True), YFinanceTools(enable_all=True)],
    instructions=dedent("""\
        You are a seasoned Wall Street analyst with deep expertise in market analysis! 📊

        Follow these steps for comprehensive financial analysis:
        1. Market Overview
           - Latest stock price
           - 52-week high and low
        2. Financial Deep Dive
           - Key metrics (P/E, Market Cap, EPS)
        3. Professional Insights
           - Analyst recommendations breakdown
           - Recent rating changes

        4. Market Context
           - Industry trends and positioning
           - Competitive analysis
           - Market sentiment indicators

        Your reporting style:
        - Begin with an executive summary
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends (📈 📉)
        - Highlight key insights with bullet points
        - Compare metrics to industry averages
        - Include technical term explanations
        - End with a forward-looking analysis

        Risk Disclosure:
        - Always highlight potential risk factors
        - Note market uncertainties
        - Mention relevant regulatory concerns\
    """),
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
    stream_intermediate_steps=True,
)

# Example usage with detailed market analysis request
finance_agent.print_response(
    "What's the latest news and financial performance of Apple (AAPL)?", stream=True
)

# Semiconductor market analysis example
finance_agent.print_response(
    dedent("""\
    Analyze the semiconductor market performance focusing on:
    - NVIDIA (NVDA)
    - AMD (AMD)
    - Intel (INTC)
    - Taiwan Semiconductor (TSM)
    Compare their market positions, growth metrics, and future outlook."""),
    stream=True,
)

# Automotive market analysis example
finance_agent.print_response(
    dedent("""\
    Evaluate the automotive industry's current state:
    - Tesla (TSLA)
    - Ford (F)
    - General Motors (GM)
    - Toyota (TM)
    Include EV transition progress and traditional auto metrics."""),
    stream=True,
)

# More example prompts to explore:
"""
Advanced analysis queries:
1. "Compare Tesla's valuation metrics with traditional automakers"
2. "Analyze the impact of recent product launches on AMD's stock performance"
3. "How do Meta's financial metrics compare to its social media peers?"
4. "Evaluate Netflix's subscriber growth impact on financial metrics"
5. "Break down Amazon's revenue streams and segment performance"

Industry-specific analyses:
Semiconductor Market:
1. "How is the chip shortage affecting TSMC's market position?"
2. "Compare NVIDIA's AI chip revenue growth with competitors"
3. "Analyze Intel's foundry strategy impact on stock performance"
4. "Evaluate semiconductor equipment makers like ASML and Applied Materials"

Automotive Industry:
1. "Compare EV manufacturers' production metrics and margins"
2. "Analyze traditional automakers' EV transition progress"
3. "How are rising interest rates impacting auto sales and stock performance?"
4. "Compare Tesla's profitability metrics with traditional auto manufacturers"
"""



================================================
FILE: cookbook/examples/agents/translation_agent.py
================================================
# aa
import asyncio
import json
import os
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.cartesia import CartesiaTools
from agno.utils.media import save_audio

agent_instructions = dedent(
    f"""Follow these steps SEQUENTIALLY to translate text and generate a localized voice note:
    1. Identify the text to translate and the target language from the user request.
    2. Translate the text accurately to the target language. Keep this translated text for the final audio generation step.
    3. Analyze the emotion conveyed by the *translated* text (e.g., neutral, happy, sad, angry, etc.).
    4. Determine the standard 2-letter language code for the target language (e.g., 'fr' for French, 'es' for Spanish).
    5. Call the 'list_voices' tool to get a list of available Cartesia voices. Wait for the result.
    6. Examine the list of voices from the 'list_voices' result. Select the 'id' of an *existing* voice that:
       a) Matches the target language code (from step 4).
       b) Best reflects the analyzed emotion (from step 3).
    7. Call the 'localize_voice' tool to create a new voice. Provide the following arguments:
       - 'voice_id': The 'base_voice_id' selected in step 6.
       - 'name': A suitable name for the new voice (e.g., "French Happy Female").
       - 'description': A description reflecting the language and emotion.
       - 'language': The target language code (from step 4).
       - 'original_speaker_gender': User specified gender or the selected base voice gender.
       Wait for the result of this tool call.
    8. Check the result of the 'localize_voice' tool call from step 8:
       a) If the call was successful and returned the details of the newly created voice, extract the 'id' of this **new** voice. This is the 'final_voice_id'.
    9. Call the 'text_to_speech' tool to generate the audio. Provide:
        - 'transcript': The translated text from step 2.
        - 'voice_id': The 'final_voice_id' determined in step 9.
    """
)

agent = Agent(
    name="Emotion-Aware Translator Agent",
    description="Translates text, analyzes emotion, selects a suitable voice,creates a localized voice, and generates a voice note (audio file) using Cartesia TTStools.",
    instructions=agent_instructions,
    model=OpenAIChat(id="gpt-4o"),
    tools=[CartesiaTools(voice_localize_enabled=True)],
    show_tool_calls=True,
)

agent.print_response(
    "Convert this phrase 'hello! how are you? Tell me more about the weather in Paris?' to French and create a voice note"
)
response = agent.run_response

print("\nChecking for Audio Artifacts on Agent...")
if response.audio:
    save_audio(
        base64_data=response.audio[0].base64_audio, output_path="tmp/greeting.mp3"
    )



================================================
FILE: cookbook/examples/agents/web_extraction_agent.py
================================================
from textwrap import dedent
from typing import Dict, List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.firecrawl import FirecrawlTools
from pydantic import BaseModel, Field
from rich.pretty import pprint


class ContentSection(BaseModel):
    """Represents a section of content from the webpage."""

    heading: Optional[str] = Field(None, description="Section heading")
    content: str = Field(..., description="Section content text")


class PageInformation(BaseModel):
    """Structured representation of a webpage."""

    url: str = Field(..., description="URL of the page")
    title: str = Field(..., description="Title of the page")
    description: Optional[str] = Field(
        None, description="Meta description or summary of the page"
    )
    features: Optional[List[str]] = Field(None, description="Key feature list")
    content_sections: Optional[List[ContentSection]] = Field(
        None, description="Main content sections of the page"
    )
    links: Optional[Dict[str, str]] = Field(
        None, description="Important links found on the page with description"
    )
    contact_info: Optional[Dict[str, str]] = Field(
        None, description="Contact information if available"
    )
    metadata: Optional[Dict[str, str]] = Field(
        None, description="Important metadata from the page"
    )


agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    tools=[FirecrawlTools(scrape=True, crawl=True)],
    instructions=dedent("""
        You are an expert web researcher and content extractor. Extract comprehensive, structured information
        from the provided webpage. Focus on:

        1. Accurately capturing the page title, description, and key features
        2. Identifying and extracting main content sections with their headings
        3. Finding important links to related pages or resources
        4. Locating contact information if available
        5. Extracting relevant metadata that provides context about the site

        Be thorough but concise. If the page has extensive content, prioritize the most important information.
    """).strip(),
    response_model=PageInformation,
)

result = agent.run("Extract all information from https://www.agno.com")
pprint(result.content)



================================================
FILE: cookbook/examples/agents/youtube_agent.py
================================================
"""🎥 YouTube Agent - Your Video Content Expert!

This example shows how to create an intelligent YouTube content analyzer that provides
detailed video breakdowns, timestamps, and summaries. Perfect for content creators,
researchers, and viewers who want to efficiently navigate video content.

Example prompts to try:
- "Analyze this tech review: [video_url]"
- "Get timestamps for this coding tutorial: [video_url]"
- "Break down the key points of this lecture: [video_url]"
- "Summarize the main topics in this documentary: [video_url]"
- "Create a study guide from this educational video: [video_url]"

Run: `pip install openai youtube_transcript_api agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.youtube import YouTubeTools

youtube_agent = Agent(
    name="YouTube Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YouTubeTools()],
    show_tool_calls=True,
    instructions=dedent("""\
        You are an expert YouTube content analyst with a keen eye for detail! 🎓
        Follow these steps for comprehensive video analysis:
        1. Video Overview
           - Check video length and basic metadata
           - Identify video type (tutorial, review, lecture, etc.)
           - Note the content structure
        2. Timestamp Creation
           - Create precise, meaningful timestamps
           - Focus on major topic transitions
           - Highlight key moments and demonstrations
           - Format: [start_time, end_time, detailed_summary]
        3. Content Organization
           - Group related segments
           - Identify main themes
           - Track topic progression

        Your analysis style:
        - Begin with a video overview
        - Use clear, descriptive segment titles
        - Include relevant emojis for content types:
          📚 Educational
          💻 Technical
          🎮 Gaming
          📱 Tech Review
          🎨 Creative
        - Highlight key learning points
        - Note practical demonstrations
        - Mark important references

        Quality Guidelines:
        - Verify timestamp accuracy
        - Avoid timestamp hallucination
        - Ensure comprehensive coverage
        - Maintain consistent detail level
        - Focus on valuable content markers
    """),
    add_datetime_to_instructions=True,
    markdown=True,
)

# Example usage with different types of videos
youtube_agent.print_response(
    "Analyze this video: https://www.youtube.com/watch?v=zjkBMFhNj_g",
    stream=True,
)

# More example prompts to explore:
"""
Tutorial Analysis:
1. "Break down this Python tutorial with focus on code examples"
2. "Create a learning path from this web development course"
3. "Extract all practical exercises from this programming guide"
4. "Identify key concepts and implementation examples"

Educational Content:
1. "Create a study guide with timestamps for this math lecture"
2. "Extract main theories and examples from this science video"
3. "Break down this historical documentary into key events"
4. "Summarize the main arguments in this academic presentation"

Tech Reviews:
1. "List all product features mentioned with timestamps"
2. "Compare pros and cons discussed in this review"
3. "Extract technical specifications and benchmarks"
4. "Identify key comparison points and conclusions"

Creative Content:
1. "Break down the techniques shown in this art tutorial"
2. "Create a timeline of project steps in this DIY video"
3. "List all tools and materials mentioned with timestamps"
4. "Extract tips and tricks with their demonstrations"
"""



================================================
FILE: cookbook/examples/multi_agent_reasoning/README.md
================================================
# Multi Agent Reasoning

This cookbook demo's multi-agent reasoning using a reasoning finance team. The team is made up of two agents:

- Web Search Agent: Searches the web for information
- Finance Agent: Gets financial data using the `yfinance` library

We'll using `sonnet-4` as the team leader and `gpt-4.1` as the team members.

> Note: Fork and clone the repository if needed

### 1. Create a virtual environment

```shell
uv venv --python 3.12
source .venv/bin/activate
```

### 2. Install libraries

```shell
uv pip install -r cookbook/examples/multi_agent_reasoning/requirements.txt
```

### 3. Run PgVector

For this example, we'll use Postgres for storing data and `PgVector` for vector search. You can use any other vectordb or storage system you like.

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

- Run using a helper script

```shell
./cookbook/scripts/run_pgvector.sh
```

- OR run using the docker run command

```shell
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16
```

### 4. Export API Keys

We recommend using claude-4-sonnet as the team leader and gpt-4.1 as the team members.

```shell
export ANTHROPIC_API_KEY=***
export OPENAI_API_KEY=***
```

### 5. Authenticate with Agno

```shell
ag setup
```

### 6. Run Playground App

```shell
python cookbook/examples/multi_agent_reasoning/playground.py
```

- Open [app.agno.com/playground](https://app.agno.com/playground?endpoint=localhost%3A7777) to chat with your new multi-agent reasoning team.

### 7. Message us on [discord](https://agno.link/discord) if you have any questions




================================================
FILE: cookbook/examples/multi_agent_reasoning/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/multi_agent_reasoning/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/multi_agent_reasoning/playground.py
================================================
from agno.playground import Playground, serve_playground_app
from reasoning_finance_team import finance_agent, reasoning_finance_team, web_agent

app = Playground(
    app_id="multi-agent-reasoning-app",
    name="Multi Agent Reasoning App",
    agents=[web_agent, finance_agent],
    teams=[reasoning_finance_team],
).get_app()

if __name__ == "__main__":
    serve_playground_app("playground:app", port=7777)



================================================
FILE: cookbook/examples/multi_agent_reasoning/reasoning_finance_team.py
================================================
from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

# ************* Database Connection *************
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# *******************************

# ************* Memory *************
memory = Memory(
    model=OpenAIChat(id="gpt-4.1"),
    db=PostgresMemoryDb(table_name="user_memories", db_url=db_url),
    delete_memories=True,
    clear_memories=True,
)
# *******************************

# ************* Members *************
web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests and general research",
    agent_id="web_agent",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[DuckDuckGoTools()],
    storage=PostgresAgentStorage(
        db_url=db_url,
        table_name="web_agent_sessions",
    ),
    memory=memory,
    instructions=[
        "Search for current and relevant information on financial topics",
        "Always include sources and publication dates",
        "Focus on reputable financial news sources",
        "Provide context and background information",
    ],
    markdown=True,
    enable_agentic_memory=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests and market analysis",
    agent_id="finance_agent",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[
        YFinanceTools(
            stock_price=True,
            company_info=True,
            stock_fundamentals=True,
            key_financial_ratios=True,
            analyst_recommendations=True,
        )
    ],
    storage=PostgresAgentStorage(
        db_url=db_url,
        table_name="finance_agent_sessions",
    ),
    memory=memory,
    instructions=[
        "You are a financial data specialist and your goal is to generate comprehensive and accurate financial reports.",
        "Use tables to display stock prices, fundamentals (P/E, Market Cap, Revenue), and recommendations.",
        "Clearly state the company name and ticker symbol.",
        "Include key financial ratios and metrics in your analysis.",
        "Focus on delivering actionable financial insights.",
        "Delegate tasks and run tools in parallel if needed.",
    ],
    markdown=True,
    enable_agentic_memory=True,
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
)
# *******************************

# ************* Team *************
reasoning_finance_team = Team(
    name="Reasoning Finance Team",
    mode="coordinate",
    team_id="reasoning_finance_team",
    model=Claude(id="claude-sonnet-4-20250514"),
    members=[web_agent, finance_agent],
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Collaborate to provide comprehensive financial and investment insights",
        "Consider both fundamental analysis and market sentiment",
        "Provide actionable investment recommendations with clear rationale",
        "Use tables and charts to display data clearly and professionally",
        "Ensure all claims are supported by data and sources",
        "Present findings in a structured, easy-to-follow format",
        "Only output the final consolidated analysis, not individual agent responses",
        "Dont use emojis",
    ],
    storage=PostgresAgentStorage(
        db_url=db_url,
        table_name="reasoning_finance_team_sessions",
    ),
    memory=memory,
    markdown=True,
    enable_agentic_memory=True,
    enable_agentic_context=True,
    add_datetime_to_instructions=True,
    enable_team_history=True,
    success_criteria="The team has provided a complete financial analysis with data, visualizations, risk assessment, and actionable investment recommendations supported by quantitative analysis and market research.",
)
# *******************************

# ************* Demo Scenarios *************
"""
DEMO SCENARIOS - Use these as example queries to showcase the multi-agent system:

1. COMPREHENSIVE INVESTMENT RESEARCH:
Analyze Apple (AAPL) as a potential investment:
1. Get current stock price and fundamentals
2. Research recent news and market sentiment
3. Calculate key financial ratios and risk metrics
4. Provide a comprehensive investment recommendation

2. SECTOR COMPARISON ANALYSIS:
Compare the tech sector giants (AAPL, GOOGL, MSFT) performance:
1. Get financial data for all three companies
2. Analyze recent news affecting the tech sector
3. Calculate comparative metrics and correlations
4. Recommend portfolio allocation weights

3. RISK ASSESSMENT SCENARIO:
Evaluate the risk profile of Tesla (TSLA):
1. Calculate volatility metrics and beta
2. Analyze recent news for risk factors
3. Compare risk vs return to market benchmarks
4. Provide risk-adjusted investment recommendation

4. MARKET SENTIMENT ANALYSIS:
Analyze current market sentiment around AI stocks:
1. Search for recent AI industry news and developments
2. Get financial data for key AI companies (NVDA, GOOGL, MSFT, AMD)
3. Provide outlook for AI sector investing

5. EARNINGS SEASON ANALYSIS:
Prepare for upcoming earnings season - analyze Microsoft (MSFT):
1. Get current financial metrics and analyst expectations
2. Research recent news and market sentiment
3. Calculate historical earnings impact on stock price
4. Provide trading strategy recommendation
"""
# *******************************



================================================
FILE: cookbook/examples/multi_agent_reasoning/requirements.in
================================================
agno
anthropic
ddgs
fastapi[standard]
openai
pgvector
psycopg[binary]
sqlalchemy
yfinance



================================================
FILE: cookbook/examples/multi_agent_reasoning/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.5.6
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in
annotated-types==0.7.0
    # via pydantic
anthropic==0.52.1
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in
anyio==4.9.0
    # via
    #   anthropic
    #   httpx
    #   openai
    #   starlette
    #   watchfiles
beautifulsoup4==4.13.4
    # via yfinance
certifi==2025.4.26
    # via
    #   curl-cffi
    #   httpcore
    #   httpx
    #   requests
cffi==1.17.1
    # via curl-cffi
charset-normalizer==3.4.2
    # via requests
click==8.2.1
    # via
    #   ddgs
    #   rich-toolkit
    #   typer
    #   uvicorn
curl-cffi==0.11.1
    # via yfinance
distro==1.9.0
    # via
    #   anthropic
    #   openai
dnspython==2.7.0
    # via email-validator
docstring-parser==0.16
    # via agno
ddgs==9.5.4
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in
email-validator==2.2.0
    # via fastapi
fastapi==0.115.12
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in
fastapi-cli==0.0.7
    # via fastapi
frozendict==2.4.6
    # via yfinance
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via agno
h11==0.16.0
    # via
    #   httpcore
    #   uvicorn
httpcore==1.0.9
    # via httpx
httptools==0.6.4
    # via uvicorn
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   fastapi
    #   openai
idna==3.10
    # via
    #   anyio
    #   email-validator
    #   httpx
    #   requests
jinja2==3.1.6
    # via fastapi
jiter==0.10.0
    # via
    #   anthropic
    #   openai
lxml==5.4.0
    # via ddgs
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
multitasking==0.0.11
    # via yfinance
numpy==2.2.6
    # via
    #   pandas
    #   pgvector
    #   yfinance
openai==1.82.1
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in
pandas==2.2.3
    # via yfinance
peewee==3.18.1
    # via yfinance
pgvector==0.4.1
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in
platformdirs==4.3.8
    # via yfinance
primp==0.15.0
    # via ddgs
protobuf==6.31.1
    # via yfinance
psycopg==3.2.9
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in
psycopg-binary==3.2.9
    # via psycopg
pycparser==2.22
    # via cffi
pydantic==2.11.5
    # via
    #   agno
    #   anthropic
    #   fastapi
    #   openai
    #   pydantic-settings
pydantic-core==2.33.2
    # via pydantic
pydantic-settings==2.9.1
    # via agno
pygments==2.19.1
    # via rich
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.1.0
    # via
    #   agno
    #   pydantic-settings
    #   uvicorn
python-multipart==0.0.20
    # via
    #   agno
    #   fastapi
pytz==2025.2
    # via
    #   pandas
    #   yfinance
pyyaml==6.0.2
    # via
    #   agno
    #   uvicorn
requests==2.32.3
    # via yfinance
rich==14.0.0
    # via
    #   agno
    #   rich-toolkit
    #   typer
rich-toolkit==0.14.7
    # via fastapi-cli
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   openai
soupsieve==2.7
    # via beautifulsoup4
sqlalchemy==2.0.41
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in
starlette==0.46.2
    # via fastapi
tomli==2.2.1
    # via agno
tqdm==4.67.1
    # via openai
typer==0.16.0
    # via
    #   agno
    #   fastapi-cli
typing-extensions==4.13.2
    # via
    #   agno
    #   anthropic
    #   anyio
    #   beautifulsoup4
    #   fastapi
    #   openai
    #   psycopg
    #   pydantic
    #   pydantic-core
    #   rich-toolkit
    #   sqlalchemy
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.4.0
    # via requests
uvicorn==0.34.2
    # via
    #   fastapi
    #   fastapi-cli
uvloop==0.21.0
    # via uvicorn
watchfiles==1.0.5
    # via uvicorn
websockets==15.0.1
    # via
    #   uvicorn
    #   yfinance
yfinance==0.2.61
    # via -r cookbook/examples/multi_agent_reasoning/requirements.in



================================================
FILE: cookbook/examples/streamlit_apps/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/agentic_rag/README.md
================================================
# Agentic RAG Agent

**Agentic RAG Agent** is a chat application that combines models with retrieval-augmented generation.
It allows users to ask questions based on custom knowledge bases, documents, and web data, retrieve context-aware answers, and maintain chat history across sessions.

> Note: Fork and clone this repository if needed

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install dependencies

```shell
pip install -r cookbook/examples/streamlit_apps/agentic_rag/requirements.txt
```

### 3. Configure API Keys

Required:
```bash
export OPENAI_API_KEY=your_openai_key_here
```

Optional (for additional models):
```bash
export ANTHROPIC_API_KEY=your_anthropic_key_here
export GOOGLE_API_KEY=your_google_key_here
export GROQ_API_KEY=your_groq_key_here
```

### 4. Run PgVector

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

- Run using a helper script

```shell
./cookbook/scripts/run_pgvector.sh
```

- OR run using the docker run command

```shell
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16
```

### 5. Run Agentic RAG App

```shell
streamlit run cookbook/examples/streamlit_apps/agentic_rag/app.py 
```

## 🔧 Customization

### Model Selection

The application supports multiple model providers:
- OpenAI (o3-mini, gpt-4o)
- Anthropic (claude-3-5-sonnet)
- Google (gemini-2.0-flash-exp)
- Groq (llama-3.3-70b-versatile)

### How to Use
- Open [localhost:8501](http://localhost:8501) in your browser.
- Upload documents or provide URLs (websites, csv, txt, and PDFs) to build a knowledge base.
- Enter questions in the chat interface and get context-aware answers.
- The app can also answer question using duckduckgo search without any external documents added.

### Troubleshooting
- **Docker Connection Refused**: Ensure `pgvector`  containers are running (`docker ps`).
- **OpenAI API Errors**: Verify that the `OPENAI_API_KEY` is set and valid.

## 📚 Documentation

For more detailed information:
- [Agno Documentation](https://docs.agno.com)
- [Streamlit Documentation](https://docs.streamlit.io)

## 🤝 Support

Need help? Join our [Discord community](https://agno.link/discord)






================================================
FILE: cookbook/examples/streamlit_apps/agentic_rag/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/agentic_rag/agentic_rag.py
================================================
"""🤖 Agentic RAG Agent - Your AI Knowledge Assistant!

This advanced example shows how to build a sophisticated RAG (Retrieval Augmented Generation) system that
leverages vector search and LLMs to provide deep insights from any knowledge base.

The agent can:
- Process and understand documents from multiple sources (PDFs, websites, text files)
- Build a searchable knowledge base using vector embeddings
- Maintain conversation context and memory across sessions
- Provide relevant citations and sources for its responses
- Generate summaries and extract key insights
- Answer follow-up questions and clarifications

Example queries to try:
- "What are the key points from this document?"
- "Can you summarize the main arguments and supporting evidence?"
- "What are the important statistics and findings?"
- "How does this relate to [topic X]?"
- "What are the limitations or gaps in this analysis?"
- "Can you explain [concept X] in more detail?"
- "What other sources support or contradict these claims?"

The agent uses:
- Vector similarity search for relevant document retrieval
- Conversation memory for contextual responses
- Citation tracking for source attribution
- Dynamic knowledge base updates

View the README for instructions on how to run the application.
"""

from typing import Optional

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge import AgentKnowledge
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


def get_agentic_rag_agent(
    model_id: str = "openai:gpt-4o",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    debug_mode: bool = True,
) -> Agent:
    """Get an Agentic RAG Agent with Memory."""
    # Parse model provider and name
    provider, model_name = model_id.split(":")

    # Select appropriate model class based on provider
    if provider == "openai":
        model = OpenAIChat(id=model_name)
    elif provider == "google":
        model = Gemini(id=model_name)
    elif provider == "anthropic":
        model = Claude(id=model_name)
    elif provider == "groq":
        model = Groq(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")
    # Define persistent memory for chat history

    # Define the knowledge base
    knowledge_base = AgentKnowledge(
        vector_db=PgVector(
            db_url=db_url,
            table_name="agentic_rag_documents",
            schema="ai",
            # Use OpenAI embeddings
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
        num_documents=3,  # Retrieve 3 most relevant documents
    )

    # Create the Agent
    return Agent(
        name="agentic_rag_agent",
        session_id=session_id,  # Track session ID for persistent conversations
        user_id=user_id,
        model=model,
        storage=PostgresAgentStorage(
            table_name="agentic_rag_agent_sessions", db_url=db_url
        ),  # Persist session data
        knowledge=knowledge_base,  # Add knowledge base
        description="You are a helpful Agent called 'Agentic RAG' and your goal is to assist the user in the best way possible.",
        instructions=[
            "1. Knowledge Base Search:",
            "   - ALWAYS start by searching the knowledge base using search_knowledge_base tool",
            "   - Analyze ALL returned documents thoroughly before responding",
            "   - If multiple documents are returned, synthesize the information coherently",
            "2. External Search:",
            "   - If knowledge base search yields insufficient results, use duckduckgo_search",
            "   - Focus on reputable sources and recent information",
            "   - Cross-reference information from multiple sources when possible",
            "3. Context Management:",
            "   - Use get_chat_history tool to maintain conversation continuity",
            "   - Reference previous interactions when relevant",
            "   - Keep track of user preferences and prior clarifications",
            "4. Response Quality:",
            "   - Provide specific citations and sources for claims",
            "   - Structure responses with clear sections and bullet points when appropriate",
            "   - Include relevant quotes from source materials",
            "   - Avoid hedging phrases like 'based on my knowledge' or 'depending on the information'",
            "5. User Interaction:",
            "   - Ask for clarification if the query is ambiguous",
            "   - Break down complex questions into manageable parts",
            "   - Proactively suggest related topics or follow-up questions",
            "6. Error Handling:",
            "   - If no relevant information is found, clearly state this",
            "   - Suggest alternative approaches or questions",
            "   - Be transparent about limitations in available information",
        ],
        search_knowledge=True,  # This setting gives the model a tool to search the knowledge base for information
        read_chat_history=True,  # This setting gives the model a tool to get chat history
        tools=[DuckDuckGoTools()],
        markdown=True,  # This setting tellss the model to format messages in markdown
        # add_chat_history_to_messages=True,
        show_tool_calls=True,
        add_history_to_messages=True,  # Adds chat history to messages
        add_datetime_to_instructions=True,
        debug_mode=debug_mode,
        read_tool_call_history=True,
        num_history_responses=3,
    )



================================================
FILE: cookbook/examples/streamlit_apps/agentic_rag/app.py
================================================
import os
import tempfile
from typing import List

import nest_asyncio
import requests
import streamlit as st
from agentic_rag import get_agentic_rag_agent
from agno.agent import Agent
from agno.document import Document
from agno.document.reader.csv_reader import CSVReader
from agno.document.reader.pdf_reader import PDFReader
from agno.document.reader.text_reader import TextReader
from agno.document.reader.website_reader import WebsiteReader
from agno.utils.log import logger
from utils import (
    CUSTOM_CSS,
    about_widget,
    add_message,
    display_tool_calls,
    export_chat_history,
    rename_session_widget,
    session_selector_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="Agentic RAG",
    page_icon="💎",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS

st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def restart_agent():
    """Reset the agent and clear chat history"""
    logger.debug("---*--- Restarting agent ---*---")
    st.session_state["agentic_rag_agent"] = None
    st.session_state["agentic_rag_agent_session_id"] = None
    st.session_state["messages"] = []
    st.rerun()


def get_reader(file_type: str):
    """Return appropriate reader based on file type."""
    readers = {
        "pdf": PDFReader(),
        "csv": CSVReader(),
        "txt": TextReader(),
    }
    return readers.get(file_type.lower(), None)


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>Agentic RAG </h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Your intelligent research assistant powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    model_options = {
        "o3-mini": "openai:o3-mini",
        "gpt-4o": "openai:gpt-4o",
        "gemini-2.0-flash-exp": "google:gemini-2.0-flash-exp",
        "claude-3-5-sonnet": "anthropic:claude-3-5-sonnet-20241022",
        "llama-3.3-70b": "groq:llama-3.3-70b-versatile",
    }
    selected_model = st.sidebar.selectbox(
        "Select a model",
        options=list(model_options.keys()),
        index=0,
        key="model_selector",
    )
    model_id = model_options[selected_model]

    ####################################################################
    # Initialize Agent
    ####################################################################
    agentic_rag_agent: Agent
    if (
        "agentic_rag_agent" not in st.session_state
        or st.session_state["agentic_rag_agent"] is None
        or st.session_state.get("current_model") != model_id
    ):
        logger.info("---*--- Creating new Agentic RAG  ---*---")
        agentic_rag_agent = get_agentic_rag_agent(model_id=model_id)
        st.session_state["agentic_rag_agent"] = agentic_rag_agent
        st.session_state["current_model"] = model_id
    else:
        agentic_rag_agent = st.session_state["agentic_rag_agent"]

    ####################################################################
    # Load Agent Session from the database
    ####################################################################
    # Check if session ID is already in session state
    session_id_exists = (
        "agentic_rag_agent_session_id" in st.session_state
        and st.session_state["agentic_rag_agent_session_id"]
    )

    if not session_id_exists:
        try:
            st.session_state["agentic_rag_agent_session_id"] = (
                agentic_rag_agent.load_session()
            )
        except Exception as e:
            logger.error(f"Session load error: {str(e)}")
            st.warning("Could not create Agent session, is the database running?")
            # Continue anyway instead of returning, to avoid breaking session switching
    elif (
        st.session_state["agentic_rag_agent_session_id"]
        and hasattr(agentic_rag_agent, "memory")
        and agentic_rag_agent.memory is not None
        and not agentic_rag_agent.memory.runs
    ):
        # If we have a session ID but no runs, try to load the session explicitly
        try:
            agentic_rag_agent.load_session(
                st.session_state["agentic_rag_agent_session_id"]
            )
        except Exception as e:
            logger.error(f"Failed to load existing session: {str(e)}")
            # Continue anyway

    ####################################################################
    # Load runs from memory
    ####################################################################
    agent_runs = []
    if hasattr(agentic_rag_agent, "memory") and agentic_rag_agent.memory is not None:
        agent_runs = agentic_rag_agent.memory.runs

    # Initialize messages if it doesn't exist yet
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # Only populate messages from agent runs if we haven't already
    if len(st.session_state["messages"]) == 0 and len(agent_runs) > 0:
        logger.debug("Loading run history")
        for _run in agent_runs:
            # Check if _run is an object with message attribute
            if hasattr(_run, "message") and _run.message is not None:
                add_message(_run.message.role, _run.message.content)
            # Check if _run is an object with response attribute
            if hasattr(_run, "response") and _run.response is not None:
                add_message("assistant", _run.response.content, _run.response.tools)
    elif len(agent_runs) == 0 and len(st.session_state["messages"]) == 0:
        logger.debug("No run history found")

    if prompt := st.chat_input("👋 Ask me anything!"):
        add_message("user", prompt)

    ####################################################################
    # Track loaded URLs and files in session state
    ####################################################################
    if "loaded_urls" not in st.session_state:
        st.session_state.loaded_urls = set()
    if "loaded_files" not in st.session_state:
        st.session_state.loaded_files = set()
    if "knowledge_base_initialized" not in st.session_state:
        st.session_state.knowledge_base_initialized = False

    st.sidebar.markdown("#### 📚 Document Management")
    input_url = st.sidebar.text_input("Add URL to Knowledge Base")
    if (
        input_url and not prompt and not st.session_state.knowledge_base_initialized
    ):  # Only load if KB not initialized
        if input_url not in st.session_state.loaded_urls:
            alert = st.sidebar.info("Processing URLs...", icon="ℹ️")
            if input_url.lower().endswith(".pdf"):
                try:
                    # Download PDF to temporary file
                    response = requests.get(input_url, stream=True, verify=False)
                    response.raise_for_status()

                    with tempfile.NamedTemporaryFile(
                        suffix=".pdf", delete=False
                    ) as tmp_file:
                        for chunk in response.iter_content(chunk_size=8192):
                            tmp_file.write(chunk)
                        tmp_path = tmp_file.name

                    reader = PDFReader()
                    docs: List[Document] = reader.read(tmp_path)

                    # Clean up temporary file
                    os.unlink(tmp_path)
                except Exception as e:
                    st.sidebar.error(f"Error processing PDF: {str(e)}")
                    docs = []
            else:
                scraper = WebsiteReader(max_links=2, max_depth=1)
                docs: List[Document] = scraper.read(input_url)

            if docs:
                agentic_rag_agent.knowledge.load_documents(docs, upsert=True)
                st.session_state.loaded_urls.add(input_url)
                st.sidebar.success("URL added to knowledge base")
            else:
                st.sidebar.error("Could not process the provided URL")
            alert.empty()
        else:
            st.sidebar.info("URL already loaded in knowledge base")

    uploaded_file = st.sidebar.file_uploader(
        "Add a Document (.pdf, .csv, or .txt)", key="file_upload"
    )
    if (
        uploaded_file and not prompt and not st.session_state.knowledge_base_initialized
    ):  # Only load if KB not initialized
        file_identifier = f"{uploaded_file.name}_{uploaded_file.size}"
        if file_identifier not in st.session_state.loaded_files:
            alert = st.sidebar.info("Processing document...", icon="ℹ️")
            file_type = uploaded_file.name.split(".")[-1].lower()
            reader = get_reader(file_type)
            if reader:
                docs = reader.read(uploaded_file)
                agentic_rag_agent.knowledge.load_documents(docs, upsert=True)
                st.session_state.loaded_files.add(file_identifier)
                st.sidebar.success(f"{uploaded_file.name} added to knowledge base")
                st.session_state.knowledge_base_initialized = True
            alert.empty()
        else:
            st.sidebar.info(f"{uploaded_file.name} already loaded in knowledge base")

    if st.sidebar.button("Clear Knowledge Base"):
        agentic_rag_agent.knowledge.vector_db.delete()
        st.session_state.loaded_urls.clear()
        st.session_state.loaded_files.clear()
        st.session_state.knowledge_base_initialized = False  # Reset initialization flag
        st.sidebar.success("Knowledge base cleared")
    ###############################################################
    # Sample Question
    ###############################################################
    st.sidebar.markdown("#### ❓ Sample Questions")
    if st.sidebar.button("📝 Summarize"):
        add_message(
            "user",
            "Can you summarize what is currently in the knowledge base (use `search_knowledge_base` tool)?",
        )

    ###############################################################
    # Utility buttons
    ###############################################################
    st.sidebar.markdown("#### 🛠️ Utilities")
    col1, col2 = st.sidebar.columns([1, 1])  # Equal width columns
    with col1:
        if st.sidebar.button(
            "🔄 New Chat", use_container_width=True
        ):  # Added use_container_width
            restart_agent()
    with col2:
        if st.sidebar.download_button(
            "💾 Export Chat",
            export_chat_history(),
            file_name="rag_chat_history.md",
            mime="text/markdown",
            use_container_width=True,  # Added use_container_width
        ):
            st.sidebar.success("Chat history exported!")

    ####################################################################
    # Display chat history
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] in ["user", "assistant"]:
            _content = message["content"]
            if _content is not None:
                with st.chat_message(message["role"]):
                    # Display tool calls if they exist in the message
                    if "tool_calls" in message and message["tool_calls"]:
                        display_tool_calls(st.empty(), message["tool_calls"])
                    st.markdown(_content)

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        with st.chat_message("assistant"):
            # Create container for tool calls
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner("🤔 Thinking..."):
                response = ""
                try:
                    # Run the agent and stream the response
                    run_response = agentic_rag_agent.run(question, stream=True)
                    for _resp_chunk in run_response:
                        # Display tool calls if available
                        if hasattr(_resp_chunk, "tool") and _resp_chunk.tool:
                            display_tool_calls(tool_calls_container, [_resp_chunk.tool])

                        # Display response
                        if _resp_chunk.content is not None:
                            response += _resp_chunk.content
                            resp_container.markdown(response)

                    add_message(
                        "assistant", response, agentic_rag_agent.run_response.tools
                    )
                except Exception as e:
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    add_message("assistant", error_message)
                    st.error(error_message)

    ####################################################################
    # Session selector
    ####################################################################
    session_selector_widget(agentic_rag_agent, model_id)
    rename_session_widget(agentic_rag_agent)

    ####################################################################
    # About section
    ####################################################################
    about_widget()


main()



================================================
FILE: cookbook/examples/streamlit_apps/agentic_rag/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/agentic_rag/requirements.in
================================================
agno
anthropic
ddgs
google-genai
groq
nest_asyncio
openai
sqlalchemy
streamlit



================================================
FILE: cookbook/examples/streamlit_apps/agentic_rag/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.7.12
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anthropic==0.64.0
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
anyio==4.10.0
    # via
    #   anthropic
    #   google-genai
    #   groq
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.8.3
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.3
    # via requests
click==8.2.1
    # via
    #   ddgs
    #   streamlit
    #   typer
ddgs==9.5.4
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
docstring-parser==0.17.0
    # via agno
gitdb==4.0.12
    # via gitpython
gitpython==3.1.45
    # via
    #   agno
    #   streamlit
google-auth==2.40.3
    # via google-genai
google-genai==1.31.0
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
groq==0.31.0
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
h11==0.16.0
    # via httpcore
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.10.0
    # via
    #   anthropic
    #   openai
jsonschema==4.25.1
    # via altair
jsonschema-specifications==2025.4.1
    # via jsonschema
lxml==6.0.1
    # via ddgs
markdown-it-py==4.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
narwhals==2.1.2
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
numpy==2.3.2
    # via
    #   pandas
    #   pydeck
    #   streamlit
openai==1.101.0
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
packaging==25.0
    # via
    #   agno
    #   altair
    #   streamlit
pandas==2.3.2
    # via streamlit
pillow==11.3.0
    # via streamlit
primp==0.15.0
    # via ddgs
protobuf==6.32.0
    # via streamlit
pyarrow==21.0.0
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.2
    # via google-auth
pydantic==2.11.7
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   openai
    #   pydantic-settings
pydantic-core==2.33.2
    # via pydantic
pydantic-settings==2.10.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.2
    # via rich
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.1.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.5
    # via
    #   google-genai
    #   streamlit
rich==14.1.0
    # via
    #   agno
    #   typer
rpds-py==0.27.0
    # via
    #   jsonschema
    #   referencing
rsa==4.9.1
    # via google-auth
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
sqlalchemy==2.0.43
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
streamlit==1.48.1
    # via -r cookbook/examples/streamlit_apps/agentic_rag/requirements.in
tenacity==9.1.2
    # via
    #   google-genai
    #   streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.5.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.16.1
    # via agno
typing-extensions==4.14.1
    # via
    #   agno
    #   altair
    #   anthropic
    #   anyio
    #   google-genai
    #   groq
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   sqlalchemy
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.5.0
    # via requests
websockets==15.0.1
    # via google-genai



================================================
FILE: cookbook/examples/streamlit_apps/agentic_rag/utils.py
================================================
from typing import Any, Dict, List, Optional

import streamlit as st
from agentic_rag import get_agentic_rag_agent
from agno.agent import Agent
from agno.models.response import ToolExecution
from agno.utils.log import logger


def add_message(
    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None
) -> None:
    """Safely add a message to the session state"""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append(
        {"role": role, "content": content, "tool_calls": tool_calls}
    )


def export_chat_history():
    """Export chat history as markdown"""
    if "messages" in st.session_state:
        chat_text = "# Auto RAG Agent - Chat History\n\n"
        for msg in st.session_state["messages"]:
            role = "🤖 Assistant" if msg["role"] == "agent" else "👤 User"
            chat_text += f"### {role}\n{msg['content']}\n\n"
            if msg.get("tool_calls"):
                chat_text += "#### Tools Used:\n"
                for tool in msg["tool_calls"]:
                    if isinstance(tool, dict):
                        tool_name = tool.get("name", "Unknown Tool")
                    else:
                        tool_name = getattr(tool, "name", "Unknown Tool")
                    chat_text += f"- {tool_name}\n"
        return chat_text
    return ""


def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):
    """Display tool calls in a streamlit container with expandable sections.

    Args:
        tool_calls_container: Streamlit container to display the tool calls
        tools: List of tool call dictionaries containing name, args, content, and metrics
    """
    if not tools:
        return

    with tool_calls_container.container():
        for tool_call in tools:
            # Handle different tool call formats
            _tool_name = tool_call.tool_name or "Unknown Tool"
            _tool_args = tool_call.tool_args or {}
            _content = tool_call.result or ""
            _metrics = tool_call.metrics or {}

            # Safely create the title with a default if tool name is None
            title = f"🛠️ {_tool_name.replace('_', ' ').title() if _tool_name else 'Tool Call'}"

            with st.expander(title, expanded=False):
                if isinstance(_tool_args, dict) and "query" in _tool_args:
                    st.code(_tool_args["query"], language="sql")
                # Handle string arguments
                elif isinstance(_tool_args, str) and _tool_args:
                    try:
                        # Try to parse as JSON
                        import json

                        args_dict = json.loads(_tool_args)
                        st.markdown("**Arguments:**")
                        st.json(args_dict)
                    except:
                        # If not valid JSON, display as string
                        st.markdown("**Arguments:**")
                        st.markdown(f"```\n{_tool_args}\n```")
                # Handle dict arguments
                elif _tool_args and _tool_args != {"query": None}:
                    st.markdown("**Arguments:**")
                    st.json(_tool_args)

                if _content:
                    st.markdown("**Results:**")
                    if isinstance(_content, (dict, list)):
                        st.json(_content)
                    else:
                        try:
                            st.json(_content)
                        except Exception:
                            st.markdown(_content)

                if _metrics:
                    st.markdown("**Metrics:**")
                    st.json(
                        _metrics if isinstance(_metrics, dict) else _metrics.to_dict()
                    )


def rename_session_widget(agent: Agent) -> None:
    """Rename the current session of the agent and save to storage"""

    container = st.sidebar.container()

    # Initialize session_edit_mode if needed
    if "session_edit_mode" not in st.session_state:
        st.session_state.session_edit_mode = False

    if st.sidebar.button("✎ Rename Session"):
        st.session_state.session_edit_mode = True
        st.rerun()

    if st.session_state.session_edit_mode:
        new_session_name = st.sidebar.text_input(
            "Enter new name:",
            value=agent.session_name,
            key="session_name_input",
        )
        if st.sidebar.button("Save", type="primary"):
            if new_session_name:
                agent.rename_session(new_session_name)
                st.session_state.session_edit_mode = False
                st.rerun()


def session_selector_widget(agent: Agent, model_id: str) -> None:
    """Display a session selector in the sidebar"""

    if agent.storage:
        agent_sessions = agent.storage.get_all_sessions()
        # print(f"Agent sessions: {agent_sessions}")

        session_options = []
        for session in agent_sessions:
            session_id = session.session_id
            session_name = (
                session.session_data.get("session_name", None)
                if session.session_data
                else None
            )
            display_name = session_name if session_name else session_id
            session_options.append({"id": session_id, "display": display_name})

        if session_options:
            selected_session = st.sidebar.selectbox(
                "Session",
                options=[s["display"] for s in session_options],
                key="session_selector",
            )
            # Find the selected session ID
            selected_session_id = next(
                s["id"] for s in session_options if s["display"] == selected_session
            )

            if (
                st.session_state.get("agentic_rag_agent_session_id")
                != selected_session_id
            ):
                logger.info(
                    f"---*--- Loading {model_id} run: {selected_session_id} ---*---"
                )

                try:
                    new_agent = get_agentic_rag_agent(
                        model_id=model_id,
                        session_id=selected_session_id,
                    )

                    st.session_state["agentic_rag_agent"] = new_agent
                    st.session_state["agentic_rag_agent_session_id"] = (
                        selected_session_id
                    )

                    st.session_state["messages"] = []

                    selected_session_obj = next(
                        (
                            s
                            for s in agent_sessions
                            if s.session_id == selected_session_id
                        ),
                        None,
                    )

                    if (
                        selected_session_obj
                        and selected_session_obj.memory
                        and "runs" in selected_session_obj.memory
                    ):
                        seen_messages = set()

                        for run in selected_session_obj.memory["runs"]:
                            if "messages" in run:
                                for msg in run["messages"]:
                                    msg_role = msg.get("role")
                                    msg_content = msg.get("content")

                                    if not msg_content or msg_role == "system":
                                        continue

                                    msg_id = f"{msg_role}:{msg_content}"

                                    if msg_id in seen_messages:
                                        continue

                                    seen_messages.add(msg_id)

                                    if msg_role == "assistant":
                                        tool_calls = None
                                        if "tool_calls" in msg:
                                            tool_calls = msg["tool_calls"]
                                        elif "metrics" in msg and msg.get("metrics"):
                                            tools = run.get("tools")
                                            if tools:
                                                tool_calls = tools

                                        add_message(msg_role, msg_content, tool_calls)
                                    else:
                                        add_message(msg_role, msg_content)

                            elif (
                                "message" in run
                                and isinstance(run["message"], dict)
                                and "content" in run["message"]
                            ):
                                user_msg = run["message"]["content"]
                                msg_id = f"user:{user_msg}"

                                if msg_id not in seen_messages:
                                    seen_messages.add(msg_id)
                                    add_message("user", user_msg)

                                if "content" in run and run["content"]:
                                    asst_msg = run["content"]
                                    msg_id = f"assistant:{asst_msg}"

                                    if msg_id not in seen_messages:
                                        seen_messages.add(msg_id)
                                        add_message(
                                            "assistant", asst_msg, run.get("tools")
                                        )

                    st.rerun()
                except Exception as e:
                    logger.error(f"Error switching sessions: {str(e)}")
                    st.sidebar.error(f"Error loading session: {str(e)}")
        else:
            st.sidebar.info("No saved sessions available.")


def about_widget() -> None:
    """Display an about section in the sidebar"""
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ℹ️ About")
    st.sidebar.markdown("""
    This Agentic RAG Assistant helps you analyze documents and web content using natural language queries.

    Built with:
    - 🚀 Agno
    - 💫 Streamlit
    """)


CUSTOM_CSS = """
    <style>
    /* Main Styles */
   .main-title {
        text-align: center;
        background: linear-gradient(45deg, #FF4B2B, #FF416C);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 3em;
        font-weight: bold;
        padding: 1em 0;
    }
    .subtitle {
        text-align: center;
        color: #666;
        margin-bottom: 2em;
    }
    .stButton button {
        width: 100%;
        border-radius: 20px;
        margin: 0.2em 0;
        transition: all 0.3s ease;
    }
    .stButton button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    }
    .chat-container {
        border-radius: 15px;
        padding: 1em;
        margin: 1em 0;
        background-color: #f5f5f5;
    }
    .tool-result {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 1em;
        margin: 1em 0;
        border-left: 4px solid #3B82F6;
    }
    .status-message {
        padding: 1em;
        border-radius: 10px;
        margin: 1em 0;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
    }
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
    }
    /* Dark mode adjustments */
    @media (prefers-color-scheme: dark) {
        .chat-container {
            background-color: #2b2b2b;
        }
        .tool-result {
            background-color: #1e1e1e;
        }
    }
    </style>
"""



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/README.md
================================================
# Sage: Advanced Answer Engine

Sage is a powerful answer engine that combines:
- Real-time web search capabilities (using DuckDuckGo)
- Deep contextual analysis (using ExaTools)
- Intelligent tool selection
- Multiple LLM support
- Session management (using Sqlite)
- Chat history export

## 🚀 Quick Start

### 1. Environment Setup

Create and activate a virtual environment:
```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 2. Install Dependencies

```bash
pip install -r cookbook/examples/streamlit_apps/answer_engine/requirements.txt
```

### 3. Configure API Keys

Required:
```bash
export GROQ_API_KEY=your_groq_key_here
export EXA_API_KEY=your_exa_key_here
```

Optional (for additional models):
```bash
export OPENAI_API_KEY=your_openai_key_here
export ANTHROPIC_API_KEY=your_anthropic_key_here
export GOOGLE_API_KEY=your_google_key_here
```

### 4. Launch the Application

```bash
streamlit run cookbook/examples/streamlit_apps/answer_engine/app.py
```

Visit [localhost:8501](http://localhost:8501) to use the answer engine.

## 🔧 Customization

### Model Selection

The application supports multiple model providers:
- Groq (llama-3.3-70b-versatile)
- OpenAI (o3-mini, gpt-4o)
- Anthropic (claude-3-5-sonnet)
- Google (gemini-2.0-flash-exp)

### Agent Configuration

The agent configuration is in `agents.py` and the prompts are in `prompts.py`.
- If you just want to modify the prompts, update the `prompts.py` file.
- If you want to add new tools, models etc. update the `agents.py` file.

## 📚 Documentation

For more detailed information:
- [Agno Documentation](https://docs.agno.com)
- [Streamlit Documentation](https://docs.streamlit.io)

## 🤝 Support

Need help? Join our [Discord community](https://agno.link/discord)



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/agents.py
================================================
"""
Sage: An Answer Engine
---------------------------------
This example shows how to build Sage, a Perplexity-like Answer Engine that intelligently
determines whether to perform a web search or conduct a deep analysis using ExaTools based on the user's query.
It also prompts the user to save the generated answer to a file using FileTools.

Usage Examples:
---------------
1. Quick real-time search:
   sage = get_sage()
   answer = sage.run("What are the latest trends in renewable energy?")

2. In-depth analysis:
   sage = get_sage()
   answer = sage.run("Perform a detailed analysis of the impact of climate change on agriculture.")

3. Combined query with saving option:
   sage = get_sage()
   answer = sage.run("What's new in AI regulations in the EU and could you save the summary for me?")

Sage integrates:
  - DuckDuckGoTools for real-time web searches.
  - ExaTools for structured, in-depth analysis.
  - FileTools for saving the output upon user confirmation.

Sage intelligently selects the optimal tool based on query complexity to provide insightful, comprehensive answers.
"""

import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional

# Importing the Agent and model classes
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat

# Importing storage and tool classes
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.file import FileTools

# Import the Agent template
from prompts import AGENT_DESCRIPTION, AGENT_INSTRUCTIONS, EXPECTED_OUTPUT_TEMPLATE

# ************* Setup Paths *************
# Define the current working directory and output directory for saving files
cwd = Path(__file__).parent
output_dir = cwd.joinpath("output")
# Create output directory if it doesn't exist
output_dir.mkdir(parents=True, exist_ok=True)
# Create tmp directory if it doesn't exist
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)
# *************************************

# ************* Agent Storage *************
# Configure SQLite storage for agent sessions
agent_storage = SqliteAgentStorage(
    table_name="answer_engine_sessions",  # Table to store agent sessions
    db_file=str(tmp_dir.joinpath("agents.db")),  # SQLite database file
    auto_upgrade_schema=True,
)
# *************************************


def get_sage(
    user_id: Optional[str] = None,
    model_id: str = "openai:gpt-4o",
    session_id: Optional[str] = None,
    num_history_responses: int = 5,
    debug_mode: bool = True,
) -> Agent:
    """
    Returns an instance of Sage, the Answer Engine Agent with integrated tools for web search,
    deep contextual analysis, and file management.

    Sage will:
      - Decide whether a query requires a real-time web search (using DuckDuckGoTools)
        or an in-depth analysis (using ExaTools).
      - Generate a comprehensive answer that includes:
          • A direct, succinct answer.
          • Detailed explanations and supporting evidence.
          • Examples and clarification of misconceptions.
      - Prompt the user:
            "Would you like to save this answer to a file? (yes/no)"
        If confirmed, it will use FileTools to save the answer in markdown format in the output directory.

    Args:
        user_id: Optional identifier for the user.
        model_id: Model identifier in the format 'provider:model_name' (e.g., "openai:gpt-4o").
        session_id: Optional session identifier for tracking conversation history.
        num_history_responses: Number of previous responses to include for context.
        debug_mode: Enable logging and debug features.

    Returns:
        An instance of the configured Agent.
    """

    # Parse model provider and name
    provider, model_name = model_id.split(":")

    # Select appropriate model class based on provider
    if provider == "openai":
        model = OpenAIChat(id=model_name)
    elif provider == "google":
        model = Gemini(id=model_name)
    elif provider == "anthropic":
        model = Claude(id=model_name)
    elif provider == "groq":
        model = Groq(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")

    # Tools for Sage
    tools = [
        ExaTools(
            start_published_date=datetime.now().strftime("%Y-%m-%d"),
            type="keyword",
            num_results=10,
        ),
        DuckDuckGoTools(
            timeout=20,
            fixed_max_results=5,
        ),
        FileTools(base_dir=output_dir),
    ]

    return Agent(
        name="Sage",
        model=model,
        user_id=user_id,
        session_id=session_id or str(uuid.uuid4()),
        storage=agent_storage,
        tools=tools,
        # Allow Sage to read both chat history and tool call history for better context.
        read_chat_history=True,
        read_tool_call_history=True,
        # Append previous conversation responses into the new messages for context.
        add_history_to_messages=True,
        num_history_responses=num_history_responses,
        add_datetime_to_instructions=True,
        add_name_to_instructions=True,
        description=AGENT_DESCRIPTION,
        instructions=AGENT_INSTRUCTIONS,
        expected_output=EXPECTED_OUTPUT_TEMPLATE,
        debug_mode=debug_mode,
        markdown=True,
    )



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/app.py
================================================
import nest_asyncio
import streamlit as st
from agents import get_sage
from agno.agent import Agent
from agno.utils.log import logger
from utils import (
    CUSTOM_CSS,
    about_widget,
    add_message,
    display_tool_calls,
    rename_session_widget,
    session_selector_widget,
    sidebar_widget,
)

nest_asyncio.apply()

# Page configuration
st.set_page_config(
    page_title="Sage: The Answer Engine",
    page_icon=":crystal_ball:",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Load custom CSS with dark mode support
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def main() -> None:
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>Sage</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Your intelligent answer engine powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    model_options = {
        "llama-3.3-70b": "groq:llama-3.3-70b-versatile",
        "gpt-4o": "openai:gpt-4o",
        "o3-mini": "openai:o3-mini",
        "gemini-2.0-flash-exp": "google:gemini-2.0-flash-exp",
        "claude-3-5-sonnet": "anthropic:claude-3-5-sonnet-20241022",
    }
    selected_model = st.sidebar.selectbox(
        "Choose a model",
        options=list(model_options.keys()),
        index=0,
        key="model_selector",
    )
    model_id = model_options[selected_model]

    ####################################################################
    # Initialize Agent
    ####################################################################
    sage: Agent
    if (
        "sage" not in st.session_state
        or st.session_state["sage"] is None
        or st.session_state.get("current_model") != model_id
    ):
        logger.info("---*--- Creating new Sage agent ---*---")
        sage = get_sage(model_id=model_id)
        st.session_state["sage"] = sage
        st.session_state["current_model"] = model_id
        # Initialize messages array if needed
        if "messages" not in st.session_state:
            st.session_state["messages"] = []
    else:
        sage = st.session_state["sage"]

    ####################################################################
    # Load Agent Session from the database
    ####################################################################
    # Initialize session state
    if "sage_session_id" not in st.session_state:
        st.session_state["sage_session_id"] = None

    # Attempt to load or create a session
    if not st.session_state["sage_session_id"]:
        try:
            logger.info("---*--- Loading Sage session ---*---")
            st.session_state["sage_session_id"] = sage.load_session()
            logger.info(
                f"---*--- Sage session: {st.session_state['sage_session_id']} ---*---"
            )
        except Exception as e:
            logger.error(f"Session load error: {str(e)}")
            st.warning("Database connection unavailable. Running in memory-only mode.")
            # Generate a temporary session ID to allow the app to function without storage
            if not st.session_state["sage_session_id"]:
                import uuid

                st.session_state["sage_session_id"] = f"temp-{str(uuid.uuid4())}"
                logger.info(
                    f"---*--- Created temporary session: {st.session_state['sage_session_id']} ---*---"
                )

    ####################################################################
    # Load runs from memory
    ####################################################################
    # Initialize the messages array if not already done
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # Only try to load runs from memory if we have a valid session and no messages yet
    if (
        len(st.session_state["messages"]) == 0
        and hasattr(sage, "memory")
        and sage.memory is not None
    ):
        agent_runs = []
        # Check if memory is a dict or an object with runs attribute
        if isinstance(sage.memory, dict) and "runs" in sage.memory:
            agent_runs = sage.memory["runs"]
        elif hasattr(sage.memory, "runs"):
            agent_runs = sage.memory.runs

        # Load messages from agent runs
        if len(agent_runs) > 0:
            logger.debug("Loading run history")
            for _run in agent_runs:
                # Check if _run is an object with message attribute
                if hasattr(_run, "message") and _run.message is not None:
                    add_message(_run.message.role, _run.message.content)
                # Check if _run is an object with response attribute
                if hasattr(_run, "response") and _run.response is not None:
                    add_message("assistant", _run.response.content, _run.response.tools)
        else:
            logger.debug("No run history found")

    ####################################################################
    # Sidebar
    ####################################################################
    sidebar_widget()

    ####################################################################
    # Get user input
    ####################################################################
    if prompt := st.chat_input("✨ What would you like to know, bestie?"):
        add_message("user", prompt)

    ####################################################################
    # Display chat history
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] in ["user", "assistant"]:
            _content = message["content"]
            if _content is not None:
                with st.chat_message(message["role"]):
                    # Display tool calls if they exist in the message
                    if "tool_calls" in message and message["tool_calls"]:
                        display_tool_calls(st.empty(), message["tool_calls"])
                    st.markdown(_content)

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        with st.chat_message("assistant"):
            # Create container for tool calls
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner(":crystal_ball: Sage is working its magic..."):
                response = ""
                try:
                    # Run the agent and stream the response
                    run_response = sage.run(question, stream=True)
                    for _resp_chunk in run_response:
                        # Display tool calls if available
                        if hasattr(_resp_chunk, "tool") and _resp_chunk.tool:
                            display_tool_calls(tool_calls_container, [_resp_chunk.tool])

                        # Display response
                        if _resp_chunk.content is not None:
                            response += _resp_chunk.content
                            resp_container.markdown(response)

                    add_message("assistant", response, sage.run_response.tools)
                except Exception as e:
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    add_message("assistant", error_message)
                    st.error(error_message)

    ####################################################################
    # Session selector
    ####################################################################
    session_selector_widget(sage, model_id)
    rename_session_widget(sage)

    ####################################################################
    # About section
    ####################################################################
    about_widget()


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/prompts.py
================================================
"""Templates for the Answer Engine."""

from textwrap import dedent

AGENT_DESCRIPTION = dedent("""\
    You are Sage, a cutting-edge Answer Engine built to deliver precise, context-rich, and engaging responses.
    You have the following tools at your disposal:
      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.
      - ExaTools for structured, in-depth analysis.
      - FileTools for saving the output upon user confirmation.

    Your response should always be clear, concise, and detailed. Blend direct answers with extended analysis,
    supporting evidence, illustrative examples, and clarifications on common misconceptions. Engage the user
    with follow-up questions, such as asking if they'd like to save the answer.

    <critical>
    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.
    - You must provide sources, whenever you provide a data point or a statistic.
    - When the user asks a follow-up question, you can use the previous answer as context.
    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.
    </critical>\
""")

AGENT_INSTRUCTIONS = dedent("""\
    Here's how you should answer the user's question:

    1. Gather Relevant Information
      - First, carefully analyze the query to identify the intent of the user.
      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.
      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.
      - Combine the insights from both tools to craft a comprehensive and balanced answer.
      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.
      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.

    2. Construct Your Response
      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query.
      - **Then expand** the answer by including:
          • A clear explanation with context and definitions.
          • Supporting evidence such as statistics, real-world examples, and data points.
          • Clarifications that address common misconceptions.
      - Expand the answer only if the query requires more detail. Simple questions like: "What is the weather in Tokyo?" or "What is the capital of France?" don't need an in-depth analysis.
      - Ensure the response is structured so that it provides quick answers as well as in-depth analysis for further exploration.

    3. Enhance Engagement
      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)"
      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.

    4. Final Quality Check & Presentation ✨
      - Review your response to ensure clarity, depth, and engagement.
      - Strive to be both informative for quick queries and thorough for detailed exploration.

    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\
""")

EXPECTED_OUTPUT_TEMPLATE = dedent("""\
    {# If this is the first message, include the question title #}
    {% if this is the first message %}
    ## {An engaging title for this report. Keep it short.}
    {% endif %}

    **{A clear and direct response that answers the question.}**

    {# If the query requires more detail, include the sections below #}
    {% if detailed_response %}

    ### {Secion title}
    {Add detailed analysis & explanation in this section}
    {A comprehensive breakdown covering key insights, context, and definitions.}

    ### {Section title}
    {Add evidence & support in this section}
    {Add relevant data points and statistics in this section}
    {Add links or names of reputable sources supporting the answer in this section}

    ### {Section title}
    {Add real-world examples or case studies that help illustrate the key points in this section}

    ### {Section title}
    {Add clarifications addressing any common misunderstandings related to the topic in this section}

    ### {Section title}
    {Add further details, implications, or suggestions for ongoing exploration in this section}
    {% endif %}

    {Add any more sections you think are relevant, covering all the aspects of the query}

    ### Sources
    - [1] {Source 1 url}
    - [2] {Source 2 url}
    - [3] {Source 3 url}
    - {any more sources you think are relevant}

    Generated by Sage on: {current_time}

    Stay curious and keep exploring ✨\
    """)



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/requirements.in
================================================
agno
anthropic
ddgs
exa_py
google-generativeai
google-search-results
groq
nest_asyncio
openai
sqlalchemy
streamlit



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.7.12
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anthropic==0.64.0
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
anyio==4.10.0
    # via
    #   anthropic
    #   groq
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.8.3
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.3
    # via requests
click==8.2.1
    # via
    #   ddgs
    #   streamlit
    #   typer
ddgs==9.5.4
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
docstring-parser==0.17.0
    # via agno
exa-py==1.15.2
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
gitdb==4.0.12
    # via gitpython
gitpython==3.1.45
    # via
    #   agno
    #   streamlit
google-ai-generativelanguage==0.6.15
    # via google-generativeai
google-api-core==2.25.1
    # via
    #   google-ai-generativelanguage
    #   google-api-python-client
    #   google-generativeai
google-api-python-client==2.179.0
    # via google-generativeai
google-auth==2.40.3
    # via
    #   google-ai-generativelanguage
    #   google-api-core
    #   google-api-python-client
    #   google-auth-httplib2
    #   google-generativeai
google-auth-httplib2==0.2.0
    # via google-api-python-client
google-generativeai==0.8.5
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
google-search-results==2.4.2
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
googleapis-common-protos==1.70.0
    # via
    #   google-api-core
    #   grpcio-status
groq==0.31.0
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
grpcio==1.74.0
    # via
    #   google-api-core
    #   grpcio-status
grpcio-status==1.71.2
    # via google-api-core
h11==0.16.0
    # via httpcore
httpcore==1.0.9
    # via httpx
httplib2==0.22.0
    # via
    #   google-api-python-client
    #   google-auth-httplib2
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   exa-py
    #   groq
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.10.0
    # via
    #   anthropic
    #   openai
jsonschema==4.25.1
    # via altair
jsonschema-specifications==2025.4.1
    # via jsonschema
lxml==6.0.1
    # via ddgs
markdown-it-py==4.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
narwhals==2.1.2
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
numpy==2.3.2
    # via
    #   pandas
    #   pydeck
    #   streamlit
openai==1.101.0
    # via
    #   -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
    #   exa-py
packaging==25.0
    # via
    #   agno
    #   altair
    #   streamlit
pandas==2.3.2
    # via streamlit
pillow==11.3.0
    # via streamlit
primp==0.15.0
    # via ddgs
proto-plus==1.26.1
    # via
    #   google-ai-generativelanguage
    #   google-api-core
protobuf==5.29.5
    # via
    #   google-ai-generativelanguage
    #   google-api-core
    #   google-generativeai
    #   googleapis-common-protos
    #   grpcio-status
    #   proto-plus
    #   streamlit
pyarrow==21.0.0
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.2
    # via google-auth
pydantic==2.11.7
    # via
    #   agno
    #   anthropic
    #   exa-py
    #   google-generativeai
    #   groq
    #   openai
    #   pydantic-settings
pydantic-core==2.33.2
    # via pydantic
pydantic-settings==2.10.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.2
    # via rich
pyparsing==3.2.3
    # via httplib2
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.1.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.5
    # via
    #   exa-py
    #   google-api-core
    #   google-search-results
    #   streamlit
rich==14.1.0
    # via
    #   agno
    #   typer
rpds-py==0.27.0
    # via
    #   jsonschema
    #   referencing
rsa==4.9.1
    # via google-auth
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
sqlalchemy==2.0.43
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
streamlit==1.48.1
    # via -r cookbook/examples/streamlit_apps/answer_engine/requirements.in
tenacity==9.1.2
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.5.2
    # via streamlit
tqdm==4.67.1
    # via
    #   google-generativeai
    #   openai
typer==0.16.1
    # via agno
typing-extensions==4.14.1
    # via
    #   agno
    #   altair
    #   anthropic
    #   anyio
    #   exa-py
    #   google-generativeai
    #   groq
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   sqlalchemy
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
uritemplate==4.2.0
    # via google-api-python-client
urllib3==2.5.0
    # via requests



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/test.py
================================================
from agents import get_sage

sage = get_sage()

if __name__ == "__main__":
    sage.show_tool_calls = True
    sage.print_response("Tell me about the tarrifs the US is imposing", stream=True)



================================================
FILE: cookbook/examples/streamlit_apps/answer_engine/utils.py
================================================
from typing import Any, Dict, List, Optional

import streamlit as st
from agents import get_sage
from agno.agent.agent import Agent
from agno.models.response import ToolExecution
from agno.utils.log import logger


def add_message(
    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None
) -> None:
    """Safely add a message to the session state."""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append(
        {"role": role, "content": content, "tool_calls": tool_calls}
    )


def restart_agent():
    """Reset the agent and clear chat history."""
    logger.debug("---*--- Restarting agent ---*---")
    st.session_state["sage"] = None
    st.session_state["sage_session_id"] = None
    st.session_state["messages"] = []
    st.rerun()


def export_chat_history():
    """Export chat history as markdown."""
    if "messages" in st.session_state:
        chat_text = "# Sage - Chat History\n\n"
        for msg in st.session_state["messages"]:
            role_label = "🤖 Assistant" if msg["role"] == "assistant" else "👤 User"
            chat_text += f"### {role_label}\n{msg['content']}\n\n"
        return chat_text
    return ""


def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):
    """Display tool calls in a streamlit container with expandable sections.

    Args:
        tool_calls_container: Streamlit container to display the tool calls
        tools: List of tool call dictionaries containing name, args, content, and metrics
    """
    try:
        with tool_calls_container.container():
            for tool_call in tools:
                tool_name = tool_call.tool_name or "Unknown Tool"
                tool_args = tool_call.tool_args or {}
                content = tool_call.result or ""
                metrics = tool_call.metrics or {}

                # Add timing information
                execution_time_str = "N/A"
                try:
                    if metrics:
                        execution_time = (
                            metrics["time"]
                            if isinstance(metrics, dict)
                            else metrics.time
                        )
                        if execution_time is not None:
                            execution_time_str = f"{execution_time:.2f}s"
                except Exception as e:
                    logger.error(f"Error displaying tool calls: {str(e)}")
                    pass

                with st.expander(
                    f"🛠️ {tool_name.replace('_', ' ').title()} ({execution_time_str})",
                    expanded=False,
                ):
                    # Show query with syntax highlighting
                    if isinstance(tool_args, dict) and "query" in tool_args:
                        st.code(tool_args["query"], language="sql")

                    # Display arguments in a more readable format
                    if tool_args and tool_args != {"query": None}:
                        st.markdown("**Arguments:**")
                        st.json(tool_args)

                    if content:
                        st.markdown("**Results:**")
                        try:
                            st.json(content)
                        except Exception as e:
                            st.markdown(content)

    except Exception as e:
        logger.error(f"Error displaying tool calls: {str(e)}")
        tool_calls_container.error("Failed to display tool results")


def sidebar_widget() -> None:
    """Display a sidebar with sample user queries for Sage."""
    with st.sidebar:
        st.markdown("#### 📜 Try me!")
        if st.button("💡 US Tariffs"):
            add_message(
                "user",
                "Tell me about the tariffs the US is imposing in 2025",
            )
        if st.button("🤔 Reasoning Models"):
            add_message(
                "user",
                "Which is a better reasoning model: o3-mini or DeepSeek R1?",
            )
        if st.button("🤖 Tell me about Agno"):
            add_message(
                "user",
                "Tell me about Agno: https://github.com/agno-agi/agno and https://docs.agno.com",
            )
        if st.button("⚖️ Impact of AI Regulations"):
            add_message(
                "user",
                "Evaluate how emerging AI regulations could influence innovation, privacy, and ethical AI deployment in the near future.",
            )

        st.markdown("---")
        st.markdown("#### 🛠️ Utilities")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("🔄 New Chat"):
                restart_agent()
        with col2:
            fn = "sage_chat_history.md"
            if "sage_session_id" in st.session_state:
                fn = f"sage_{st.session_state.sage_session_id}.md"
            if st.download_button(
                "💾 Export Chat",
                export_chat_history(),
                file_name=fn,
                mime="text/markdown",
            ):
                st.sidebar.success("Chat history exported!")


def session_selector_widget(agent: Agent, model_id: str) -> None:
    """Display a session selector in the sidebar."""
    if agent.storage:
        agent_sessions = agent.storage.get_all_sessions()

        session_options = []
        for session in agent_sessions:
            session_id = session.session_id
            session_name = (
                session.session_data.get("session_name", None)
                if session.session_data
                else None
            )
            display_name = session_name if session_name else session_id
            session_options.append({"id": session_id, "display": display_name})

        selected_session = st.sidebar.selectbox(
            "Session",
            options=[s["display"] for s in session_options],
            key="session_selector",
        )

        selected_session_id = next(
            s["id"] for s in session_options if s["display"] == selected_session
        )

        if st.session_state.get("sage_session_id") != selected_session_id:
            logger.info(
                f"---*--- Loading {model_id} run: {selected_session_id} ---*---"
            )

            try:
                new_agent = get_sage(
                    model_id=model_id,
                    session_id=selected_session_id,
                )

                st.session_state["sage"] = new_agent
                st.session_state["sage_session_id"] = selected_session_id

                st.session_state["messages"] = []

                selected_session_obj = next(
                    (s for s in agent_sessions if s.session_id == selected_session_id),
                    None,
                )

                if (
                    selected_session_obj
                    and selected_session_obj.memory
                    and "runs" in selected_session_obj.memory
                ):
                    seen_messages = set()

                    for run in selected_session_obj.memory["runs"]:
                        if "messages" in run:
                            for msg in run["messages"]:
                                msg_role = msg.get("role")
                                msg_content = msg.get("content")

                                if not msg_content or msg_role == "system":
                                    continue

                                msg_id = f"{msg_role}:{msg_content}"

                                if msg_id in seen_messages:
                                    continue

                                seen_messages.add(msg_id)

                                if msg_role == "assistant":
                                    tool_calls = None
                                    if "tool_calls" in msg:
                                        tool_calls = msg["tool_calls"]
                                    elif "metrics" in msg and msg.get("metrics"):
                                        tools = run.get("tools")
                                        if tools:
                                            tool_calls = tools

                                    add_message(msg_role, msg_content, tool_calls)
                                else:
                                    # For user and other messages
                                    add_message(msg_role, msg_content)

                        elif (
                            "message" in run
                            and isinstance(run["message"], dict)
                            and "content" in run["message"]
                        ):
                            user_msg = run["message"]["content"]
                            msg_id = f"user:{user_msg}"

                            if msg_id not in seen_messages:
                                seen_messages.add(msg_id)
                                add_message("user", user_msg)

                            if "content" in run and run["content"]:
                                asst_msg = run["content"]
                                msg_id = f"assistant:{asst_msg}"

                                if msg_id not in seen_messages:
                                    seen_messages.add(msg_id)
                                    add_message("assistant", asst_msg, run.get("tools"))

                st.rerun()
            except Exception as e:
                logger.error(f"Error switching sessions: {str(e)}")
                st.sidebar.error(f"Error loading session: {str(e)}")


def rename_session_widget(agent: Agent) -> None:
    """Rename the current session of the agent and save to storage."""
    container = st.sidebar.container()
    session_row = container.columns([3, 1], vertical_alignment="center")

    # Initialize session_edit_mode if needed.
    if "session_edit_mode" not in st.session_state:
        st.session_state.session_edit_mode = False

    with session_row[0]:
        if st.session_state.session_edit_mode:
            new_session_name = st.text_input(
                "Session Name",
                value=agent.session_name,
                key="session_name_input",
                label_visibility="collapsed",
            )
        else:
            st.markdown(f"Session Name: **{agent.session_name}**")

    with session_row[1]:
        if st.session_state.session_edit_mode:
            if st.button("✓", key="save_session_name", type="primary"):
                if new_session_name:
                    agent.rename_session(new_session_name)
                    st.session_state.session_edit_mode = False
                    container.success("Renamed!")
        else:
            if st.button("✎", key="edit_session_name"):
                st.session_state.session_edit_mode = True


def about_widget() -> None:
    """Display an about section in the sidebar."""
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ℹ️ About")
    st.sidebar.markdown(
        """
        Sage is a cutting-edge answer engine that delivers real-time insights and in-depth analysis on a wide range of topics.

        Built with:
        - 🚀 Agno
        - 💫 Streamlit
        """
    )


CUSTOM_CSS = """
    <style>
    /* Main Styles */
    .main-title {
        text-align: center;
        background: linear-gradient(45deg, #FF4B2B, #FF416C);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 3em;
        font-weight: bold;
        padding: 1em 0;
    }
    .subtitle {
        text-align: center;
        color: #666;
        margin-bottom: 2em;
    }
    .stButton button {
        width: 100%;
        border-radius: 20px;
        margin: 0.2em 0;
        transition: all 0.3s ease;
    }
    .stButton button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    }
    .chat-container {
        border-radius: 15px;
        padding: 1em;
        margin: 1em 0;
        background-color: #f5f5f5;
    }
    .sql-result {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 1em;
        margin: 1em 0;
        border-left: 4px solid #FF4B2B;
    }
    .status-message {
        padding: 1em;
        border-radius: 10px;
        margin: 1em 0;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
    }
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
    }
    /* Dark mode adjustments */
    @media (prefers-color-scheme: dark) {
        .chat-container {
            background-color: #2b2b2b;
        }
        .sql-result {
            background-color: #1e1e1e;
        }
    }
    </style>
"""



================================================
FILE: cookbook/examples/streamlit_apps/chess_team/README.md
================================================
# Chess Team Battle

This example shows how to build an interactive Chess game where AI agents compete against each other. The application showcases how to:
- Coordinate multiple AI agents in a turn-based chess game
- Use different language models for different players
- Create an interactive web interface with Streamlit
- Handle chess game state and move validation
- Display real-time game progress and move history

## Features
- Multiple AI models support (GPT-4, Claude, Gemini, etc.)
- Real-time chess visualization
- Move history tracking with board states
- Interactive player selection
- Game state management
- Move validation and coordination
- Pause/resume functionality

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install dependencies

```shell
pip install -r cookbook/examples/streamlit_apps/chess_team/requirements.txt
```

### 3. Export API Keys

The game supports multiple AI models. Export the API keys for the models you want to use:

```shell
# Required for OpenAI models
export OPENAI_API_KEY=***

# Optional - for additional models
export ANTHROPIC_API_KEY=***  # For Claude models
export GOOGLE_API_KEY=***     # For Gemini models
export GROQ_API_KEY=***       # For Groq models
```

### 4. Run the Game

```shell
streamlit run cookbook/examples/streamlit_apps/chess_team/app.py
```

- Open [localhost:8501](http://localhost:8501) to view the game interface

## How It Works

The game consists of three agents:

1. **Master Agent (Referee)**
   - Coordinates the game
   - Validates chess moves
   - Maintains game state
   - Determines game outcome
   - Provides analysis after each turn

2. **Two Player Agents (White and Black)**
   - Make strategic chess moves
   - Analyze board positions
   - Follow chess rules
   - Respond to opponent moves

## Available Models

The game supports various AI models:
- GPT-4o (OpenAI)
- GPT-o3-mini (OpenAI)
- Gemini (Google)
- Llama 3 (Groq)
- Claude (Anthropic)

## Game Features

1. **Interactive Chess Board**
   - Real-time updates
   - Visual move tracking
   - Clear game status display
   - Legal move validation

2. **Move History**
   - Detailed move tracking
   - Board state visualization
   - Player action timeline
   - Move analysis

3. **Game Controls**
   - Start/Pause game
   - Reset board
   - Select AI models
   - View game history

4. **Performance Analysis**
   - Move timing
   - Strategy tracking
   - Game statistics
   - Position evaluation

## Support

Join our [Discord community](https://agno.link/discord) for help and discussions.




================================================
FILE: cookbook/examples/streamlit_apps/chess_team/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/chess_team/agents.py
================================================
"""
Chess Team Battle
---------------------------------
This example shows how to build a Chess game where AI agents play different roles in a team.
The game features specialized agents for strategy for white pieces, strategy for black pieces,
and a master agent overseeing the game. Move validation is handled by python-chess.

Usage Examples:
---------------
1. Quick game with default settings:
   team = get_chess_team()

2. Game with debug mode off:
   team = get_chess_team(debug_mode=False)

The game integrates:
  - Multiple AI models (Claude, GPT-4, etc.)
  - Specialized agent roles (strategist, master)
  - Turn-based gameplay coordination
  - Move validation using python-chess
"""

import sys
from pathlib import Path

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.utils.log import logger

project_root = str(Path(__file__).parent.parent.parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)


def get_model_for_provider(provider: str, model_name: str):
    """
    Creates and returns the appropriate model instance based on the provider.

    Args:
        provider: The model provider (e.g., 'openai', 'google', 'anthropic', 'groq')
        model_name: The specific model name/ID

    Returns:
        An instance of the appropriate model class

    Raises:
        ValueError: If the provider is not supported
    """
    if provider == "openai":
        return OpenAIChat(id=model_name)
    elif provider == "google":
        return Gemini(id=model_name)
    elif provider == "anthropic":
        if model_name == "claude-3-5-sonnet":
            return Claude(id="claude-3-5-sonnet-20241022", max_tokens=8192)
        elif model_name == "claude-3-7-sonnet":
            return Claude(
                id="claude-3-7-sonnet-20250219",
                max_tokens=8192,
            )
        elif model_name == "claude-3-7-sonnet-thinking":
            return Claude(
                id="claude-3-7-sonnet-20250219",
                max_tokens=8192,
                thinking={"type": "enabled", "budget_tokens": 4096},
            )
        else:
            return Claude(id=model_name)
    elif provider == "groq":
        return Groq(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")


def get_chess_team(
    white_model: str = "openai:gpt-4o",
    black_model: str = "anthropic:claude-3-7-sonnet",
    master_model: str = "openai:gpt-4o",
    debug_mode: bool = True,
) -> Team:
    """
    Returns a chess team with specialized agents for white pieces, black pieces, and game master.

    Args:
        white_model: Model for white piece strategy
        black_model: Model for black piece strategy
        master_model: Model for game state evaluation
        debug_mode: Enable logging and debug features

    Returns:
        Team instance configured for chess gameplay
    """
    try:
        white_provider, white_name = white_model.split(":")
        black_provider, black_name = black_model.split(":")
        master_provider, master_name = master_model.split(":")

        white_piece_model = get_model_for_provider(white_provider, white_name)
        black_piece_model = get_model_for_provider(black_provider, black_name)
        master_model = get_model_for_provider(master_provider, master_name)

        white_piece_agent = Agent(
            name="white_piece_agent",
            role="White Piece Strategist",
            description="""You are a chess strategist for white pieces. Given a list of legal moves,
                    analyze them and choose the best one based on standard chess strategy.
                    Consider piece development, center control, and king safety.
                    Respond ONLY with your chosen move in UCI notation (e.g., 'e2e4').""",
            model=white_piece_model,
            debug_mode=debug_mode,
        )

        black_piece_agent = Agent(
            name="black_piece_agent",
            role="Black Piece Strategist",
            description="""You are a chess strategist for black pieces. Given a list of legal moves,
                    analyze them and choose the best one based on standard chess strategy.
                    Consider piece development, center control, and king safety.
                    Respond ONLY with your chosen move in UCI notation (e.g., 'e7e5').""",
            model=black_piece_model,
            debug_mode=debug_mode,
        )

        return Team(
            name="Chess Team",
            mode="route",
            model=master_model,
            success_criteria="The game is completed with a win, loss, or draw",
            members=[white_piece_agent, black_piece_agent],
            instructions=[
                "You are the chess game coordinator and master analyst.",
                "Your role is to coordinate between two player agents and provide game analysis:",
                "1. white_piece_agent - Makes moves for white pieces",
                "2. black_piece_agent - Makes moves for black pieces",
                "",
                "When receiving a task:",
                "1. Check the 'current_player' in the context",
                "2. If current_player is white_piece_agent or black_piece_agent:",
                "   - Forward the move request to that agent",
                "   - Return their move response directly without modification",
                "3. If no current_player is specified:",
                "   - This indicates a request for position analysis",
                "   - Analyze the position yourself and respond with a JSON object:",
                "   {",
                "       'game_over': true/false,",
                "       'result': 'white_win'/'black_win'/'draw'/null,",
                "   }",
                "",
                "Do not modify player agent responses.",
                "For analysis requests, provide detailed evaluation of the position.",
            ],
            debug_mode=debug_mode,
            markdown=True,
            show_members_responses=True,
        )

    except Exception as e:
        logger.error(f"Error initializing chess team: {str(e)}")
        raise



================================================
FILE: cookbook/examples/streamlit_apps/chess_team/app.py
================================================
import logging
from typing import Dict, List

import chess
import nest_asyncio
import streamlit as st
from agents import get_chess_team
from agno.utils.log import logger
from utils import (
    CUSTOM_CSS,
    WHITE,
    ChessBoard,
    display_board,
    display_move_history,
    parse_move,
    show_agent_status,
)

# Configure logging
logging.basicConfig(
    level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

nest_asyncio.apply()

# Page configuration
st.set_page_config(
    page_title="Chess Team Battle",
    page_icon="♟️",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Load custom CSS with dark mode support
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def get_legal_moves_with_descriptions(board: ChessBoard) -> List[Dict]:
    """
    Get all legal moves with descriptions for the current player.

    Args:
        board: ChessBoard instance

    Returns:
        List of dictionaries with move information
    """
    legal_moves = []

    # Get python-chess board
    chess_board = board.board

    # Get all legal moves
    for move in chess_board.legal_moves:
        # Get source and destination squares
        from_square = chess.square_name(move.from_square)
        to_square = chess.square_name(move.to_square)

        # Get piece type
        piece = chess_board.piece_at(move.from_square)
        piece_type = piece.symbol().upper() if piece else "?"

        # Check if it's a capture
        is_capture = chess_board.is_capture(move)

        # Check if it's a promotion
        promotion = None
        if move.promotion:
            promotion = chess.piece_name(move.promotion)

        # Check if it's a castling move
        is_kingside_castle = chess_board.is_kingside_castling(move)
        is_queenside_castle = chess_board.is_queenside_castling(move)

        # Create move description
        if is_kingside_castle:
            description = "Kingside castle (O-O)"
        elif is_queenside_castle:
            description = "Queenside castle (O-O-O)"
        elif promotion:
            description = f"Pawn {from_square} to {to_square}, promote to {promotion}"
        elif is_capture:
            captured_piece = chess_board.piece_at(move.to_square)
            captured_type = captured_piece.symbol().upper() if captured_piece else "?"
            description = f"{piece_type} from {from_square} captures {captured_type} at {to_square}"
        else:
            description = f"{piece_type} from {from_square} to {to_square}"

        # Add move to list
        legal_moves.append(
            {
                "uci": move.uci(),
                "san": chess_board.san(move),
                "description": description,
                "is_capture": is_capture,
                "is_castle": is_kingside_castle or is_queenside_castle,
                "promotion": promotion,
            }
        )

    return legal_moves


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>♟️ Chess Team Battle</h1>",
        unsafe_allow_html=True,
    )
    ####################################################################
    # Initialize session state
    ####################################################################
    if "game_started" not in st.session_state:
        st.session_state.game_started = False
    if "game_paused" not in st.session_state:
        st.session_state.game_paused = False
    if "move_history" not in st.session_state:
        st.session_state.move_history = []

    ####################################################################
    # Sidebar controls
    ####################################################################
    with st.sidebar:
        st.markdown("### Game Settings")

        # Model selection
        model_options = {
            "gpt-4o": "openai:gpt-4o",
            "o3-mini": "openai:o3-mini",
            "claude-3.5": "anthropic:claude-3-5-sonnet",
            "claude-3.7": "anthropic:claude-3-7-sonnet",
            "claude-3.7-thinking": "anthropic:claude-3-7-sonnet-thinking",
            "gemini-flash": "google:gemini-2.0-flash",
            "gemini-pro": "google:gemini-2.0-pro-exp-02-05",
        }
        ################################################################
        # Model selection
        ################################################################
        st.markdown("#### White Player")
        selected_white = st.selectbox(
            "Select White Player",
            list(model_options.keys()),
            index=list(model_options.keys()).index("gpt-4o"),
            key="model_white",
        )

        st.markdown("#### Black Player")
        selected_black = st.selectbox(
            "Select Black Player",
            list(model_options.keys()),
            index=list(model_options.keys()).index("claude-3.7"),
            key="model_black",
        )

        st.markdown("#### Game Master")
        selected_master = st.selectbox(
            "Select Game Master",
            list(model_options.keys()),
            index=list(model_options.keys()).index("gpt-4o"),
            key="model_master",
        )

        ################################################################
        # Game controls
        ################################################################
        col1, col2 = st.columns(2)
        with col1:
            if not st.session_state.game_started:
                if st.button("▶️ Start Game"):
                    st.session_state.team = get_chess_team(
                        white_model=model_options[selected_white],
                        black_model=model_options[selected_black],
                        master_model=model_options[selected_master],
                        debug_mode=True,
                    )
                    st.session_state.game_board = ChessBoard()
                    st.session_state.game_started = True
                    st.session_state.game_paused = False
                    st.session_state.move_history = []
                    st.rerun()
            else:
                game_over, _ = st.session_state.game_board.get_game_state()
                if not game_over:
                    if st.button(
                        "⏸️ Pause" if not st.session_state.game_paused else "▶️ Resume"
                    ):
                        st.session_state.game_paused = not st.session_state.game_paused
                        st.rerun()
        with col2:
            if st.session_state.game_started:
                if st.button("🔄 New Game"):
                    st.session_state.team = get_chess_team(
                        white_model=model_options[selected_white],
                        black_model=model_options[selected_black],
                        master_model=model_options[selected_master],
                        debug_mode=True,
                    )
                    st.session_state.game_board = ChessBoard()
                    st.session_state.game_paused = False
                    st.session_state.move_history = []
                    st.rerun()

    ####################################################################
    # Header showing current models
    ####################################################################
    if st.session_state.game_started:
        st.markdown(
            f"<h3 style='color:#87CEEB; text-align:center;'>{selected_white} vs {selected_black}</h3>",
            unsafe_allow_html=True,
        )

    ####################################################################
    # Main game area
    ####################################################################
    if st.session_state.game_started:
        game_over, state_info = st.session_state.game_board.get_game_state()

        display_board(st.session_state.game_board)

        # Show game status (winner/draw/current player)
        if game_over:
            result = state_info.get("result", "")
            reason = state_info.get("reason", "")

            if "white_win" in result:
                st.success(f"🏆 Game Over! White ({selected_white}) wins by {reason}!")
            elif "black_win" in result:
                st.success(f"🏆 Game Over! Black ({selected_black}) wins by {reason}!")
            else:
                st.info(f"🤝 Game Over! It's a draw by {reason}!")
        else:
            # Show current player status
            current_color = st.session_state.game_board.current_color
            current_model_name = (
                selected_white if current_color == WHITE else selected_black
            )

            show_agent_status(
                f"{current_color.capitalize()} Player ({current_model_name})",
                "It's your turn",
                is_white=(current_color == WHITE),
            )

        display_move_history(st.session_state.move_history)

        if not st.session_state.game_paused and not game_over:
            # Thinking indicator
            st.markdown(
                f"""<div class="thinking-container">
                    <div class="agent-thinking">
                        <div style="margin-right: 10px; display: inline-block;">🔄</div>
                        {current_color.capitalize()} Player ({current_model_name}) is thinking...
                    </div>
                </div>""",
                unsafe_allow_html=True,
            )

            # Get legal moves using python-chess directly
            legal_moves = get_legal_moves_with_descriptions(st.session_state.game_board)

            # Format legal moves for the agent
            legal_moves_descriptions = "\n".join(
                [
                    f"- {move['san']} ({move['uci']}): {move['description']}"
                    for move in legal_moves
                ]
            )

            # Get board state
            board_state = st.session_state.game_board.get_board_state()
            fen = st.session_state.game_board.get_fen()

            # Get move from current player agent through team
            current_agent_name = (
                "white_piece_agent" if current_color == WHITE else "black_piece_agent"
            )

            # Create the task message for the team
            task_message = f"""\
Current board state (FEN): {fen}
Board visualization:
{board_state}

Legal moves available:
{legal_moves_descriptions}

Choose your next move from the legal moves above.
Respond with ONLY your chosen move in UCI notation (e.g., 'e2e4').
Do not include any other text in your response."""

            response = st.session_state.team.run(
                task_message,
                stream=False,
                context={
                    "current_player": current_agent_name,
                    "board_state": board_state,
                    "legal_moves": legal_moves,
                },
            )

            try:
                # Parse the move from the response
                move_str = parse_move(response.content if response else "")

                # Verify the move is in the list of legal moves
                legal_move_ucis = [move["uci"] for move in legal_moves]

                if move_str not in legal_move_ucis:
                    # Try to find a matching move
                    for move in legal_moves:
                        if move["san"].lower() == move_str.lower():
                            move_str = move["uci"]
                            break

                # Make the move
                success, message = st.session_state.game_board.make_move(move_str)

                if success:
                    # Find the move description
                    move_description = next(
                        (
                            move["description"]
                            for move in legal_moves
                            if move["uci"] == move_str
                        ),
                        "",
                    )

                    move_number = len(st.session_state.move_history) + 1
                    st.session_state.move_history.append(
                        {
                            "number": move_number,
                            "player": f"{current_color.capitalize()} ({current_model_name})",
                            "move": move_str,
                            "description": move_description,
                        }
                    )

                    logger.info(
                        f"Move {move_number}: {current_color.capitalize()} ({current_model_name}) played {move_str} - {move_description}"
                    )
                    logger.info(
                        f"Board state:\n{st.session_state.game_board.get_board_state()}"
                    )

                    # Check game state after move
                    game_over, state_info = st.session_state.game_board.get_game_state()

                    # If game is not over, get analysis from coordinator after black's move
                    if not game_over and move_number % 2 == 0:  # After black's move
                        analysis_message = f"""\
Current board state (FEN): {fen}
Board visualization:
{board_state}

Last move: {move_str} ({move_description})

Analyze the current position and provide your evaluation.
Respond with a JSON object containing:
{{
    "game_over": false,
    "result": null,
    "reason": null,
    "commentary": "Your analysis of the position",
    "advantage": "white"/"black"/"equal"
}}"""

                        st.session_state.team.run(
                            message=analysis_message,
                            stream=False,
                            context={
                                "board_state": board_state,
                                "last_move": move_str,
                                "last_move_description": move_description,
                            },
                        )

                    if game_over:
                        result = state_info.get("result", "")
                        reason = state_info.get("reason", "")

                        if "white_win" in result:
                            logger.info(f"Game Over - White wins by {reason}")
                            st.success(
                                f"🏆 Game Over! White ({selected_white}) wins by {reason}!"
                            )
                        elif "black_win" in result:
                            logger.info(f"Game Over - Black wins by {reason}")
                            st.success(
                                f"🏆 Game Over! Black ({selected_black}) wins by {reason}!"
                            )
                        else:
                            logger.info(f"Game Over - Draw by {reason}")
                            st.info(f"🤝 Game Over! It's a draw by {reason}!")

                        st.session_state.game_paused = True

                    st.rerun()
                else:
                    logger.error(f"Invalid move attempt: {message}")
                    st.session_state.team.run(
                        message=f"""\
Invalid move: {message}

Current board state (FEN): {fen}
Board visualization:
{board_state}

Legal moves available:
{legal_moves_descriptions}

Please choose a valid move from the list above.
Respond with ONLY your chosen move in UCI notation (e.g., 'e2e4').
Do not include any other text in your response.""",
                        stream=False,
                        context={
                            "current_player": current_agent_name,
                            "board_state": board_state,
                            "legal_moves": legal_moves,
                            "last_error": message,
                        },
                    )
                    st.rerun()

            except Exception as e:
                logger.error(f"Error processing move: {str(e)}")
                st.rerun()
    else:
        st.info("👈 Press 'Start Game' to begin!")

    ####################################################################
    # About section
    ####################################################################
    st.sidebar.markdown(f"""
    ### ♟️ Chess Team Battle
    Watch AI agents play chess with specialized roles!

    **Current Teams:**
    * ♔ White: `{selected_white}`
    * ♚ Black: `{selected_black}`
    * 🧠 Game Master: `{selected_master}`

    **How it Works:**
    1. Python-chess validates all legal moves
    2. The White/Black Player agents choose the best move
    3. The Game Master analyzes the position
    4. The process repeats until the game ends
    """)


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/chess_team/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/chess_team/requirements.in
================================================
agno
anthropic
groq
google-genai
nest-asyncio
openai
pathlib
Pillow
rich
streamlit
pydantic
typing-extensions
python-dotenv
python-chess


================================================
FILE: cookbook/examples/streamlit_apps/chess_team/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.6.0
    # via -r cookbook/examples/apps/chess_team/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anthropic==0.49.0
    # via -r cookbook/examples/apps/chess_team/requirements.in
anyio==4.9.0
    # via
    #   anthropic
    #   google-genai
    #   groq
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.1
    # via requests
chess==1.11.2
    # via python-chess
click==8.1.8
    # via
    #   streamlit
    #   typer
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
docstring-parser==0.16
    # via agno
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
google-auth==2.38.0
    # via google-genai
google-genai==1.7.0
    # via -r cookbook/examples/apps/chess_team/requirements.in
groq==0.20.0
    # via -r cookbook/examples/apps/chess_team/requirements.in
h11==0.14.0
    # via httpcore
httpcore==1.0.7
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.9.0
    # via
    #   anthropic
    #   openai
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2024.10.1
    # via jsonschema
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
narwhals==1.32.0
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/apps/chess_team/requirements.in
numpy==2.2.4
    # via
    #   pandas
    #   pydeck
    #   streamlit
openai==1.68.2
    # via -r cookbook/examples/apps/chess_team/requirements.in
packaging==24.2
    # via
    #   altair
    #   streamlit
pandas==2.2.3
    # via streamlit
pathlib==1.0.1
    # via -r cookbook/examples/apps/chess_team/requirements.in
pillow==11.1.0
    # via
    #   -r cookbook/examples/apps/chess_team/requirements.in
    #   streamlit
protobuf==5.29.4
    # via streamlit
pyarrow==19.0.1
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.1
    # via google-auth
pydantic==2.10.6
    # via
    #   -r cookbook/examples/apps/chess_team/requirements.in
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   openai
    #   pydantic-settings
pydantic-core==2.27.2
    # via pydantic
pydantic-settings==2.8.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.1
    # via rich
python-chess==1.999
    # via -r cookbook/examples/apps/chess_team/requirements.in
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.0.1
    # via
    #   -r cookbook/examples/apps/chess_team/requirements.in
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.1
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via
    #   google-genai
    #   streamlit
rich==13.9.4
    # via
    #   -r cookbook/examples/apps/chess_team/requirements.in
    #   agno
    #   typer
rpds-py==0.23.1
    # via
    #   jsonschema
    #   referencing
rsa==4.9
    # via google-auth
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
streamlit==1.43.2
    # via -r cookbook/examples/apps/chess_team/requirements.in
tenacity==9.0.0
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.15.2
    # via agno
typing-extensions==4.12.2
    # via
    #   -r cookbook/examples/apps/chess_team/requirements.in
    #   agno
    #   altair
    #   anthropic
    #   anyio
    #   google-genai
    #   groq
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   streamlit
    #   typer
tzdata==2025.2
    # via pandas
urllib3==2.3.0
    # via requests
websockets==15.0.1
    # via google-genai



================================================
FILE: cookbook/examples/streamlit_apps/chess_team/utils.py
================================================
from dataclasses import dataclass
from typing import Dict, List, Tuple

import chess
import streamlit as st
from agno.agent import Agent

WHITE = "white"
BLACK = "black"

# Custom CSS for the chess board
CUSTOM_CSS = """
<style>
    /* Dark mode styling */
    .main-title {
        color: #87CEEB;
        text-align: center;
        margin-bottom: 20px;
    }
    
    .chess-board {
        width: 100%;
        max-width: 500px;
        margin: 0 auto;
        border: 2px solid #555;
        border-radius: 5px;
        overflow: hidden;
    }
    
    .chess-square {
        width: 12.5%;
        aspect-ratio: 1;
        display: inline-block;
        text-align: center;
        font-size: 24px;
        line-height: 60px;
        vertical-align: middle;
    }
    
    .white-square {
        background-color: #f0d9b5;
        color: #000;
    }
    
    .black-square {
        background-color: #b58863;
        color: #000;
    }
    
    .piece {
        font-size: 32px;
        line-height: 60px;
    }
    
    .move-history {
        margin-top: 20px;
        border: 1px solid #555;
        border-radius: 5px;
        padding: 10px;
        max-height: 300px;
        overflow-y: auto;
    }
    
    .move-entry {
        padding: 5px;
        border-bottom: 1px solid #444;
    }
    
    .move-entry:last-child {
        border-bottom: none;
    }
    
    .agent-thinking {
        display: flex;
        align-items: center;
        background-color: rgba(100, 100, 100, 0.2);
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
        animation: pulse 1.5s infinite;
    }
    
    @keyframes pulse {
        0% { opacity: 0.6; }
        50% { opacity: 1; }
        100% { opacity: 0.6; }
    }
    
    .agent-status {
        display: flex;
        align-items: center;
        background-color: rgba(100, 100, 100, 0.2);
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
    
    .agent-status.white {
        border-left: 4px solid #f0d9b5;
    }
    
    .agent-status.black {
        border-left: 4px solid #b58863;
    }
    
    .thinking-container {
        margin: 10px 0;
    }
    
    /* Additional CSS for the simple board representation */
    .chess-board-wrapper {
        font-family: 'Courier New', monospace;
        background: #2b2b2b;
        padding: 20px;
        border-radius: 10px;
        display: inline-block;
        margin: 20px auto;
        text-align: center;
    }
    
    .board-container {
        display: flex;
        justify-content: center;
        width: 100%;
    }
    
    .chess-files {
        color: #888;
        text-align: center;
        padding: 5px 0;
        margin-left: 30px;
        display: flex;
        justify-content: space-around;
        width: calc(100% - 30px);
        margin-bottom: 5px;
    }
    
    .chess-file-label {
        width: 40px;
        text-align: center;
    }
    
    .chess-grid {
        border: 1px solid #666;
        display: inline-block;
    }
    
    .chess-row {
        display: flex;
        align-items: center;
    }
    
    .chess-rank {
        color: #888;
        width: 25px;
        text-align: center;
        padding-right: 5px;
    }
    
    .chess-cell {
        width: 40px;
        height: 40px;
        display: flex;
        align-items: center;
        justify-content: center;
        border: 1px solid #666;
        font-size: 24px;
    }
    
    .piece-white {
        color: #fff;
    }
    
    .piece-black {
        color: #aaa;
    }
    
    .piece-empty {
        color: transparent;
    }
    
    .chess-row:nth-child(odd) .chess-cell:nth-child(even),
    .chess-row:nth-child(even) .chess-cell:nth-child(odd) {
        background-color: #3c3c3c;
    }
    
    .chess-row:nth-child(even) .chess-cell:nth-child(even),
    .chess-row:nth-child(odd) .chess-cell:nth-child(odd) {
        background-color: #262626;
    }
    
    /* Additional CSS for move history grid */
    .move-history-grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
        gap: 10px;
        padding: 10px;
        max-height: 600px;  /* Height of approximately 2 rows of boards */
        overflow-y: auto;
        scrollbar-width: thin;
        scrollbar-color: #666 #333;
    }
    
    /* Webkit scrollbar styling */
    .move-history-grid::-webkit-scrollbar {
        width: 8px;
    }

    .move-history-grid::-webkit-scrollbar-track {
        background: #333;
        border-radius: 4px;
    }

    .move-history-grid::-webkit-scrollbar-thumb {
        background: #666;
        border-radius: 4px;
    }

    .move-history-grid::-webkit-scrollbar-thumb:hover {
        background: #888;
    }
    
    .move-history-item {
        background: rgba(40, 40, 40, 0.8);
        border-radius: 5px;
        padding: 10px;
        text-align: center;
        border: 1px solid #444;
    }
    
    .move-history-item .move-text {
        font-family: monospace;
        font-size: 1.1em;
        margin: 5px 0;
    }

    .move-history-item .move-text.white-move {
        color: #4CAF50;
    }

    .move-history-item .move-text.black-move {
        color: #ff4444;
    }
    
    .move-history-item .description {
        color: #888;
        font-size: 0.9em;
        margin-top: 5px;
    }
    
    /* Mini chess board for history */
    .mini-chess-board {
        display: grid;
        grid-template-columns: repeat(8, 1fr);
        width: 160px;
        margin: 10px auto;
        border: 1px solid #555;
        position: relative;
    }
    
    .mini-square {
        aspect-ratio: 1;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 14px;
        position: relative;
    }
    
    .mini-white-square {
        background-color: #f0d9b5;
    }
    
    .mini-black-square {
        background-color: #b58863;
    }
    
    .mini-piece {
        font-size: 14px;
        line-height: 20px;
        z-index: 2;
    }

    .mini-piece.white-piece {
        color: #ffffff;
        text-shadow: 0 0 2px #000;
    }

    .mini-piece.black-piece {
        color: #000000;
        text-shadow: 0 0 2px #fff;
    }

    .mini-square.move-from.white-move {
        background-color: rgba(76, 175, 80, 0.5) !important;
    }

    .mini-square.move-to.white-move {
        background-color: rgba(76, 175, 80, 0.7) !important;
    }

    .mini-square.move-from.black-move {
        background-color: rgba(255, 68, 68, 0.5) !important;
    }

    .mini-square.move-to.black-move {
        background-color: rgba(255, 68, 68, 0.7) !important;
    }
</style>
"""


@dataclass
class SimpleChessBoard:
    """Represents a simple chess board state without full rules validation"""

    def __init__(self):
        # Use Unicode chess pieces for better visualization
        self.piece_map = {
            "K": "♔",
            "Q": "♕",
            "R": "♖",
            "B": "♗",
            "N": "♘",
            "P": "♙",  # White pieces
            "k": "♚",
            "q": "♛",
            "r": "♜",
            "b": "♝",
            "n": "♞",
            "p": "♟",  # Black pieces
            ".": " ",  # Empty square
        }

        self.board = [
            ["r", "n", "b", "q", "k", "b", "n", "r"],  # Black pieces
            ["p", "p", "p", "p", "p", "p", "p", "p"],  # Black pawns
            [".", ".", ".", ".", ".", ".", ".", "."],  # Empty row
            [".", ".", ".", ".", ".", ".", ".", "."],  # Empty row
            [".", ".", ".", ".", ".", ".", ".", "."],  # Empty row
            [".", ".", ".", ".", ".", ".", ".", "."],  # Empty row
            ["P", "P", "P", "P", "P", "P", "P", "P"],  # White pawns
            ["R", "N", "B", "Q", "K", "B", "N", "R"],  # White pieces
        ]

    def get_board_state(self) -> str:
        """Returns a formatted string representation of the current board state with HTML/CSS classes"""
        # First create the HTML structure with CSS classes
        html_output = [
            '<div class="chess-board-wrapper">',
            '<div class="chess-files">',
        ]

        # Add individual file labels
        for file in "abcdefgh":
            html_output.append(f'<div class="chess-file-label">{file}</div>')

        html_output.extend(
            [
                "</div>",  # Close chess-files
                '<div class="chess-grid">',
            ]
        )

        for i, row in enumerate(self.board):
            html_output.append('<div class="chess-row">')
            html_output.append(f'<div class="chess-rank">{8 - i}</div>')

            for piece in row:
                piece_char = self.piece_map[piece]
                piece_class = "piece-white" if piece.isupper() else "piece-black"
                if piece == ".":
                    piece_class = "piece-empty"
                html_output.append(
                    f'<div class="chess-cell {piece_class}">{piece_char}</div>'
                )

            html_output.append("</div>")

        html_output.append("</div>")
        html_output.append("</div>")

        return "\n".join(html_output)

    def update_position(
        self, from_pos: Tuple[int, int], to_pos: Tuple[int, int]
    ) -> None:
        """Updates the board with a new move"""
        piece = self.board[from_pos[0]][from_pos[1]]
        self.board[from_pos[0]][from_pos[1]] = "."
        self.board[to_pos[0]][to_pos[1]] = piece

    def is_valid_position(self, pos: Tuple[int, int]) -> bool:
        """Checks if a position is within the board boundaries"""
        return 0 <= pos[0] < 8 and 0 <= pos[1] < 8

    def is_valid_move(self, move: str) -> bool:
        """Validates if a move string is in the correct format (e.g., 'e2e4')"""
        if len(move) != 4:
            return False

        file_chars = "abcdefgh"
        rank_chars = "12345678"

        from_file, from_rank = move[0], move[1]
        to_file, to_rank = move[2], move[3]

        return all(
            [
                from_file in file_chars,
                from_rank in rank_chars,
                to_file in file_chars,
                to_rank in rank_chars,
            ]
        )

    def algebraic_to_index(self, move: str) -> tuple[tuple[int, int], tuple[int, int]]:
        """Converts algebraic notation (e.g., 'e2e4') to board indices"""
        file_map = {"a": 0, "b": 1, "c": 2, "d": 3, "e": 4, "f": 5, "g": 6, "h": 7}

        from_file, from_rank = move[0], int(move[1])
        to_file, to_rank = move[2], int(move[3])

        from_pos = (8 - from_rank, file_map[from_file])
        to_pos = (8 - to_rank, file_map[to_file])

        return from_pos, to_pos

    def get_piece_name(self, piece: str) -> str:
        """Returns the full name of a piece from its symbol"""
        piece_names = {
            "K": "King",
            "Q": "Queen",
            "R": "Rook",
            "B": "Bishop",
            "N": "Knight",
            "P": "Pawn",
            "k": "King",
            "q": "Queen",
            "r": "Rook",
            "b": "Bishop",
            "n": "Knight",
            "p": "Pawn",
            ".": "Empty",
        }
        return piece_names.get(piece, "Unknown")

    def get_piece_at_position(self, pos: Tuple[int, int]) -> str:
        """Returns the piece at the given position"""
        return self.board[pos[0]][pos[1]]


class ChessBoard:
    def __init__(self):
        self.board = chess.Board()
        self.current_color = WHITE
        self.move_history = []

    def make_move(self, move_str: str) -> Tuple[bool, str]:
        """
        Make a move on the board using python-chess for validation.

        Args:
            move_str: Move in UCI notation (e.g., "e2e4")

        Returns:
            Tuple[bool, str]: (Success status, Message with current board state or error)
        """
        try:
            # Convert to python-chess move
            move = chess.Move.from_uci(move_str)

            # Check if move is legal
            if move not in self.board.legal_moves:
                return False, f"Invalid move: {move_str} is not a legal move."

            self.board.push(move)

            # Switch player
            self.current_color = BLACK if self.current_color == WHITE else WHITE

            return True, f"Move successful!\n{self.get_board_state()}"
        except ValueError:
            return False, f"Invalid move format: {move_str}. Use format like 'e2e4'."
        except Exception as e:
            return False, f"Error making move: {str(e)}"

    def get_board_state(self) -> str:
        """
        Get a string representation of the current board state.

        Returns:
            String representation of the board
        """
        return str(self.board)

    def get_fen(self) -> str:
        """
        Get the FEN notation of the current board state.

        Returns:
            FEN string
        """
        return self.board.fen()

    def get_legal_moves(self, color: str = None) -> List[str]:
        """
        Get all legal moves for the current player or specified color.

        Args:
            color: Optional color to get moves for (WHITE or BLACK)

        Returns:
            List of legal moves in UCI notation
        """

        # If it's not the specified color's turn, return empty list
        if (color == WHITE and not self.board.turn) or (
            color == BLACK and self.board.turn
        ):
            return []

        return [move.uci() for move in self.board.legal_moves]

    def is_game_over(self) -> bool:
        """
        Check if the game is over.

        Returns:
            True if game is over, False otherwise
        """
        return self.board.is_game_over()

    def get_game_state(self) -> Tuple[bool, Dict]:
        """
        Get the current game state.

        Returns:
            Tuple[bool, Dict]: (is_game_over, state_info)
        """
        is_game_over = self.board.is_game_over()

        state_info = {
            "current_player": self.current_color,
            "fen": self.board.fen(),
            "halfmove_clock": self.board.halfmove_clock,
            "fullmove_number": self.board.fullmove_number,
        }

        if is_game_over:
            if self.board.is_checkmate():
                winner = BLACK if self.board.turn else WHITE
                state_info["result"] = f"{winner}_win"
                state_info["reason"] = "checkmate"
            elif self.board.is_stalemate():
                state_info["result"] = "draw"
                state_info["reason"] = "stalemate"
            elif self.board.is_insufficient_material():
                state_info["result"] = "draw"
                state_info["reason"] = "insufficient_material"
            elif self.board.is_fifty_moves():
                state_info["result"] = "draw"
                state_info["reason"] = "fifty_move_rule"
            elif self.board.is_repetition():
                state_info["result"] = "draw"
                state_info["reason"] = "repetition"
            else:
                state_info["result"] = "draw"
                state_info["reason"] = "unknown"
        else:
            state_info["result"] = None
            state_info["reason"] = None

        return is_game_over, state_info


def display_board(board: ChessBoard):
    """
    Display the chess board in the Streamlit app.

    Args:
        board: ChessBoard instance
    """
    board_obj = board.board

    # Create HTML for the chess board
    html = '<div class="chess-board">'

    # Unicode chess pieces
    pieces = {
        "r": "♜",
        "n": "♞",
        "b": "♝",
        "q": "♛",
        "k": "♚",
        "p": "♟",
        "R": "♖",
        "N": "♘",
        "B": "♗",
        "Q": "♕",
        "K": "♔",
        "P": "♙",
        ".": "",
    }

    # Convert board to a 2D array for easier rendering
    board_array = []
    for row in str(board_obj).split("\n"):
        board_array.append(row.split(" "))

    for i in range(8):
        for j in range(8):
            square_color = "white-square" if (i + j) % 2 == 0 else "black-square"
            piece = board_array[i][j]
            piece_unicode = pieces.get(piece, "")

            html += f'<div class="chess-square {square_color}">'
            if piece_unicode:
                html += f'<span class="piece">{piece_unicode}</span>'
            html += "</div>"

    html += "</div>"

    st.markdown(html, unsafe_allow_html=True)


def show_agent_status(agent_name: str, status: str, is_white: bool = True):
    """
    Display the status of an agent.

    Args:
        agent_name: Name of the agent
        status: Status message
        is_white: Whether the agent plays white pieces
    """
    color_class = "white" if is_white else "black"
    st.markdown(
        f"""<div class="agent-status {color_class}">
            <div style="margin-right: 10px;">{"♔" if is_white else "♚"}</div>
            <div>
                <strong>{agent_name}</strong><br>
                {status}
            </div>
        </div>""",
        unsafe_allow_html=True,
    )


def display_move_history(move_history):
    """
    Display the move history with miniature chess boards.

    Args:
        move_history: List of move history entries
    """
    if not move_history:
        return

    st.markdown("<h3>Move History</h3>", unsafe_allow_html=True)

    html = '<div class="move-history-grid">'

    # Unicode chess pieces
    pieces = {
        "r": "♜",
        "n": "♞",
        "b": "♝",
        "q": "♛",
        "k": "♚",
        "p": "♟",
        "R": "♖",
        "N": "♘",
        "B": "♗",
        "Q": "♕",
        "K": "♔",
        "P": "♙",
        ".": "",
    }

    for move in move_history:
        board = chess.Board()
        # Play all moves up to this point
        for i in range(move["number"]):
            if i < len(move_history):
                try:
                    board.push(chess.Move.from_uci(move_history[i]["move"]))
                except (chess.InvalidMoveError, ValueError) as e:
                    continue

        # Get the current move's from and to squares
        current_move = move["move"]
        from_square = current_move[:2]
        to_square = current_move[2:]

        # Convert algebraic notation to board coordinates
        file_map = {"a": 0, "b": 1, "c": 2, "d": 3, "e": 4, "f": 5, "g": 6, "h": 7}
        from_file, from_rank = from_square[0], int(from_square[1])
        to_file, to_rank = to_square[0], int(to_square[1])
        from_coords = (8 - from_rank, file_map[from_file])
        to_coords = (8 - to_rank, file_map[to_file])

        # Determine if it's a white or black move
        is_white_move = "white" in move["player"].lower()
        move_color_class = "white-move" if is_white_move else "black-move"

        # Board to 2D array
        board_array = []
        for row in str(board).split("\n"):
            board_array.append(row.split(" "))

        # Create move history item with mini board
        html += '<div class="move-history-item">'
        html += f"<strong>Move {move['number']}</strong><br>"
        html += f"{move['player']}<br>"
        html += f'<div class="move-text {move_color_class}">{move["move"]}</div>'

        # Add mini chess board
        html += '<div class="mini-chess-board">'
        for i in range(8):
            for j in range(8):
                square_color = (
                    "mini-white-square" if (i + j) % 2 == 0 else "mini-black-square"
                )
                piece = board_array[i][j]
                piece_unicode = pieces.get(piece, "")

                # highlighting moves
                highlight_class = ""
                if (i, j) == from_coords:
                    highlight_class = f" move-from {move_color_class}"
                elif (i, j) == to_coords:
                    highlight_class = f" move-to {move_color_class}"

                html += f'<div class="mini-square {square_color}{highlight_class}">'
                if piece_unicode:
                    piece_color = "white-piece" if piece.isupper() else "black-piece"
                    html += (
                        f'<span class="mini-piece {piece_color}">{piece_unicode}</span>'
                    )
                html += "</div>"
        html += "</div>"

        if move.get("description"):
            html += f'<div class="description">{move["description"]}</div>'

        html += "</div>"

    html += "</div>"

    st.markdown(html, unsafe_allow_html=True)


def parse_move(move_text: str) -> str:
    """
    Parse a move from agent response.

    Args:
        move_text: Text containing the move

    Returns:
        Extracted move in UCI format
    """
    move_text = move_text.strip()

    # If the move is already in UCI format (e.g., "e2e4"), return it
    if (
        len(move_text) == 4
        and move_text[0].isalpha()
        and move_text[1].isdigit()
        and move_text[2].isalpha()
        and move_text[3].isdigit()
    ):
        return move_text

    # Try to extract the move from text
    import re

    move_match = re.search(r"([a-h][1-8][a-h][1-8])", move_text)
    if move_match:
        return move_match.group(1)

    return move_text


def is_claude_thinking_model(agent: Agent) -> bool:
    """
    Args:
        agent: The agent to check
    Returns:
        bool: True if the agent uses a Claude model with thinking enabled
    """
    return (
        hasattr(agent.model, "id")
        and isinstance(agent.model.id, str)
        and "claude" in agent.model.id.lower()
        and "thinking" in agent.model.id.lower()
    )



================================================
FILE: cookbook/examples/streamlit_apps/deep_researcher/README.md
================================================
# Deep Researcher Agent

A multi-stage AI-powered research workflow agent that automates comprehensive web research, analysis, and report generation using Agno, Scrapegraph, and Nebius AI.

1. **Searcher**: Finds and extracts high-quality, up-to-date information from the web using Scrapegraph and Nebius AI.
2. **Analyst**: Synthesizes, interprets, and organizes the research findings, highlighting key insights and trends.
3. **Writer**: Crafts a clear, structured, and actionable report, including references and recommendations.


## Installation

> Note: Fork and clone this repository if needed

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install dependencies

```shell
pip install -r cookbook/examples/streamlit_apps/deep_researcher/requirements.txt
```

### 3. Configure API Keys

Required:
```bash
export OPENAI_API_KEY=your_openai_key_here
export NEBIUS_API_KEY=your_nebius_api_key_here
export SGAI_API_KEY=your_scrapegraph_api_key_here
```


## Usage

You can use the Deep Researcher Agent in three ways. Each method below includes a demo image so you know what to expect.

### Web Interface

Run the Streamlit app:

```bash
streamlit run cookbook/examples/streamlit_apps/deep_researcher/app.py
```

Open your browser at [http://localhost:8501](http://localhost:8501)

### MCP Server

Add the following configuration to your .cursor/mcp.json or Claude/claude_desktop_config.json file (adjust paths and API keys as needed):

```json
{
  "mcpServers": {
    "deep_researcher_agent": {
      "command": "python",
      "args": [
        "--directory",
        "/Your/Path/to/directory/cookbook/examples/streamlit_apps/deep_researcher/server.py",
        "run",
        "server.py"
      ],
      "env": {
        "NEBIUS_API_KEY": "your_nebius_api_key_here",
        "SGAI_API_KEY": "your_scrapegraph_api_key_here"
      }
    }
  }
}
```

This allows tools like Claude Desktop to manage and launch the MCP server automatically.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request or open an issue.

## 📚 Documentation

For more detailed information:
- [Agno Documentation](https://docs.agno.com)
- [Streamlit Documentation](https://docs.streamlit.io)

## 🤝 Support

Need help? Join our [Discord community](https://agno.link/discord)



================================================
FILE: cookbook/examples/streamlit_apps/deep_researcher/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/deep_researcher/agents.py
================================================
from typing import Iterator

from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.scrapegraph import ScrapeGraphTools
from agno.utils.log import logger
from agno.workflow import RunResponse, Workflow


class DeepResearcherAgent(Workflow):
    """
    A multi-stage research workflow that:
    1. Gathers information from the web using advanced scraping tools.
    2. Analyzes and synthesizes the findings.
    3. Produces a clear, well-structured report.
    """

    searcher: Agent = Agent(
        tools=[ScrapeGraphTools()],
        model=Nebius(id="deepseek-ai/DeepSeek-V3-0324"),
        show_tool_calls=True,
        markdown=True,
        description=(
            "You are ResearchBot-X, an expert at finding and extracting high-quality, "
            "up-to-date information from the web. Your job is to gather comprehensive, "
            "reliable, and diverse sources on the given topic."
        ),
        instructions=(
            "1. Search for the most recent and authoritative and up-to-date sources (news, blogs, official docs, research papers, forums, etc.) on the topic.\n"
            "2. Extract key facts, statistics, and expert opinions.\n"
            "3. Cover multiple perspectives and highlight any disagreements or controversies.\n"
            "4. Include relevant statistics, data, and expert opinions where possible.\n"
            "5. Organize your findings in a clear, structured format (e.g., markdown table or sections by source type).\n"
            "6. If the topic is ambiguous, clarify with the user before proceeding.\n"
            "7. Be as comprehensive and verbose as possible—err on the side of including more detail.\n"
            "8. Mention the References & Sources of the Content. (It's Must)"
        ),
    )

    # Analyst: Synthesizes and interprets the research findings
    analyst: Agent = Agent(
        model=Nebius(id="deepseek-ai/DeepSeek-V3-0324"),
        markdown=True,
        description=(
            "You are AnalystBot-X, a critical thinker who synthesizes research findings "
            "into actionable insights. Your job is to analyze, compare, and interpret the "
            "information provided by the researcher."
        ),
        instructions=(
            "1. Identify key themes, trends, and contradictions in the research.\n"
            "2. Highlight the most important findings and their implications.\n"
            "3. Suggest areas for further investigation if gaps are found.\n"
            "4. Present your analysis in a structured, easy-to-read format.\n"
            "5. Extract and list ONLY the reference links or sources that were ACTUALLY found and provided by the researcher in their findings. Do NOT create, invent, or hallucinate any links.\n"
            "6. If no links were provided by the researcher, do not include a References section.\n"
            "7. Don't add hallucinations or make up information. Use ONLY the links that were explicitly passed to you by the researcher.\n"
            "8. Verify that each link you include was actually present in the researcher's findings before listing it.\n"
            "9. If there's no Link found from the previous agent then just say, No reference Found."
        ),
    )

    # Writer: Produces a final, polished report
    writer: Agent = Agent(
        model=Nebius(id="deepseek-ai/DeepSeek-V3-0324"),
        markdown=True,
        description=(
            "You are WriterBot-X, a professional technical writer. Your job is to craft "
            "a clear, engaging, and well-structured report based on the analyst's summary."
        ),
        instructions=(
            "1. Write an engaging introduction that sets the context.\n"
            "2. Organize the main findings into logical sections with headings.\n"
            "3. Use bullet points, tables, or lists for clarity where appropriate.\n"
            "4. Conclude with a summary and actionable recommendations.\n"
            "5. Include a References & Sources section ONLY if the analyst provided actual links from their analysis.\n"
            "6. Use ONLY the reference links that were explicitly provided by the analyst in their analysis. Do NOT create, invent, or hallucinate any links.\n"
            "7. If the analyst provided links, format them as clickable markdown links in the References section.\n"
            "8. If no links were provided by the analyst, do not include a References section at all.\n"
            "9. Never add fake or made-up links - only use links that were actually found and passed through the research chain."
        ),
    )

    def run(self, topic: str) -> Iterator[RunResponse]:
        """
        Orchestrates the research, analysis, and report writing process for a given topic.
        """
        logger.info(f"Running deep researcher agent for topic: {topic}")

        # Step 1: Research
        research_content = self.searcher.run(topic)
        # logger.info(f"Searcher content: {research_content.content}")

        logger.info("Analysis started")
        # Step 2: Analysis
        analysis = self.analyst.run(research_content.content)
        # logger.info(f"Analyst analysis: {analysis.content}")

        logger.info("Report Writing Started")
        # Step 3: Report Writing
        report = self.writer.run(analysis.content, stream=True)
        yield from report


def run_research(query: str) -> str:
    agent = DeepResearcherAgent()
    final_report_iterator = agent.run(
        topic=query,
    )
    logger.info("Report Generated")

    full_report = ""
    for chunk in final_report_iterator:
        if chunk.content:
            full_report += chunk.content

    return full_report



================================================
FILE: cookbook/examples/streamlit_apps/deep_researcher/app.py
================================================
import base64
import json
import time
from datetime import datetime
from os import getenv

import streamlit as st
from agents import DeepResearcherAgent

st.set_page_config(
    page_title="Deep Research Agent",
    page_icon="🔎",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Initialize session state for chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

with open(
    "cookbook/examples/streamlit_apps/deep_researcher/assets/scrapegraph.png", "rb"
) as scrapegraph_file:
    scrapegraph_base64 = base64.b64encode(scrapegraph_file.read()).decode()

    title_html = f"""
    <div style="display: flex; justify-content: center; align-items: center; width: 100%; padding: 32px 0 24px 0;">
        <h1 style="margin: 0; padding: 0; font-size: 2.5rem; font-weight: bold;">
            <span style="font-size:2.5rem;">🔎</span> Agentic Deep Searcher with 
            <span style="color: #fb542c;">Agno</span> & 
            <span style="color: #8564ff;">Scrapegraph</span>
            <img src="data:image/png;base64,{scrapegraph_base64}" style="height: 60px; margin-left: 12px; vertical-align: middle;"/>
        </h1>
    </div>
    """
    st.markdown(title_html, unsafe_allow_html=True)

with st.sidebar:
    st.image(
        "cookbook/examples/streamlit_apps/deep_researcher/assets/nebius.png", width=150
    )
    nebius_api_key = getenv("NEBIUS_API_KEY")
    if not nebius_api_key:
        nebius_api_key = st.text_input("Enter your Nebius API key", type="password")

    scrapegraph_api_key = getenv("SGAI_API_KEY")
    if not scrapegraph_api_key:
        scrapegraph_api_key = st.text_input(
            "Enter your Scrapegraph API key", type="password"
        )

    st.divider()

    # Example research topics
    st.header("🔍 Try These Examples")
    st.markdown("Click any topic below to start an instant deep research:")

    example_topics = [
        "🚀 Latest developments in AI and machine learning in 2024",
        "🌱 Current trends in sustainable energy technologies",
        "💊 Recent breakthroughs in personalized medicine and genomics",
    ]

    if "trigger_research" not in st.session_state:
        st.session_state.trigger_research = None

    for topic in example_topics:
        topic_text = topic.split(" ", 1)[1]  # Remove emoji and space
        if st.button(topic, use_container_width=True, key=f"example_{topic_text}"):
            st.session_state.trigger_research = topic_text
            st.rerun()

    st.divider()

    # Chat management buttons
    col1, col2 = st.columns(2)
    with col1:
        if st.button("🆕 New Chat", use_container_width=True):
            st.session_state.chat_history = []
            st.rerun()

        with col2:
            if st.session_state.chat_history:
                markdown_content = "# 🔎 Deep Research Agent - Chat History\n\n"

                for i, conversation in enumerate(st.session_state.chat_history, 1):
                    markdown_content += f"## {conversation['question']}\n\n"
                    markdown_content += f"{conversation['response']}\n\n"
                    if i < len(st.session_state.chat_history):
                        markdown_content += "---\n\n"

                st.download_button(
                    label="📥 Export Chat",
                    data=markdown_content,
                    file_name=f"deep_research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    mime="text/markdown",
                    use_container_width=True,
                )
            else:
                st.button("📥 Export Chat", use_container_width=True, disabled=True)

    st.divider()

    st.header("About")
    st.markdown(
        """
    This Deep Researcher workflow leverages multiple AI agents for a comprehensive research process:
    - **Searcher**: Finds and extracts information from the web.
    - **Analyst**: Synthesizes and interprets the research findings.
    - **Writer**: Produces a final, polished report.

    Built with:
    - 🚀 Agno
    - 💫 Streamlit
    """
    )

# Display chat history
if st.session_state.chat_history:
    st.subheader("💬 Chat History")

    for i, conversation in enumerate(st.session_state.chat_history):
        with st.container():
            with st.chat_message("user"):
                st.write(conversation["question"])

            with st.chat_message("assistant"):
                st.markdown(conversation["response"])
                st.caption(f"Research completed at: {conversation['timestamp']}")

            if i < len(st.session_state.chat_history) - 1:
                st.divider()

user_input = st.chat_input("Ask a question...")

if st.session_state.trigger_research:
    user_input = st.session_state.trigger_research
    st.session_state.trigger_research = None

    with st.chat_message("user"):
        st.write(user_input)

if user_input:
    try:
        agent = DeepResearcherAgent()

        current_conversation = {
            "question": user_input,
            "response": "",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        }

        with st.status("Executing research plan...", expanded=True) as status:
            # PHASE 1: Researching
            phase1_msg = "🧠 **Phase 1: Researching** - Finding and extracting relevant information from the web..."
            status.write(phase1_msg)
            research_content = agent.searcher.run(user_input)

            # PHASE 2: Analyzing
            phase2_msg = "🔬 **Phase 2: Analyzing** - Synthesizing and interpreting the research findings..."
            status.write(phase2_msg)
            analysis = agent.analyst.run(research_content.content)

            # PHASE 3: Writing Report
            phase3_msg = (
                "✍️ **Phase 3: Writing Report** - Producing a final, polished report..."
            )
            status.write(phase3_msg)
            report_iterator = agent.writer.run(analysis.content, stream=True)

        # Collect the full report
        full_report = ""
        report_container = st.empty()
        with st.spinner("🤔 Thinking..."):
            for chunk in report_iterator:
                if chunk.content:
                    full_report += chunk.content
                    report_container.markdown(full_report)

        # Store the complete conversation
        current_conversation["response"] = full_report
        st.session_state.chat_history.append(current_conversation)

        st.rerun()

    except Exception as e:
        st.error(f"An error occurred: {e}")



================================================
FILE: cookbook/examples/streamlit_apps/deep_researcher/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/deep_researcher/requirements.in
================================================
agno
streamlit
nest_asyncio
scrapegraph-py



================================================
FILE: cookbook/examples/streamlit_apps/deep_researcher/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.7.4
    # via -r cookbook/examples/streamlit_apps/deep_researcher/requirements.in
aiohappyeyeballs==2.6.1
    # via aiohttp
aiohttp==3.12.14
    # via scrapegraph-py
aiosignal==1.4.0
    # via aiohttp
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anyio==4.9.0
    # via httpx
attrs==25.3.0
    # via
    #   aiohttp
    #   jsonschema
    #   referencing
beautifulsoup4==4.13.4
    # via scrapegraph-py
blinker==1.9.0
    # via streamlit
cachetools==6.1.0
    # via streamlit
certifi==2025.7.14
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.2
    # via requests
click==8.2.1
    # via
    #   streamlit
    #   typer
docstring-parser==0.16
    # via agno
frozenlist==1.7.0
    # via
    #   aiohttp
    #   aiosignal
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
h11==0.16.0
    # via httpcore
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via agno
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
    #   yarl
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jsonschema==4.24.0
    # via altair
jsonschema-specifications==2025.4.1
    # via jsonschema
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
multidict==6.6.3
    # via
    #   aiohttp
    #   yarl
narwhals==1.47.0
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/streamlit_apps/deep_researcher/requirements.in
numpy==2.3.1
    # via
    #   pandas
    #   pydeck
    #   streamlit
packaging==25.0
    # via
    #   altair
    #   streamlit
pandas==2.3.1
    # via streamlit
pillow==11.3.0
    # via streamlit
propcache==0.3.2
    # via
    #   aiohttp
    #   yarl
protobuf==6.31.1
    # via streamlit
pyarrow==20.0.0
    # via streamlit
pydantic==2.11.7
    # via
    #   agno
    #   pydantic-settings
    #   scrapegraph-py
pydantic-core==2.33.2
    # via pydantic
pydantic-settings==2.10.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.2
    # via rich
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.1.1
    # via
    #   agno
    #   pydantic-settings
    #   scrapegraph-py
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.4
    # via
    #   scrapegraph-py
    #   streamlit
rich==14.0.0
    # via
    #   agno
    #   typer
rpds-py==0.26.0
    # via
    #   jsonschema
    #   referencing
scrapegraph-py==1.14.2
    # via -r cookbook/examples/streamlit_apps/deep_researcher/requirements.in
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via anyio
soupsieve==2.7
    # via beautifulsoup4
streamlit==1.47.0
    # via -r cookbook/examples/streamlit_apps/deep_researcher/requirements.in
tenacity==9.1.2
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.5.1
    # via streamlit
typer==0.16.0
    # via agno
typing-extensions==4.14.1
    # via
    #   agno
    #   aiosignal
    #   altair
    #   anyio
    #   beautifulsoup4
    #   pydantic
    #   pydantic-core
    #   referencing
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.5.0
    # via requests
yarl==1.20.1
    # via aiohttp



================================================
FILE: cookbook/examples/streamlit_apps/deep_researcher/server.py
================================================
import asyncio

from agents import run_research
from mcp.server.fastmcp import FastMCP

# Create FastMCP instance
mcp = FastMCP("deep_researcher_agent")


@mcp.tool()
def deep_researcher_agent(query: str) -> str:
    """Run Deep Researcher Agent for given user query. Can do both standard and deep web search.

    Args:
        query (str): The research query or question.

    Returns:
        str: The research response from the Deep Researcher Agent.
    """

    return run_research(query)


# Run the server
if __name__ == "__main__":
    mcp.run(transport="stdio")


# add this inside ./.cursor/mcp.json
# {
#   "mcpServers": {
#     "deep_researcher_agent": {
#       "command": "python",
#       "args": [
#         "--directory",
#         "/Users/arindammajumder/Developer/Python/awesome-llm-apps/advance_ai_agents/deep_researcher_agent",
#         "run",
#         "server.py"
#       ],
#       "env": {
#         "NEBIUS_API_KEY": "your_nebius_api_key_here",
#         "SGAI_API_KEY": "your_scrapegraph_api_key_here"
#       }
#     }
#   }
# }



================================================
FILE: cookbook/examples/streamlit_apps/game_generator/README.md
================================================
# Game Generator Workflow

This is a simple game generator workflow that generates a single-page HTML5 game based on a user's prompt.

### 1. Create a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Install requirements

```shell
pip install -r cookbook/examples/streamlit_apps/game_generator/requirements.txt
```

### 3. Export `OPENAI_API_KEY`

```shell
export OPENAI_API_KEY=sk-***
```

### 4. Run Streamlit App

```shell
streamlit run cookbook/examples/streamlit_apps/game_generator/app.py
```



================================================
FILE: cookbook/examples/streamlit_apps/game_generator/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/game_generator/app.py
================================================
from pathlib import Path

import streamlit as st
from agno.utils.string import hash_string_sha256
from game_generator import GameGenerator, SqliteWorkflowStorage

st.set_page_config(
    page_title="HTML5 Game Generator",
    page_icon="🎮",
    layout="wide",
)


st.title("Game Generator")
st.markdown("##### 🎮 built using [Agno](https://github.com/agno-agi/agno)")


def main() -> None:
    game_description = st.sidebar.text_area(
        "🎮 Describe your game",
        value="An asteroids game. Make sure the asteroids move randomly and are random sizes.",
        height=100,
    )

    generate_game = st.sidebar.button("Generate Game! 🚀")

    st.sidebar.markdown("## Example Games")
    example_games = [
        "A simple snake game where the snake grows longer as it eats food",
        "A breakout clone with colorful blocks and power-ups",
        "A space invaders game with multiple enemy types",
        "A simple platformer with jumping mechanics",
    ]

    for game in example_games:
        if st.sidebar.button(game):
            st.session_state["game_description"] = game
            generate_game = True

    if generate_game:
        with st.spinner("Generating your game... This might take a minute..."):
            try:
                hash_of_description = hash_string_sha256(game_description)
                game_generator = GameGenerator(
                    session_id=f"game-gen-{hash_of_description}",
                    storage=SqliteWorkflowStorage(
                        table_name="game_generator_workflows",
                        db_file="tmp/workflows.db",
                    ),
                )

                result = list(game_generator.run(game_description=game_description))

                games_dir = Path(__file__).parent.joinpath("games")
                game_path = games_dir / "game_output_file.html"

                if game_path.exists():
                    game_code = game_path.read_text()

                    with st.status(
                        "Game Generated Successfully!", expanded=True
                    ) as status:
                        st.subheader("Play the Game")
                        st.components.v1.html(game_code, height=700, scrolling=False)

                        st.subheader("Game Instructions")
                        st.write(result[-1].content)

                        st.download_button(
                            label="Download Game HTML",
                            data=game_code,
                            file_name="game.html",
                            mime="text/html",
                        )

                        status.update(
                            label="Game ready to play!",
                            state="complete",
                            expanded=True,
                        )

            except Exception as e:
                st.error(f"Failed to generate game: {str(e)}")

    st.sidebar.markdown("---")
    if st.sidebar.button("Restart"):
        st.rerun()


main()



================================================
FILE: cookbook/examples/streamlit_apps/game_generator/game_generator.py
================================================
"""
1. Install dependencies using: `pip install openai agno`
2. Run the script using: `python cookbook/examples/streamlit/game_generator/game_generator.py`
"""

import json
from pathlib import Path
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowCompletedEvent
from agno.storage.workflow.sqlite import SqliteWorkflowStorage
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.utils.string import hash_string_sha256
from agno.utils.web import open_html_file
from agno.workflow import Workflow
from pydantic import BaseModel, Field

games_dir = Path(__file__).parent.joinpath("games")
games_dir.mkdir(parents=True, exist_ok=True)
game_output_path = games_dir / "game_output_file.html"
game_output_path.unlink(missing_ok=True)


class GameOutput(BaseModel):
    reasoning: str = Field(..., description="Explain your reasoning")
    code: str = Field(..., description="The html5 code for the game")
    instructions: str = Field(..., description="Instructions how to play the game")


class QAOutput(BaseModel):
    reasoning: str = Field(..., description="Explain your reasoning")
    correct: bool = Field(False, description="Does the game pass your criteria?")


class GameGenerator(Workflow):
    # This description is only used in the workflow UI
    description: str = "Generator for single-page HTML5 games"

    game_developer: Agent = Agent(
        name="Game Developer Agent",
        description="You are a game developer that produces working HTML5 code.",
        model=OpenAIChat(id="gpt-4o"),
        instructions=[
            "Create a game based on the user's prompt. "
            "The game should be HTML5, completely self-contained and must be runnable simply by opening on a browser",
            "Ensure the game has a alert that pops up if the user dies and then allows the user to restart or exit the game.",
            "add full screen mode to the game",
            "Ensure instructions for the game are displayed on the HTML page."
            "Use user-friendly colours and make the game canvas large enough for the game to be playable on a larger screen.",
        ],
        response_model=GameOutput,
    )

    qa_agent: Agent = Agent(
        name="QA Agent",
        model=OpenAIChat(id="gpt-4o"),
        description="You are a game QA and you evaluate html5 code for correctness.",
        instructions=[
            "You will be given some HTML5 code."
            "Your task is to read the code and evaluate it for correctness, but also that it matches the original task description.",
        ],
        response_model=QAOutput,
    )

    def run(self, game_description: str) -> Iterator[RunResponse]:
        logger.info(f"Game description: {game_description}")

        game_output = self.game_developer.run(game_description)

        if (
            game_output
            and game_output.content
            and isinstance(game_output.content, GameOutput)
        ):
            game_code = game_output.content.code
            logger.info(f"Game code: {game_code}")
        else:
            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content="Sorry, could not generate a game.",
            )
            return

        logger.info("QA'ing the game code")
        qa_input = {
            "game_description": game_description,
            "game_code": game_code,
        }
        qa_output = self.qa_agent.run({"role": "user", "content": json.dumps(qa_input)})

        if qa_output and qa_output.content and isinstance(qa_output.content, QAOutput):
            logger.info(qa_output.content)
            if not qa_output.content.correct:
                raise Exception(f"QA failed for code: {game_code}")

            # Store the resulting code
            game_output_path.write_text(game_code)

            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content=game_output.content.instructions,
            )
        else:
            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content="Sorry, could not QA the game.",
            )
            return


# Run the workflow if the script is executed directly
if __name__ == "__main__":
    from rich.prompt import Prompt

    game_description = Prompt.ask(
        "[bold]Describe the game you want to make (keep it simple)[/bold]\n✨",
        # default="An asteroids game."
        default="An asteroids game. Make sure the asteroids move randomly and are random sizes. They should continually spawn more and become more difficult over time. Keep score. Make my spaceship's movement realistic.",
    )

    hash_of_description = hash_string_sha256(game_description)

    # Initialize the investment analyst workflow
    game_generator = GameGenerator(
        session_id=f"game-gen-{hash_of_description}",
        storage=SqliteWorkflowStorage(
            table_name="game_generator_workflows",
            db_file="tmp/workflows.db",
        ),
    )

    # Execute the workflow
    result: Iterator[RunResponse] = game_generator.run(
        game_description=game_description
    )

    # Print the report
    pprint_run_response(result)

    if game_output_path.exists():
        open_html_file(game_output_path)



================================================
FILE: cookbook/examples/streamlit_apps/game_generator/requirements.txt
================================================
agno
openai
streamlit




================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/README.md
================================================
# Gemini Multimodal Learning Tutor 📚🧠

Gemini Multimodal Learning Tutor is an advanced educational AI assistant powered by Google's Gemini 2.5 Pro Experimental. It provides personalized, interactive, and multimodal learning experiences tailored to various education levels.

---

## 🚀 Features

### Multimodal Capabilities

- 🖼️ **Image Analysis**: Interpret diagrams, charts, equations, and visual content.
- 🔊 **Audio Processing**: Extract insights from lectures, podcasts, and spoken explanations.
- 🎬 **Video Analysis**: Learn from educational videos, demonstrations, and presentations.
- 🔄 **Cross-Modal Learning**: Combine multiple media types for enhanced understanding.

### Advanced Search & Information

- 🔍 **Google Search**: Comprehensive web results for broad context and current events.
- 📚 **Exa Search**: Academic and structured educational content.
- 🦆 **DuckDuckGo**: Additional search perspectives for balanced information.
- 📊 **Multi-source Validation**: Cross-reference information from multiple sources.

### Advanced AI Features

- 🧠 Advanced reasoning for complex problems.
- 💭 Visible step-by-step reasoning.
- 🤖 Agentic AI for multi-step educational tasks.
- 🔢 Expert at math, science, and coding challenges.
- 📊 1 million token context window.
- 📚 Personalized learning experiences.
- 💾 Save lessons for future reference.

### Educational Features

- **Reasoning Modes**: Standard responses or detailed thinking processes.
- **Step-by-Step Problem Solving**: Detailed explanations of complex concepts.
- **Visual Learning**: Visual explanations and diagrams.
- **Interactive Learning**: Practice questions and assessments.
- **Session Management**: Save and organize learning sessions.

---

## 🛠️ Tech Stack

- 🤖 **Gemini 2.5 Pro Experimental** (March 2025) from Google
- 🚀 **Agno Framework** for AI agents
- 💫 **Streamlit** for interactive UI
- 🔍 **Multiple Search Engines** (Google, DuckDuckGo, Exa)
- 💾 **File System** for saving lessons

---

## ⚙️ Setup Instructions

### 1. Create a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Install requirements

```shell
pip install -r cookbook/examples/streamlit_apps/gemini-tutor/requirements.txt
```

### 3. Export `GOOGLE_API_KEY`

```shell
export GOOGLE_API_KEY=***
```

### 4. Run Streamlit App

```shell
streamlit run cookbook/examples/streamlit_apps/gemini-tutor/app.py
```

---

## 📂 Project Structure

## Multimodal Learning Features in Detail

### Image Analysis

- **Visual Problem Solving**: Analyze mathematical equations, diagrams, and problems
- **Chart and Graph Interpretation**: Extract data and insights from visual representations
- **Text in Images**: Recognize and interpret text within images
- **Spatial Reasoning**: Understand spatial relationships in visual content
- **Scientific Diagrams**: Interpret complex scientific visualizations

### Audio Analysis

- **Lecture Understanding**: Extract key concepts from educational audio
- **Speech Comprehension**: Process spoken explanations and instructions
- **Language Learning**: Analyze pronunciation and language patterns
- **Music Education**: Interpret musical concepts and theory
- **Sound Pattern Recognition**: Identify patterns in audio data

### Video Analysis

- **Tutorial Comprehension**: Extract step-by-step instructions from video tutorials
- **Demo Understanding**: Process demonstrations of concepts or experiments
- **Presentation Analysis**: Extract key points from educational presentations
- **Motion Analysis**: Understand physical processes shown in videos
- **Visual Storytelling**: Interpret narrative and sequential information

### Advanced Search Features

- **Multi-engine Search**: Leverages Google Search, Exa, and DuckDuckGo simultaneously
- **Information Synthesis**: Combines results from multiple sources for comprehensive answers
- **Current Events**: Access up-to-date information on recent developments
- **Academic Content**: Retrieve scholarly and educational resources
- **Source Credibility**: Cross-validate information across different search providers

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.



================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/agents.py
================================================
"""
Gemini Tutor: Advanced Educational AI Assistant powered by Gemini 2.5
"""

import json
import uuid
from pathlib import Path
from typing import Any, Dict, Optional

from agno.agent import Agent, RunResponse
from agno.models.google import Gemini
from agno.models.message import Message
from agno.tools.file import FileTools
from agno.tools.googlesearch import GoogleSearchTools
from agno.utils.log import logger

# Import prompt templates
from prompts import (
    SEARCH_GROUNDING_INSTRUCTIONS,
    TUTOR_DESCRIPTION_TEMPLATE,
    TUTOR_INSTRUCTIONS_TEMPLATE,
)


class TutorAppAgent:
    """
    Central agent that handles all tutoring functionality.
    Offloads search, content preparation, and learning experience generation.
    """

    def __init__(
        self, model_id="gemini-2.5-pro-exp-03-25", education_level="High School"
    ):
        """
        Initialize the TutorAppAgent.

        Args:
            model_id: Model identifier to use
            education_level: Target education level for content
        """
        self.model_id = model_id
        self.education_level = education_level
        self.agent = self._create_agent()
        logger.info(
            f"TutorAppAgent initialized with {model_id} model and {education_level} education level"
        )

    def _create_agent(self):
        """Create and configure the agent with all necessary capabilities"""

        gemini_model = Gemini(
            id=self.model_id,
            temperature=1,
            top_p=0.9,
            top_k=40,
        )

        # Enable grounding if supported by the model
        if "gemini-2." in self.model_id or "gemini-1.5" in self.model_id:
            try:
                setattr(gemini_model, "grounding", True)
                logger.info("Enabled model grounding (google_search_retrieval)")
            except AttributeError:
                logger.warning(
                    f"Model {self.model_id} does not support grounding attribute."
                )
            except Exception as e:
                logger.warning(f"Could not enable model grounding: {e}")

        # Format description and instructions directly
        tutor_description = TUTOR_DESCRIPTION_TEMPLATE.format(
            education_level=self.education_level
        )
        tutor_instructions = TUTOR_INSTRUCTIONS_TEMPLATE.format(
            education_level=self.education_level,
        )

        # Create agent with tutor capabilities, passing the created model
        return Agent(
            name="Gemini Tutor",
            model=gemini_model,
            session_id=str(uuid.uuid4()),
            read_chat_history=True,
            read_tool_call_history=True,
            add_history_to_messages=True,
            num_history_responses=5,
            description=tutor_description,  # Pass formatted description
            instructions=tutor_instructions,  # Pass formatted instructions
            debug_mode=True,
            markdown=True,
        )

    def create_learning_experience(self, search_topic, education_level=None):
        """
        Create a complete learning experience from search topic to final content.
        This method offloads the entire process to the agent.

        Args:
            search_topic: The topic to create a learning experience for
            education_level: Override the default education level for this specific call.

        Returns:
            The learning experience response from the agent
        """
        # Determine the education level for this specific request
        current_education_level = education_level or self.education_level
        if education_level and self.education_level != education_level:
            logger.info(
                f"Using temporary education level for this request: {education_level}"
            )
        else:
            # Use the agent's default education level if not overridden
            current_education_level = self.education_level

        logger.info(
            f"Creating learning experience for '{search_topic}' at {current_education_level} level"
        )

        # Construct a focused prompt for the agent, relying on its core instructions
        grounding_instructions = (
            SEARCH_GROUNDING_INSTRUCTIONS
            if "gemini-2." in self.model_id or "gemini-1.5" in self.model_id
            else ""
        )
        # The agent's core instructions (set during init) already contain formatting rules.
        # This prompt focuses on the specific task.
        prompt = f"""
        Create a complete and engaging learning experience about '{search_topic}' specifically tailored for {current_education_level} students.

        **Task:**
        Generate a comprehensive learning module covering the key aspects of '{search_topic}'.

        **Follow your core instructions regarding:**
        *   Adapting content complexity and style for the {current_education_level} level.
        *   Structuring the response logically (introduction, key concepts, examples, etc.).
        *   Including interactive elements (thought experiments/questions) and assessments (2-3 simple questions with answers).
        *   Strictly adhering to the rules for embedding images and videos (using direct, stable URLs only or omitting embeds).
        *   Citing up to 5 key sources if external information was used.

        {grounding_instructions}
        """

        # Create message
        user_message = Message(role="user", content=prompt)
        return self.agent.run(prompt=prompt, messages=[user_message], stream=True)



================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/app.py
================================================
"""
Gemini Tutor: Advanced Educational AI Assistant with Multimodal Learning
"""

import os

import nest_asyncio
import streamlit as st
from agents import TutorAppAgent
from agno.utils.log import logger
from utils import display_grounding_metadata, display_tool_calls

# Initialize asyncio support
nest_asyncio.apply()

# Page configuration
st.set_page_config(
    page_title="Gemini Multimodal Tutor",
    page_icon="🧠",
    layout="wide",
    initial_sidebar_state="expanded",
)

# --- Constants ---
MODEL_OPTIONS = {
    "Gemini 2.5 Pro Experimental (Recommended)": "gemini-2.5-pro-exp-03-25",
    "Gemini 2.0 Pro": "gemini-2.0-pro",
    "Gemini 2.0 Pro": "gemini-2.0-pro",
    "Gemini 1.5 Pro": "gemini-1.5-pro",
}

EDUCATION_LEVELS = [
    "Elementary School",
    "High School",
    "College",
    "Graduate",
    "PhD",
]


def initialize_session_state():
    """Initialize Streamlit session state variables if they don't exist."""
    if "tutor_agent" not in st.session_state:
        st.session_state.tutor_agent = None
    if "model_id" not in st.session_state:
        st.session_state.model_id = MODEL_OPTIONS[
            "Gemini 2.5 Pro Experimental (Recommended)"
        ]
    if "education_level" not in st.session_state:
        st.session_state.education_level = "High School"
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "processing" not in st.session_state:
        st.session_state.processing = False
    if "agent_needs_reset" not in st.session_state:
        st.session_state.agent_needs_reset = True  # Start needing initialization


def render_sidebar():
    """Render the sidebar for configuration and reset."""
    with st.sidebar:
        st.header("Configuration")

        # Store previous values to detect changes
        prev_model_id = st.session_state.model_id
        prev_education_level = st.session_state.education_level

        selected_model_name = st.selectbox(
            "Select Gemini Model",
            options=list(MODEL_OPTIONS.keys()),
            index=list(MODEL_OPTIONS.values()).index(
                st.session_state.model_id
            ),  # Maintain selection
            key="selected_model_name",
        )
        st.session_state.model_id = MODEL_OPTIONS[selected_model_name]

        st.session_state.education_level = st.selectbox(
            "Select Education Level",
            options=EDUCATION_LEVELS,
            index=EDUCATION_LEVELS.index(
                st.session_state.education_level
            ),  # Maintain selection
            key="education_level_selector",
        )

        # Check if settings changed
        if (
            st.session_state.model_id != prev_model_id
            or st.session_state.education_level != prev_education_level
        ):
            st.session_state.agent_needs_reset = True
            st.info(
                "Settings changed. Agent will be updated on next interaction or reset."
            )

        if st.button("New chat", key="apply_reset"):
            st.session_state.agent_needs_reset = True
            st.session_state.messages = []  # Clear history on reset
            st.toast("Settings applied. Agent updated and chat reset.")


def initialize_or_update_agent():
    """Initialize or update the agent if settings have changed."""
    if st.session_state.agent_needs_reset or st.session_state.tutor_agent is None:
        logger.info(
            f"Initializing/Updating Tutor Agent: Model={st.session_state.model_id}, Level={st.session_state.education_level}"
        )
        try:
            st.session_state.tutor_agent = TutorAppAgent(
                model_id=st.session_state.model_id,
                education_level=st.session_state.education_level,
            )
            st.session_state.agent_needs_reset = False
        except Exception as e:
            st.error(f"Failed to initialize agent: {e}")
            st.session_state.tutor_agent = None
            st.stop()


def render_chat_history():
    """Display the chat messages stored in session state."""
    st.markdown("### Learning Session")
    for message in st.session_state.messages:
        # Skip empty messages if any occurred (e.g., during stream error)
        if (
            not message.get("content")
            and not message.get("tools")
            and not message.get("citations")
        ):
            continue
        with st.chat_message(message["role"]):
            # Display content if it exists
            if message.get("content"):
                st.markdown(message["content"])

            # If assistant message, display tools and citations *within the same bubble*
            if message["role"] == "assistant":
                if message.get("tools"):
                    with st.expander("🛠️ Tool Calls", expanded=False):
                        display_tool_calls(message["tools"])
                if message.get("citations"):
                    display_grounding_metadata(message["citations"])


def handle_user_input():
    """Render the chat input form and handle submission."""
    with st.form(key="topic_form"):
        search_topic = st.text_input(
            "What would you like to learn about?",
            key="search_topic_input",
            placeholder="e.g., Quantum Physics, History of Rome, Python programming",
        )
        submitted = st.form_submit_button("Start Learning", type="primary")

        if submitted and search_topic and not st.session_state.processing:
            st.session_state.processing = True
            user_message = {
                "role": "user",
                "content": f"Teach me about: {search_topic}",
            }
            st.session_state.messages.append(user_message)
            st.rerun()  # Rerun to display user message immediately


def process_agent_response():
    """Process the agent response if the last message was from the user."""
    if (
        st.session_state.processing
        and st.session_state.messages
        and st.session_state.messages[-1]["role"] == "user"
    ):
        if st.session_state.tutor_agent is None:
            st.error("Agent is not initialized. Cannot process request.")
            st.session_state.processing = False
            return

        try:
            search_topic = st.session_state.messages[-1]["content"].replace(
                "Teach me about: ", ""
            )

            with st.spinner("🤔 Thinking..."):
                response_stream = (
                    st.session_state.tutor_agent.create_learning_experience(
                        search_topic=search_topic,
                        education_level=st.session_state.education_level,
                    )
                )

                st.session_state.current_tools = None
                st.session_state.current_citations = None

                def stream_handler(stream_generator):
                    logger.info("Starting stream processing...")
                    full_content = ""
                    for chunk in stream_generator:
                        content_delta = getattr(chunk, "content", None)
                        if content_delta:
                            full_content += content_delta
                            yield content_delta
                        tools = getattr(chunk, "tools", None)
                        if tools:
                            st.session_state.current_tools = tools
                        citations = getattr(chunk, "citations", None)
                        if citations:
                            st.session_state.current_citations = citations
                    logger.info("Finished stream processing.")
                    st.session_state.full_content_from_stream = full_content

                with st.chat_message("assistant"):
                    st.write_stream(stream_handler(response_stream))

                assistant_message = {"role": "assistant"}
                full_content = st.session_state.pop(
                    "full_content_from_stream", "[No content received]"
                )
                assistant_message["content"] = (
                    full_content if full_content else "[No content received]"
                )
                if not full_content:
                    logger.warning("Stream finished with no content.")

                # --- Post-processing to remove duplicate grounding sources ---
                grounding_marker = "\n🌐 Sources"
                if grounding_marker in full_content:
                    logger.info("Removing duplicate grounding sources section.")
                    full_content = full_content.split(grounding_marker)[0].rstrip()
                # -------------------------------------------------------------

                final_tools = st.session_state.pop("current_tools", None)
                final_citations = st.session_state.pop("current_citations", None)
                if final_tools:
                    assistant_message["tools"] = final_tools
                if final_citations:
                    assistant_message["citations"] = final_citations

                st.session_state.final_assistant_message = assistant_message

            if "final_assistant_message" in st.session_state:
                st.session_state.messages.append(
                    st.session_state.pop("final_assistant_message")
                )

        except Exception as e:
            logger.error(f"Error during agent run: {e}", exc_info=True)
            st.session_state.messages.append(
                {"role": "assistant", "content": f"An error occurred: {e}"}
            )
        finally:
            st.session_state.processing = False
            st.rerun()


# Custom CSS
CUSTOM_CSS = """
<style>
    .main-title {
        font-size: 2.5rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
        color: #5186EC;
    }
    .subtitle {
        font-size: 1.2rem;
        font-weight: 400;
        margin-bottom: 2rem;
        opacity: 0.8;
    }
    [data-testid="stChatMessageContent"] img {
        max-width: 350px;
        max-height: 300px;
        display: block;
        margin-top: 10px;
        margin-bottom: 10px;
        border-radius: 5px;
    }
</style>
"""


st.title("🔍 Gemini Tutor 📚")
st.markdown(
    '<p class="subtitle">Your AI-powered guide for exploring any topic</p>',
    unsafe_allow_html=True,
)
initialize_session_state()
render_sidebar()
initialize_or_update_agent()
render_chat_history()
handle_user_input()
process_agent_response()
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)



================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/prompts.py
================================================
"""
Prompt templates for the Gemini Tutor application.
"""

# Instructions specific to using search grounding
SEARCH_GROUNDING_INSTRUCTIONS = """
Use search to get accurate, up-to-date information and cite your sources **as specified in the formatting instructions**.
"""

# Base template for the tutor description
# The {education_level} will be formatted in agents.py
TUTOR_DESCRIPTION_TEMPLATE = """You are expertGemini Tutor, an educational AI assistant that provides personalized
learning for {education_level} students. You can analyze text and content
to create comprehensive learning experiences."""

# Base template for the tutor's core instructions
# The {education_level} will be formatted in agents.py
TUTOR_INSTRUCTIONS_TEMPLATE = """
**Your Role: Expert Gemini Tutor**
You are an expert educational AI assistant designed to create personalized and engaging learning experiences for {education_level} students. Your goal is to foster understanding, not just present information. Adapt your tone, vocabulary, depth, and examples appropriately for the specified education level.

**Core Task: Create Learning Experiences**
1.  **Understand & Research:** Analyze the user's query/topic. Use grounded search (if available) to gather accurate, up-to-date information. If the topic is ambiguous, either ask a clarifying question or make a reasonable assumption and state it clearly.
2.  **Structure the Content:** Organize the information logically with clear headings, introductions, explanations of key concepts, and summaries. Use Markdown for formatting (lists, emphasis, code blocks, tables).
3.  **Explain Clearly:** Provide explanations tailored to the {education_level} level. Use analogies, examples, and simple language where appropriate.
4.  **Engage & Assess:** Make the experience interactive. Include:
    *   **Interactive Elements:** At least one relevant thought experiment, practical analogy, or open-ended question to stimulate critical thinking.
    *   **Assessment:** 2-3 simple assessment questions (e.g., multiple-choice, true/false, or short fill-in-the-blank) with answers provided to check understanding.
5.  **Media Integration (Strict Rules):**
    *   Enhance explanations with relevant images or videos *only* if you can find stable, direct URLs.
    *   **Images:** Use `![Description](URL)`. **CRITICAL: The URL MUST be a direct link to the image file itself (ending in .png, .jpg, .jpeg, .gif). DO NOT use URLs pointing to webpages, intermediate services, or URLs with excessive query parameters.** Prioritize Wikimedia Commons direct file links if available.
    *   **Videos:** Use `[Video Title](URL)`. **CRITICAL: ONLY use standard, publicly accessible YouTube video URLs (e.g., https://www.youtube.com/watch?v=...).**
    *   **If you cannot find a URL meeting these strict criteria, DO NOT include the markdown embed.** Instead, describe the concept the media would illustrate or mention it textually (e.g., "A helpful diagram showing X can be found online").
6.  **Cite Sources:** Ensure factual accuracy. If you used search results or specific external documents to answer, cite **no more than 5** of the most relevant sources in a 'Sources' section at the end. Use the format: `* [Source Title](URL)`. **CRITICAL: Ensure this is the *only* list of sources provided. Do not include any automatically generated source lists (e.g., those labeled '🌐 Sources') that might come from the search tool.**
7.  **Formatting:** Follow the specific following formatting instructions provided in the user prompt for overall structure and citations.

Format your response as Markdown with:
- Clear headings and subheadings
- Lists and emphasis for important concepts
- Tables and code blocks when relevant
- Only provide sources if you used them to answer the question. Limit to 5 sources.
- **Source Citations:** At the end of your response, include a 'Sources' section. List **no more than 5** of the most relevant sources you used. Format each source as a markdown link: `* [Source Title](URL)`.
- **Images:** Use `![Description](URL)`. **CRITICAL: The URL MUST be a direct link to the image file itself (ending in .png, .jpg, .jpeg, .gif). DO NOT use URLs pointing to webpages, intermediate services, or URLs with excessive query parameters.** Prioritize Wikimedia Commons direct file links if available.
- **Videos:** Use `[Video Title](URL)`. **CRITICAL: ONLY use standard, publicly accessible YouTube video URLs (e.g., https://www.youtube.com/watch?v=...).**

"""



================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/requirements.in
================================================
# Core dependencies
agno>=0.1.0
google-genai>=0.3.2
streamlit>=1.31.0

# Utilities
nest-asyncio>=1.5.8
python-dotenv>=1.0.0
pydantic>=2.0.0
typing-extensions>=4.0.0

# Media and data handling
Pillow>=10.0.0
matplotlib>=3.7.0
numpy>=1.24.0
pandas>=2.0.0

# Search and web utilities
googlesearch-python>=1.2.3
requests>=2.31.0
beautifulsoup4>=4.12.0

# Optional utilities for enhanced functionality
pycountry>=22.1.10  # For handling country data when discussing geography



================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.6.0
    # via -r cookbook/examples/apps/gemini-tutor/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anyio==4.9.0
    # via
    #   google-genai
    #   httpx
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
beautifulsoup4==4.13.4
    # via
    #   -r cookbook/examples/apps/gemini-tutor/requirements.in
    #   googlesearch-python
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.1
    # via requests
click==8.1.8
    # via
    #   streamlit
    #   typer
contourpy==1.3.2
    # via matplotlib
cycler==0.12.1
    # via matplotlib
docstring-parser==0.16
    # via agno
fonttools==4.57.0
    # via matplotlib
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
google-auth==2.39.0
    # via google-genai
google-genai==1.11.0
    # via -r cookbook/examples/apps/gemini-tutor/requirements.in
googlesearch-python==1.3.0
    # via -r cookbook/examples/apps/gemini-tutor/requirements.in
h11==0.14.0
    # via httpcore
httpcore==1.0.8
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   google-genai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2024.10.1
    # via jsonschema
kiwisolver==1.4.8
    # via matplotlib
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
matplotlib==3.10.1
    # via -r cookbook/examples/apps/gemini-tutor/requirements.in
mdurl==0.1.2
    # via markdown-it-py
narwhals==1.35.0
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/apps/gemini-tutor/requirements.in
numpy==2.2.4
    # via
    #   -r cookbook/examples/apps/gemini-tutor/requirements.in
    #   contourpy
    #   matplotlib
    #   pandas
    #   pydeck
    #   streamlit
packaging==24.2
    # via
    #   altair
    #   matplotlib
    #   streamlit
pandas==2.2.3
    # via
    #   -r cookbook/examples/apps/gemini-tutor/requirements.in
    #   streamlit
pillow==11.2.1
    # via
    #   -r cookbook/examples/apps/gemini-tutor/requirements.in
    #   matplotlib
    #   streamlit
protobuf==5.29.4
    # via streamlit
pyarrow==19.0.1
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.2
    # via google-auth
pycountry==24.6.1
    # via -r cookbook/examples/apps/gemini-tutor/requirements.in
pydantic==2.11.3
    # via
    #   -r cookbook/examples/apps/gemini-tutor/requirements.in
    #   agno
    #   google-genai
    #   pydantic-settings
pydantic-core==2.33.1
    # via pydantic
pydantic-settings==2.8.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.1
    # via rich
pyparsing==3.2.3
    # via matplotlib
python-dateutil==2.9.0.post0
    # via
    #   matplotlib
    #   pandas
python-dotenv==1.1.0
    # via
    #   -r cookbook/examples/apps/gemini-tutor/requirements.in
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via
    #   -r cookbook/examples/apps/gemini-tutor/requirements.in
    #   google-genai
    #   googlesearch-python
    #   streamlit
rich==14.0.0
    # via
    #   agno
    #   typer
rpds-py==0.24.0
    # via
    #   jsonschema
    #   referencing
rsa==4.9.1
    # via google-auth
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via anyio
soupsieve==2.6
    # via beautifulsoup4
streamlit==1.44.1
    # via -r cookbook/examples/apps/gemini-tutor/requirements.in
tenacity==9.1.2
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
typer==0.15.2
    # via agno
typing-extensions==4.13.2
    # via
    #   -r cookbook/examples/apps/gemini-tutor/requirements.in
    #   agno
    #   altair
    #   anyio
    #   beautifulsoup4
    #   google-genai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.0
    # via pydantic
tzdata==2025.2
    # via pandas
urllib3==2.4.0
    # via requests
websockets==15.0.1
    # via google-genai



================================================
FILE: cookbook/examples/streamlit_apps/gemini-tutor/utils.py
================================================
"""
Utility functions for Gemini Tutor
"""

import json
from typing import Any, Dict, List, Optional

import streamlit as st
from agno.models.message import Citations
from agno.models.response import ToolExecution
from agno.utils.log import logger


def add_message(
    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None, **kwargs
) -> None:
    """
    Safely add a message to the session state.

    Args:
        role: The role of the message sender (user/assistant)
        content: The text content of the message
        tool_calls: Optional tool calls to include
        **kwargs: Additional message attributes (image, audio, video paths)
    """
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []

    message = {"role": role, "content": content, "tool_calls": tool_calls}

    # Add any additional attributes like image, audio, or video paths
    for key, value in kwargs.items():
        message[key] = value

    st.session_state["messages"].append(message)


def display_tool_calls(container: Any, tool_calls: List[ToolExecution]) -> None:
    """
    Display tool calls in a formatted way.

    Args:
        container: Streamlit container to display the tool calls
        tool_calls: List of tool call dictionaries
    """
    if not tool_calls:
        return

    with container:
        st.markdown("**Tool Calls:**")

        for i, tool_call in enumerate(tool_calls):
            # Format the tool call name
            tool_name = tool_call.tool_name or "Unknown Tool"

            # Format the args as pretty JSON
            args = tool_call.tool_args or {}
            formatted_args = json.dumps(args, indent=2)

            expander_label = f"📋 Tool Call {i + 1}: {tool_name}"
            with st.expander(expander_label, expanded=False):
                st.code(formatted_args, language="json")


def display_grounding_metadata(citations: Optional[Citations]) -> None:
    """
    Display search grounding metadata (sources) if available.

    Args:
        citations: Citations object from the agent response chunk or final message.
    """
    # Check if citations object exists and has the 'urls' attribute and it's not empty
    if not citations or not hasattr(citations, "urls") or not citations.urls:
        return

    try:
        st.markdown("---")
        st.markdown("### 🌐 Sources")

        # Display grounding sources from the pre-parsed list
        for citation_url in citations.urls:
            # Ensure url and title exist
            if (
                hasattr(citation_url, "url")
                and citation_url.url
                and hasattr(citation_url, "title")
                and citation_url.title
            ):
                st.markdown(f"- [{citation_url.title}]({citation_url.url})")
            elif (
                hasattr(citation_url, "url") and citation_url.url
            ):  # Fallback if title is missing
                st.markdown(f"- [{citation_url.url}]({citation_url.url})")

        # Optionally, display raw metadata in an expander for debugging if needed
        # if hasattr(citations, 'raw') and citations.raw:
        #     with st.expander("Raw Grounding Metadata (Debug)"):
        #         st.json(citations.raw)

    except Exception as e:
        logger.error(f"Error displaying grounding metadata: {e}", exc_info=True)
        st.warning("Could not display sources.")



================================================
FILE: cookbook/examples/streamlit_apps/geobuddy/readme.md
================================================
# GeoBuddy 🌍

GeoBuddy is an AI-powered geography agent that analyzes images to predict locations based on visible cues like landmarks, architecture, and cultural symbols.

## Features

- **Location Identification**: Predicts location details from uploaded images.
- **Detailed Reasoning**: Explains predictions based on visual cues.
- **User-Friendly UI**: Built with Streamlit for an intuitive experience.

---

## Setup Instructions

### 1. Create a virtual environment

```shell
python3 -m venv ~/.venvs/geobuddyenv
source ~/.venvs/geobuddyenv/bin/activate
```

### 2. Install requirements

```shell
pip install -r cookbook/examples/streamlit_apps/geobuddy/requirements.txt
```

### 3. Export `GOOGLE_API_KEY`

```shell
export GOOGLE_API_KEY=***
```

### 4. Run Streamlit App

```shell
streamlit run cookbook/examples/streamlit_apps/geobuddy/app.py
```



================================================
FILE: cookbook/examples/streamlit_apps/geobuddy/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/geobuddy/app.py
================================================
import os
from pathlib import Path

import streamlit as st
from geography_buddy import analyze_image
from PIL import Image

# Streamlit App Configuration
st.set_page_config(
    page_title="Geography Location Buddy",
    page_icon="🌍",
)
st.title("GeoBuddy 🌍")
st.markdown("##### :orange_heart: built by [agno](https://github.com/agno-agi/agno)")
st.markdown(
    """
    **Upload your image** and let model guess the location based on visual cues such as landmarks, architecture, and more.
    """
)


def main() -> None:
    # Sidebar Design
    with st.sidebar:
        st.markdown("<br><br>", unsafe_allow_html=True)
        st.markdown("let me guess the location based on visible cues from your image!")

        # Upload Image
        uploaded_file = st.file_uploader(
            "📷 Upload here..", type=["jpg", "jpeg", "png"]
        )
        st.markdown("---")

    # App Logic
    if uploaded_file:
        col1, col2 = st.columns([1, 2])

        # Display Uploaded Image
        with col1:
            st.markdown("#### Uploaded Image")
            image = Image.open(uploaded_file)
            resized_image = image.resize((400, 400))
            image_path = Path("temp_image.png")
            with open(image_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            st.image(resized_image, caption="Your Image", use_container_width=True)

        # Analyze Button and Output
        with col2:
            st.markdown("#### Location Analysis")
            analyze_button = st.button("🔍 Analyze Image")

            if analyze_button:
                with st.spinner("Analyzing the image... please wait."):
                    try:
                        result = analyze_image(image_path)
                        if result:
                            st.success("🌍 Here's my guess:")
                            st.markdown(result)
                        else:
                            st.warning(
                                "Sorry, I couldn't determine the location. Try another image."
                            )
                    except Exception as e:
                        st.error(f"An error occurred: {e}")

                # Cleanup after analysis
                if image_path.exists():
                    os.remove(image_path)
            else:
                st.info("Click the **Analyze** button to get started!")
    else:
        st.info("📷 Please upload an image to begin location analysis.")

    # Footer Section
    st.markdown("---")
    st.markdown(
        """
        **🌟 Features**:
        - Identify locations based on uploaded images.
        - Advanced reasoning based on landmarks, architecture, and cultural clues.

        **📢 Disclaimer**: GeoBuddy's guesses are based on visual cues and analysis and may not always be accurate.
        """
    )
    st.markdown(":orange_heart: Thank you for using GeoBuddy!")


main()



================================================
FILE: cookbook/examples/streamlit_apps/geobuddy/geography_buddy.py
================================================
import os
from pathlib import Path
from typing import Optional

from agno.agent import Agent
from agno.media import Image
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# Define the query for geography identification
geo_query = """
You are a geography expert. Your task is to analyze the given image and provide a reasoned guess of the location based on visible clues such as:
- Landmarks
- Architecture
- Natural features (mountains, rivers, coastlines)
- Language or symbols (text, street signs, billboards, any names mentioned in the picture as clue)
- People’s clothing or cultural aspects
- Environmental clues like weather, time of day

Return in this format:
Location Name, City, Country and Reasoning
Structure the response in markdown.

Instructions:
1. Examine the image thoroughly.
2. Provide a reasoned guess for the street name, city, state, and country.
3. Explain your reasoning in detail by pointing out the visual clues that led to your conclusion.
4. If uncertain, offer possible guesses with reasoning.
"""

# Initialize the GeoBuddy agent
geo_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"), tools=[DuckDuckGoTools()], markdown=True
)


# Function to analyze the image and return location information
def analyze_image(image_path: Path) -> Optional[str]:
    try:
        response = geo_agent.run(geo_query, images=[Image(filepath=image_path)])
        return response.content
    except Exception as e:
        raise RuntimeError(f"An error occurred while analyzing the image: {e}")



================================================
FILE: cookbook/examples/streamlit_apps/geobuddy/requirements.txt
================================================
agno
google-generativeai
openai
streamlit
pillow
ddgs



================================================
FILE: cookbook/examples/streamlit_apps/github_mcp_agent/README.md
================================================
# 🐙 MCP GitHub Agent

A Streamlit application that allows you to explore and analyze GitHub repositories using natural language queries through the Model Context Protocol (MCP).

## Features

- **Natural Language Interface**: Ask questions about repositories in plain English
- **Comprehensive Analysis**: Explore issues, pull requests, repository activity, and code statistics
- **Interactive UI**: User-friendly interface with example queries and custom input
- **MCP Integration**: Leverages the Model Context Protocol to interact with GitHub's API
- **Real-time Results**: Get immediate insights on repository activity and health

## Setup

### Requirements

- Python 3.8+
- Node.js and npm (for MCP GitHub server)
  - This is a critical requirement! The app uses `npx` to run the MCP GitHub server
  - Download and install from [nodejs.org](https://nodejs.org/)
- GitHub Personal Access Token with appropriate permissions
- OpenAI API Key

### Installation

1. Install the required Python packages:
   ```bash
   pip install -r cookbook/examples/streamlit_apps/github_mcp_agent/requirements.txt
   ```

2. Verify Node.js and npm are installed:
   ```bash
   node --version
   npm --version
   npx --version
   ```
   All of these commands should return version numbers. If they don't, please install Node.js.

3. Set up your API keys:
   - Set OpenAI API Key as an environment variable:
     ```bash
     export OPENAI_API_KEY=your-openai-api-key
     ```
   - GitHub token will be entered directly in the app interface

4. Create a GitHub Personal Access Token:
   - Visit https://github.com/settings/tokens
   - Create a new token with `repo` and `user` scopes
   - Save the token somewhere secure

### Running the App

1. Start the Streamlit app:
   ```bash
   streamlit run cookbook/examples/streamlit_apps/github_mcp_agent/app.py
   ```

2. In the app interface:
   - Enter your GitHub token in the sidebar
   - Specify a repository to analyze
   - Select a query type or write your own
   - Click "Run Query"

### Example Queries

#### Issues
- "Show me issues by label"
- "What issues are being actively discussed?"
- "Find issues labeled as bugs"

#### Pull Requests
- "What PRs need review?"
- "Show me recent merged PRs"
- "Find PRs with conflicts"

#### Repository
- "Show repository health metrics"
- "Show repository activity patterns"
- "Analyze code quality trends"


================================================
FILE: cookbook/examples/streamlit_apps/github_mcp_agent/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/github_mcp_agent/agents.py
================================================
import os
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


async def run_github_agent(message):
    if not os.getenv("GITHUB_TOKEN"):
        return "Error: GitHub token not provided"

    try:
        server_params = StdioServerParameters(
            command="npx",
            args=["-y", "@modelcontextprotocol/server-github"],
        )

        # Create client session
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # Initialize MCP toolkit
                mcp_tools = MCPTools(session=session)
                await mcp_tools.initialize()

                # Create agent
                agent = Agent(
                    tools=[mcp_tools],
                    instructions=dedent("""\
                        You are a GitHub assistant. Help users explore repositories and their activity.
                        - Provide organized, concise insights about the repository
                        - Focus on facts and data from the GitHub API
                        - Use markdown formatting for better readability
                        - Present numerical data in tables when appropriate
                        - Include links to relevant GitHub pages when helpful
                    """),
                    markdown=True,
                    show_tool_calls=True,
                )

                # Run agent
                response = await agent.arun(message)
                return response.content
    except Exception as e:
        return f"Error: {str(e)}"



================================================
FILE: cookbook/examples/streamlit_apps/github_mcp_agent/app.py
================================================
import asyncio
import os

import streamlit as st
from agents import run_github_agent

# Page config
st.set_page_config(page_title="🐙 GitHub MCP Agent", page_icon="🐙", layout="wide")

# Title and description
st.markdown("<h1 class='main-header'>🐙 GitHub MCP Agent</h1>", unsafe_allow_html=True)
st.markdown(
    "Explore GitHub repositories with natural language using the Model Context Protocol"
)

# Setup sidebar for API key
with st.sidebar:
    st.header("🔑 Authentication")
    github_token = st.text_input(
        "GitHub Token",
        type="password",
        help="Create a token with repo scope at github.com/settings/tokens",
    )

    if github_token:
        os.environ["GITHUB_TOKEN"] = github_token

    st.markdown("---")
    st.markdown("### Example Queries")

    st.markdown("**Issues**")
    st.markdown("- Show me issues by label")
    st.markdown("- What issues are being actively discussed?")

    st.markdown("**Pull Requests**")
    st.markdown("- What PRs need review?")
    st.markdown("- Show me recent merged PRs")

    st.markdown("**Repository**")
    st.markdown("- Show repository health metrics")
    st.markdown("- Show repository activity patterns")

    st.markdown("---")
    st.caption(
        "Note: Always specify the repository in your query if not already selected in the main input."
    )

# Query input
col1, col2 = st.columns([3, 1])
with col1:
    repo = st.text_input("Repository", value="agno-agi/agno", help="Format: owner/repo")
with col2:
    query_type = st.selectbox(
        "Query Type", ["Issues", "Pull Requests", "Repository Activity", "Custom"]
    )

# Create predefined queries based on type
if query_type == "Issues":
    query_template = f"Find issues labeled as bugs in {repo}"
elif query_type == "Pull Requests":
    query_template = f"Show me recent merged PRs in {repo}"
elif query_type == "Repository Activity":
    query_template = f"Analyze code quality trends in {repo}"
else:
    query_template = ""

query = st.text_area(
    "Your Query",
    value=query_template,
    placeholder="What would you like to know about this repository?",
)

# Run button
if st.button("🚀 Run Query", type="primary", use_container_width=True):
    if not github_token:
        st.error("Please enter your GitHub token in the sidebar")
    elif not query:
        st.error("Please enter a query")
    else:
        with st.spinner("Analyzing GitHub repository..."):
            # Ensure the repository is explicitly mentioned in the query
            if repo and repo not in query:
                full_query = f"{query} in {repo}"
            else:
                full_query = query

            result = asyncio.run(run_github_agent(full_query))

        # Display results in a nice container
        st.markdown("### Results")
        st.markdown(result)

# Display help text for first-time users
if "result" not in locals():
    st.markdown(
        """<div class='info-box'>
        <h4>How to use this app:</h4>
        <ol>
            <li>Enter your GitHub token in the sidebar</li>
            <li>Specify a repository (e.g., agno-agi/agno)</li>
            <li>Select a query type or write your own</li>
            <li>Click 'Run Query' to see results</li>
        </ol>
        <p><strong>Important Notes:</strong></p>
        <ul>
            <li>The Model Context Protocol (MCP) provides real-time access to GitHub repositories</li>
            <li>Queries work best when they focus on specific aspects like issues, PRs, or repository info</li>
            <li>More specific queries yield better results</li>
            <li>This app requires Node.js to be installed (for the npx command)</li>
        </ul>
        </div>""",
        unsafe_allow_html=True,
    )

# Footer
st.markdown("---")
st.write("Built with Streamlit, Agno, and Model Context Protocol ❤️")



================================================
FILE: cookbook/examples/streamlit_apps/github_mcp_agent/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/github_mcp_agent/requirements.in
================================================
streamlit
agno
mcp
openai
asyncio


================================================
FILE: cookbook/examples/streamlit_apps/github_mcp_agent/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.6.0
    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anyio==4.8.0
    # via
    #   httpx
    #   mcp
    #   openai
    #   sse-starlette
    #   starlette
asyncio==3.4.3
    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in
attrs==25.1.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via streamlit
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.1
    # via requests
click==8.1.8
    # via
    #   streamlit
    #   typer
    #   uvicorn
distro==1.9.0
    # via openai
docstring-parser==0.16
    # via agno
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
h11==0.14.0
    # via
    #   httpcore
    #   uvicorn
httpcore==1.0.7
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   mcp
    #   openai
httpx-sse==0.4.0
    # via mcp
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.8.2
    # via openai
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2024.10.1
    # via jsonschema
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mcp==1.3.0
    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in
mdurl==0.1.2
    # via markdown-it-py
narwhals==1.29.1
    # via altair
numpy==2.2.3
    # via
    #   pandas
    #   pydeck
    #   streamlit
openai==1.65.4
    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in
packaging==24.2
    # via
    #   altair
    #   streamlit
pandas==2.2.3
    # via streamlit
pillow==11.1.0
    # via streamlit
protobuf==5.29.3
    # via streamlit
pyarrow==19.0.1
    # via streamlit
pydantic==2.10.6
    # via
    #   agno
    #   mcp
    #   openai
    #   pydantic-settings
pydantic-core==2.27.2
    # via pydantic
pydantic-settings==2.8.1
    # via
    #   agno
    #   mcp
pydeck==0.9.1
    # via streamlit
pygments==2.19.1
    # via rich
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.0.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.1
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via streamlit
rich==13.9.4
    # via
    #   agno
    #   typer
rpds-py==0.23.1
    # via
    #   jsonschema
    #   referencing
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anyio
    #   openai
sse-starlette==2.2.1
    # via mcp
starlette==0.46.0
    # via
    #   mcp
    #   sse-starlette
streamlit==1.43.0
    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in
tenacity==9.0.0
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.15.2
    # via agno
typing-extensions==4.12.2
    # via
    #   agno
    #   altair
    #   anyio
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   streamlit
    #   typer
tzdata==2025.1
    # via pandas
urllib3==2.3.0
    # via requests
uvicorn==0.34.0
    # via mcp



================================================
FILE: cookbook/examples/streamlit_apps/github_repo_analyzer/README.md
================================================
# GitHub Repository Analyzer

This application provides a chat-based interface to interact with and analyze GitHub repositories using the Agno framework and OpenAI models. Users can select a repository and ask questions about its code, issues, pull requests, statistics, and more.

## Features

- **Chat Interface:** Interact with an AI agent knowledgeable about a selected GitHub repository.
- **Repository Selection:** Choose from a predefined list of popular open-source repositories or potentially add your own (requires code modification or environment setup).
- **Comprehensive Analysis:** Ask about:
  - Repository statistics (stars, forks, languages).
  - Open/Closed issues and pull requests.
  - Detailed pull request information, including code changes (diff/patch analysis).
  - File contents and directory structures.
  - Code searching within the repository.
- **Powered by Agno & OpenAI:** Leverages the `agno` framework for agent creation and tool usage.

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install dependencies

```shell
pip install -r cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.txt
```

### 3. Export API Keys

Export the API keys:

```shell
export OPENAI_API_KEY=***
export GITHUB_ACCESS_TOKEN=**
```

### 4. Run the app

```shell
streamlit run cookbook/examples/streamlit_apps/github_repo_analyzer/app.py
```

Navigate to the URL provided by Streamlit (usually `http://localhost:8501`) in your web browser. Select a repository from the sidebar and start chatting!

## Project Structure

The project uses a streamlined structure with all functionality in a single file:

```
github-repo-analyzer/
├── app.py            # Main application with all functionality
├── agent.py          # Agent initialization
├── requirements.txt  # Dependencies
├── README.md         # Documentation
└── output/           # Generated analysis reports
```

## Technologies Used

- [Agno](https://docs.agno.com) - AI agent framework for GitHub analysis
- [Streamlit](https://streamlit.io/) - Interactive web interface
- [PyGithub](https://pygithub.readthedocs.io/) - GitHub API access



================================================
FILE: cookbook/examples/streamlit_apps/github_repo_analyzer/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/github_repo_analyzer/agents.py
================================================
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.github import GithubTools


def get_github_agent(debug_mode: bool = True) -> Optional[Agent]:
    """
    Args:
        repo_name: Optional repository name ("owner/repo"). If None, agent relies on user query.
        debug_mode: Whether to enable debug mode for tool calls.
    """

    return Agent(
        model=OpenAIChat(id="gpt-4.1"),
        description=dedent("""
            You are an expert Code Reviewing Agent specializing in analyzing GitHub repositories,
            with a strong focus on detailed code reviews for Pull Requests.
            Use your tools to answer questions accurately and provide insightful analysis.
        """),
        instructions=dedent(f"""\
        **Core Task:** Analyze GitHub repositories and answer user questions based on the available tools and conversation history.

        **Repository Context Management:**
        1.  **Context Persistence:** Once a target repository (owner/repo) is identified (either initially or from a user query like 'analyze owner/repo'), **MAINTAIN THAT CONTEXT** for all subsequent questions in the current conversation unless the user clearly specifies a *different* repository.
        2.  **Determining Context:** If no repository is specified in the *current* user query, **CAREFULLY REVIEW THE CONVERSATION HISTORY** to find the most recently established target repository. Use that repository context.
        3.  **Accuracy:** When extracting a repository name (owner/repo) from the query or history, **BE EXTREMELY CAREFUL WITH SPELLING AND FORMATTING**. Double-check against the user's exact input.
        4.  **Ambiguity:** If no repository context has been established in the conversation history and the current query doesn't specify one, **YOU MUST ASK THE USER** to clarify which repository (using owner/repo format) they are interested in before using tools that require a repository name.

        **How to Answer Questions:**
        *   **Identify Key Information:** Understand the user's goal and the target repository (using the context rules above).
        *   **Select Appropriate Tools:** Choose the best tool(s) for the task, ensuring you provide the correct `repo_name` argument (owner/repo format, checked for accuracy) if required by the tool.
            *   Project Overview: `get_repository`, `get_file_content` (for README.md).
            *   Libraries/Dependencies: `get_file_content` (for requirements.txt, pyproject.toml, etc.), `get_directory_content`, `search_code`.
            *   PRs/Issues: Use relevant PR/issue tools.
            *   List User Repos: `list_repositories` (no repo_name needed).
            *   Search Repos: `search_repositories` (no repo_name needed).
        *   **Execute Tools:** Run the selected tools.
        *   **Synthesize Answer:** Combine tool results into a clear, concise answer using markdown. If a tool fails (e.g., 404 error because the repo name was incorrect), state that you couldn't find the specified repository and suggest checking the name.
        *   **Cite Sources:** Mention specific files (e.g., "According to README.md...").

        **Specific Analysis Areas (Most require a specific repository):**
        *   Issues: Listing, summarizing, searching.
        *   Pull Requests (PRs): Listing, summarizing, searching, getting details/changes.
        *   Code & Files: Searching code, getting file content, listing directory contents.
        *   Repository Stats & Activity: Stars, contributors, recent activity.

        **Code Review Guidelines (Requires repository and PR):**
        *   Fetch Changes: Use `get_pull_request_changes` or `get_pull_request_with_details`.
        *   Analyze Patch: Evaluate based on functionality, best practices, style, clarity, efficiency.
        *   Present Review: Structure clearly, cite lines/code, be constructive.
        """),
        tools=[
            GithubTools(
                get_repository=True,
                search_repositories=True,
                get_pull_request=True,
                get_pull_request_changes=True,
                list_branches=True,
                get_pull_request_count=True,
                get_pull_requests=True,
                get_pull_request_comments=True,
                get_pull_request_with_details=True,
                list_issues=True,
                get_issue=True,
                update_file=True,
                get_file_content=True,
                get_directory_content=True,
                search_code=True,
            ),
        ],
        markdown=True,
        debug_mode=debug_mode,
        add_history_to_messages=True,
    )



================================================
FILE: cookbook/examples/streamlit_apps/github_repo_analyzer/app.py
================================================
from os import getenv

import nest_asyncio
import streamlit as st
from agents import get_github_agent
from agno.agent import Agent
from agno.utils.log import logger
from utils import (
    CUSTOM_CSS,
    about_widget,
    add_message,
    display_tool_calls,
    sidebar_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="GitHub Repo Analyzer",
    page_icon="👨‍💻",
    layout="wide",
)

# Load custom CSS with dark mode support
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def main() -> None:
    #####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-header'>👨‍💻 GitHub Repo Analyzer</h1>", unsafe_allow_html=True
    )
    st.markdown("Analyze GitHub repositories")

    ####################################################################
    # Initialize Agent
    ####################################################################
    github_agent: Agent
    if (
        "github_agent" not in st.session_state
        or st.session_state["github_agent"] is None
    ):
        logger.info("---*--- Creating new Github agent ---*---")
        github_agent = get_github_agent()
        st.session_state["github_agent"] = github_agent
        st.session_state["messages"] = []
        st.session_state["github_token"] = getenv("GITHUB_ACCESS_TOKEN")
    else:
        github_agent = st.session_state["github_agent"]

    ####################################################################
    # Load Agent Session from the database
    ####################################################################
    try:
        st.session_state["github_agent_session_id"] = github_agent.load_session()
    except Exception:
        st.warning("Could not create Agent session, is the database running?")
        return

    ####################################################################
    # Load runs from memory (v2 Memory) only on initial load
    ####################################################################
    if github_agent.memory is not None and not st.session_state.get("messages"):
        session_id = st.session_state.get("github_agent_session_id")
        # Fetch stored runs for this session
        agent_runs = github_agent.memory.get_runs(session_id)
        if agent_runs:
            logger.debug("Loading run history")
            st.session_state["messages"] = []
            for run_response in agent_runs:
                # Iterate through stored messages in the run
                for msg in run_response.messages or []:
                    if msg.role in ["user", "assistant"] and msg.content is not None:
                        # Include any tool calls attached to this message
                        add_message(
                            msg.role, msg.content, getattr(msg, "tool_calls", None)
                        )
        else:
            logger.debug("No run history found")
            st.session_state["messages"] = []

    ####################################################################
    # Sidebar
    ####################################################################
    sidebar_widget()

    ####################################################################
    # Get user input
    ####################################################################
    if prompt := st.chat_input("👋 Ask me about GitHub repositories!"):
        add_message("user", prompt)

    ####################################################################
    # Display chat history
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] in ["user", "assistant"]:
            _content = message["content"]
            if _content is not None:
                with st.chat_message(message["role"]):
                    # Display tool calls if they exist in the message
                    if "tool_calls" in message and message["tool_calls"]:
                        display_tool_calls(st.empty(), message["tool_calls"])
                    st.markdown(_content)

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        with st.chat_message("assistant"):
            # Create container for tool calls
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner("🤔 Thinking..."):
                response = ""
                try:
                    # Run the agent and stream the response
                    run_response = github_agent.run(
                        question, stream=True, stream_intermediate_steps=True
                    )
                    for _resp_chunk in run_response:
                        # Display tool calls if available
                        if hasattr(_resp_chunk, "tool") and _resp_chunk.tool:
                            display_tool_calls(tool_calls_container, [_resp_chunk.tool])

                        # Display response if available and event is RunResponse
                        if (
                            _resp_chunk.event == "RunResponse"
                            and _resp_chunk.content is not None
                        ):
                            response += _resp_chunk.content
                            resp_container.markdown(response)

                    add_message("assistant", response, github_agent.run_response.tools)
                except Exception as e:
                    logger.exception(e)
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    add_message("assistant", error_message)
                    st.error(error_message)

    ####################################################################
    # About section
    ####################################################################
    about_widget()


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/github_repo_analyzer/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.in
================================================
# Direct dependencies for the GitHub Repo Chat App
agno>=0.1.0
PyGithub>=2.1.1
python-dotenv>=1.0.0
matplotlib>=3.7.2
pandas>=2.0.3
streamlit>=1.24.0
openai>=1.67.0



================================================
FILE: cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.6.0
    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anyio==4.9.0
    # via
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via streamlit
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
cffi==1.17.1
    # via
    #   cryptography
    #   pynacl
charset-normalizer==3.4.1
    # via requests
click==8.1.8
    # via
    #   streamlit
    #   typer
contourpy==1.3.2
    # via matplotlib
cryptography==44.0.2
    # via pyjwt
cycler==0.12.1
    # via matplotlib
deprecated==1.2.18
    # via pygithub
distro==1.9.0
    # via openai
docstring-parser==0.16
    # via agno
fonttools==4.57.0
    # via matplotlib
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
h11==0.14.0
    # via httpcore
httpcore==1.0.8
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.9.0
    # via openai
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2024.10.1
    # via jsonschema
kiwisolver==1.4.8
    # via matplotlib
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
matplotlib==3.10.1
    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in
mdurl==0.1.2
    # via markdown-it-py
narwhals==1.35.0
    # via altair
numpy==2.2.4
    # via
    #   contourpy
    #   matplotlib
    #   pandas
    #   pydeck
    #   streamlit
openai==1.75.0
    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in
packaging==24.2
    # via
    #   altair
    #   matplotlib
    #   streamlit
pandas==2.2.3
    # via
    #   -r cookbook/examples/apps/github_repo_analyzer/requirements.in
    #   streamlit
pillow==11.2.1
    # via
    #   matplotlib
    #   streamlit
protobuf==5.29.4
    # via streamlit
pyarrow==19.0.1
    # via streamlit
pycparser==2.22
    # via cffi
pydantic==2.11.3
    # via
    #   agno
    #   openai
    #   pydantic-settings
pydantic-core==2.33.1
    # via pydantic
pydantic-settings==2.8.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygithub==2.6.1
    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in
pygments==2.19.1
    # via rich
pyjwt==2.10.1
    # via pygithub
pynacl==1.5.0
    # via pygithub
pyparsing==3.2.3
    # via matplotlib
python-dateutil==2.9.0.post0
    # via
    #   matplotlib
    #   pandas
python-dotenv==1.1.0
    # via
    #   -r cookbook/examples/apps/github_repo_analyzer/requirements.in
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via
    #   pygithub
    #   streamlit
rich==14.0.0
    # via
    #   agno
    #   typer
rpds-py==0.24.0
    # via
    #   jsonschema
    #   referencing
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anyio
    #   openai
streamlit==1.44.1
    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in
tenacity==9.1.2
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.15.2
    # via agno
typing-extensions==4.13.2
    # via
    #   agno
    #   altair
    #   anyio
    #   openai
    #   pydantic
    #   pydantic-core
    #   pygithub
    #   referencing
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.0
    # via pydantic
tzdata==2025.2
    # via pandas
urllib3==2.4.0
    # via
    #   pygithub
    #   requests
wrapt==1.17.2
    # via deprecated



================================================
FILE: cookbook/examples/streamlit_apps/github_repo_analyzer/utils.py
================================================
import json
from typing import Any, Dict, List, Optional

import streamlit as st
from agno.models.response import ToolExecution
from agno.utils.log import log_debug, log_error, log_info


def add_message(
    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None
) -> None:
    """Safely add a message to the session state"""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append(
        {"role": role, "content": content, "tool_calls": tool_calls}
    )


def sidebar_widget() -> None:
    """Renders the sidebar for configuration and example queries."""
    with st.sidebar:
        # Configuration
        st.header("Configuration")

        st.markdown("**GitHub Token**")
        token_input = st.text_input(
            "Enter your GitHub Personal Access Token (required for most queries):",
            type="password",
            key="github_token_input",
            value=st.session_state.get("github_token", ""),
            help="Allows the agent to access GitHub API, including your private/org data.",
        )
        st.markdown(
            "[How to create a GitHub PAT?](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic)",
            unsafe_allow_html=True,
        )

        # Update session state if token input changes
        current_token_in_state = st.session_state.get("github_token")
        if token_input != current_token_in_state and (
            token_input or current_token_in_state is not None
        ):
            st.session_state.github_token = token_input if token_input else None
            log_info(
                f"GitHub token updated via sidebar input {'(cleared)' if not token_input else ''}."
            )
            st.session_state.github_agent = None
            st.rerun()

        st.markdown("---")

        st.markdown("#### 🏆 Sample Queries")
        if st.button("📋 Summarize 'agno-agi/agno'"):
            # Run this query in the current session
            add_message("user", "Summarize 'agno-agi/agno' repo")
        if st.button("🥇 List my recent repositories"):
            add_message("user", "List my recent repositories")
        if st.button("🏆 List latest issues in 'agno-agi/agno' "):
            add_message("user", "List latest issues in 'agno-agi/agno'")
        if st.button("🥇 List recent PRs in 'agno-agi/agno'"):
            add_message("user", "List recent PRs in 'agno-agi/agno' repo")
        # Chat controls
        st.header("Chat")
        if st.button("🆕 New Chat"):
            # Use restart logic to clear everything and rerun
            restart_agent()


def about_widget() -> None:
    """Display an about section in the sidebar"""
    with st.sidebar:
        st.markdown("### About Agno ✨")
        st.markdown("""
        Agno is a lightweight library for building Reasoning Agents.

        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)
        """)

        st.markdown("### Need Help?")
        st.markdown(
            "If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community)."
        )


def is_json(myjson):
    """Check if a string is valid JSON"""
    try:
        json.loads(myjson)
    except (ValueError, TypeError):
        return False
    return True


def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):
    """Display tool calls in a streamlit container with expandable sections.

    Args:
        tool_calls_container: Streamlit container to display the tool calls
        tools: List of tool call dictionaries containing name, args, content, and metrics
    """
    try:
        with tool_calls_container.container():
            for tool_call in tools:
                tool_name = tool_call.tool_name or "Unknown Tool"
                tool_args = tool_call.tool_args or {}
                content = tool_call.result or None
                metrics = tool_call.metrics or None

                # Add timing information
                execution_time_str = "N/A"
                try:
                    if metrics is not None and hasattr(metrics, "time"):
                        execution_time = metrics.time
                        if execution_time is not None:
                            execution_time_str = f"{execution_time:.4f}s"
                except Exception as e:
                    log_error(f"Error displaying tool calls: {str(e)}")
                    pass

                with st.expander(
                    f"🛠️ {tool_name.replace('_', ' ').title()} ({execution_time_str})",
                    expanded=False,
                ):
                    # Show query with syntax highlighting
                    if isinstance(tool_args, dict) and "query" in tool_args:
                        st.code(tool_args["query"], language="sql")

                    # Display arguments in a more readable format
                    if tool_args and tool_args != {"query": None}:
                        st.markdown("**Arguments:**")
                        st.json(tool_args)

                    if content is not None:
                        try:
                            if is_json(content):
                                st.markdown("**Results:**")
                                st.json(content)
                        except Exception as e:
                            log_debug(f"Skipped tool call content: {e}")
    except Exception as e:
        log_error(f"Error displaying tool calls: {str(e)}")
        tool_calls_container.error("Failed to display tool results")


def restart_agent():
    """Reset the agent and clear chat history"""
    log_debug("---*--- Restarting agent ---*---")
    st.session_state["sql_agent"] = None
    st.session_state["sql_agent_session_id"] = None
    st.session_state["messages"] = []
    st.session_state["github_agent"] = None
    st.rerun()


# Keep only necessary CSS styles
CUSTOM_CSS = """
<style>
    .main-header {
        font-size: 2.5rem;
        margin-bottom: 1rem;
        color: #0366d6;
        font-weight: 600;
    }
    .sub-header {
        font-size: 1.5rem;
        margin-top: 1rem;
        margin-bottom: 0.5rem;
        color: #2f363d;
        font-weight: 500;
    }
    .metric-card {
        background-color: #f6f8fa;
        border-radius: 8px;
        padding: 1.2rem;
        margin-bottom: 1rem;
        border-left: 5px solid #0366d6;
    }
    .pr-card {
        background-color: #f1f8ff;
        border-radius: 8px;
        padding: 1.2rem;
        margin-bottom: 1.2rem;
        border-left: 5px solid #6f42c1;
    }
</style>
"""



================================================
FILE: cookbook/examples/streamlit_apps/image_generation/README.md
================================================
# Recipe Image Generator 🍳

Recipe Image Generator is an interactive Streamlit application that leverages Agno to generate step-by-step cooking images from recipes. Upload your own recipe PDF or use the built-in sample recipe book, then ask for a recipe (e.g., "Recipe for Pad Thai") and watch the app generate visual cooking instructions.


---

## 🚀 Setup Instructions

> Note: Fork and clone the repository if needed.

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install dependencies

```shell
pip install -r cookbook/examples/streamlit_apps/image_generation/requirements.txt
```

### 3. Export API Keys

This app uses the Llama family of models offer via Groq

```shell
export GROQ_API_KEY=***       # for Groq models
```

### 4. Run the Recipe Image Generator

```shell
streamlit run cookbook/examples/streamlit_apps/image_generation/app.py
```

- Open [http://localhost:8501](http://localhost:8501) in your browser to view the app.

---

## 🎯 Features

- **Recipe Upload**: Upload a PDF of your favorite recipes or use the default sample recipe book.
- **Interactive Chat**: Ask for a recipe by name and receive a streamed, step-by-step visual guide.
- **Example Recipes**: Quick-start buttons for common recipes like Pad Thai, Som Tum, Massaman Curry, and Tom Kha Gai.
- **Tool Call Visualization**: View intermediate tool calls and results within the chat interface.
- **Image Rendering**: Inline display of generated images or fallback on URLs.

---

## 🛠 How to Use

1. **Select Model**: Use the sidebar dropdown to pick a model.
2. **Load Recipes**: Click "Load recipes" (optional) to preload sample recipes.
3. **Try Examples**: Click an example recipe in the sidebar under "Try an example recipe".
4. **Upload PDF**: Upload your own recipe PDF file or check "Use default sample recipe book".
5. **Chat**: Type your request in the chat input (e.g., "Recipe for Pad Thai").
6. **View Images**: Scroll through the chat to see the generated images and instructions.

---

## 💬 Support

Join our [Discord community](https://agno.link/discord) for questions and discussion.




================================================
FILE: cookbook/examples/streamlit_apps/image_generation/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/image_generation/agents.py
================================================
# Recipe agent for image generation
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.document.reader.pdf_reader import PDFImageReader
from agno.embedder.cohere import CohereEmbedder
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.groq import Groq
from agno.tools.openai import OpenAITools
from agno.vectordb.pgvector import PgVector

# Database connection string for recipe knowledge base
# Adjust as needed for your environment
DB_URL = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Constants for recipe agent
DEFAULT_RECIPE_URL = "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
DEFAULT_RECIPE_TABLE = "recipe_documents"


def get_recipe_agent(
    local_pdf_path: Optional[str] = None,
) -> Agent:
    """
    Returns a RecipeImageAgent backed by a recipe PDF knowledge base.
    """
    # Choose the appropriate knowledge base
    if local_pdf_path:
        knowledge_base = PDFKnowledgeBase(
            path=local_pdf_path,
            reader=PDFImageReader(),
            vector_db=PgVector(
                db_url=DB_URL,
                table_name=DEFAULT_RECIPE_TABLE,
                embedder=CohereEmbedder(id="embed-v4.0"),
            ),
        )
    else:
        knowledge_base = PDFUrlKnowledgeBase(
            urls=[DEFAULT_RECIPE_URL],
            vector_db=PgVector(
                db_url=DB_URL,
                table_name=DEFAULT_RECIPE_TABLE,
                embedder=CohereEmbedder(id="embed-v4.0"),
            ),
        )

    model = Groq(id="meta-llama/llama-4-scout-17b-16e-instruct")

    # Instantiate and return the recipe agent
    return Agent(
        name="RecipeImageAgent",
        model=model,
        knowledge=knowledge_base,
        tools=[OpenAITools(image_model="gpt-image-1")],
        instructions=[
            dedent("""\
            You are a specialized recipe assistant.
            When asked for a recipe:
            1. Use the `search_knowledge_base` tool to find and load the most relevant recipe from the knowledge base.
            2. Extract and output exactly two formatted markdown sections:
               ## Ingredients
               - List each ingredient with a hyphen and space prefix.
               ## Directions
               1. Describe each cooking step succinctly, numbering steps starting at 1.
            3. After listing the Directions, invoke the `generate_image` tool exactly once, passing the entire recipe text and using a prompt like '<DishName>: a step-by-step visual guide showing all steps in one overhead image with bright natural lighting. In the prompt make sure to include the all the recipe ingredients and directions that were listed in the Ingredients and Directions sections.'.
            4. Maintain a consistent visual style across the image.
            5. After the image is generated, conclude with 'Recipe generation complete.'
        """),
        ],
        markdown=True,
        debug_mode=True,
    )



================================================
FILE: cookbook/examples/streamlit_apps/image_generation/app.py
================================================
import base64
import io

import nest_asyncio
import streamlit as st
from agents import get_recipe_agent
from agno.utils.log import logger
from PIL import Image
from utils import (
    CUSTOM_CSS,
    about_widget,
    add_message,
    display_tool_calls,
    example_inputs,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="Recipe Image Generator",
    page_icon="🍳",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Load custom CSS with dark mode support
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def main() -> None:
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>Recipe Image Generator</h1>",
        unsafe_allow_html=True,
    )
    st.markdown(
        "<p class='subtitle'>Upload your recipe PDF or use the default. Ask for a recipe and receive step-by-step images!</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    model_options = {
        "llama-4-scout": "groq:meta-llama/llama-4-scout-17b-16e-instruct",
    }
    selected_model = st.sidebar.selectbox(
        "Select a model",
        options=list(model_options.keys()),
        index=0,
        key="model_selector",
    )
    model_id = model_options[selected_model]

    example_inputs()
    ####################################################################
    # Recipe source selector & Agent initialization
    ####################################################################
    uploaded_file = st.sidebar.file_uploader("Upload recipe PDF", type=["pdf"])
    use_default = st.sidebar.checkbox(
        "Use default sample recipe book", value=(uploaded_file is None)
    )
    pdf_path = None
    if uploaded_file:
        import tempfile

        tf = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
        tf.write(uploaded_file.read())
        tf.flush()
        pdf_path = tf.name
    if use_default:
        pdf_path = None

    if (
        "recipe_agent" not in st.session_state
        or st.session_state.get("pdf_path") != pdf_path
        or st.session_state.get("current_model") != model_id
    ):
        logger.info("---*--- Creating new Recipe agent ---*---")
        recipe_agent = get_recipe_agent(
            local_pdf_path=pdf_path,
        )
        st.session_state["recipe_agent"] = recipe_agent
        st.session_state["pdf_path"] = pdf_path
        st.session_state["current_model"] = model_id
    else:
        recipe_agent = st.session_state["recipe_agent"]

    # Track knowledge load state
    if "knowledge_loaded" not in st.session_state:
        st.session_state["knowledge_loaded"] = False

    # Manual load button
    if st.sidebar.button("Load recipes"):
        st.sidebar.info("Loading default recipes...")
        recipe_agent.knowledge.load(recreate=True)
        st.session_state["knowledge_loaded"] = True
        st.sidebar.success("Recipes loaded!")

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    ####################################################################
    # Get user input
    ####################################################################
    if prompt := st.chat_input("👋 Ask me for a recipe (e.g., 'Recipe for Pad Thai')"):
        add_message("user", prompt)

    ####################################################################
    # Display chat history
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] in ["user", "assistant"]:
            _content = message["content"]
            if _content is not None:
                with st.chat_message(message["role"]):
                    # Display tool calls if they exist in the message
                    if "tool_calls" in message and message["tool_calls"]:
                        display_tool_calls(st.empty(), message["tool_calls"])
                    st.markdown(_content)

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        # Auto-load knowledge if needed
        if not st.session_state.get("knowledge_loaded", False):
            info = st.info("Loading default recipes...")
            recipe_agent.knowledge.load(recreate=True)
            st.session_state["knowledge_loaded"] = True
            info.empty()
        with st.chat_message("assistant"):
            # Create container for tool calls
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner("🤔 Thinking..."):
                response = ""
                try:
                    # Run the agent and stream the response
                    run_response = recipe_agent.run(
                        question, stream=True, stream_intermediate_steps=True
                    )
                    for _resp_chunk in run_response:
                        # Display tool calls if available
                        if hasattr(_resp_chunk, "tool") and _resp_chunk.tool:
                            display_tool_calls(tool_calls_container, [_resp_chunk.tool])

                        # Display response if available and event is RunResponse
                        if (
                            _resp_chunk.event == "RunResponse"
                            and _resp_chunk.content is not None
                        ):
                            response += _resp_chunk.content
                            resp_container.markdown(response)

                    # Display generated images
                    for img in recipe_agent.run_response.images or []:
                        # Inline base64 content
                        if getattr(img, "content", None):
                            try:
                                # img.content is base64-encoded bytes
                                decoded = base64.b64decode(img.content)
                                image = Image.open(io.BytesIO(decoded))
                                resp_container.image(image)
                            except Exception as e:
                                logger.error(f"Failed to render inline image: {e}")
                                # Fallback to URL if available
                                if getattr(img, "url", None):
                                    resp_container.image(img.url)
                        # URL fallback
                        elif getattr(img, "url", None):
                            resp_container.image(img.url)
                    add_message("assistant", response, recipe_agent.run_response.tools)
                except Exception as e:
                    logger.exception(e)
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    add_message("assistant", error_message)
                    st.error(error_message)

    ####################################################################
    # About section
    ####################################################################
    about_widget()


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/image_generation/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/image_generation/requirements.in
================================================
agno
cohere
groq
httpx
nest_asyncio
openai
pypdf
pgvector
psycopg[binary]
rapidocr-onnxruntime
sqlalchemy
streamlit


================================================
FILE: cookbook/examples/streamlit_apps/image_generation/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.6.0
    # via -r cookbook/examples/apps/image_generation/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anyio==4.9.0
    # via
    #   groq
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via streamlit
certifi==2025.4.26
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.2
    # via requests
click==8.1.8
    # via
    #   streamlit
    #   typer
cohere==5.15.0
    # via -r cookbook/examples/apps/image_generation/requirements.in
coloredlogs==15.0.1
    # via onnxruntime
distro==1.9.0
    # via
    #   groq
    #   openai
docstring-parser==0.16
    # via agno
fastavro==1.10.0
    # via cohere
filelock==3.18.0
    # via huggingface-hub
flatbuffers==25.2.10
    # via onnxruntime
fsspec==2025.3.2
    # via huggingface-hub
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
groq==0.24.0
    # via -r cookbook/examples/apps/image_generation/requirements.in
h11==0.16.0
    # via httpcore
hf-xet==1.1.0
    # via huggingface-hub
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via
    #   -r cookbook/examples/apps/image_generation/requirements.in
    #   agno
    #   cohere
    #   groq
    #   openai
httpx-sse==0.4.0
    # via cohere
huggingface-hub==0.31.1
    # via tokenizers
humanfriendly==10.0
    # via coloredlogs
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.9.0
    # via openai
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2025.4.1
    # via jsonschema
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
mpmath==1.3.0
    # via sympy
narwhals==1.38.2
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/apps/image_generation/requirements.in
numpy==2.2.5
    # via
    #   onnxruntime
    #   opencv-python
    #   pandas
    #   pgvector
    #   pydeck
    #   rapidocr-onnxruntime
    #   shapely
    #   streamlit
onnxruntime==1.22.0
    # via rapidocr-onnxruntime
openai==1.78.0
    # via -r cookbook/examples/apps/image_generation/requirements.in
opencv-python==4.11.0.86
    # via rapidocr-onnxruntime
packaging==24.2
    # via
    #   altair
    #   huggingface-hub
    #   onnxruntime
    #   streamlit
pandas==2.2.3
    # via streamlit
pgvector==0.4.1
    # via -r cookbook/examples/apps/image_generation/requirements.in
pillow==11.2.1
    # via
    #   rapidocr-onnxruntime
    #   streamlit
protobuf==6.30.2
    # via
    #   onnxruntime
    #   streamlit
psycopg==3.2.7
    # via -r cookbook/examples/apps/image_generation/requirements.in
psycopg-binary==3.2.7
    # via psycopg
pyarrow==20.0.0
    # via streamlit
pyclipper==1.3.0.post6
    # via rapidocr-onnxruntime
pydantic==2.11.4
    # via
    #   agno
    #   cohere
    #   groq
    #   openai
    #   pydantic-settings
pydantic-core==2.33.2
    # via
    #   cohere
    #   pydantic
pydantic-settings==2.9.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.1
    # via rich
pypdf==5.4.0
    # via -r cookbook/examples/apps/image_generation/requirements.in
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.1.0
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via
    #   agno
    #   huggingface-hub
    #   rapidocr-onnxruntime
rapidocr-onnxruntime==1.4.4
    # via -r cookbook/examples/apps/image_generation/requirements.in
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via
    #   cohere
    #   huggingface-hub
    #   streamlit
rich==14.0.0
    # via
    #   agno
    #   typer
rpds-py==0.24.0
    # via
    #   jsonschema
    #   referencing
shapely==2.1.0
    # via rapidocr-onnxruntime
shellingham==1.5.4
    # via typer
six==1.17.0
    # via
    #   python-dateutil
    #   rapidocr-onnxruntime
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anyio
    #   groq
    #   openai
sqlalchemy==2.0.40
    # via -r cookbook/examples/apps/image_generation/requirements.in
streamlit==1.45.0
    # via -r cookbook/examples/apps/image_generation/requirements.in
sympy==1.14.0
    # via onnxruntime
tenacity==9.1.2
    # via streamlit
tokenizers==0.21.1
    # via cohere
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
tqdm==4.67.1
    # via
    #   huggingface-hub
    #   openai
    #   rapidocr-onnxruntime
typer==0.15.3
    # via agno
types-requests==2.32.0.20250328
    # via cohere
typing-extensions==4.13.2
    # via
    #   agno
    #   altair
    #   anyio
    #   cohere
    #   groq
    #   huggingface-hub
    #   openai
    #   psycopg
    #   pydantic
    #   pydantic-core
    #   referencing
    #   sqlalchemy
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.0
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.4.0
    # via
    #   requests
    #   types-requests



================================================
FILE: cookbook/examples/streamlit_apps/image_generation/utils.py
================================================
import json
from typing import Any, Dict, List, Optional

import streamlit as st
from agno.models.response import ToolExecution
from agno.utils.log import logger


def is_json(myjson: str) -> bool:
    """Check if a string is valid JSON"""
    try:
        json.loads(myjson)
    except (ValueError, TypeError):
        return False
    return True


def add_message(
    role: str,
    content: str,
    tool_calls: Optional[List[Dict[str, Any]]] = None,
) -> None:
    """Safely add a message to the session state"""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append(
        {"role": role, "content": content, "tool_calls": tool_calls}
    )


def display_tool_calls(tool_calls_container: Any, tools: List[ToolExecution]) -> None:
    """Display tool calls in a Streamlit container"""
    try:
        with tool_calls_container.container():
            for tool_call in tools:
                tool_name = tool_call.tool_name or "Unknown Tool"
                tool_args = tool_call.tool_args or {}
                content = tool_call.result or None
                metrics = tool_call.metrics or None

                execution_time_str = "N/A"
                if metrics is not None and hasattr(metrics, "time"):
                    t = metrics.time
                    execution_time_str = f"{t:.4f}s" if t else execution_time_str

                with st.expander(
                    f"🛠️ {tool_name.replace('_', ' ').title()} ({execution_time_str})",
                    expanded=False,
                ):
                    if isinstance(tool_args, dict) and "query" in tool_args:
                        st.code(tool_args["query"], language="sql")
                    if tool_args and tool_args != {"query": None}:
                        st.markdown("**Arguments:**")
                        st.json(tool_args)
                    if content is not None and is_json(content):
                        st.markdown("**Results:**")
                        st.json(content)
    except Exception as e:
        logger.error(f"Error displaying tool calls: {e}")
        tool_calls_container.error("Failed to display tool results")


def about_widget() -> None:
    """Display an about section in the sidebar"""
    with st.sidebar:
        st.markdown("### About Recipe Generator ✨")
        st.markdown(
            "Recipe Image Generator powered by Agno. Upload or use default recipe PDF and get step-by-step visual cooking instructions."
        )


# Added example inputs for recipe generation
def example_inputs() -> None:
    """Show example recipe inputs on the sidebar."""
    with st.sidebar:
        st.markdown("#### :sparkles: Try an example recipe")
        if st.button("Recipe for Pad Thai"):
            add_message("user", "Recipe for Pad Thai")
        if st.button("Recipe for Som Tum"):
            add_message("user", "Recipe for Som Tum / Papaya Salad")
        if st.button("Recipe for Massaman Curry"):
            add_message("user", "Recipe for Massaman Curry / Massaman Gai")
        if st.button("Recipe for Tom Kha Gai"):
            add_message("user", "Recipe for Tom Kha Gai")


CUSTOM_CSS = """
<style>
.main-title {
    text-align: center;
    background: linear-gradient(45deg, #FF4B2B, #FF416C);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    font-size: 3em;
    font-weight: bold;
    padding: 1em 0;
}
.subtitle {
    text-align: center;
    color: #666;
    margin-bottom: 2em;
}
.stButton button {
    width: 100%;
    border-radius: 20px;
    margin: 0.2em 0;
    transition: all 0.3s ease;
}
.stButton button:hover {
    transform: translateY(-2px);
    box-shadow: 0 5px 15px rgba(0,0,0,0.1);
}
.chat-container {
    border-radius: 15px;
    padding: 1em;
    margin: 1em 0;
    background-color: #f5f5f5;
}
.success-message {
    background-color: #d4edda;
    color: #155724;
}
.error-message {
    background-color: #f8d7da;
    color: #721c24;
}
@media (prefers-color-scheme: dark) {
    .chat-container { background-color: #2b2b2b; }
}
</style>
"""



================================================
FILE: cookbook/examples/streamlit_apps/llama_tutor/README.md
================================================
# Llama Tutor: Advanced Educational AI Assistant

Llama Tutor is a powerful educational AI assistant that combines:

- Personalized learning experiences tailored to various education levels
- Real-time information retrieval
- In-depth analysis and explanations

## Features
- 📚 Personalized education at various academic levels
- 🔍 Real-time information retrieval
- 📊 In-depth analysis and explanations
- 🧠 Interactive learning with quizzes and follow-up questions
- 💾 Save lessons for future reference

## Tech stack

- Llama 3.1 70B from Meta for the LLM
- Groq for LLM inference
- DuckDuckGo and Exa for the search API

## Cloning & running

> Note: Fork and clone this repository if needed

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install dependencies

```shell
pip install -r cookbook/examples/streamlit_apps/llama_tutor/requirements.txt
```

### 3. Configure API Keys

Copy .env.example to .env and replace the API keys:
```bash
GROQ_API_KEY=your_groq_key_here
EXA_API_KEY=your_exa_key_here
```
### 4. Run Llama Tutor

```shell
streamlit run cookbook/examples/streamlit_apps/llama_tutor/app.py
```


================================================
FILE: cookbook/examples/streamlit_apps/llama_tutor/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/llama_tutor/agents.py
================================================
"""
Llama tutor integrates:
  - DuckDuckGoTools for real-time web searches.
  - ExaTools for structured, in-depth analysis.
  - FileTools for saving the output upon user confirmation.
"""

import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv(override=True)

# Importing the Agent and model classes
from agno.agent import Agent
from agno.models.groq import Groq

# Importing storage and tool classes
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.file import FileTools

# Import the Agent template
from prompts import AGENT_DESCRIPTION, AGENT_INSTRUCTIONS, EXPECTED_OUTPUT_TEMPLATE

# ************* Setup Paths *************
# Define the current working directory and output directory for saving files
cwd = Path(__file__).parent
output_dir = cwd.joinpath("output")
# Create output directory if it doesn't exist
output_dir.mkdir(parents=True, exist_ok=True)
# Create tmp directory if it doesn't exist
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)
# *************************************

# ************* Agent Storage *************
# Configure SQLite storage for agent sessions
agent_storage = SqliteAgentStorage(
    table_name="answer_engine_sessions",  # Table to store agent sessions
    db_file=str(tmp_dir.joinpath("agents.db")),  # SQLite database file
)
# *************************************


def tutor_agent(
    user_id: Optional[str] = None,
    model_id: str = "groq:llama-3.3-70b-versatile",
    session_id: Optional[str] = None,
    num_history_responses: int = 5,
    debug_mode: bool = True,
    education_level: str = "High School",
) -> Agent:
    """
    Returns an instance of Llama Tutor, an educational AI assistant with integrated tools for web search,
    deep contextual analysis, and file management.

    Llama Tutor will:
      - Use DuckDuckGoTools for real-time web searches and ExaTools for in-depth analysis to gather information.
      - Generate comprehensive educational answers tailored to the specified education level that include:
          • Direct, succinct answers appropriate for the student's level.
          • Detailed explanations with supporting evidence.
          • Examples and clarification of common misconceptions.
          • Interactive elements like questions to check understanding.
      - Prompt the user:
            "Would you like to save this answer to a file? (yes/no)"
        If confirmed, it will use FileTools to save the answer in markdown format in the output directory.

    Args:
        user_id: Optional identifier for the user.
        model_id: Model identifier in the format 'groq:model_name' (e.g., "groq:llama-3.3-70b-versatile").
                 Will always use Groq with a Llama model regardless of provider specified.
        session_id: Optional session identifier for tracking conversation history.
        num_history_responses: Number of previous responses to include for context.
        debug_mode: Enable logging and debug features.
        education_level: Education level for tailoring responses (e.g., "Elementary School", "High School", "College").

    Returns:
        An instance of the configured Agent.
    """

    # Parse model provider and name
    provider, model_name = model_id.split(":")

    # Always use Groq with Llama model
    groq_api_key = os.environ.get("GROQ_API_KEY")

    # Default to llama-3.3-70b-versatile if the model name doesn't contain "llama"
    if "llama" not in model_name.lower():
        model_name = "llama-3.3-70b-versatile"

    model = Groq(id=model_name, api_key=groq_api_key)

    # Get Exa API key from environment variable
    exa_api_key = os.environ.get("EXA_API_KEY")

    # Tools for Llama Tutor
    tools = [
        ExaTools(
            api_key=exa_api_key,
            start_published_date=datetime.now().strftime("%Y-%m-%d"),
            type="keyword",
            num_results=10,
        ),
        DuckDuckGoTools(
            timeout=20,
            fixed_max_results=5,
        ),
        FileTools(base_dir=output_dir),
    ]

    # Modify the description to include the education level
    tutor_description = f"""You are Llama Tutor, an educational AI assistant designed to teach concepts at a {education_level} level.
    You have the following tools at your disposal:
      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.
      - ExaTools for structured, in-depth analysis.
      - FileTools for saving the output upon user confirmation.

    Your response should always be clear, concise, and detailed, tailored to a {education_level} student's understanding.
    Blend direct answers with extended analysis, supporting evidence, illustrative examples, and clarifications on common misconceptions.
    Engage the user with follow-up questions to check understanding and deepen learning.

    <critical>
    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.
    - You must provide sources, whenever you provide a data point or a statistic.
    - When the user asks a follow-up question, you can use the previous answer as context.
    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.
    - Always adapt your explanations to a {education_level} level of understanding.
    </critical>"""

    # Modify the instructions to include the education level
    tutor_instructions = f"""Here's how you should answer the user's question:

    1. Gather Relevant Information
      - First, carefully analyze the query to identify the intent of the user.
      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.
      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.
      - Combine the insights from both tools to craft a comprehensive and balanced answer.
      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.
      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.

    2. Construct Your Response
      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query, tailored to a {education_level} level.
      - **Then expand** the answer by including:
          • A clear explanation with context and definitions appropriate for {education_level} students.
          • Supporting evidence such as statistics, real-world examples, and data points that are understandable at a {education_level} level.
          • Clarifications that address common misconceptions students at this level might have.
      - Structure your response with clear headings, bullet points, and organized paragraphs to make it easy to follow.
      - Include interactive elements like questions to check understanding or mini-quizzes when appropriate.
      - Use analogies and examples that would be familiar to students at a {education_level} level.

    3. Enhance Engagement
      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)"
      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.
      - Suggest follow-up topics or questions that might deepen their understanding.

    4. Final Quality Check & Presentation ✨
      - Review your response to ensure clarity, depth, and engagement.
      - Ensure the language and concepts are appropriate for a {education_level} level.
      - Make complex ideas accessible without oversimplifying to the point of inaccuracy.

    5. In case of any uncertainties, clarify limitations and encourage follow-up queries."""

    return Agent(
        name="Llama Tutor",
        model=model,
        user_id=user_id,
        session_id=session_id or str(uuid.uuid4()),
        storage=agent_storage,
        tools=tools,
        # Allow Llama Tutor to read both chat history and tool call history for better context.
        read_chat_history=True,
        read_tool_call_history=True,
        # Append previous conversation responses into the new messages for context.
        add_history_to_messages=True,
        num_history_responses=num_history_responses,
        add_datetime_to_instructions=True,
        add_name_to_instructions=True,
        description=tutor_description,
        instructions=tutor_instructions,
        expected_output=EXPECTED_OUTPUT_TEMPLATE,
        debug_mode=debug_mode,
        markdown=True,
    )



================================================
FILE: cookbook/examples/streamlit_apps/llama_tutor/app.py
================================================
import nest_asyncio
import streamlit as st
from agents import tutor_agent
from agno.utils.log import logger
from utils import (
    CUSTOM_CSS,
    about_widget,
    add_message,
    display_tool_calls,
    rename_session_widget,
    session_selector_widget,
    sidebar_widget,
)

nest_asyncio.apply()

# Page configuration
st.set_page_config(
    page_title="Llama Tutor: Learn Anything",
    page_icon=":book:",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Load custom CSS with dark mode support
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def main() -> None:
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>Llama Tutor</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Your intelligent answer engine powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model configuration - always use Llama 3.3 70B
    ####################################################################
    model_id = "groq:llama-3.3-70b-versatile"

    ####################################################################
    # Education level selector
    ####################################################################
    education_levels = [
        "Elementary School",
        "Middle School",
        "High School",
        "College",
        "Undergrad",
        "Graduate",
    ]

    selected_education_level = st.sidebar.selectbox(
        "Education Level",
        options=education_levels,
        index=2,  # Default to High School
        key="education_level_selector",
    )

    # Store the education level in session state
    if "education_level" not in st.session_state:
        st.session_state["education_level"] = selected_education_level
    elif st.session_state["education_level"] != selected_education_level:
        st.session_state["education_level"] = selected_education_level
        # Reset the agent if education level changes
        if "llama_tutor" in st.session_state:
            st.session_state["llama_tutor"] = None

    ####################################################################
    # Initialize Agent
    ####################################################################
    llama_tutor: Agent
    if (
        "llama_tutor" not in st.session_state
        or st.session_state["llama_tutor"] is None
        or st.session_state.get("current_model") != model_id
    ):
        logger.info("---*--- Creating new Llama Tutor agent ---*---")
        llama_tutor = tutor_agent(
            model_id=model_id, education_level=st.session_state["education_level"]
        )
        st.session_state["llama_tutor"] = llama_tutor
        st.session_state["current_model"] = model_id
    else:
        llama_tutor = st.session_state["llama_tutor"]

    ####################################################################
    # Load Agent Session from the database
    ####################################################################
    try:
        st.session_state["llama_tutor_session_id"] = llama_tutor.load_session()
    except Exception:
        st.warning("Could not create Agent session, is the database running?")
        return

    ####################################################################
    # Load runs from memory
    ####################################################################
    agent_runs = llama_tutor.memory.runs
    if len(agent_runs) > 0:
        logger.debug("Loading run history")
        st.session_state["messages"] = []
        for _run in agent_runs:
            if _run.message is not None:
                add_message(_run.message.role, _run.message.content)
            if _run.response is not None:
                add_message("assistant", _run.response.content, _run.response.tools)
    else:
        logger.debug("No run history found")
        st.session_state["messages"] = []

    ####################################################################
    # Sidebar
    ####################################################################
    sidebar_widget()

    ####################################################################
    # Get user input
    ####################################################################
    if prompt := st.chat_input("✨ What would you like to learn about?"):
        add_message("user", prompt)

    ####################################################################
    # Display chat history
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] in ["user", "assistant"]:
            _content = message["content"]
            if _content is not None:
                with st.chat_message(message["role"]):
                    # Display tool calls if they exist in the message
                    if "tool_calls" in message and message["tool_calls"]:
                        display_tool_calls(st.empty(), message["tool_calls"])
                    st.markdown(_content)

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        with st.chat_message("assistant"):
            # Create container for tool calls
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner(":book: Llama Tutor is preparing your lesson..."):
                response = ""
                try:
                    # Run the agent and stream the response
                    run_response = llama_tutor.run(question, stream=True)
                    for _resp_chunk in run_response:
                        # Display tool calls if available
                        if hasattr(_resp_chunk, "tool") and _resp_chunk.tool:
                            display_tool_calls(tool_calls_container, [_resp_chunk.tool])

                        # Display response
                        if _resp_chunk.content is not None:
                            response += _resp_chunk.content
                            resp_container.markdown(response)

                    add_message("assistant", response, llama_tutor.run_response.tools)
                except Exception as e:
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    add_message("assistant", error_message)
                    st.error(error_message)

    ####################################################################
    # Session selector
    ####################################################################
    session_selector_widget(llama_tutor, model_id)
    rename_session_widget(llama_tutor)

    ####################################################################
    # About section
    ####################################################################
    about_widget()


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/llama_tutor/prompts.py
================================================
"""Templates for the Answer Engine."""

from textwrap import dedent

AGENT_DESCRIPTION = dedent("""\
    You are Llama Tutor, a cutting-edge Answer Engine built to deliver precise, context-rich, and engaging responses.
    You have the following tools at your disposal:
      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.
      - ExaTools for structured, in-depth analysis.
      - FileTools for saving the output upon user confirmation.

    Your response should always be clear, concise, and detailed. Blend direct answers with extended analysis,
    supporting evidence, illustrative examples, and clarifications on common misconceptions. Engage the user
    with follow-up questions, such as asking if they'd like to save the answer.

    <critical>
    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.
    - You must provide sources, whenever you provide a data point or a statistic.
    - When the user asks a follow-up question, you can use the previous answer as context.
    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.
    </critical>\
""")

AGENT_INSTRUCTIONS = dedent("""\
    Here's how you should answer the user's question:

    1. Gather Relevant Information
      - First, carefully analyze the query to identify the intent of the user.
      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.
      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.
      - Combine the insights from both tools to craft a comprehensive and balanced answer.
      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.
      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.

    2. Construct Your Response
      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query.
      - **Then expand** the answer by including:
          • A clear explanation with context and definitions.
          • Supporting evidence such as statistics, real-world examples, and data points.
          • Clarifications that address common misconceptions.
      - Expand the answer only if the query requires more detail. Simple questions like: "What is the weather in Tokyo?" or "What is the capital of France?" don't need an in-depth analysis.
      - Ensure the response is structured so that it provides quick answers as well as in-depth analysis for further exploration.

    3. Enhance Engagement
      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)"
      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.

    4. Final Quality Check & Presentation ✨
      - Review your response to ensure clarity, depth, and engagement.
      - Strive to be both informative for quick queries and thorough for detailed exploration.

    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\
""")

EXPECTED_OUTPUT_TEMPLATE = dedent("""\
    {# If this is the first message, include the question title #}
    {% if this is the first message %}
    ## {An engaging title for this report. Keep it short.}
    {% endif %}

    **{A clear and direct response that answers the question.}**

    {# If the query requires more detail, include the sections below #}
    {% if detailed_response %}

    ### {Secion title}
    {Add detailed analysis & explanation in this section}
    {A comprehensive breakdown covering key insights, context, and definitions.}

    ### {Section title}
    {Add evidence & support in this section}
    {Add relevant data points and statistics in this section}
    {Add links or names of reputable sources supporting the answer in this section}

    ### {Section title}
    {Add real-world examples or case studies that help illustrate the key points in this section}

    ### {Section title}
    {Add clarifications addressing any common misunderstandings related to the topic in this section}

    ### {Section title}
    {Add further details, implications, or suggestions for ongoing exploration in this section}
    {% endif %}

    {Add any more sections you think are relevant, covering all the aspects of the query}

    ### Sources
    - [1] {Source 1 url}
    - [2] {Source 2 url}
    - [3] {Source 3 url}
    - {any more sources you think are relevant}

    Generated by Llama Tutor on: {current_time}

    Stay curious and keep exploring ✨\
    """)



================================================
FILE: cookbook/examples/streamlit_apps/llama_tutor/requirements.txt
================================================
agno==1.6.0
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anyio==4.8.0
    # via
    #   anthropic
    #   groq
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.1
    # via requests
click==8.1.8
    # via
    #   ddgs
    #   streamlit
    #   typer
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
docstring-parser==0.16
    # via agno
ddgs==9.5.4
exa-py==1.9.0
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
groq==0.19.0
    # via -r cookbook/examples/apps/answer_engine/requirements.in
grpcio==1.71.0
    # via
    #   google-api-core
    #   grpcio-status
grpcio-status==1.71.0
    # via google-api-core
h11==0.14.0
    # via httpcore
httpcore==1.0.7
    # via httpx
httplib2==0.22.0
    # via
    #   google-api-python-client
    #   google-auth-httplib2
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   groq
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.9.0
    # via
    #   anthropic
    #   openai
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2024.10.1
    # via jsonschema
lxml==5.3.1
    # via ddgs
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
narwhals==1.30.0
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/apps/answer_engine/requirements.in
numpy==2.2.3
    # via
    #   pandas
    #   pydeck
    #   streamlit
openai==1.66.3
    # via
    #   -r cookbook/examples/apps/answer_engine/requirements.in
    #   exa-py
packaging==24.2
    # via
    #   altair
    #   streamlit
pandas==2.2.3
    # via streamlit
pillow==11.1.0
    # via streamlit
primp==0.14.0
    # via ddgs
protobuf==5.29.3
    # via
    #   google-ai-generativelanguage
    #   google-api-core
    #   google-generativeai
    #   googleapis-common-protos
    #   grpcio-status
    #   proto-plus
    #   streamlit
pyarrow==19.0.1
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.1
    # via google-auth
pydantic==2.10.6
    # via
    #   agno
    #   anthropic
    #   google-generativeai
    #   groq
    #   openai
    #   pydantic-settings
pydantic-core==2.27.2
    # via pydantic
pydantic-settings==2.8.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.1
    # via rich
pyparsing==3.2.1
    # via httplib2
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.0.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.1
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via
    #   exa-py
    #   google-api-core
    #   google-search-results
    #   streamlit
rich==13.9.4
    # via
    #   agno
    #   typer
rpds-py==0.23.1
    # via
    #   jsonschema
    #   referencing
rsa==4.9
    # via google-auth
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
sqlalchemy==2.0.39
streamlit==1.43.2
tenacity==9.0.0
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
tqdm==4.67.1
    # via
    #   google-generativeai
    #   openai
typer==0.15.2
    # via agno
typing-extensions==4.12.2
    # via
    #   agno
    #   altair
    #   anthropic
    #   anyio
    #   exa-py
    #   google-generativeai
    #   groq
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   sqlalchemy
    #   streamlit
    #   typer
tzdata==2025.1
    # via pandas
uritemplate==4.1.1
    # via google-api-python-client
urllib3==2.3.0
    # via requests



================================================
FILE: cookbook/examples/streamlit_apps/llama_tutor/utils.py
================================================
from typing import Any, Dict, List, Optional

import streamlit as st
from agents import tutor_agent
from agno.agent.agent import Agent
from agno.models.response import ToolExecution
from agno.utils.log import logger


def add_message(
    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None
) -> None:
    """Safely add a message to the session state."""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append(
        {"role": role, "content": content, "tool_calls": tool_calls}
    )


def restart_agent():
    """Reset the agent and clear chat history."""
    logger.debug("---*--- Restarting agent ---*---")
    st.session_state["llama_tutor"] = None
    st.session_state["llama_tutor_session_id"] = None
    st.session_state["messages"] = []
    st.rerun()


def export_chat_history():
    """Export chat history as markdown."""
    if "messages" in st.session_state:
        chat_text = "# Llama Tutor - Chat History\n\n"
        for msg in st.session_state["messages"]:
            role_label = "🤖 Assistant" if msg["role"] == "assistant" else "👤 User"
            chat_text += f"### {role_label}\n{msg['content']}\n\n"
        return chat_text
    return ""


def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):
    """Display tool calls in a streamlit container with expandable sections.

    Args:
        tool_calls_container: Streamlit container to display the tool calls
        tools: List of tool call dictionaries containing name, args, content, and metrics
    """
    try:
        with tool_calls_container.container():
            for tool_call in tools:
                tool_name = tool_call.tool_name or "Unknown Tool"
                tool_args = tool_call.tool_args or {}
                content = tool_call.result or None
                metrics = tool_call.metrics or None

                # Add timing information
                execution_time_str = "N/A"
                try:
                    if metrics:
                        execution_time = metrics.time
                        if execution_time is not None:
                            execution_time_str = f"{execution_time:.2f}s"
                except Exception as e:
                    logger.error(f"Error displaying tool calls: {str(e)}")
                    pass

                with st.expander(
                    f"🛠️ {tool_name.replace('_', ' ').title()} ({execution_time_str})",
                    expanded=False,
                ):
                    # Show query with syntax highlighting
                    if isinstance(tool_args, dict) and "query" in tool_args:
                        st.code(tool_args["query"], language="sql")

                    # Display arguments in a more readable format
                    if tool_args and tool_args != {"query": None}:
                        st.markdown("**Arguments:**")
                        st.json(tool_args)

                    if content:
                        st.markdown("**Results:**")
                        try:
                            st.json(content)
                        except Exception as e:
                            st.markdown(content)

    except Exception as e:
        logger.error(f"Error displaying tool calls: {str(e)}")
        tool_calls_container.error("Failed to display tool results")


def sidebar_widget() -> None:
    """Display a sidebar with sample user queries for Llama Tutor."""
    with st.sidebar:
        st.markdown("#### 📜 Try me!")
        if st.button("💡 US Tariffs"):
            add_message(
                "user",
                "Tell me about the tariffs the US is imposing in 2025",
            )
        if st.button("🤔 Reasoning Models"):
            add_message(
                "user",
                "Which is a better reasoning model: o3-mini or DeepSeek R1?",
            )
        if st.button("🤖 Tell me about Agno"):
            add_message(
                "user",
                "Tell me about Agno: https://github.com/agno-agi/agno and https://docs.agno.com",
            )
        if st.button("⚖️ Impact of AI Regulations"):
            add_message(
                "user",
                "Evaluate how emerging AI regulations could influence innovation, privacy, and ethical AI deployment in the near future.",
            )

        st.markdown("---")
        st.markdown("#### 🛠️ Utilities")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("🔄 New Chat"):
                restart_agent()
        with col2:
            fn = "llama_tutor_chat_history.md"
            if "llama_tutor_session_id" in st.session_state:
                fn = f"llama_tutor_{st.session_state.llama_tutor_session_id}.md"
            if st.download_button(
                "💾 Export Chat",
                export_chat_history(),
                file_name=fn,
                mime="text/markdown",
            ):
                st.sidebar.success("Chat history exported!")


def session_selector_widget(agent: Agent, model_id: str) -> None:
    """Display a session selector in the sidebar."""
    if agent.storage:
        agent_sessions = agent.storage.get_all_sessions()
        # Get session names if available, otherwise use IDs.
        session_options = []
        for session in agent_sessions:
            session_id = session.session_id
            session_name = (
                session.session_data.get("session_name", None)
                if session.session_data
                else None
            )
            display_name = session_name if session_name else session_id
            session_options.append({"id": session_id, "display": display_name})

        # Display session selector.
        selected_session = st.sidebar.selectbox(
            "Session",
            options=[s["display"] for s in session_options],
            key="session_selector",
        )
        # Find the selected session ID.
        selected_session_id = next(
            s["id"] for s in session_options if s["display"] == selected_session
        )

        if st.session_state.get("llama_tutor_session_id") != selected_session_id:
            logger.info(
                f"---*--- Loading {model_id} run: {selected_session_id} ---*---"
            )
            st.session_state["llama_tutor"] = tutor_agent(
                model_id=model_id,
                session_id=selected_session_id,
            )
            st.rerun()


def rename_session_widget(agent: Agent) -> None:
    """Rename the current session of the agent and save to storage."""
    container = st.sidebar.container()
    session_row = container.columns([3, 1], vertical_alignment="center")

    # Initialize session_edit_mode if needed.
    if "session_edit_mode" not in st.session_state:
        st.session_state.session_edit_mode = False

    with session_row[0]:
        if st.session_state.session_edit_mode:
            new_session_name = st.text_input(
                "Session Name",
                value=agent.session_name,
                key="session_name_input",
                label_visibility="collapsed",
            )
        else:
            st.markdown(f"Session Name: **{agent.session_name}**")

    with session_row[1]:
        if st.session_state.session_edit_mode:
            if st.button("✓", key="save_session_name", type="primary"):
                if new_session_name:
                    agent.rename_session(new_session_name)
                    st.session_state.session_edit_mode = False
                    container.success("Renamed!")
        else:
            if st.button("✎", key="edit_session_name"):
                st.session_state.session_edit_mode = True


def about_widget() -> None:
    """Display an about section in the sidebar."""
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ℹ️ About Llama Tutor")
    st.sidebar.markdown(
        """
        Llama Tutor is an educational AI assistant that delivers personalized learning experiences tailored to your education level.
        
        Features:
        - 📚 Personalized education at various academic levels
        - 🔍 Real-time information retrieval
        - 📊 In-depth analysis and explanations
        - 🧠 Interactive learning with quizzes and follow-up questions
        - 💾 Save lessons for future reference
        
        Built with:
        - 🦙 Llama 3.3 70B from Meta
        - 🚀 Agno framework
        - 💫 Streamlit
        """
    )


CUSTOM_CSS = """
    <style>
    /* Main Styles */
    .main-title {
        font-size: 3rem !important;
        font-weight: 700 !important;
        color: #2A8EF9 !important;
        margin-bottom: 0 !important;
        text-align: center;
    }
    
    .subtitle {
        font-size: 1.2rem !important;
        color: #555 !important;
        margin-top: 0 !important;
        text-align: center;
        margin-bottom: 2rem !important;
    }
    
    /* Dark mode support */
    @media (prefers-color-scheme: dark) {
        .main-title {
            color: #4DA3FF !important;
        }
        
        .subtitle {
            color: #CCC !important;
        }
    }
    
    /* Tool Call Styling */
    .stExpander {
        border-radius: 8px !important;
        border: 1px solid rgba(49, 51, 63, 0.2) !important;
        margin-bottom: 0.5rem !important;
    }
    
    .stExpander summary {
        font-weight: 600 !important;
        padding: 0.5rem 1rem !important;
    }
    
    /* Sidebar Styling */
    .css-1544g2n {
        padding-top: 2rem !important;
    }
    
    /* Education Level Display */
    .education-level-display {
        padding: 8px;
        background-color: #f8f9fa;
        border-radius: 6px;
        margin-top: 8px;
        text-align: center;
        border: 1px solid #e9ecef;
        font-size: 0.9rem;
    }
    
    /* Dark mode support for education level */
    @media (prefers-color-scheme: dark) {
        .education-level-display {
            background-color: #262730;
            border-color: #4a4d56;
            color: #f8f9fa;
        }
    }
    </style>
"""



================================================
FILE: cookbook/examples/streamlit_apps/llama_tutor/.env.example
================================================
GROQ_API_KEY=<your_groq_api_key>
EXA_API_KEY=<your_exa_api_key>


================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/README.md
================================================
# UAgI: Universal Agent Interface powered by MCP

> [!IMPORTANT]
> This is a work in progress (see [open issues](#-open-issues) below), please contribute and help improve.

UAgI (Universal Agent Interface) is a powerful agent application that leverages the Model Context Protocol (MCP) to provide a unified interface for interacting with various MCP servers. This application allows you to connect to different data sources and tools through MCP servers, providing a seamless experience for working with external services.

## 🌟 Features

- **Multiple Model Support**: Works with various LLM providers including:
  - OpenAI (o3-mini, gpt-4o, gpt-4.5)
  - Anthropic (claude-3-7-sonnet, claude-3-7-sonnet-thinking)
  - Google (gemini-2.0-flash, gemini-2.0-pro)
  - Groq (llama-3.3-70b-versatile)

- **MCP Server Integration**: Connect to the following MCP servers:
  - GitHub: Access repositories, issues, and more
  - Filesystem: Browse and manipulate files on your local system

- **Knowledge Base**: Built-in knowledge of MCP documentation to help answer questions about the protocol

- **Session Management**: Save and restore chat sessions using SQLite storage

- **Chat History Export**: Export your conversations as markdown files

- **Streamlit UI**: User-friendly interface with customizable settings

## 🐞 Open Issues

- Only works with 1 MCP server at a time
- Changing MCP servers resets the agent
- Only supports 2 MCP servers at the moment
- Chat history is broken
- MCP Cleanup is not working, so memory leaks are possible

## 🚀 Quick Start

### 1. Environment Setup

Create and activate a virtual environment:
```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 2. Install Dependencies

```bash
pip install -r cookbook/examples/streamlit_apps/mcp_agent/requirements.txt
```

### 3. Configure API Keys

Required:
```bash
export OPENAI_API_KEY=your_openai_key_here
```

Optional (for additional models):
```bash
export ANTHROPIC_API_KEY=your_anthropic_key_here
export GOOGLE_API_KEY=your_google_key_here
export GROQ_API_KEY=your_groq_key_here
```

For GitHub MCP server:
```bash
export GITHUB_TOKEN=your_github_token_here
```

### 4. Launch the Application

```bash
streamlit run cookbook/examples/streamlit_apps/mcp_agent/app.py
```

Visit [localhost:8501](http://localhost:8501) to access the UAgI application.

## 🔧 How It Works

UAgI connects to MCP servers using the Model Context Protocol, which standardizes how applications provide context to LLMs. When you ask a question:

1. The agent analyzes your request and determines which MCP tools might be helpful
2. It connects to the appropriate MCP server (GitHub, Filesystem, etc.)
3. The agent executes the necessary tools through the MCP server
4. Results are processed and returned in a natural language response
5. All interactions are saved in your session history

## 📚 Understanding MCP

The Model Context Protocol (MCP) is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications - it provides a standardized way to connect AI models to different data sources and tools.

MCP helps you build agents and complex workflows on top of LLMs by providing:
- A growing list of pre-built integrations that your LLM can directly plug into
- The flexibility to switch between LLM providers and vendors
- Best practices for securing your data within your infrastructure

## 🛠️ Customization

### Adding New MCP Servers

The application is designed to be extensible. To add new MCP servers:

1. Update the `get_mcp_server_config()` function in `utils.py`
2. Add server-specific example inputs in the `example_inputs()` function

### Modifying Agent Behavior

The agent configuration is in `agents.py`:
- Adjust the agent description and instructions to change its behavior
- Modify the knowledge base to include additional documentation
- Add new tools or capabilities as needed

## 📚 Documentation

For more detailed information:
- [Agno Documentation](https://docs.agno.com)
- [Streamlit Documentation](https://docs.streamlit.io)

## 🤝 Support

Need help? Join our [Discord community](https://agno.link/discord)



================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/agents.py
================================================
from pathlib import Path
from textwrap import dedent
from typing import List, Optional

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.tools.mcp import MCPTools
from agno.vectordb.lancedb import LanceDb, SearchType

# ************* Setup Paths *************
# Define the current working directory
cwd = Path(__file__).parent
# Create a tmp directory for storing agent sessions and knowledge
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)
# *************************************

# ************* Agent Storage *************
# Store agent sessions in a SQLite database
agent_storage = SqliteAgentStorage(
    table_name="mcp_agent_sessions",  # Table to store agent sessions
    db_file=str(tmp_dir.joinpath("agents.db")),  # SQLite database file
)
# *************************************

# ************* Agent Knowledge *************
# Store MCP Documentation in a knowledge base
agent_knowledge = UrlKnowledge(
    urls=["https://modelcontextprotocol.io/llms-full.txt"],
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("mcp_documentation")),
        table_name="mcp_documentation",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# *************************************


def get_mcp_agent(
    user_id: Optional[str] = None,
    model_str: str = "openai:gpt-4o",
    session_id: Optional[str] = None,
    num_history_responses: int = 5,
    mcp_tools: Optional[List[MCPTools]] = None,
    mcp_server_ids: Optional[List[str]] = None,
    debug_mode: bool = True,
) -> Agent:
    model = get_model_for_provider(model_str)

    description = dedent("""\
        You are UAgI, a universal MCP (Model Context Protocol) agent designed to interact with MCP servers.
        You can connect to various MCP servers to access resources and execute tools.

        As an MCP agent, you can:
        - Connect to file systems, databases, APIs, and other data sources through MCP servers
        - Execute tools provided by MCP servers to perform actions
        - Access resources exposed by MCP servers

        Note: You only have access to the MCP Servers provided below, if you need to access other MCP Servers, please ask the user to enable them.

        <critical>
        - When a user mentions a task that might require external data or tools, check if an appropriate MCP server is available
        - If an MCP server is available, use its capabilities to fulfill the user's request
        - You have a knowledge base full of MCP documentation, search it using the `search_knowledge` tool to answer questions about MCP and the different tools available.
        - Provide clear explanations of which MCP servers and tools you're using
        - If you encounter errors with an MCP server, explain the issue and suggest alternatives
        - Always cite sources when providing information retrieved through MCP servers
        </critical>\
    """)

    if mcp_server_ids:
        description += dedent(
            """\n
            You have access to the following MCP servers:
            {}
        """.format("\n".join([f"- {server_id}" for server_id in mcp_server_ids]))
        )

    instructions = dedent("""\
        Here's how you should fulfill a user request:

        1. Understand the user's request
        - Read the user's request carefully
        - Determine if the request requires MCP server interaction
        - Search your knowledge base using the `search_knowledge` tool to answer questions about MCP or to learn how to use different MCP tools.
        - To interact with an MCP server, follow these steps:
            - Identify which tools are available to you
            - Select the appropriate tool for the user's request
            - Explain to the user which tool you're using
            - Execute the tool
            - Provide clear feedback about tool execution results

        2. Error Handling
        - If an MCP tool fails, explain the issue clearly and provide details about the error.
        - Suggest alternatives when MCP capabilities are unavailable

        3. Security and Privacy
        - Be transparent about which servers and tools you're using
        - Request explicit permission before executing tools that modify data
        - Respect access limitations of connected MCP servers

        MCP Knowledge
        - You have access to a knowledge base of MCP documentation
        - To answer questions about MCP, use the knowledge base
        - If you don't know the answer or can't find the information in the knowledge base, say so\
    """)

    return Agent(
        name="UAgI: The Universal MCP Agent",
        model=model,
        user_id=user_id,
        session_id=session_id,
        tools=mcp_tools,
        # Store Agent sessions in the database
        storage=agent_storage,
        # Store MCP Documentation in a knowledge base
        knowledge=agent_knowledge,
        # Agent description, instructions and expected output format
        description=description,
        instructions=instructions,
        # Allow MCP Agent to read both chat history and tool call history for better context.
        read_chat_history=True,
        read_tool_call_history=True,
        # Append previous conversation responses into the new messages for context.
        add_history_to_messages=True,
        num_history_responses=num_history_responses,
        add_datetime_to_instructions=True,
        add_name_to_instructions=True,
        debug_mode=debug_mode,
        # Respond in markdown format
        markdown=True,
    )


def get_model_for_provider(model_str: str):
    """
    Creates and returns the appropriate model for a model string.

    Args:
        model_str: The model string (e.g., 'openai:gpt-4o', 'google:gemini-2.0-flash', 'anthropic:claude-3-5-sonnet', 'groq:llama-3.3-70b-versatile')

    Returns:
        An instance of the appropriate model class

    Raises:
        ValueError: If the provider is not supported
    """
    provider, model_name = model_str.split(":")
    if provider == "openai":
        return OpenAIChat(id=model_name)
    elif provider == "gemini":
        return Gemini(id=model_name)
    elif provider == "anthropic":
        if "thinking" in model_name:
            return Claude(
                id=model_name,
                max_tokens=16384,
                thinking={"type": "enabled", "budget_tokens": 8192},
            )
        return Claude(id=model_name, max_tokens=16384)
    elif provider == "groq":
        return Groq(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")



================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/app.py
================================================
import asyncio

import nest_asyncio
import streamlit as st
from agents import get_mcp_agent
from agno.utils.log import logger
from mcp_client import MCPClient
from utils import (
    about_widget,
    add_message,
    apply_theme,
    display_tool_calls,
    example_inputs,
    get_mcp_server_config,
    get_num_history_responses,
    get_selected_model,
    session_selector_widget,
    utilities_widget,
)

nest_asyncio.apply()
apply_theme()


async def main() -> None:
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>Universal Agent Interface powered by MCP</h1>",
        unsafe_allow_html=True,
    )
    st.markdown(
        "<p class='subtitle'>A unified Agentic interface for MCP servers</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Settings
    ####################################################################
    selected_model = get_selected_model()
    mcp_server_config = get_mcp_server_config()
    mcp_server_id = mcp_server_config.id
    num_history_responses = get_num_history_responses()

    ####################################################################
    # Initialize MCP Client and Agent
    ####################################################################
    try:
        # Check if we need to reinitialize the MCP client
        if (
            "mcp_client" not in st.session_state
            or st.session_state.get("mcp_server_id") != mcp_server_id
            or getattr(st.session_state.get("mcp_client", None), "session", None)
            is None
        ):
            # # Clean up existing client if it exists
            # if "mcp_client" in st.session_state:
            #     logger.info("Cleaning up existing MCP client")
            #     await st.session_state["mcp_client"].cleanup()

            # Initialize new MCP client
            logger.info(f"Creating new MCPClient for {mcp_server_id}")
            st.session_state["mcp_client"] = MCPClient()

        mcp_client = st.session_state["mcp_client"]
        # Connect to the MCP server and get tools
        mcp_tools = await mcp_client.connect_to_server(mcp_server_config)

        # Initialize or retrieve the agent
        if (
            "mcp_agent" not in st.session_state
            or st.session_state["mcp_agent"] is None
            or st.session_state.get("current_model") != selected_model
            or st.session_state.get("mcp_server_id") != mcp_server_id
        ):
            logger.info("---*--- Creating new MCP Agent ---*---")
            mcp_agent = get_mcp_agent(
                model_str=selected_model,
                num_history_responses=num_history_responses,
                mcp_tools=[mcp_tools],
                mcp_server_ids=[mcp_server_id],
            )
            st.session_state["mcp_agent"] = mcp_agent
            st.session_state["current_model"] = selected_model
        else:
            mcp_agent = st.session_state["mcp_agent"]

        # Update the agent's MCP tools incase the session has been reinitialized
        mcp_agent.tools = [mcp_tools]
        ####################################################################
        # Load the current Agent session from the database
        ####################################################################
        try:
            st.session_state["mcp_agent_session_id"] = mcp_agent.load_session()
        except Exception as e:
            st.warning(
                f"Could not create Agent session: {str(e)}. Is the database running?"
            )
            return

        ####################################################################
        # Load agent runs (i.e. chat history) from memory
        ####################################################################
        agent_runs = mcp_agent.memory.runs
        if len(agent_runs) > 0:
            # If there are runs, load the messages
            logger.debug("Loading run history")
            st.session_state["messages"] = []
            # Loop through the runs and add the messages to the messages list
            for _run in agent_runs:
                if _run.message is not None:
                    add_message(_run.message.role, _run.message.content)
                if _run.response is not None:
                    add_message("assistant", _run.response.content, _run.response.tools)
        else:
            # If there are no runs, create an empty messages list
            logger.debug("No run history found")
            st.session_state["messages"] = []

        ####################################################################
        # Get user input
        ####################################################################
        if prompt := st.chat_input("✨ How can I help, bestie?"):
            add_message("user", prompt)

        ####################################################################
        # Show example inputs
        ####################################################################
        example_inputs(server_id=mcp_server_id)

        ####################################################################
        # Display agent messages
        ####################################################################
        for message in st.session_state["messages"]:
            if message["role"] in ["user", "assistant"]:
                _content = message["content"]
                if _content is not None:
                    with st.chat_message(message["role"]):
                        # Display tool calls if they exist in the message
                        if "tool_calls" in message and message["tool_calls"]:
                            display_tool_calls(st.empty(), message["tool_calls"])
                        st.markdown(_content)

        ####################################################################
        # Generate response for user message
        ####################################################################
        last_message = (
            st.session_state["messages"][-1] if st.session_state["messages"] else None
        )
        if last_message and last_message.get("role") == "user":
            question = last_message["content"]
            with st.chat_message("assistant"):
                # Create container for tool calls
                tool_calls_container = st.empty()
                resp_container = st.empty()
                with st.spinner(":thinking_face: Thinking..."):
                    response = ""
                    try:
                        # Run the agent and stream the response
                        run_response = await mcp_agent.arun(question, stream=True)
                        async for _resp_chunk in run_response:
                            # Display tool calls if available
                            if hasattr(_resp_chunk, "tool") and _resp_chunk.tool:
                                display_tool_calls(
                                    tool_calls_container, [_resp_chunk.tool]
                                )

                            # Display response
                            if _resp_chunk.content is not None:
                                response += _resp_chunk.content
                                resp_container.markdown(response)

                        add_message("assistant", response, mcp_agent.run_response.tools)
                    except Exception as e:
                        logger.error(f"Error during agent run: {str(e)}", exc_info=True)
                        error_message = f"Sorry, I encountered an error: {str(e)}"
                        add_message("assistant", error_message)
                        st.error(error_message)

        ####################################################################
        # Session selector
        ####################################################################
        session_selector_widget(
            agent=mcp_agent,
            model_str=selected_model,
            num_history_responses=num_history_responses,
            mcp_tools=[mcp_tools],
            mcp_server_ids=[mcp_server_id],
        )

        ####################################################################
        # About section
        ####################################################################
        utilities_widget(agent=mcp_agent)
        about_widget()

    except Exception as e:
        logger.error(f"Error during agent run: {str(e)}", exc_info=True)
        error_message = f"Sorry, I encountered an error: {str(e)}"
        add_message("assistant", error_message)
        st.error(error_message)
    finally:
        # Don't clean up resources here - we want to keep the connection alive
        # between Streamlit reruns. We'll clean up when we need to reinitialize.
        pass


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/mcp_client.py
================================================
from contextlib import AsyncExitStack
from typing import List, Optional

from agno.tools.mcp import MCPTools
from agno.utils.log import logger
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from pydantic import BaseModel


class MCPServerConfig(BaseModel):
    """Configuration for an MCP server."""

    id: str
    command: str
    args: Optional[List[str]] = None


class MCPClient:
    def __init__(self):
        # Initialize session and client objects
        self.session = None
        self.exit_stack = AsyncExitStack()
        self.tools = []
        self.server_id = None

    async def connect_to_server(self, server_config):
        """Connect to an MCP server using the provided configuration

        Args:
            server_config: Configuration for the MCP server
        """
        self.server_id = server_config.id

        server_params = StdioServerParameters(
            command=server_config.command,
            args=server_config.args,
        )
        logger.info(f"Connecting to server {self.server_id}")

        # Create client session
        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(self.stdio, self.write)
        )

        # Initialize the session
        await self.session.initialize()

        # Create MCPTools for this server
        mcp_tools = MCPTools(session=self.session)
        await mcp_tools.initialize()
        logger.info(f"Connected to server {self.server_id}")

        return mcp_tools

    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()



================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/requirements.in
================================================
agno
anthropic
google-genai
groq
lancedb
mcp
nest_asyncio
openai
sqlalchemy
streamlit
tantivy



================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.6.0
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anthropic==0.49.0
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
anyio==4.8.0
    # via
    #   anthropic
    #   google-genai
    #   groq
    #   httpx
    #   mcp
    #   openai
    #   sse-starlette
    #   starlette
attrs==25.1.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.1
    # via requests
click==8.1.8
    # via
    #   streamlit
    #   typer
    #   uvicorn
deprecation==2.1.0
    # via lancedb
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
docstring-parser==0.16
    # via agno
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
google-auth==2.38.0
    # via google-genai
google-genai==1.5.0
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
groq==0.18.0
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
h11==0.14.0
    # via
    #   httpcore
    #   uvicorn
httpcore==1.0.7
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   mcp
    #   openai
httpx-sse==0.4.0
    # via mcp
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.8.2
    # via
    #   anthropic
    #   openai
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2024.10.1
    # via jsonschema
lancedb==0.20.0
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mcp==1.3.0
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
mdurl==0.1.2
    # via markdown-it-py
narwhals==1.29.1
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
numpy==2.2.3
    # via
    #   pandas
    #   pydeck
    #   pylance
    #   streamlit
openai==1.65.5
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
overrides==7.7.0
    # via lancedb
packaging==24.2
    # via
    #   altair
    #   deprecation
    #   lancedb
    #   streamlit
pandas==2.2.3
    # via streamlit
pillow==11.1.0
    # via streamlit
protobuf==5.29.3
    # via streamlit
pyarrow==19.0.1
    # via
    #   pylance
    #   streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.1
    # via google-auth
pydantic==2.10.6
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   lancedb
    #   mcp
    #   openai
    #   pydantic-settings
pydantic-core==2.27.2
    # via pydantic
pydantic-settings==2.8.1
    # via
    #   agno
    #   mcp
pydeck==0.9.1
    # via streamlit
pygments==2.19.1
    # via rich
pylance==0.23.2
    # via lancedb
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.0.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.1
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via
    #   google-genai
    #   streamlit
rich==13.9.4
    # via
    #   agno
    #   typer
rpds-py==0.23.1
    # via
    #   jsonschema
    #   referencing
rsa==4.9
    # via google-auth
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
sqlalchemy==2.0.38
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
sse-starlette==2.2.1
    # via mcp
starlette==0.46.1
    # via
    #   mcp
    #   sse-starlette
streamlit==1.43.1
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
tantivy==0.22.0
    # via -r cookbook/examples/apps/mcp_agent/requirements.in
tenacity==9.0.0
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
tqdm==4.67.1
    # via
    #   lancedb
    #   openai
typer==0.15.2
    # via agno
typing-extensions==4.12.2
    # via
    #   agno
    #   altair
    #   anthropic
    #   anyio
    #   google-genai
    #   groq
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   sqlalchemy
    #   streamlit
    #   typer
tzdata==2025.1
    # via pandas
urllib3==2.3.0
    # via requests
uvicorn==0.34.0
    # via mcp
websockets==14.2
    # via google-genai



================================================
FILE: cookbook/examples/streamlit_apps/mcp_agent/utils.py
================================================
import os
from pathlib import Path
from typing import Any, Dict, List, Optional

import streamlit as st
from agents import get_mcp_agent
from agno.agent import Agent
from agno.models.response import ToolExecution
from agno.tools.mcp import MCPTools
from agno.utils.log import logger
from mcp_client import MCPServerConfig


def get_selected_model() -> str:
    """Return the selected model identifier based on user selection in the sidebar.

    Returns:
        str: The model identifier string in the format 'provider:model-name'
    """
    model_options = {
        "gpt-4o": "openai:gpt-4o",
        "gpt-4.5": "openai:gpt-4.5-preview",
        "gpt-4o-mini": "openai:gpt-4o-mini",
        "o3-mini": "openai:o3-mini",
        "sonnet-3-7": "anthropic:claude-3-7-sonnet-latest",
        "sonnet-3.7-thinking": "anthropic:claude-3-7-sonnet-thinking",
        "gemini-flash": "gemini:gemini-2.0-flash",
        "gemini-pro": "gemini:gemini-2.0-pro-exp-02-05",
        "llama-3.3-70b": "groq:llama-3.3-70b-versatile",
    }
    st.sidebar.markdown("#### :sparkles: Select a model")
    selected_model = st.sidebar.selectbox(
        "Select a model",
        options=list(model_options.keys()),
        index=list(model_options.keys()).index("gpt-4o"),
        key="selected_model",
        label_visibility="collapsed",
    )
    return model_options[selected_model]


def get_num_history_responses() -> int:
    """Return the number of messages from history to send to the LLM.

    Returns:
        int: The number of messages from history to include
    """
    num_history = st.sidebar.slider(
        "Number of previous messages to include",
        min_value=1,
        max_value=20,
        value=5,
        step=1,
        help="Controls how many previous messages are sent to the LLM for context",
    )
    return num_history


def get_mcp_server_config() -> Optional[MCPServerConfig]:
    """Get a single MCP server config to add to the agent.

    Returns:
        Optional[MCPServerConfig]: A single MCP server config, or None if none selected.
    """
    with st.sidebar:
        st.markdown("#### 🛠️ Select MCP Tool")

        # Use radio button for single selection
        selected_tool = st.radio(
            "Select a tool to use:",
            options=["GitHub", "Filesystem"],
            key="selected_mcp_tool",
            label_visibility="collapsed",
        )

        if selected_tool == "GitHub":
            github_token_from_env = os.getenv("GITHUB_TOKEN")
            github_token = st.text_input(
                "GitHub Token",
                type="password",
                help="Create a token with repo scope at github.com/settings/tokens",
                value=github_token_from_env,
            )
            if github_token:
                os.environ["GITHUB_TOKEN"] = github_token
                return MCPServerConfig(
                    id="github",
                    command="npx",
                    args=["-y", "@modelcontextprotocol/server-github"],
                    env_vars=["GITHUB_TOKEN"],
                )
            else:
                st.error("GitHub Token is required to use GitHub MCP Tools")

        elif selected_tool == "Filesystem":
            # Get the repository root
            cwd = Path(__file__).parent
            repo_root = cwd.parent.parent.parent.parent.resolve()
            st.info(f"Repository path: {repo_root}")
            return MCPServerConfig(
                id="filesystem",
                command="npx",
                args=["-y", "@modelcontextprotocol/server-filesystem"]
                + [str(repo_root)],
            )

    return None


def add_message(
    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None
) -> None:
    """Safely add a message to the session state."""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append(
        {"role": role, "content": content, "tool_calls": tool_calls}
    )


def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):
    """Display tool calls in a streamlit container with expandable sections.

    Args:
        tool_calls_container: Streamlit container to display the tool calls
        tools: List of tool call dictionaries containing name, args, content, and metrics
    """
    if not tools:
        return

    try:
        with tool_calls_container.container():
            for tool_call in tools:
                tool_name = tool_call.tool_name or "Unknown Tool"
                tool_args = tool_call.tool_args or {}
                content = tool_call.result or None
                metrics = tool_call.metrics or None

                # Add timing information
                execution_time_str = "N/A"
                if metrics and hasattr(metrics, "time"):
                    execution_time = metrics.time
                    if execution_time is None:
                        execution_time_str = "N/A"
                    else:
                        execution_time_str = f"{execution_time:.2f}s"

                with st.expander(
                    f"🛠️ {tool_name.replace('_', ' ').title()} ({execution_time_str})",
                    expanded=False,
                ):
                    # Show query with syntax highlighting
                    if isinstance(tool_args, dict) and tool_args.get("query"):
                        st.code(tool_args["query"], language="sql")

                    # Display arguments in a more readable format
                    if tool_args and tool_args != {"query": None}:
                        st.markdown("**Arguments:**")
                        st.json(tool_args)

                    if content:
                        st.markdown("**Results:**")
                        try:
                            # Check if content is already a dictionary or can be parsed as JSON
                            if isinstance(content, dict) or (
                                isinstance(content, str)
                                and content.strip().startswith(("{", "["))
                            ):
                                st.json(content)
                            else:
                                # If not JSON, show as markdown
                                st.markdown(content)
                        except Exception:
                            # If JSON display fails, show as markdown
                            st.markdown(content)

    except Exception as e:
        logger.error(f"Error displaying tool calls: {str(e)}")
        tool_calls_container.error(f"Failed to display tool results: {str(e)}")


def example_inputs(server_id: str) -> None:
    """Show example inputs for the MCP Agent."""
    with st.sidebar:
        st.markdown("#### :thinking_face: Try me!")
        if st.button("Who are you?"):
            add_message(
                "user",
                "Who are you?",
            )
        if st.button("What is your purpose?"):
            add_message(
                "user",
                "What is your purpose?",
            )
        # Common examples for all server types
        if st.button("What can you help me with?"):
            add_message(
                "user",
                "What can you help me with?",
            )
        if st.button("How do MCP tools work?"):
            add_message(
                "user",
                "How do MCP tools work? Explain the Model Context Protocol.",
            )

        # Server-specific examples
        if server_id == "github":
            if st.button("Tell me about Agno"):
                add_message(
                    "user",
                    "Tell me about Agno. Github repo: https://github.com/agno-agi/agno. You can read the README for more information.",
                )
            if st.button("Find issues in the Agno repo"):
                add_message(
                    "user",
                    "Find open issues in the agno-agi/agno repository and summarize the top 3 most recent ones.",
                )
        elif server_id == "filesystem":
            if st.button("Summarize the README"):
                add_message(
                    "user",
                    "If there is a README file in the current directory, summarize it.",
                )


def session_selector_widget(
    agent: Agent,
    model_str: str,
    num_history_responses: int,
    mcp_tools: List[MCPTools],
    mcp_server_ids: List[str],
) -> None:
    """Display a session selector in the sidebar, if a new session is selected, the agent is restarted with the new session."""

    if not agent.storage:
        return

    try:
        # -*- Get all agent sessions.
        agent_sessions = agent.storage.get_all_sessions()

        if not agent_sessions:
            st.sidebar.info("No saved sessions found.")
            return

        # -*- Get session names if available, otherwise use IDs.
        sessions_list = []
        for session in agent_sessions:
            session_id = session.session_id
            session_name = (
                session.session_data.get("session_name", None)
                if session.session_data
                else None
            )
            display_name = session_name if session_name else session_id
            sessions_list.append({"id": session_id, "display_name": display_name})

        # -*- Display session selector.
        st.sidebar.markdown("#### 💬 Session")
        selected_session = st.sidebar.selectbox(
            "Session",
            options=[s["display_name"] for s in sessions_list],
            key="session_selector",
            label_visibility="collapsed",
        )
        # -*- Find the selected session ID.
        selected_session_id = next(
            s["id"] for s in sessions_list if s["display_name"] == selected_session
        )
        # -*- Update the selected session if it has changed.
        if st.session_state.get("mcp_agent_session_id") != selected_session_id:
            logger.info(
                f"---*--- Loading {model_str} run: {selected_session_id} ---*---"
            )
            st.session_state["mcp_agent"] = get_mcp_agent(
                model_str=model_str,
                session_id=selected_session_id,
                num_history_responses=num_history_responses,
                mcp_tools=mcp_tools,
                mcp_server_ids=mcp_server_ids,
            )
            st.rerun()

        # -*- Show the rename session widget.
        container = st.sidebar.container()
        session_row = container.columns([3, 1], vertical_alignment="center")

        # -*- Initialize session_edit_mode if needed.
        if "session_edit_mode" not in st.session_state:
            st.session_state.session_edit_mode = False

        # -*- Show the session name.
        with session_row[0]:
            if st.session_state.session_edit_mode:
                new_session_name = st.text_input(
                    "Session Name",
                    value=agent.session_name,
                    key="session_name_input",
                    label_visibility="collapsed",
                )
            else:
                st.markdown(f"Session Name: **{agent.session_name}**")

        # -*- Show the rename session button.
        with session_row[1]:
            if st.session_state.session_edit_mode:
                if st.button("✓", key="save_session_name", type="primary"):
                    if new_session_name:
                        agent.rename_session(new_session_name)
                        st.session_state.session_edit_mode = False
                        container.success("Renamed!")
                        # Trigger a rerun to refresh the sessions list
                        st.rerun()
            else:
                if st.button("✎", key="edit_session_name"):
                    st.session_state.session_edit_mode = True
    except Exception as e:
        logger.error(f"Error in session selector: {str(e)}")
        st.sidebar.error("Failed to load sessions")


def restart_agent():
    """Reset the agent and clear chat history."""
    logger.debug("---*--- Restarting agent ---*---")
    st.session_state["mcp_agent"] = None
    st.session_state["mcp_agent_session_id"] = None
    st.session_state["messages"] = []
    st.rerun()


def export_chat_history():
    """Export chat history as markdown.

    Returns:
        str: Formatted markdown string of the chat history
    """
    if "messages" not in st.session_state or not st.session_state["messages"]:
        return "# MCP Agent - Chat History\n\nNo messages to export."

    chat_text = "# MCP Agent - Chat History\n\n"
    for msg in st.session_state["messages"]:
        role_label = "🤖 Assistant" if msg["role"] == "assistant" else "👤 User"
        chat_text += f"### {role_label}\n{msg['content']}\n\n"

        # Include tool calls if present
        if msg.get("tool_calls"):
            chat_text += "#### Tool Calls:\n"
            for i, tool_call in enumerate(msg["tool_calls"]):
                tool_name = tool_call.get("name", "Unknown Tool")
                chat_text += f"**{i + 1}. {tool_name}**\n\n"
                if "arguments" in tool_call:
                    chat_text += (
                        f"Arguments: ```json\n{tool_call['arguments']}\n```\n\n"
                    )
                if "content" in tool_call:
                    chat_text += f"Results: ```\n{tool_call['content']}\n```\n\n"

    return chat_text


def utilities_widget(agent: Agent) -> None:
    """Display a utilities widget in the sidebar."""
    st.sidebar.markdown("---")
    st.sidebar.markdown("#### 🛠️ Utilities")
    col1, col2 = st.sidebar.columns(2)
    with col1:
        if st.button("🔄 New Chat"):
            restart_agent()
    with col2:
        fn = "mcp_agent_chat_history.md"
        if "mcp_agent_session_id" in st.session_state:
            fn = f"mcp_agent_{st.session_state.mcp_agent_session_id}.md"
        if st.download_button(
            ":file_folder: Export Chat",
            export_chat_history(),
            file_name=fn,
            mime="text/markdown",
        ):
            st.sidebar.success("Chat history exported!")
    if agent is not None and agent.knowledge is not None:
        if st.sidebar.button("📚 Load Knowledge"):
            agent.knowledge.load()
            st.sidebar.success("Knowledge loaded!")


def about_widget() -> None:
    """Display an about section in the sidebar."""
    st.sidebar.markdown("#### ℹ️ About")
    st.sidebar.markdown(
        """
        The Universal MCP Agent lets you interact with MCP servers using a chat interface.

        Built with:
        - 🚀 [Agno](https://github.com/agno-agi/agno)
        - 💫 [Streamlit](https://streamlit.io)
        """
    )


CUSTOM_CSS = """
    <style>
    /* Main Styles */
    .main-title {
        text-align: center;
        background: linear-gradient(45deg, #FF4B2B, #FF416C);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .subtitle {
        text-align: center;
        color: #666;
        margin-bottom: 2em;
    }
    .stButton button {
        width: 100%;
        border-radius: 20px;
        margin: 0.2em 0;
        transition: all 0.3s ease;
    }
    .stButton button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    }
    .chat-container {
        border-radius: 15px;
        padding: 1em;
        margin: 1em 0;
        background-color: #f5f5f5;
    }
    .sql-result {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 1em;
        margin: 1em 0;
        border-left: 4px solid #FF4B2B;
    }
    .status-message {
        padding: 1em;
        border-radius: 10px;
        margin: 1em 0;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
    }
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
    }
    /* Dark mode adjustments */
    @media (prefers-color-scheme: dark) {
        .chat-container {
            background-color: #2b2b2b;
        }
        .sql-result {
            background-color: #1e1e1e;
        }
    }
    </style>
"""


# Add a function to handle theme customization
def apply_theme():
    """Apply custom theme settings to the Streamlit app."""
    # Set page configuration
    st.set_page_config(
        page_title="Universal MCP Agent",
        page_icon=":crystal_ball:",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    # Apply custom CSS
    st.markdown(CUSTOM_CSS, unsafe_allow_html=True)



================================================
FILE: cookbook/examples/streamlit_apps/medical_imaging/README.md
================================================
# Medical Imaging Diagnosis Agent

Medical Imaging Diagnosis Agent is a medical imaging analysis agent that analyzes medical images and provides detailed findings by utilizing models and external tools.

### 1. Create a virtual environment

```shell
./scripts/cookbook_setup.py
source ./agnoenv/bin/activate

### 2. Install requirements

```shell
pip install -r cookbook/examples/streamlit_apps/medical_imaging/requirements.txt
```

### 3. Export `GOOGLE_API_KEY`

```shell
export GOOGLE_API_KEY=****
```

### 4. Run Streamlit App

```shell
streamlit run cookbook/examples/streamlit_apps/medical_imaging/app.py
```



================================================
FILE: cookbook/examples/streamlit_apps/medical_imaging/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/medical_imaging/app.py
================================================
import os

import streamlit as st
from agno.media import Image as AgnoImage
from medical_agent import agent
from PIL import Image as PILImage

st.set_page_config(
    page_title="Medical Imaging Analysis",
    page_icon="🏥",
    layout="wide",
)
st.markdown("##### 🏥 built using [Agno](https://github.com/agno-agi/agno)")


def main():
    with st.sidebar:
        st.info(
            "This tool provides AI-powered analysis of medical imaging data using "
            "advanced computer vision and radiological expertise."
        )
        st.warning(
            "⚠DISCLAIMER: This tool is for educational and informational purposes only. "
            "All analyses should be reviewed by qualified healthcare professionals. "
            "Do not make medical decisions based solely on this analysis."
        )

    st.title("🏥 Medical Imaging Diagnosis Agent")
    st.write("Upload a medical image for professional analysis")

    # Create containers for better organization
    upload_container = st.container()
    image_container = st.container()
    analysis_container = st.container()

    with upload_container:
        uploaded_file = st.file_uploader(
            "Upload Medical Image",
            type=["jpg", "jpeg", "png", "dicom"],
            help="Supported formats: JPG, JPEG, PNG, DICOM",
        )

    if uploaded_file is not None:
        with image_container:
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                # Use PILImage for display
                pil_image = PILImage.open(uploaded_file)
                width, height = pil_image.size
                aspect_ratio = width / height
                new_width = 500
                new_height = int(new_width / aspect_ratio)
                resized_image = pil_image.resize((new_width, new_height))

                st.image(
                    resized_image,
                    caption="Uploaded Medical Image",
                    use_container_width=True,
                )

                analyze_button = st.button(
                    "🔍 Analyze Image", type="primary", use_container_width=True
                )

                additional_info = st.text_area(
                    "Provide additional context about the image (e.g., patient history, symptoms)",
                    placeholder="Enter any relevant information here...",
                )

        with analysis_container:
            if analyze_button:
                image_path = "temp_medical_image.png"
                # Save the resized image
                resized_image.save(image_path, format="PNG")

                with st.spinner("🔄 Analyzing image... Please wait."):
                    try:
                        # Read the image file as binary
                        with open(image_path, "rb") as f:
                            image_bytes = f.read()
                        # creating an instance of Image
                        agno_image = AgnoImage(content=image_bytes, format="png")

                        prompt = (
                            f"Analyze this medical image considering the following context: {additional_info}"
                            if additional_info
                            else "Analyze this medical image and provide detailed findings."
                        )
                        response = agent.run(prompt, images=[agno_image])
                        st.markdown("### 📋 Analysis Results")
                        st.markdown("---")
                        if hasattr(response, "content"):
                            st.markdown(response.content)
                        elif isinstance(response, str):
                            st.markdown(response)
                        elif isinstance(response, dict) and "content" in response:
                            st.markdown(response["content"])
                        else:
                            st.markdown(str(response))
                        st.markdown("---")
                        st.caption(
                            "Note: This analysis is generated by AI and should be reviewed by "
                            "a qualified healthcare professional."
                        )

                    except Exception as e:
                        st.error(f"Analysis error: {str(e)}")
                        st.info(
                            "Please try again or contact support if the issue persists."
                        )
                        print(f"Detailed error: {e}")
                    finally:
                        if os.path.exists(image_path):
                            os.remove(image_path)

    else:
        st.info("👆 Please upload a medical image to begin analysis")


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/medical_imaging/medical_agent.py
================================================
"""
Medical Imaging Analysis Agent Tutorial
=====================================

This example demonstrates how to create an AI agent specialized in medical imaging analysis.
The agent can analyze various types of medical images (X-ray, MRI, CT, Ultrasound) and provide
detailed professional analysis along with patient-friendly explanations.

"""

from pathlib import Path

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

# Base prompt that defines the agent's expertise and response structure
BASE_PROMPT = """You are a highly skilled medical imaging expert with extensive knowledge in radiology 
and diagnostic imaging. Your role is to provide comprehensive, accurate, and ethical analysis of medical images.

Key Responsibilities:
1. Maintain patient privacy and confidentiality
2. Provide objective, evidence-based analysis
3. Highlight any urgent or critical findings
4. Explain findings in both professional and patient-friendly terms

For each image analysis, structure your response as follows:"""

# Detailed instructions for image analysis
ANALYSIS_TEMPLATE = """
### 1. Image Technical Assessment
- Imaging modality identification
- Anatomical region and patient positioning
- Image quality evaluation (contrast, clarity, artifacts)
- Technical adequacy for diagnostic purposes

### 2. Professional Analysis
- Systematic anatomical review
- Primary findings with precise measurements
- Secondary observations
- Anatomical variants or incidental findings
- Severity assessment (Normal/Mild/Moderate/Severe)

### 3. Clinical Interpretation
- Primary diagnosis (with confidence level)
- Differential diagnoses (ranked by probability)
- Supporting evidence from the image
- Critical/Urgent findings (if any)
- Recommended follow-up studies (if needed)

### 4. Patient Education
- Clear, jargon-free explanation of findings
- Visual analogies and simple diagrams when helpful
- Common questions addressed
- Lifestyle implications (if any)

### 5. Evidence-Based Context
Using DuckDuckGo search:
- Recent relevant medical literature
- Standard treatment guidelines
- Similar case studies
- Technological advances in imaging/treatment
- 2-3 authoritative medical references

Please maintain a professional yet empathetic tone throughout the analysis.
"""

# Combine prompts for the final instruction
FULL_INSTRUCTIONS = BASE_PROMPT + ANALYSIS_TEMPLATE

# Initialize the Medical Imaging Expert agent
agent = Agent(
    name="Medical Imaging Expert",
    model=Gemini(id="gemini-2.0-flash-exp"),
    tools=[DuckDuckGoTools()],  # Enable web search for medical literature
    markdown=True,  # Enable markdown formatting for structured output
    instructions=FULL_INSTRUCTIONS,
)

# Example usage
if __name__ == "__main__":
    # Example image path (users should replace with their own image)
    image_path = Path(__file__).parent.joinpath("test.jpg")

    # Uncomment to run the analysis
    # agent.print_response("Please analyze this medical image.", images=[image_path])

    # Example with specific focus
    # agent.print_response(
    #     "Please analyze this image with special attention to bone density.",
    #     images=[image_path]
    # )



================================================
FILE: cookbook/examples/streamlit_apps/medical_imaging/requirements.txt
================================================
agno
openai
streamlit
ddgs



================================================
FILE: cookbook/examples/streamlit_apps/paperpal/README.md
================================================
# Paperpal Workflow

Paperpal is a research and technical blog writer workflow that writes a detailed blog on research topics referencing research papers by utilizing models and external tools: Exa and ArXiv

### 1. Create a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Install requirements

```shell
pip install -r cookbook/examples/streamlit_apps/paperpal/requirements.txt
```

### 3. Export `OPENAI_API_KEY` and `EXA_API_KEY`

```shell
export OPENAI_API_KEY=sk-***
export EXA_API_KEY=***
```

### 4. Run Streamlit App

```shell
streamlit run cookbook/examples/streamlit_apps/paperpal/app.py
```



================================================
FILE: cookbook/examples/streamlit_apps/paperpal/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/paperpal/app.py
================================================
import json
from typing import Optional

import pandas as pd
import streamlit as st
from technical_writer import (
    ArxivSearchResults,
    SearchTerms,
    WebSearchResults,
    arxiv_search_agent,
    arxiv_toolkit,
    exa_search_agent,
    research_editor,
    search_term_generator,
)

# Streamlit App Configuration
st.set_page_config(
    page_title="AI Researcher Workflow",
    page_icon=":orange_heart:",
)
st.title("Paperpal")
st.markdown("##### :orange_heart: built by [agno](https://github.com/agno-agi/agno)")


def main() -> None:
    # Get topic for report
    input_topic = st.sidebar.text_input(
        ":female-scientist: Enter a topic",
        value="LLM evals in multi-agentic space",
    )
    # Button to generate blog
    generate_report = st.sidebar.button("Generate Blog")
    if generate_report:
        st.session_state["topic"] = input_topic

    # Checkboxes for search
    st.sidebar.markdown("## Agents")
    search_exa = st.sidebar.checkbox("Exa Search", value=True)
    search_arxiv = st.sidebar.checkbox("ArXiv Search", value=False)
    # search_pubmed = st.sidebar.checkbox("PubMed Search", disabled=True)  # noqa
    # search_google_scholar = st.sidebar.checkbox("Google Scholar Search", disabled=True)  # noqa
    # use_cache = st.sidebar.toggle("Use Cache", value=False, disabled=True)  # noqa
    num_search_terms = st.sidebar.number_input(
        "Number of Search Terms",
        value=2,
        min_value=2,
        max_value=3,
        help="This will increase latency.",
    )

    st.sidebar.markdown("---")
    st.sidebar.markdown("## Trending Topics")
    topic = "Humanoid and Autonomous Agents"
    if st.sidebar.button(topic):
        st.session_state["topic"] = topic

    topic = "Gene Editing for Disease Treatment"
    if st.sidebar.button(topic):
        st.session_state["topic"] = topic

    topic = "Multimodal AI in healthcare"
    if st.sidebar.button(topic):
        st.session_state["topic"] = topic

    topic = "Brain Aging and Neurodegenerative Diseases"
    if st.sidebar.button(topic):
        st.session_state["topic"] = topic

    if "topic" in st.session_state:
        report_topic = st.session_state["topic"]

        search_terms: Optional[SearchTerms] = None
        with st.status("Generating Search Terms", expanded=True) as status:
            with st.container():
                search_terms_container = st.empty()
                search_generator_input = {
                    "topic": report_topic,
                    "num_terms": num_search_terms,
                }
                search_terms = search_term_generator.run(
                    json.dumps(search_generator_input)
                ).content
                if search_terms:
                    search_terms_container.json(search_terms.model_dump())
            status.update(
                label="Search Terms Generated", state="complete", expanded=False
            )

        if not search_terms:
            st.write("Sorry report generation failed. Please try again.")
            return

        exa_content: Optional[str] = None
        arxiv_content: Optional[str] = None

        if search_exa:
            with st.status("Searching Exa", expanded=True) as status:
                with st.container():
                    exa_container = st.empty()
                    try:
                        exa_search_results = exa_search_agent.run(
                            search_terms.model_dump_json(indent=4)
                        )
                        if isinstance(exa_search_results, str):
                            raise ValueError(
                                "Unexpected string response from exa_search_agent"
                            )

                        if isinstance(exa_search_results.content, WebSearchResults):
                            exa_container.json(exa_search_results.content.results)
                            if (
                                exa_search_results
                                and exa_search_results.content
                                and len(exa_search_results.content.results) > 0
                            ):
                                exa_content = (
                                    exa_search_results.content.model_dump_json(indent=4)
                                )
                                exa_container.json(exa_search_results.content.results)
                                status.update(
                                    label="Exa Search Complete",
                                    state="complete",
                                    expanded=False,
                                )
                        else:
                            raise TypeError("Unexpected response from exa_search_agent")

                    except Exception as e:
                        st.error(f"An error occurred during Exa search: {e}")
                        status.update(
                            label="Exa Search Failed", state="error", expanded=True
                        )
                        exa_content = None

        if search_arxiv:
            with st.status(
                "Searching ArXiv (this takes a while)", expanded=True
            ) as status:
                with st.container():
                    arxiv_container = st.empty()
                    arxiv_search_results = arxiv_search_agent.run(
                        search_terms.model_dump_json(indent=4)
                    )
                    if isinstance(arxiv_search_results.content, ArxivSearchResults):
                        if (
                            arxiv_search_results
                            and arxiv_search_results.content
                            and arxiv_search_results.content.results
                        ):
                            arxiv_container.json(
                                [
                                    result.model_dump()
                                    for result in arxiv_search_results.content.results
                                ]
                            )
                    else:
                        raise TypeError("Unexpected response from arxiv_search_agent")

                status.update(
                    label="ArXiv Search Complete", state="complete", expanded=False
                )

            if (
                arxiv_search_results
                and arxiv_search_results.content
                and arxiv_search_results.content.results
            ):
                paper_summaries = []
                for result in arxiv_search_results.content.results:
                    summary = {
                        "ID": result.id,
                        "Title": result.title,
                        "Authors": ", ".join(result.authors)
                        if result.authors
                        else "No authors available",
                        "Summary": result.summary[:200] + "..."
                        if len(result.summary) > 200
                        else result.summary,
                    }
                    paper_summaries.append(summary)

                if paper_summaries:
                    with st.status(
                        "Displaying ArXiv Paper Summaries", expanded=True
                    ) as status:
                        with st.container():
                            st.subheader("ArXiv Paper Summaries")
                            df = pd.DataFrame(paper_summaries)
                            st.dataframe(df, use_container_width=True)
                        status.update(
                            label="ArXiv Paper Summaries Displayed",
                            state="complete",
                            expanded=False,
                        )

                    arxiv_paper_ids = [summary["ID"] for summary in paper_summaries]
                    if arxiv_paper_ids:
                        with st.status("Reading ArXiv Papers", expanded=True) as status:
                            with st.container():
                                arxiv_content = arxiv_toolkit.read_arxiv_papers(
                                    arxiv_paper_ids, pages_to_read=2
                                )
                                st.write(f"Read {len(arxiv_paper_ids)} ArXiv papers")
                            status.update(
                                label="Reading ArXiv Papers Complete",
                                state="complete",
                                expanded=False,
                            )

        report_input = ""
        report_input += f"# Topic: {report_topic}\n\n"
        report_input += "## Search Terms\n\n"
        report_input += f"{search_terms}\n\n"
        if arxiv_content:
            report_input += "## ArXiv Papers\n\n"
            report_input += "<arxiv_papers>\n\n"
            report_input += f"{arxiv_content}\n\n"
            report_input += "</arxiv_papers>\n\n"
        if exa_content:
            report_input += "## Web Search Content from Exa\n\n"
            report_input += "<exa_content>\n\n"
            report_input += f"{exa_content}\n\n"
            report_input += "</exa_content>\n\n"

        # Only generate the report if we have content
        if arxiv_content or exa_content:
            with st.spinner("Generating Blog"):
                final_report_container = st.empty()
                research_report = research_editor.run(report_input)
                final_report_container.markdown(research_report.content)
        else:
            st.error(
                "Report generation cancelled due to search failure. Please try again or select another search option."
            )

    st.sidebar.markdown("---")
    if st.sidebar.button("Restart"):
        st.rerun()


main()



================================================
FILE: cookbook/examples/streamlit_apps/paperpal/requirements.txt
================================================
agno
openai
streamlit
exa_py
arxiv



================================================
FILE: cookbook/examples/streamlit_apps/paperpal/technical_writer.py
================================================
import os
from pathlib import Path
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.arxiv import ArxivTools
from agno.tools.exa import ExaTools
from dotenv import load_dotenv
from pydantic import BaseModel, Field

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


# Define data models
class SearchTerms(BaseModel):
    terms: List[str] = Field(
        ..., description="List of search terms related to a topic."
    )


class ArxivSearchResult(BaseModel):
    title: str = Field(..., description="Title of the article.")
    id: str = Field(..., description="The ID of the article.")
    authors: List[str] = Field(..., description="Authors of the article.")
    summary: str = Field(..., description="Summary of the article.")
    pdf_url: str = Field(..., description="URL of the PDF of the article.")
    links: List[str] = Field(..., description="Links related to the article.")
    reasoning: str = Field(..., description="Reason for selecting this article.")


class ArxivSearchResults(BaseModel):
    results: List[ArxivSearchResult] = Field(
        ..., description="List of top search results."
    )


class WebSearchResult(BaseModel):
    title: str = Field(..., description="Title of the article.")
    summary: str = Field(..., description="Summary of the article.")
    links: List[str] = Field(..., description="Links related to the article.")
    reasoning: str = Field(..., description="Reason for selecting this article.")


class WebSearchResults(BaseModel):
    results: List[WebSearchResult] = Field(
        ..., description="List of top search results."
    )


# Initialize tools
arxiv_toolkit = ArxivTools(
    download_dir=Path(__file__).parent.parent.parent.parent.joinpath(
        "wip", "arxiv_pdfs"
    )
)
exa_tools = ExaTools()

# Initialize agents
search_term_generator = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="""
You are an expert research strategist. Generate 2 specific and distinct search terms that will capture different key aspects of the given topic.
Focus on terms that are:
    - Specific enough to yield relevant results
    - Cover both technical and practical aspects of the topic
    - Relevant to current developments
    - Optimized for searching academic and web resources effectively

Provide the search terms as a list of strings like ["xyz", "abc", ...]
""",
    response_model=SearchTerms,
)

arxiv_search_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="""
You are an expert in academic research with access to ArXiv's database.

Your task is to:
1. Search ArXiv for the top 10 papers related to the provided search term.
2. Select the 3 most relevant research papers based on:
    - Direct relevance to the search term.
    - Scientific impact (e.g., citations, journal reputation).
    - Recency of publication.

For each selected paper, the output should be in json structure have these details:
    - title
    - id
    - authors
    - a concise summary
    - the PDF link of the research paper
    - links related to the research paper
    - reasoning for why the paper was chosen

Ensure the selected research papers directly address the topic and offer valuable insights.
""",
    tools=[arxiv_toolkit],
    response_model=ArxivSearchResults,
)

exa_search_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="""
You are a web search expert specializing in extracting high-quality information.

Your task is to:
1. Given a topic, search Exa for the top 10 articles about that topic.
2. Select the 3 most relevant articles based on:
    - Source credibility.
    - Content depth and relevance.

For each selected article, the output should have:
    - title
    - a concise summary
    - related links to the article
    - reasoning for why the article was chosen and how it contributes to understanding the topic.

Ensure the selected articles are credible, relevant, and provide significant insights into the topic.
""",
    tools=[ExaTools()],
    response_model=WebSearchResults,
)

research_editor = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="""
You are a senior research editor specializing in breaking complex topics and information into understandable, engaging, high-quality blogs.

Your task is to:
1. Create a detailed blog within 1000 words based on the given topic.
2. The blog should be of max 7-8 paragraphs, understandable, intuitive, making things easy to understand for the reader.
3. Highlight key findings and provide a clear, high-level overview of the topic.
4. At the end add the supporting articles link, paper link or any findings you think is necessary to add.

The blog should help the reader in getting a decent understanding of the topic.
The blog should me in markdown format.
""",
    instructions=[
        "Analyze all materials before writing.",
        "Build a clear narrative structure.",
        "Balance technical accuracy with explainability.",
    ],
    markdown=True,
)



================================================
FILE: cookbook/examples/streamlit_apps/parallel_world_builder/README.md
================================================
# Parallel World

This advanced example shows how to build a parallel world builder using Agno and imagination and creativity.

> Note: Fork and clone this repository if needed

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install requirements

```shell
pip install -r cookbook/examples/streamlit_apps/parallel_world_builder/requirements.txt
```

### 3. Export `OPENAI_API_KEY`

```shell
export OPENAI_API_KEY=sk-***
```

Other API keys are optional, but if you'd like to test:

```shell
export ANTHROPIC_API_KEY=***
export GOOGLE_API_KEY=***
```

### 4. Run Streamlit App

```shell
streamlit run cookbook/examples/streamlit_apps/parallel_world_builder/app.py
```

- Open [localhost:8501](http://localhost:8501) to view the Parallel World Builder.

### 5. Message us on [discord](https://agno.link/discord) if you have any questions



================================================
FILE: cookbook/examples/streamlit_apps/parallel_world_builder/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/parallel_world_builder/agents.py
================================================
"""🌎 World Builder - Your AI World Creator!

This advanced example shows how to build a sophisticated world building system that
creates rich, detailed fictional worlds.

Example prompts to try:
- "Create a world where time flows backwards"
- "Design a steampunk world powered by dreams"
- "Build an underwater civilization in a gas giant"
- "Make a world where music is the source of magic"
- "Design a world where plants are sentient and rule"
- "Create a world inside a giant computer simulation"

View the README for instructions on how to run the application.
"""

from textwrap import dedent
from typing import List

from agno.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field


class World(BaseModel):
    """Model representing a fictional world with its key attributes."""

    name: str = Field(
        ...,
        description=(
            "The name of this world. Be exceptionally creative and unique. "
            "Avoid using simple names like Futura, Earth, or other common names."
        ),
    )
    characteristics: List[str] = Field(
        ...,
        description=(
            "Key attributes of the world. Examples: Ethereal, Arcane, Quantum-Fueled, "
            "Dreamlike, Mechanized, Harmonious. Think outside the box."
        ),
    )
    currency: str = Field(
        ...,
        description=(
            "The monetary system or trade medium in the world. "
            "Consider unusual or symbolic currencies (e.g., Memory Crystals, Void Shards)."
        ),
    )
    languages: List[str] = Field(
        ...,
        description=(
            "The languages spoken in the world. Invent languages with unique phonetics, "
            "scripts, or origins. Examples: Elurian, Syneth, Aeon's Glyph."
        ),
    )
    history: str = Field(
        ...,
        description=(
            "The detailed history of the world spanning at least 100,000 years. "
            "Include pivotal events, revolutions, cataclysms, golden ages, and more. "
            "Make it immersive and richly detailed."
        ),
    )
    wars: List[str] = Field(
        ...,
        description=(
            "List of major wars or conflicts that shaped the world. Each should have unique "
            "motivations, participants, and consequences."
        ),
    )
    drugs: List[str] = Field(
        ...,
        description=(
            "Substances used in the world, either recreationally, medically, or spiritually. "
            "Invent intriguing names and effects (e.g., Lunar Nectar, Dreamweaver Elixir)."
        ),
    )


def get_world_builder(
    model_id: str = "openai:gpt-4o",
    debug_mode: bool = False,
) -> Agent:
    """Returns an instance of the World Builder Agent.

    Args:
        model: Model identifier to use
        debug_mode: Enable debug logging
    """
    # Parse model provider and name
    provider, model_name = model_id.split(":")

    # Select appropriate model class based on provider
    if provider == "openai":
        model = OpenAIChat(id=model_name)
    elif provider == "google":
        model = Gemini(id=model_name)
    elif provider == "anthropic":
        model = Claude(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")

    return Agent(
        name="world_builder",
        model=model,
        description=dedent("""\
        You are WorldCrafter-X, an elite world building specialist focused on:
        
        - Unique world concepts
        - Rich cultural details  
        - Complex histories
        - Innovative systems
        - Compelling conflicts
        - Immersive atmospheres
        
        You combine boundless creativity with meticulous attention to detail to craft unforgettable worlds."""),
        instructions=dedent("""\
        You are tasked with creating entirely unique and intricate worlds.
        
        When a user provides a world description:
        1. Carefully analyze all aspects of the requested world
        2. Think deeply about how different elements would interact
        3. Create rich, interconnected systems and histories
        4. Ensure internal consistency while being creative
        5. Focus on unique and memorable details
        6. Avoid clichés and common tropes
        7. Consider long-term implications of world features
        8. Create compelling conflicts and dynamics
        
        Remember to:
        - Push creative boundaries
        - Use vivid, evocative language
        - Create memorable names and terms
        - Maintain internal logic
        - Consider multiple cultural perspectives
        - Add unexpected but fitting elements"""),
        response_model=World,
        debug_mode=debug_mode,
    )



================================================
FILE: cookbook/examples/streamlit_apps/parallel_world_builder/app.py
================================================
from typing import Optional

import streamlit as st
from agents import World, get_world_builder
from agno.agent import Agent
from agno.utils.log import logger
from utils import add_message, display_tool_calls, sidebar_widget

# set page config
st.set_page_config(
    page_title="World Building",
    page_icon=":ringed_planet:",
    layout="wide",
    initial_sidebar_state="expanded",
)


def main() -> None:
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>Parallel World Building</h1>", unsafe_allow_html=True
    )
    st.markdown(
        "<p class='subtitle'>Your intelligent world creator powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    model_options = {
        "gpt-4o": "openai:gpt-4o",
        "gemini-2.0-flash-exp": "google:gemini-2.0-flash-exp",
        "claude-3-5-sonnet": "anthropic:claude-3-5-sonnet-20241022",
    }
    selected_model = st.sidebar.selectbox(
        "Select a model",
        options=list(model_options.keys()),
        index=0,
        key="model_selector",
    )
    model_id = model_options[selected_model]

    ####################################################################
    # Initialize Agent
    ####################################################################
    world_builder: Agent
    if (
        "world_builder" not in st.session_state
        or st.session_state["world_builder"] is None
        or st.session_state.get("current_model") != model_id
    ):
        logger.info("---*--- Creating new World Builder agent ---*---")
        world_builder = get_world_builder(model_id=model_id)
        st.session_state["world_builder"] = world_builder
        st.session_state["current_model"] = model_id
    else:
        world_builder = st.session_state["world_builder"]

    ####################################################################
    # Initialize messages if not exists
    ####################################################################
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    ####################################################################
    # Sidebar
    ####################################################################
    sidebar_widget()

    ####################################################################
    # Get user input
    ####################################################################
    if prompt := st.chat_input("Describe your world! 🌏"):
        add_message("user", prompt)

    ####################################################################
    # Display chat history
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] in ["user", "assistant"]:
            with st.chat_message(message["role"]):
                if "tool_calls" in message and message["tool_calls"]:
                    display_tool_calls(st.empty(), message["tool_calls"])
                st.markdown(message["content"])

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        with st.chat_message("assistant"):
            # Create container for tool calls
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner("🤔 Generating world..."):
                try:
                    # Run the agent and get response
                    run_response = world_builder.run(question)
                    world_data: World = run_response.content

                    # Display world details in a single column layout
                    st.header(world_data.name)

                    st.subheader("🌟 Characteristics")
                    for char in world_data.characteristics:
                        st.markdown(f"- {char}")

                    st.subheader("💰 Currency")
                    st.markdown(world_data.currency)

                    st.subheader("🗣️ Languages")
                    for lang in world_data.languages:
                        st.markdown(f"- {lang}")

                    st.subheader("⚔️ Major Wars & Conflicts")
                    for war in world_data.wars:
                        st.markdown(f"- {war}")

                    st.subheader("🧪 Notable Substances")
                    for drug in world_data.drugs:
                        st.markdown(f"- {drug}")

                    st.subheader("📜 History")
                    st.markdown(world_data.history)

                    # Store the formatted response for chat history
                    response = f"""# {world_data.name}

### Characteristics
{chr(10).join("- " + char for char in world_data.characteristics)}

### Currency
{world_data.currency}

### Languages
{chr(10).join("- " + lang for lang in world_data.languages)}

### History
{world_data.history}

### Major Wars & Conflicts
{chr(10).join("- " + war for war in world_data.wars)}

### Notable Substances
{chr(10).join("- " + drug for drug in world_data.drugs)}"""

                    # Display tool calls if available+
                    if run_response.tools and len(run_response.tools) > 0:
                        display_tool_calls(tool_calls_container, run_response.tools)

                    add_message("assistant", response, run_response.tools)

                except Exception as e:
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    add_message("assistant", error_message)
                    st.error(error_message)


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/parallel_world_builder/requirements.txt
================================================
agno
openai
streamlit
google-generativeai
anthropic



================================================
FILE: cookbook/examples/streamlit_apps/parallel_world_builder/utils.py
================================================
from typing import Any, Dict, List, Optional

import streamlit as st
from agno.models.response import ToolExecution
from agno.utils.log import logger


def add_message(
    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None
) -> None:
    """Safely add a message to the session state"""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append(
        {"role": role, "content": content, "tool_calls": tool_calls}
    )


def restart_agent():
    """Reset the agent and clear chat history"""
    logger.debug("---*--- Restarting agent ---*---")
    st.session_state["world_builder"] = None
    st.session_state["world"] = None
    st.session_state["messages"] = []
    st.rerun()


def sidebar_widget() -> None:
    """Display a sidebar with sample user queries"""
    with st.sidebar:
        # Basic Information
        st.markdown("#### Sample Queries")
        if st.button(
            "An advanced futuristic city on distant planet with only 1 island. Dark history. Population 1 trillion."
        ):
            add_message(
                "user",
                "An advanced futuristic city on distant planet with only 1 island. Dark history. Population 1 trillion.",
            )
        if st.button("A medieval fantasy world with dragons, castles, and knights."):
            add_message(
                "user",
                "A medieval fantasy world with dragons, castles, and knights.",
            )
        if st.button(
            "A post-apocalyptic world with a nuclear wasteland and a small community living in a dome."
        ):
            add_message(
                "user",
                "A post-apocalyptic world with a nuclear wasteland and a small community living in a dome.",
            )
        if st.button(
            "A world with a mix of ancient and modern civilizations, where magic and technology coexist."
        ):
            add_message(
                "user",
                "A world with a mix of ancient and modern civilizations, where magic and technology coexist.",
            )

        st.markdown("---")
        if st.button("🔄 New World"):
            restart_agent()


def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):
    """Display tool calls in a streamlit container with expandable sections.

    Args:
        tool_calls_container: Streamlit container to display the tool calls
        tools: List of tool call dictionaries containing name, args, content, and metrics
    """
    with tool_calls_container.container():
        for tool_call in tools:
            _tool_name = tool_call.tool_name or "Unknown Tool"
            _tool_args = tool_call.tool_args or {}
            _content = tool_call.result or None
            _metrics = tool_call.metrics or None

            with st.expander(
                f"🛠️ {_tool_name.replace('_', ' ').title()}", expanded=False
            ):
                if isinstance(_tool_args, dict) and "query" in _tool_args:
                    st.code(_tool_args["query"], language="sql")

                if _tool_args and _tool_args != {"query": None}:
                    st.markdown("**Arguments:**")
                    st.json(_tool_args)

                if _content:
                    st.markdown("**Results:**")
                    try:
                        st.json(_content)
                    except Exception as e:
                        st.markdown(_content)

                if _metrics:
                    st.markdown("**Metrics:**")
                    st.json(
                        _metrics if isinstance(_metrics, dict) else _metrics.to_dict()
                    )



================================================
FILE: cookbook/examples/streamlit_apps/podcast_generator/readme.md
================================================
# Podify AI 🎙

Podify AI is an AI-powered podcast agent that generates high-quality podcasts on any topic.
It uses real-time search using DuckDuckGo and AI-generated narration to create professional podcast scripts with realistic voices.

🎧 Enter a topic → model writes a script → narrates it → you listen & download!

## Features

- **AI-Generated Podcasts**: Automatically researches & generates podcast scripts.
- **Realistic AI Voices**: Choose from multiple AI voices for narration.
- **Download & Share**: Save and share your generated podcasts.
- **Real-Time Research**: Uses DuckDuckGo for up-to-date insights.
---

## Setup Instructions

### 1. Create a virtual environment

```shell
python3 -m venv ~/.venvs/podifyenv
source ~/.venvs/podifyenv/bin/activate
```

### 2. Install requirements

```shell
pip install -r cookbook/examples/streamlit_apps/podcast_generator/requirements.txt
```

### 3. Export `OPENAI_API_KEY`

```shell
export OPENAI_API_KEY=***
```

### 4. Run Streamlit App

```shell
streamlit run cookbook/examples/streamlit_apps/podcast_generator/app.py
```



================================================
FILE: cookbook/examples/streamlit_apps/podcast_generator/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/podcast_generator/agents.py
================================================
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.audio import write_audio_to_file
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# OpenAI API Key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

os.makedirs("tmp", exist_ok=True)


def generate_podcast(topic, voice="alloy"):
    """
    Generates a podcast script using a agnodata Agent and converts it to speech using OpenAI TTS.

    Args:
        topic (str): The topic of the podcast.
        voice (str): Voice model for OpenAI TTS. Options: ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
    """

    # Create a agnodata agent to generate the podcast script
    audio_agent = Agent(
        system_message="You are a podcast scriptwriter specializing in concise and engaging narratives. Your task is to research a given topic using Exa and DuckDuckGo, gather relevant insights, and compose a short, compelling podcast script.",
        instructions="""### **Instructions:**
            1. **Research Phase:**
            - Use Exa and DuckDuckGo to gather the most recent and relevant information on the given topic.
            - Prioritize trustworthy sources such as news sites, academic articles, or well-established blogs.
            - Identify key points, statistics, expert opinions, and interesting facts.

            2. **Scripting Phase:**
            - Write a concise podcast script in a conversational tone.
            - Begin with a strong hook to capture the listener's attention.
            - Present the key insights in an engaging, easy-to-follow manner.
            - Include a smooth transition between ideas to maintain narrative flow.
            - End with a closing remark that summarizes the main takeaways and encourages further curiosity.

            ### **Formatting Guidelines:**
            - Use simple, engaging language.
            - Keep the script under 300 words (around 2 minutes of audio).
            - Write in a natural, spoken format, avoiding overly formal or technical jargon.
            - Start with a short intro of the topic, then cover the main content, and conclude.

            ### **Example Output:**
            #### **Today we will be covering the topic The Future of AI in Healthcare**
            "Imagine walking into a hospital where AI instantly diagnoses your illness, prescribes treatment, and even assists in surgery. Sounds like science fiction? Well, it's closer to reality than you think! Welcome to today's episode, where we explore how AI is revolutionizing healthcare."

            "AI is making waves in medical research, diagnostics, and patient care. For instance, Google's DeepMind developed an AI that can detect over 50 eye diseases with a single scan—just as accurately as top doctors! Meanwhile, robotic surgeries assisted by AI are reducing complications and recovery time for patients. But it's not just about tech—AI is also addressing healthcare accessibility. In rural areas, AI-powered chatbots provide medical advice where doctors are scarce."

            "While AI in healthcare is promising, it also raises ethical concerns. Who takes responsibility for a wrong diagnosis? How do we ensure data privacy? These are crucial questions for the future. One thing's for sure—AI is here to stay, and it's reshaping medicine as we know it. Thanks for tuning in, and see you next time!"
            """,
        model=OpenAIChat(
            id="gpt-4o-audio-preview",
            modalities=["text", "audio"],
            audio={"voice": voice, "format": "wav"},
        ),
        tools=[DuckDuckGoTools()],
    )

    # Generate the podcast script
    audio_agent.run(f"Write the content of podcast for the topic: {topic}")
    audio_file_path = "tmp/generated_podcast.wav"

    if audio_agent.run_response.response_audio is not None:
        audio_content = audio_agent.run_response.response_audio.content

        if audio_content:
            write_audio_to_file(
                audio=audio_content,
                filename=audio_file_path,
            )
            return audio_file_path
    return None



================================================
FILE: cookbook/examples/streamlit_apps/podcast_generator/app.py
================================================
import streamlit as st
from agents import generate_podcast

# Streamlit App Configuration
st.set_page_config(
    page_title="Podify AI 🎙️",
    page_icon="🎧",
    layout="wide",
)

# Sidebar Section
with st.sidebar:
    st.title("🎧 Podify AI")
    st.markdown("AI voices to generate an **engaging podcast**!")

    # Voice Selection
    voice_options = ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
    selected_voice = st.selectbox("🎤 Choose a Voice:", voice_options, index=0)

    st.markdown("---")
    st.subheader("🔥 Suggested Topics:")
    trending_topics = [
        "🎭 Impact of AI on Creativity and Art",
        "💡 Elon Musk vs Sam Altman",
        "🏥 Using AI in healthcare",
        "🚀 The Future of Space Exploration",
    ]

    # Create row-wise aligned buttons
    num_cols = 1  # Change this to 3 for three buttons in a row
    cols = st.columns(num_cols)  # Define columns

    for i, topic in enumerate(trending_topics):
        with cols[i % num_cols]:  # Distribute buttons evenly across columns
            if st.button(topic):
                st.session_state["topic"] = topic
                st.session_state["generate"] = True

    st.markdown("---")
    st.subheader("ℹ️ About")
    st.markdown(
        """
        - Enter a **topic** <br>
        - **Select a voice** <br>
        - **Click Generate Podcast** <br>
        - **Listen & Download** the AI-generated audio
        """,
        unsafe_allow_html=True,
    )

# Main Content
st.title("Podify AI🎙️")
st.markdown(":orange_heart: **powered by Agno**")

st.markdown(
    "Create high-quality podcasts on **any topic**! Simply enter a topic and let Podify AI generate a professional podcast with **realistic AI voices**. 🚀"
)

# Get pre-selected topic from sidebar
pre_selected_topic = st.session_state.get("topic", "")

# Input for Podcast Topic
topic = st.text_input(
    "📖 **Enter Your Podcast Topic Below:**",
    placeholder="E.g., How AI is Changing the Job Market",
    value=pre_selected_topic,
)

# Check if auto-generation is triggered
generate_now = st.session_state.get("generate", False)


# Generate Podcast Function
def generate_and_display_podcast(topic):
    with st.spinner("⏳ Generating Podcast... This may take up to 2 minute..."):
        audio_path = generate_podcast(topic, selected_voice)

    if audio_path:
        st.success("✅ Podcast generated successfully!")

        st.subheader("🎧 Your AI Podcast")
        st.audio(audio_path, format="audio/wav")

        with open(audio_path, "rb") as audio_file:
            st.download_button(
                "⬇️ Download Podcast",
                audio_file,
                file_name="podcast.wav",
                mime="audio/wav",
            )

    else:
        st.error("❌ Failed to generate podcast. Please try again.")


# Auto-generate podcast if a trending topic is selected
if generate_now and topic:
    generate_and_display_podcast(topic)
    st.session_state["generate"] = False  # Reset the flag after generation

# Manual Generate Podcast Button
if st.button("🎬 Generate Podcast"):
    if topic:
        generate_and_display_podcast(topic)
    else:
        st.warning("⚠️ Please enter a topic before generating.")

# Footer Section
st.markdown("---")
st.markdown(
    """
    🌟 **Features:**
    - 🎙️ AI-generated podcast scripts based on **real-time research**.
    - 🗣️ Multiple **realistic voices** for narration.
    - 📥 **Download & share** your podcasts instantly.

    📢 **Disclaimer:** AI-generated content is based on available online data and may not always be accurate.
    """,
    unsafe_allow_html=True,
)
st.markdown("---")
st.markdown(":orange_heart: **Thank you for using Podify AI!**")



================================================
FILE: cookbook/examples/streamlit_apps/podcast_generator/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/podcast_generator/requirements.in
================================================
agno
openai
streamlit
ddgs



================================================
FILE: cookbook/examples/streamlit_apps/podcast_generator/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.7.12
    # via -r cookbook/examples/streamlit_apps/podcast_generator/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anyio==4.10.0
    # via
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==6.1.0
    # via streamlit
certifi==2025.8.3
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.3
    # via requests
click==8.2.1
    # via
    #   ddgs
    #   streamlit
    #   typer
ddgs==9.5.4
    # via -r cookbook/examples/streamlit_apps/podcast_generator/requirements.in
distro==1.9.0
    # via openai
docstring-parser==0.17.0
    # via agno
gitdb==4.0.12
    # via gitpython
gitpython==3.1.45
    # via
    #   agno
    #   streamlit
h11==0.16.0
    # via httpcore
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.10.0
    # via openai
jsonschema==4.25.1
    # via altair
jsonschema-specifications==2025.4.1
    # via jsonschema
lxml==6.0.1
    # via ddgs
markdown-it-py==4.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
narwhals==2.1.2
    # via altair
numpy==2.3.2
    # via
    #   pandas
    #   pydeck
    #   streamlit
openai==1.101.0
    # via -r cookbook/examples/streamlit_apps/podcast_generator/requirements.in
packaging==25.0
    # via
    #   agno
    #   altair
    #   streamlit
pandas==2.3.2
    # via streamlit
pillow==11.3.0
    # via streamlit
primp==0.15.0
    # via ddgs
protobuf==6.32.0
    # via streamlit
pyarrow==21.0.0
    # via streamlit
pydantic==2.11.7
    # via
    #   agno
    #   openai
    #   pydantic-settings
pydantic-core==2.33.2
    # via pydantic
pydantic-settings==2.10.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.2
    # via rich
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.1.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.5
    # via streamlit
rich==14.1.0
    # via
    #   agno
    #   typer
rpds-py==0.27.0
    # via
    #   jsonschema
    #   referencing
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anyio
    #   openai
streamlit==1.48.1
    # via -r cookbook/examples/streamlit_apps/podcast_generator/requirements.in
tenacity==9.1.2
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.5.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.16.1
    # via agno
typing-extensions==4.14.1
    # via
    #   agno
    #   altair
    #   anyio
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.5.0
    # via requests



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/README.md
================================================
# SQrL: A Text2SQL Reasoning Agent

SQrL is an advanced text-to-SQL system that leverages Reasoning Agents to provide deep insights into any data. We'll use the F1 dataset as an example, but the system is designed to be easily extensible to other datasets.

The agent uses Reasoning Agents to search for table metadata and rules, enabling it to write and run better SQL queries. This process, called `Dynamic Few Shot Prompting`, is a technique that allows the agent to dynamically search for few shot examples to improve its performance.

SQrL also "thinks" before it acts. It will think about the user's question, and then decide to search its knowledge base before writing and running the SQL query.

SQrL also "analyzes" the result of the SQL query, which yield much better results.

> Note: Fork and clone the repository if needed

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install libraries

```shell
pip install -r cookbook/examples/streamlit_apps/sql_agent/requirements.txt
```

### 3. Run PgVector

Let's use Postgres for storing our data, but the SQL Agent should work with any database. This will also help us use `PgVector` for vector search.

> Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) first.

- Run using a helper script

```shell
./cookbook/scripts/run_pgvector.sh
```

- OR run using the docker run command

```shell
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16
```

### 4. Load F1 data

```shell
python cookbook/examples/streamlit_apps/sql_agent/load_f1_data.py
```

### 5. Export API Keys

We recommend using claude-3-7-sonnet for this task, but you can use any Model you like.

```shell
export ANTHROPIC_API_KEY=***
```

Other API keys are optional, but if you'd like to test:

```shell
export OPENAI_API_KEY=***
export GOOGLE_API_KEY=***
export GROQ_API_KEY=***
```

### 6. Load the knowledge base

The knowledge base contains table metadata, rules and sample queries, which are used by the Agent to improve responses. This is a dynamic few shot prompting technique. This data, stored in `cookbook/examples/streamlit_apps/sql_agent/knowledge/` folder, is used by the Agent at run-time to search for sample queries and rules. We only add a minimal amount of data to the knowledge base, but you can add as much as you like.

We recommend adding the following as you go along:
  - Add `table_rules` and `column_rules` to the table metadata. The Agent is prompted to follow them. This is useful when you want to guide the Agent to always query date in a particular format, or avoid certain columns.
  - Add sample SQL queries to the `cookbook/examples/streamlit_apps/sql_agent/knowledge/sample_queries.sql` file. This will give the Assistant a head start on how to write complex queries.

```shell
python cookbook/examples/streamlit_apps/sql_agent/load_knowledge.py
```

### 7. Run SQL Agent

```shell
streamlit run cookbook/examples/streamlit_apps/sql_agent/app.py
```

- Open [localhost:8501](http://localhost:8501) to view the SQL Agent.

### 8. Message us on [discord](https://agno.link/discord) if you have any questions




================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/agents.py
================================================
"""💎 Reasoning SQL Agent - Your AI Data Analyst!

This advanced example shows how to build a sophisticated text-to-SQL system that
leverages Reasoning Agents to provide deep insights into any data.

Example queries to try:
- "Who are the top 5 drivers with the most race wins?"
- "Compare Mercedes vs Ferrari performance in constructors championships"
- "Show me the progression of fastest lap times at Monza"
- "Which drivers have won championships with multiple teams?"
- "What tracks have hosted the most races?"
- "Show me Lewis Hamilton's win percentage by season"

Examples with table joins:
- "How many races did the championship winners win each year?"
- "Compare the number of race wins vs championship positions for constructors in 2019"
- "Show me Lewis Hamilton's race wins and championship positions by year"
- "Which drivers have both won races and set fastest laps at Monaco?"
- "Show me Ferrari's race wins and constructor championship positions from 2015-2020"

View the README for instructions on how to run the application.
"""

import json
from pathlib import Path
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.combined import CombinedKnowledgeBase
from agno.knowledge.json import JSONKnowledgeBase
from agno.knowledge.text import TextKnowledgeBase
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.tools.file import FileTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.sql import SQLTools
from agno.vectordb.pgvector import PgVector

# ************* Database Connection *************
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# *******************************

# ************* Paths *************
cwd = Path(__file__).parent
knowledge_dir = cwd.joinpath("knowledge")
output_dir = cwd.joinpath("output")

# Create the output directory if it does not exist
output_dir.mkdir(parents=True, exist_ok=True)
# *******************************

# ************* Storage & Knowledge *************
agent_storage = PostgresAgentStorage(
    db_url=db_url,
    # Store agent sessions in the ai.sql_agent_sessions table
    table_name="sql_agent_sessions",
    schema="ai",
)
agent_knowledge = CombinedKnowledgeBase(
    sources=[
        # Reads text files, SQL files, and markdown files
        TextKnowledgeBase(
            path=knowledge_dir,
            formats=[".txt", ".sql", ".md"],
        ),
        # Reads JSON files
        JSONKnowledgeBase(path=knowledge_dir),
    ],
    # Store agent knowledge in the ai.sql_agent_knowledge table
    vector_db=PgVector(
        db_url=db_url,
        table_name="sql_agent_knowledge",
        schema="ai",
        # Use OpenAI embeddings
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
    # 5 references are added to the prompt
    num_documents=5,
)
# *******************************

# ************* Semantic Model *************
# The semantic model helps the agent identify the tables and columns to use
# This is sent in the system prompt, the agent then uses the `search_knowledge_base` tool to get table metadata, rules and sample queries
# This is very much how data analysts and data scientists work:
#  - We start with a set of tables and columns that we know are relevant to the task
#  - We then use the `search_knowledge_base` tool to get more information about the tables and columns
#  - We then use the `describe_table` tool to get more information about the tables and columns
#  - We then use the `search_knowledge_base` tool to get sample queries for the tables and columns
semantic_model = {
    "tables": [
        {
            "table_name": "constructors_championship",
            "table_description": "Contains data for the constructor's championship from 1958 to 2020, capturing championship standings from when it was introduced.",
            "Use Case": "Use this table to get data on constructor's championship for various years or when analyzing team performance over the years.",
        },
        {
            "table_name": "drivers_championship",
            "table_description": "Contains data for driver's championship standings from 1950-2020, detailing driver positions, teams, and points.",
            "Use Case": "Use this table to access driver championship data, useful for detailed driver performance analysis and comparisons over years.",
        },
        {
            "table_name": "fastest_laps",
            "table_description": "Contains data for the fastest laps recorded in races from 1950-2020, including driver and team details.",
            "Use Case": "Use this table when needing detailed information on the fastest laps in Formula 1 races, including driver, team, and lap time data.",
        },
        {
            "table_name": "race_results",
            "table_description": "Race data for each Formula 1 race from 1950-2020, including positions, drivers, teams, and points.",
            "Use Case": "Use this table answer questions about a drivers career. Race data includes driver standings, teams, and performance.",
        },
        {
            "table_name": "race_wins",
            "table_description": "Documents race win data from 1950-2020, detailing venue, winner, team, and race duration.",
            "Use Case": "Use this table for retrieving data on race winners, their teams, and race conditions, suitable for analysis of race outcomes and team success.",
        },
    ]
}
semantic_model_str = json.dumps(semantic_model, indent=2)
# *******************************


def get_sql_agent(
    name: str = "SQL Agent",
    user_id: Optional[str] = None,
    model_id: str = "openai:gpt-4o",
    session_id: Optional[str] = None,
    debug_mode: bool = True,
) -> Agent:
    """Returns an instance of the SQL Agent.

    Args:
        user_id: Optional user identifier
        debug_mode: Enable debug logging
        model_id: Model identifier in format 'provider:model_name'
    """
    # Parse model provider and name
    provider, model_name = model_id.split(":")

    # Select appropriate model class based on provider
    if provider == "openai":
        model = OpenAIChat(id=model_name)
    elif provider == "google":
        model = Gemini(id=model_name)
    elif provider == "anthropic":
        model = Claude(id=model_name)
    elif provider == "groq":
        model = Groq(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")

    return Agent(
        name=name,
        model=model,
        user_id=user_id,
        session_id=session_id,
        storage=agent_storage,
        knowledge=agent_knowledge,
        # Enable Agentic RAG i.e. the ability to search the knowledge base on-demand
        search_knowledge=True,
        # Enable the ability to read the chat history
        read_chat_history=True,
        # Enable the ability to read the tool call history
        read_tool_call_history=True,
        # Add tools to the agent
        tools=[
            SQLTools(db_url=db_url, list_tables=False),
            FileTools(base_dir=output_dir),
            ReasoningTools(add_instructions=True, add_few_shot=True),
        ],
        debug_mode=debug_mode,
        description=dedent("""\
        You are SQrL, an elite Text2SQL Engine specializing in:

        - Historical race analysis
        - Driver performance metrics
        - Team championship insights
        - Track statistics and records
        - Performance trend analysis
        - Race strategy evaluation

        You combine deep F1 knowledge with advanced SQL expertise to uncover insights from decades of racing data."""),
        instructions=dedent(f"""\
        You are a SQL expert focused on writing precise, efficient queries.

        When a user messages you, determine if you need query the database or can respond directly.
        If you can respond directly, do so.

        If you need to query the database to answer the user's question, follow these steps:
        1. First identify the tables you need to query from the semantic model.
        2. Then, ALWAYS use the `search_knowledge_base(table_name)` tool to get table metadata, rules and sample queries.
        3. If table rules are provided, ALWAYS follow them.
        4. Then, "think" about query construction, don't rush this step.
        5. Follow a chain of thought approach before writing SQL, ask clarifying questions where needed.
        6. If sample queries are available, use them as a reference.
        7. If you need more information about the table, use the `describe_table` tool.
        8. Then, using all the information available, create one single syntactically correct PostgreSQL query to accomplish your task.
        9. If you need to join tables, check the `semantic_model` for the relationships between the tables.
            - If the `semantic_model` contains a relationship between tables, use that relationship to join the tables even if the column names are different.
            - If you cannot find a relationship in the `semantic_model`, only join on the columns that have the same name and data type.
            - If you cannot find a valid relationship, ask the user to provide the column name to join.
        10. If you cannot find relevant tables, columns or relationships, stop and ask the user for more information.
        11. Once you have a syntactically correct query, run it using the `run_sql_query` function.
        12. When running a query:
            - Do not add a `;` at the end of the query.
            - Always provide a limit unless the user explicitly asks for all results.
        13. After you run the query, "analyze" the results and return the answer in markdown format.
        14. Make sure to always "analyze" the results of the query before returning the answer.
        15. You Analysis should Reason about the results of the query, whether they make sense, whether they are complete, whether they are correct, could there be any data quality issues, etc.
        16. It is really important that you "analyze" and "validate" the results of the query.
        17. Always show the user the SQL you ran to get the answer.
        18. Continue till you have accomplished the task.
        19. Show results as a table or a chart if possible.

        After finishing your task, ask the user relevant followup questions like "was the result okay, would you like me to fix any problems?"
        If the user says yes, get the previous query using the `get_tool_call_history(num_calls=3)` function and fix the problems.
        If the user wants to see the SQL, get it using the `get_tool_call_history(num_calls=3)` function.

        Finally, here are the set of rules that you MUST follow:

        <rules>
        - Use the `search_knowledge_base(table_name)` tool to get table information from your knowledge base before writing a query.
        - Do not use phrases like "based on the information provided" or "from the knowledge base".
        - Always show the SQL queries you use to get the answer.
        - Make sure your query accounts for duplicate records.
        - Make sure your query accounts for null values.
        - If you run a query, explain why you ran it.
        - Always derive your answer from the data and the query.
        - **NEVER, EVER RUN CODE TO DELETE DATA OR ABUSE THE LOCAL SYSTEM**
        - ALWAYS FOLLOW THE `table rules` if provided. NEVER IGNORE THEM.
        </rules>\
        """),
        additional_context=dedent("""\n
        The `semantic_model` contains information about tables and the relationships between them.
        If the users asks about the tables you have access to, simply share the table names from the `semantic_model`.
        <semantic_model>
        """)
        + semantic_model_str
        + dedent("""
        </semantic_model>\
        """),
    )



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/app.py
================================================
import nest_asyncio
import streamlit as st
from agents import get_sql_agent
from agno.agent import Agent
from agno.utils.log import logger
from utils import (
    CUSTOM_CSS,
    about_widget,
    add_message,
    display_tool_calls,
    rename_session_widget,
    session_selector_widget,
    sidebar_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="SQrL: Text2SQL Reasoning Agent",
    page_icon="💎",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Load custom CSS with dark mode support
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def main() -> None:
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>SQrL: Text2SQL Reasoning Agent</h1>",
        unsafe_allow_html=True,
    )
    st.markdown(
        "<p class='subtitle'>SQrL is an intelligent SQL Agent that can think, analyze and reason, powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    model_options = {
        "o4-mini": "openai:o4-mini",
        "claude-3-7-sonnet": "anthropic:claude-3-7-sonnet-latest",
        "gpt-4.1": "openai:gpt-4.1",
        "o3": "openai:o3",
        "gemini-2.5-pro": "google:gemini-2.5-pro-preview-03-25",
        "llama-4-scout": "groq:meta-llama/llama-4-scout-17b-16e-instruct",
        "gpt-4o": "openai:gpt-4o",
    }
    selected_model = st.sidebar.selectbox(
        "Select a model",
        options=list(model_options.keys()),
        index=0,
        key="model_selector",
    )
    model_id = model_options[selected_model]

    ####################################################################
    # Initialize Agent
    ####################################################################
    sql_agent: Agent
    if (
        "sql_agent" not in st.session_state
        or st.session_state["sql_agent"] is None
        or st.session_state.get("current_model") != model_id
    ):
        logger.info("---*--- Creating new SQL agent ---*---")
        sql_agent = get_sql_agent(model_id=model_id)
        st.session_state["sql_agent"] = sql_agent
        st.session_state["current_model"] = model_id
    else:
        sql_agent = st.session_state["sql_agent"]

    ####################################################################
    # Load Agent Session from the database
    ####################################################################
    try:
        st.session_state["sql_agent_session_id"] = sql_agent.load_session()
    except Exception:
        st.warning("Could not create Agent session, is the database running?")
        return

    ####################################################################
    # Load runs from memory
    ####################################################################
    agent_runs = sql_agent.memory.runs
    if len(agent_runs) > 0:
        logger.debug("Loading run history")
        st.session_state["messages"] = []
        for _run in agent_runs:
            if _run.message is not None:
                add_message(_run.message.role, _run.message.content)
            if _run.response is not None:
                add_message("assistant", _run.response.content, _run.response.tools)
    else:
        logger.debug("No run history found")
        st.session_state["messages"] = []

    ####################################################################
    # Sidebar
    ####################################################################
    sidebar_widget()

    ####################################################################
    # Get user input
    ####################################################################
    if prompt := st.chat_input("👋 Ask me about F1 data from 1950 to 2020!"):
        add_message("user", prompt)

    ####################################################################
    # Display chat history
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] in ["user", "assistant"]:
            _content = message["content"]
            if _content is not None:
                with st.chat_message(message["role"]):
                    # Display tool calls if they exist in the message
                    if "tool_calls" in message and message["tool_calls"]:
                        display_tool_calls(st.empty(), message["tool_calls"])
                    st.markdown(_content)

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        with st.chat_message("assistant"):
            # Create container for tool calls
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner("🤔 Thinking..."):
                response = ""
                try:
                    # Run the agent and stream the response
                    run_response = sql_agent.run(
                        question, stream=True, stream_intermediate_steps=True
                    )
                    for _resp_chunk in run_response:
                        # Display tool calls if available
                        if hasattr(_resp_chunk, "tool") and _resp_chunk.tool:
                            display_tool_calls(tool_calls_container, [_resp_chunk.tool])

                        # Display response if available and event is RunResponse
                        if (
                            _resp_chunk.event == "RunResponseContent"
                            and _resp_chunk.content is not None
                        ):
                            response += _resp_chunk.content
                            resp_container.markdown(response)

                    add_message("assistant", response, sql_agent.run_response.tools)
                except Exception as e:
                    logger.exception(e)
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    add_message("assistant", error_message)
                    st.error(error_message)

    ####################################################################
    # Session selector
    ####################################################################
    session_selector_widget(sql_agent, model_id)
    rename_session_widget(sql_agent)

    ####################################################################
    # About section
    ####################################################################
    about_widget()


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/load_f1_data.py
================================================
from io import StringIO

import pandas as pd
import requests
from agents import db_url
from agno.utils.log import logger
from sqlalchemy import create_engine

s3_uri = "https://agno-public.s3.amazonaws.com/f1"

# List of files and their corresponding table names
files_to_tables = {
    f"{s3_uri}/constructors_championship_1958_2020.csv": "constructors_championship",
    f"{s3_uri}/drivers_championship_1950_2020.csv": "drivers_championship",
    f"{s3_uri}/fastest_laps_1950_to_2020.csv": "fastest_laps",
    f"{s3_uri}/race_results_1950_to_2020.csv": "race_results",
    f"{s3_uri}/race_wins_1950_to_2020.csv": "race_wins",
}


def load_f1_data():
    """Load F1 data into the database"""

    logger.info("Loading database.")
    engine = create_engine(db_url)

    # Load each CSV file into the corresponding PostgreSQL table
    for file_path, table_name in files_to_tables.items():
        logger.info(f"Loading {file_path} into {table_name} table.")
        # Download the file using requests
        response = requests.get(file_path, verify=False)
        response.raise_for_status()  # Raise an exception for bad status codes

        # Read the CSV data from the response content
        csv_data = StringIO(response.text)
        df = pd.read_csv(csv_data)

        df.to_sql(table_name, engine, if_exists="replace", index=False)
        logger.info(f"{file_path} loaded into {table_name} table.")

    logger.info("Database loaded.")


if __name__ == "__main__":
    # Disable SSL verification warnings
    import urllib3

    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    load_f1_data()



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/load_knowledge.py
================================================
from agents import agent_knowledge
from agno.utils.log import logger


def load_knowledge(recreate: bool = True):
    logger.info("Loading SQL agent knowledge.")
    agent_knowledge.load(recreate=recreate)
    logger.info("SQL agent knowledge loaded.")


if __name__ == "__main__":
    load_knowledge()



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/playground.py
================================================
from agents import get_sql_agent
from agno.playground import Playground, serve_playground_app

sql_agent = get_sql_agent(name="SQL Agent", model_id="openai:o4-mini")
reasoning_sql_agent = get_sql_agent(
    name="Reasoning SQL Agent", model_id="anthropic:claude-3-7-sonnet-latest"
)

app = Playground(agents=[sql_agent, reasoning_sql_agent]).get_app()

if __name__ == "__main__":
    serve_playground_app("playground:app", reload=True)



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/requirements.in
================================================
agno
anthropic
google-genai
groq
nest_asyncio
openai
pandas
pgvector
psycopg[binary]
simplejson
sqlalchemy
streamlit


================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.7.10
    # via -r requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anthropic==0.63.0
    # via -r requirements.in
anyio==4.10.0
    # via
    #   anthropic
    #   google-genai
    #   groq
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.8.3
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.3
    # via requests
click==8.2.1
    # via
    #   streamlit
    #   typer
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
docstring-parser==0.17.0
    # via agno
gitdb==4.0.12
    # via gitpython
gitpython==3.1.45
    # via
    #   agno
    #   streamlit
google-auth==2.40.3
    # via google-genai
google-genai==1.29.0
    # via -r requirements.in
groq==0.31.0
    # via -r requirements.in
h11==0.16.0
    # via httpcore
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.10.0
    # via
    #   anthropic
    #   openai
jsonschema==4.25.0
    # via altair
jsonschema-specifications==2025.4.1
    # via jsonschema
markdown-it-py==4.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
narwhals==2.1.1
    # via altair
nest-asyncio==1.6.0
    # via -r requirements.in
numpy==2.3.2
    # via
    #   pandas
    #   pgvector
    #   pydeck
    #   streamlit
openai==1.99.9
    # via -r requirements.in
packaging==25.0
    # via
    #   agno
    #   altair
    #   streamlit
pandas==2.3.1
    # via
    #   -r requirements.in
    #   streamlit
pgvector==0.4.1
    # via -r requirements.in
pillow==11.3.0
    # via streamlit
protobuf==6.31.1
    # via streamlit
psycopg==3.2.9
    # via -r requirements.in
psycopg-binary==3.2.9
    # via psycopg
pyarrow==21.0.0
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.2
    # via google-auth
pydantic==2.11.7
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   openai
    #   pydantic-settings
pydantic-core==2.33.2
    # via pydantic
pydantic-settings==2.10.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.2
    # via rich
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.1.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.4
    # via
    #   google-genai
    #   streamlit
rich==14.1.0
    # via
    #   agno
    #   typer
rpds-py==0.27.0
    # via
    #   jsonschema
    #   referencing
rsa==4.9.1
    # via google-auth
shellingham==1.5.4
    # via typer
simplejson==3.20.1
    # via -r requirements.in
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
sqlalchemy==2.0.43
    # via -r requirements.in
streamlit==1.48.0
    # via -r requirements.in
tenacity==9.1.2
    # via
    #   google-genai
    #   streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.5.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.16.0
    # via agno
typing-extensions==4.14.1
    # via
    #   agno
    #   altair
    #   anthropic
    #   google-genai
    #   groq
    #   openai
    #   pydantic
    #   pydantic-core
    #   sqlalchemy
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.5.0
    # via requests
websockets==15.0.1
    # via google-genai



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/utils.py
================================================
import json
from dataclasses import asdict
from typing import Any, Dict, List, Optional

import streamlit as st
from agents import get_sql_agent
from agno.agent.agent import Agent
from agno.models.response import ToolExecution
from agno.utils.log import logger


def is_json(myjson):
    """Check if a string is valid JSON"""
    try:
        json.loads(myjson)
    except (ValueError, TypeError):
        return False
    return True


def load_data_and_knowledge():
    """Load F1 data and knowledge base if not already done"""
    from load_f1_data import load_f1_data
    from load_knowledge import load_knowledge

    if "data_loaded" not in st.session_state:
        with st.spinner("🔄 Loading data into database..."):
            load_f1_data()
        with st.spinner("📚 Loading knowledge base..."):
            load_knowledge()
        st.session_state["data_loaded"] = True
        st.success("✅ Data and knowledge loaded successfully!")


def add_message(
    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None
) -> None:
    """Safely add a message to the session state"""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append(
        {"role": role, "content": content, "tool_calls": tool_calls}
    )


def restart_agent():
    """Reset the agent and clear chat history"""
    logger.debug("---*--- Restarting agent ---*---")
    st.session_state["sql_agent"] = None
    st.session_state["sql_agent_session_id"] = None
    st.session_state["messages"] = []
    st.rerun()


def export_chat_history():
    """Export chat history as markdown"""
    if "messages" in st.session_state:
        chat_text = "# Reasoning SQL Agent - Chat History\n\n"
        for msg in st.session_state["messages"]:
            role = "🤖 Assistant" if msg["role"] == "agent" else "👤 User"
            chat_text += f"### {role}\n{msg['content']}\n\n"
        return chat_text
    return ""


def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):
    """Display tool calls in a streamlit container with expandable sections.

    Args:
        tool_calls_container: Streamlit container to display the tool calls
        tools: List of tool call dictionaries containing name, args, content, and metrics
    """
    try:
        with tool_calls_container.container():
            for tool_call in tools:
                tool_name = tool_call.tool_name or "Unknown Tool"
                tool_args = tool_call.tool_args or {}
                content = tool_call.result or None
                metrics = tool_call.metrics or None

                # Add timing information
                execution_time_str = "N/A"
                try:
                    if metrics is not None and hasattr(metrics, "time"):
                        execution_time = metrics.time
                        if execution_time is not None:
                            execution_time_str = f"{execution_time:.4f}s"
                except Exception as e:
                    logger.error(f"Error displaying tool calls: {str(e)}")
                    pass

                with st.expander(
                    f"🛠️ {tool_name.replace('_', ' ').title()} ({execution_time_str})",
                    expanded=False,
                ):
                    # Show query with syntax highlighting
                    if isinstance(tool_args, dict) and "query" in tool_args:
                        st.code(tool_args["query"], language="sql")

                    # Display arguments in a more readable format
                    if tool_args and tool_args != {"query": None}:
                        st.markdown("**Arguments:**")
                        st.json(tool_args)

                    if content is not None:
                        try:
                            if is_json(content):
                                st.markdown("**Results:**")
                                st.json(content)
                        except Exception as e:
                            logger.debug(f"Skipped tool call content: {e}")
    except Exception as e:
        logger.error(f"Error displaying tool calls: {str(e)}")
        tool_calls_container.error("Failed to display tool results")


def sidebar_widget() -> None:
    """Display a sidebar with sample user queries"""
    with st.sidebar:
        st.markdown("#### 🏆 Sample Queries")
        if st.button("📋 Show Tables"):
            add_message("user", "Which tables do you have access to?")
        if st.button("🥇 Most Race Wins"):
            add_message("user", "Which driver has the most race wins?")

        if st.button("🏆 Constructor Champs"):
            add_message("user", "Which team won the most Constructors Championships?")

        if st.button("⏳ Longest Career"):
            add_message(
                "user",
                "Tell me the name of the driver with the longest racing career? Also tell me when they started and when they retired.",
            )
        if st.button("📈 Races per Year"):
            add_message("user", "Show me the number of races per year.")

        if st.button("🔍 Team Performance"):
            add_message(
                "user",
                "Write a query to identify the drivers that won the most races per year from 2010 onwards and the position of their team that year.",
            )

        st.markdown("---")
        st.markdown("#### 🛠️ Utilities")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("🔄 New Chat"):
                restart_agent()
        with col2:
            fn = "sql_agent_chat_history.md"
            if "sql_agent_session_id" in st.session_state:
                fn = f"sql_agent_{st.session_state.sql_agent_session_id}.md"
            if st.download_button(
                "💾 Export Chat",
                export_chat_history(),
                file_name=fn,
                mime="text/markdown",
            ):
                st.sidebar.success("Chat history exported!")

        if st.sidebar.button("🚀 Load Data & Knowledge"):
            load_data_and_knowledge()


def session_selector_widget(agent: Agent, model_id: str) -> None:
    """Display a session selector in the sidebar"""
    if agent.storage:
        agent_sessions = agent.storage.get_all_sessions()
        # Get session names if available, otherwise use IDs
        session_options = []
        for session in agent_sessions:
            session_id = session.session_id
            session_name = (
                session.session_data.get("session_name", None)
                if session.session_data
                else None
            )
            display_name = session_name if session_name else session_id
            session_options.append({"id": session_id, "display": display_name})

        # Display session selector
        selected_session = st.sidebar.selectbox(
            "Session",
            options=[s["display"] for s in session_options],
            key="session_selector",
        )
        # Find the selected session ID
        selected_session_id = next(
            s["id"] for s in session_options if s["display"] == selected_session
        )

        if st.session_state["sql_agent_session_id"] != selected_session_id:
            logger.info(
                f"---*--- Loading {model_id} run: {selected_session_id} ---*---"
            )
            st.session_state["sql_agent"] = get_sql_agent(
                model_id=model_id,
                session_id=selected_session_id,
            )
            st.rerun()


def rename_session_widget(agent: Agent) -> None:
    """Rename the current session of the agent and save to storage"""
    container = st.sidebar.container()
    session_row = container.columns([3, 1], vertical_alignment="center")

    # Initialize session_edit_mode if needed
    if "session_edit_mode" not in st.session_state:
        st.session_state.session_edit_mode = False

    with session_row[0]:
        if st.session_state.session_edit_mode:
            new_session_name = st.text_input(
                "Session Name",
                value=agent.session_name,
                key="session_name_input",
                label_visibility="collapsed",
            )
        else:
            st.markdown(f"Session Name: **{agent.session_name}**")

    with session_row[1]:
        if st.session_state.session_edit_mode:
            if st.button("✓", key="save_session_name", type="primary"):
                if new_session_name:
                    agent.rename_session(new_session_name)
                    st.session_state.session_edit_mode = False
                    container.success("Renamed!")
        else:
            if st.button("✎", key="edit_session_name"):
                st.session_state.session_edit_mode = True


def about_widget() -> None:
    """Display an about section in the sidebar"""
    with st.sidebar:
        st.markdown("### About Agno ✨")
        st.markdown("""
        Agno is a lightweight library for building Reasoning Agents.

        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)
        """)

        st.markdown("### Need Help?")
        st.markdown(
            "If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community)."
        )


CUSTOM_CSS = """
    <style>
    /* Main Styles */
    .main-title {
        text-align: center;
        background: linear-gradient(45deg, #FF4B2B, #FF416C);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 3em;
        font-weight: bold;
        padding: 1em 0;
    }
    .subtitle {
        text-align: center;
        color: #666;
        margin-bottom: 2em;
    }
    .stButton button {
        width: 100%;
        border-radius: 20px;
        margin: 0.2em 0;
        transition: all 0.3s ease;
    }
    .stButton button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    }
    .chat-container {
        border-radius: 15px;
        padding: 1em;
        margin: 1em 0;
        background-color: #f5f5f5;
    }
    .sql-result {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 1em;
        margin: 1em 0;
        border-left: 4px solid #FF4B2B;
    }
    .status-message {
        padding: 1em;
        border-radius: 10px;
        margin: 1em 0;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
    }
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
    }
    /* Dark mode adjustments */
    @media (prefers-color-scheme: dark) {
        .chat-container {
            background-color: #2b2b2b;
        }
        .sql-result {
            background-color: #1e1e1e;
        }
    }
    </style>
"""



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/knowledge/constructors_championship.json
================================================
{
    "table_name": "constructors_championship",
    "table_description": "Contains data for the constructor's championship from 1958 to 2020, capturing championship positions from when it was introduced.",
    "table_columns": [
      {
          "name": "index",
          "type": "int",
          "description": "Index of the row."
      },
      {
          "name": "year",
          "type": "int",
          "description": "Year of the championship."
      },
      {
          "name": "position",
          "type": "int",
          "description": "Final standing position of the team in the championship. Use position = 1 to get the champion team."
      },
      {
          "name": "team",
          "type": "text",
          "description": "Name of the Formula 1 team."
      },
      {
          "name": "points",
          "type": "int",
          "description": "Total points accumulated by the team during the championship year."
      }
  ]
}



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/knowledge/drivers_championship.json
================================================
{
    "table_name": "drivers_championship",
    "table_description": "Contains data for driver's championship standings from 1950-2020, detailing driver positions, teams, and points.",
    "table_columns": [
        {
            "name": "index",
            "type": "int",
            "description": "Index number of the record."
        },
        {
            "name": "year",
            "type": "int",
            "description": "The year of the championship."
        },
        {
            "name": "position",
            "type": "text",
            "description": "Final position of the driver in the championship."
        },
        {
            "name": "name",
            "type": "text",
            "description": "Full name of the driver."
        },
        {
            "name": "driver_tag",
            "type": "text",
            "description": "Abbreviated tag of the driver."
        },
        {
            "name": "nationality",
            "type": "text",
            "description": "Nationality of the driver."
        },
        {
            "name": "team",
            "type": "text",
            "description": "Racing team of the driver."
        },
        {
            "name": "points",
            "type": "float",
            "description": "Total points accumulated by the driver that season."
        }
    ]
}



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/knowledge/fastest_laps.json
================================================
{
    "table_name": "fastest_laps",
    "table_description": "Contains data for the fastest laps recorded in races from 1950-2020, including driver and team details.",
    "table_columns": [
      {
          "name": "index",
          "type": "int",
          "description": "Unique index for each entry."
      },
      {
          "name": "year",
          "type": "int",
          "description": "Year of the race."
      },
      {
          "name": "venue",
          "type": "text",
          "description": "Name of the race venue."
      },
      {
          "name": "name",
          "type": "text",
          "description": "Name of the driver."
      },
      {
          "name": "driver_tag",
          "type": "text",
          "description": "Abbreviated tag of the driver's name."
      },
      {
          "name": "team",
          "type": "text",
          "description": "Name of the racing team."
      },
      {
          "name": "lap_time",
          "type": "text",
          "description": "Fastest lap time recorded."
      }
    ]
}



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/knowledge/race_results.json
================================================
{
    "table_name": "race_results",
    "table_description": "Holds comprehensive race data for each Formula 1 race from 1950-2020, including positions, drivers, teams, and points.",
    "table_columns": [
        {
            "name": "index",
            "type": "int",
            "description": "Unique index for each entry."
        },
        {
            "name": "year",
            "type": "int",
            "description": "The year of the race."
        },
        {
            "name": "position",
            "type": "text",
            "description": "The finishing position of the driver."
        },
        {
            "name": "driver_no",
            "type": "int",
            "description": "Driver number."
        },
        {
            "name": "venue",
            "type": "text",
            "description": "Location of the race."
        },
        {
            "name": "name",
            "type": "text",
            "description": "Name of the driver."
        },
        {
            "name": "name_tag",
            "type": "text",
            "description": "Abbreviated tag of the driver's name."
        },
        {
            "name": "team",
            "type": "text",
            "description": "The racing team of the driver."
        },
        {
            "name": "laps",
            "type": "float",
            "description": "Number of laps completed."
        },
        {
            "name": "time",
            "type": "text",
            "description": "Finishing time or gap to the leader."
        },
        {
            "name": "points",
            "type": "float",
            "description": "Points earned in the race."
        }
    ]
}



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/knowledge/race_wins.json
================================================
{
    "table_name": "race_wins",
    "table_description": "Documents race win data from 1950-2020, detailing venue, winner, team, and race duration.",
    "table_columns": [
      {
          "name": "index",
          "type": "int",
          "description": "Unique index for each entry."
      },
      {
          "name": "venue",
          "type": "text",
          "description": "Venue where the race was held."
      },
      {
          "name": "date",
          "type": "text",
          "description": "Date of the race in the format 'DD Mon YYYY'.",
          "tip": "Use the `TO_DATE` function to convert the date to a date type."
      },
      {
          "name": "name",
          "type": "text",
          "description": "Name of the winning driver."
      },
      {
          "name": "name_tag",
          "type": "text",
          "description": "Tag associated with the driver's name."
      },
      {
          "name": "team",
          "type": "text",
          "description": "Team of the winning driver."
      },
      {
          "name": "laps",
          "type": "float",
          "description": "Number of laps completed in the race."
      },
      {
          "name": "time",
          "type": "text",
          "description": "Winning time of the race."
      }
  ]
}



================================================
FILE: cookbook/examples/streamlit_apps/sql_agent/knowledge/sample_queries.sql
================================================
-- Here are some sample queries for reference

-- <query description>
-- How many races did the championship winners win each year?
-- </query description>
-- <query>
SELECT
    dc.year,
    dc.name AS champion_name,
    COUNT(rw.name) AS race_wins
FROM
    drivers_championship dc
JOIN
    race_wins rw
ON
    dc.name = rw.name AND dc.year = EXTRACT(YEAR FROM TO_DATE(rw.date, 'DD Mon YYYY'))
WHERE
    dc.position = '1'
GROUP BY
    dc.year, dc.name
ORDER BY
    dc.year;
-- </query>


-- <query description>
-- Compare the number of race wins vs championship positions for constructors in 2019
-- </query description>
-- <query>
WITH race_wins_2019 AS (
    SELECT team, COUNT(*) AS wins
    FROM race_wins
    WHERE EXTRACT(YEAR FROM TO_DATE(date, 'DD Mon YYYY')) = 2019
    GROUP BY team
),
constructors_positions_2019 AS (
    SELECT team, position
    FROM constructors_championship
    WHERE year = 2019
)

SELECT cp.team, cp.position, COALESCE(rw.wins, 0) AS wins
FROM constructors_positions_2019 cp
LEFT JOIN race_wins_2019 rw ON cp.team = rw.team
ORDER BY cp.position;
-- </query>

-- <query description>
-- Most race wins by a driver
-- </query description>
-- <query>
SELECT name, COUNT(*) AS win_count
FROM race_wins
GROUP BY name
ORDER BY win_count DESC
LIMIT 1;
-- </query>

-- <query description>
-- Which team won the most Constructors Championships?
-- </query description>
-- <query>
SELECT team, COUNT(*) AS championship_wins
FROM constructors_championship
WHERE position = 1
GROUP BY team
ORDER BY championship_wins DESC
LIMIT 1;
-- </query>



================================================
FILE: cookbook/examples/streamlit_apps/tic_tac_toe/README.md
================================================
# Agent Tic Tac Toe

This example shows how to build an interactive Tic Tac Toe game where AI agents compete against each other. The application showcases how to:
- Coordinate multiple AI agents in a turn-based game
- Use different language models for different players
- Create an interactive web interface with Streamlit
- Handle game state and move validation
- Display real-time game progress and move history

## Features
- Multiple AI models support (GPT-4, Claude, Gemini, etc.)
- Real-time game visualization
- Move history tracking with board states
- Interactive player selection
- Game state management
- Move validation and coordination

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install dependencies

```shell
pip install -r cookbook/examples/streamlit_apps/tic_tac_toe/requirements.txt
```

### 3. Export API Keys

The game supports multiple AI models. Export the API keys for the models you want to use:

```shell
# Required for OpenAI models
export OPENAI_API_KEY=***

# Optional - for additional models
export ANTHROPIC_API_KEY=***  # For Claude models
export GOOGLE_API_KEY=***     # For Gemini models
export GROQ_API_KEY=***       # For Groq models
```

### 4. Run the Game

```shell
streamlit run cookbook/examples/streamlit_apps/tic_tac_toe/app.py
```

- Open [localhost:8501](http://localhost:8501) to view the game interface

## How It Works

The game consists of three agents:

1. **Master Agent (Referee)**
   - Coordinates the game
   - Validates moves
   - Maintains game state
   - Determines game outcome

2. **Two Player Agents**
   - Make strategic moves
   - Analyze board state
   - Follow game rules
   - Respond to opponent moves

## Available Models

The game supports various AI models:
- GPT-4o (OpenAI)
- GPT-o3-mini (OpenAI)
- Gemini (Google)
- Llama 3 (Groq)
- Claude (Anthropic)

## Game Features

1. **Interactive Board**
   - Real-time updates
   - Visual move tracking
   - Clear game status display

2. **Move History**
   - Detailed move tracking
   - Board state visualization
   - Player action timeline

3. **Game Controls**
   - Start/Pause game
   - Reset board
   - Select AI models
   - View game history

4. **Performance Analysis**
   - Move timing
   - Strategy tracking
   - Game statistics

## Support

Join our [Discord community](https://agno.link/discord) for help and discussions.



================================================
FILE: cookbook/examples/streamlit_apps/tic_tac_toe/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/tic_tac_toe/agents.py
================================================
"""
Tic Tac Toe Battle
---------------------------------
This example shows how to build a Tic Tac Toe game where two AI agents play against each other.

The game integrates:
  - Multiple AI models (Claude, GPT-4, etc.)
  - Turn-based gameplay coordination
  - Move validation and game state management
"""

import sys
from pathlib import Path
from textwrap import dedent
from typing import Tuple

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat, OpenAIResponses

project_root = str(Path(__file__).parent.parent.parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)


def get_model_for_provider(provider: str, model_name: str):
    """
    Creates and returns the appropriate model instance based on the provider.

    Args:
        provider: The model provider (e.g., 'openai', 'google', 'anthropic', 'groq')
        model_name: The specific model name/ID

    Returns:
        An instance of the appropriate model class

    Raises:
        ValueError: If the provider is not supported
    """
    if provider == "openai":
        if model_name == "o1-pro":
            return OpenAIResponses(id=model_name)
        else:
            return OpenAIChat(id=model_name)
    elif provider == "google":
        return Gemini(id=model_name)
    elif provider == "anthropic":
        if model_name == "claude-3-5-sonnet":
            return Claude(id="claude-3-5-sonnet-20241022", max_tokens=8192)
        elif model_name == "claude-3-7-sonnet":
            return Claude(
                id="claude-3-7-sonnet-20250219",
                max_tokens=8192,
            )
        elif model_name == "claude-3-7-sonnet-thinking":
            return Claude(
                id="claude-3-7-sonnet-20250219",
                max_tokens=8192,
                thinking={"type": "enabled", "budget_tokens": 4096},
            )
        else:
            return Claude(id=model_name)
    elif provider == "groq":
        return Groq(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")


def get_tic_tac_toe_players(
    model_x: str = "openai:gpt-4o",
    model_o: str = "openai:o3-mini",
    debug_mode: bool = True,
) -> Tuple[Agent, Agent]:
    """
    Returns an instance of the Tic Tac Toe Referee Agent that coordinates the game.

    Args:
        model_x: ModelConfig for player X
        model_o: ModelConfig for player O
        model_referee: ModelConfig for the referee agent
        debug_mode: Enable logging and debug features

    Returns:
        An instance of the configured Referee Agent
    """
    # Parse model provider and name
    provider_x, model_name_x = model_x.split(":")
    provider_o, model_name_o = model_o.split(":")

    # Create model instances using the helper function
    model_x = get_model_for_provider(provider_x, model_name_x)
    model_o = get_model_for_provider(provider_o, model_name_o)

    player_x = Agent(
        name="Player X",
        description=dedent("""\
        You are Player X in a Tic Tac Toe game. Your goal is to win by placing three X's in a row (horizontally, vertically, or diagonally).

        BOARD LAYOUT:
        - The board is a 3x3 grid with coordinates from (0,0) to (2,2)
        - Top-left is (0,0), bottom-right is (2,2)

        RULES:
        - You can only place X in empty spaces (shown as " " on the board)
        - Players take turns placing their marks
        - First to get 3 marks in a row (horizontal, vertical, or diagonal) wins
        - If all spaces are filled with no winner, the game is a draw

        YOUR RESPONSE:
        - Provide ONLY two numbers separated by a space (row column)
        - Example: "1 2" places your X in row 1, column 2
        - Choose only from the valid moves list provided to you

        STRATEGY TIPS:
        - Study the board carefully and make strategic moves
        - Block your opponent's potential winning moves
        - Create opportunities for multiple winning paths
        - Pay attention to the valid moves and avoid illegal moves
        """),
        model=model_x,
        # Gemini models have a rate limit of 5 requests per minute
        retries=3,
        # Make sure to wait 30 seconds between retries
        delay_between_retries=30,
        debug_mode=debug_mode,
    )

    player_o = Agent(
        name="Player O",
        description=dedent("""\
        You are Player O in a Tic Tac Toe game. Your goal is to win by placing three O's in a row (horizontally, vertically, or diagonally).

        BOARD LAYOUT:
        - The board is a 3x3 grid with coordinates from (0,0) to (2,2)
        - Top-left is (0,0), bottom-right is (2,2)

        RULES:
        - You can only place X in empty spaces (shown as " " on the board)
        - Players take turns placing their marks
        - First to get 3 marks in a row (horizontal, vertical, or diagonal) wins
        - If all spaces are filled with no winner, the game is a draw

        YOUR RESPONSE:
        - Provide ONLY two numbers separated by a space (row column)
        - Example: "1 2" places your X in row 1, column 2
        - Choose only from the valid moves list provided to you

        STRATEGY TIPS:
        - Study the board carefully and make strategic moves
        - Block your opponent's potential winning moves
        - Create opportunities for multiple winning paths
        - Pay attention to the valid moves and avoid illegal moves
        """),
        model=model_o,
        # Gemini models have a rate limit of 5 requests per minute
        retries=3,
        # Make sure to wait 30 seconds between retries
        delay_between_retries=30,
        debug_mode=debug_mode,
    )

    return player_x, player_o



================================================
FILE: cookbook/examples/streamlit_apps/tic_tac_toe/app.py
================================================
import nest_asyncio
import streamlit as st
from agents import get_tic_tac_toe_players
from agno.utils.log import logger
from utils import (
    CUSTOM_CSS,
    TicTacToeBoard,
    display_board,
    display_move_history,
    show_agent_status,
)

nest_asyncio.apply()

# Page configuration
st.set_page_config(
    page_title="Agent Tic Tac Toe",
    page_icon="🎮",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Load custom CSS with dark mode support
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>Agents play Tic Tac Toe</h1>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Initialize session state
    ####################################################################
    if "game_started" not in st.session_state:
        st.session_state.game_started = False
        st.session_state.game_paused = False
        st.session_state.move_history = []

    with st.sidebar:
        st.markdown("### Game Controls")
        model_options = {
            "gpt-4o": "openai:gpt-4o",
            "gpt-4o-mini": "openai:gpt-4o-mini",
            "gpt-4.5": "openai:gpt-4.5-preview",
            "o1-pro": "openai:o1-pro",
            "o3-mini": "openai:o3-mini",
            "claude-3.5": "anthropic:claude-3-5-sonnet",
            "claude-3.7": "anthropic:claude-3-7-sonnet",
            "claude-3.7-thinking": "anthropic:claude-3-7-sonnet-thinking",
            "gemini-pro": "google:gemini-2.5-pro-exp-03-25",
            "gemini-flash": "google:gemini-2.0-flash",
            "llama-3.3": "groq:llama-3.3-70b-versatile",
        }
        ################################################################
        # Model selection
        ################################################################
        selected_p_x = st.selectbox(
            "Select Player X",
            list(model_options.keys()),
            index=list(model_options.keys()).index("gpt-4.5"),
            key="model_p1",
        )
        selected_p_o = st.selectbox(
            "Select Player O",
            list(model_options.keys()),
            index=list(model_options.keys()).index("claude-3.7"),
            key="model_p2",
        )

        ################################################################
        # Game controls
        ################################################################
        col1, col2 = st.columns(2)
        with col1:
            if not st.session_state.game_started:
                if st.button("▶️ Start Game"):
                    st.session_state.player_x, st.session_state.player_o = (
                        get_tic_tac_toe_players(
                            model_x=model_options[selected_p_x],
                            model_o=model_options[selected_p_o],
                            debug_mode=True,
                        )
                    )
                    st.session_state.game_board = TicTacToeBoard()
                    st.session_state.game_started = True
                    st.session_state.game_paused = False
                    st.session_state.move_history = []
                    st.rerun()
            else:
                game_over, _ = st.session_state.game_board.get_game_state()
                if not game_over:
                    if st.button(
                        "⏸️ Pause" if not st.session_state.game_paused else "▶️ Resume"
                    ):
                        st.session_state.game_paused = not st.session_state.game_paused
                        st.rerun()
        with col2:
            if st.session_state.game_started:
                if st.button("🔄 New Game"):
                    st.session_state.player_x, st.session_state.player_o = (
                        get_tic_tac_toe_players(
                            model_x=model_options[selected_p_x],
                            model_o=model_options[selected_p_o],
                            debug_mode=True,
                        )
                    )
                    st.session_state.game_board = TicTacToeBoard()
                    st.session_state.game_paused = False
                    st.session_state.move_history = []
                    st.rerun()

    ####################################################################
    # Header showing current models
    ####################################################################
    if st.session_state.game_started:
        st.markdown(
            f"<h3 style='color:#87CEEB; text-align:center;'>{selected_p_x} vs {selected_p_o}</h3>",
            unsafe_allow_html=True,
        )

    ####################################################################
    # Main game area
    ####################################################################
    if st.session_state.game_started:
        game_over, status = st.session_state.game_board.get_game_state()

        display_board(st.session_state.game_board)

        # Show game status (winner/draw/current player)
        if game_over:
            winner_player = (
                "X" if "X wins" in status else "O" if "O wins" in status else None
            )
            if winner_player:
                winner_num = "1" if winner_player == "X" else "2"
                winner_model = selected_p_x if winner_player == "X" else selected_p_o
                st.success(f"🏆 Game Over! Player {winner_num} ({winner_model}) wins!")
            else:
                st.info("🤝 Game Over! It's a draw!")
        else:
            # Show current player status
            current_player = st.session_state.game_board.current_player
            player_num = "1" if current_player == "X" else "2"
            current_model_name = selected_p_x if current_player == "X" else selected_p_o

            show_agent_status(
                f"Player {player_num} ({current_model_name})",
                "It's your turn",
            )

        display_move_history()

        if not st.session_state.game_paused and not game_over:
            # Thinking indicator
            st.markdown(
                f"""<div class="thinking-container">
                    <div class="agent-thinking">
                        <div style="margin-right: 10px; display: inline-block;">🔄</div>
                        Player {player_num} ({current_model_name}) is thinking...
                    </div>
                </div>""",
                unsafe_allow_html=True,
            )

            valid_moves = st.session_state.game_board.get_valid_moves()

            current_agent = (
                st.session_state.player_x
                if current_player == "X"
                else st.session_state.player_o
            )
            response = current_agent.run(
                f"""\
Current board state:\n{st.session_state.game_board.get_board_state()}\n
Available valid moves (row, col): {valid_moves}\n
Choose your next move from the valid moves above.
Respond with ONLY two numbers for row and column, e.g. "1 2".""",
                stream=False,
            )

            try:
                import re

                numbers = re.findall(r"\d+", response.content if response else "")
                row, col = map(int, numbers[:2])
                success, message = st.session_state.game_board.make_move(row, col)

                if success:
                    move_number = len(st.session_state.move_history) + 1
                    st.session_state.move_history.append(
                        {
                            "number": move_number,
                            "player": f"Player {player_num} ({current_model_name})",
                            "move": f"{row},{col}",
                        }
                    )

                    logger.info(
                        f"Move {move_number}: Player {player_num} ({current_model_name}) placed at position ({row}, {col})"
                    )
                    logger.info(
                        f"Board state:\n{st.session_state.game_board.get_board_state()}"
                    )

                    # Check game state after move
                    game_over, status = st.session_state.game_board.get_game_state()
                    if game_over:
                        logger.info(f"Game Over - {status}")
                        if "wins" in status:
                            st.success(f"🏆 Game Over! {status}")
                        else:
                            st.info(f"🤝 Game Over! {status}")
                        st.session_state.game_paused = True
                    st.rerun()
                else:
                    logger.error(f"Invalid move attempt: {message}")
                    response = current_agent.run(
                        f"""\
Invalid move: {message}

Current board state:\n{st.session_state.game_board.get_board_state()}\n
Available valid moves (row, col): {valid_moves}\n
Please choose a valid move from the list above.
Respond with ONLY two numbers for row and column, e.g. "1 2".""",
                        stream=False,
                    )
                    st.rerun()

            except Exception as e:
                logger.error(f"Error processing move: {str(e)}")
                st.error(f"Error processing move: {str(e)}")
                st.rerun()
    else:
        st.info("👈 Press 'Start Game' to begin!")

    ####################################################################
    # About section
    ####################################################################
    st.sidebar.markdown(f"""
    ### 🎮 Agent Tic Tac Toe Battle
    Watch two agents compete in real-time!

    **Current Players:**
    * 🔵 Player X: `{selected_p_x}`
    * 🔴 Player O: `{selected_p_o}`

    **How it Works:**
    Each Agent analyzes the board and employs strategic thinking to:
    * 🏆 Find winning moves
    * 🛡️ Block opponent victories
    * ⭐ Control strategic positions
    * 🤔 Plan multiple moves ahead

    Built with Streamlit and Agno
    """)


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/tic_tac_toe/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/tic_tac_toe/requirements.in
================================================
agno
anthropic
groq
google-genai
nest-asyncio
ollama
openai
pathlib
Pillow
pip-tools
python-dotenv
rich
streamlit



================================================
FILE: cookbook/examples/streamlit_apps/tic_tac_toe/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.6.0
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anthropic==0.49.0
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
anyio==4.9.0
    # via
    #   anthropic
    #   google-genai
    #   groq
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
build==1.2.2.post1
    # via pip-tools
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.1
    # via requests
click==8.1.8
    # via
    #   pip-tools
    #   streamlit
    #   typer
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
docstring-parser==0.16
    # via agno
gitdb==4.0.12
    # via gitpython
gitpython==3.1.44
    # via
    #   agno
    #   streamlit
google-auth==2.38.0
    # via google-genai
google-genai==1.7.0
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
groq==0.20.0
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
h11==0.14.0
    # via httpcore
httpcore==1.0.7
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   ollama
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.9.0
    # via
    #   anthropic
    #   openai
jsonschema==4.23.0
    # via altair
jsonschema-specifications==2024.10.1
    # via jsonschema
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
narwhals==1.32.0
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
numpy==2.2.4
    # via
    #   pandas
    #   pydeck
    #   streamlit
ollama==0.4.7
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
openai==1.68.2
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
packaging==24.2
    # via
    #   altair
    #   build
    #   streamlit
pandas==2.2.3
    # via streamlit
pathlib==1.0.1
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
pillow==11.1.0
    # via
    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in
    #   streamlit
pip==25.0.1
    # via pip-tools
pip-tools==7.4.1
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
protobuf==5.29.4
    # via streamlit
pyarrow==19.0.1
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.1
    # via google-auth
pydantic==2.10.6
    # via
    #   agno
    #   anthropic
    #   google-genai
    #   groq
    #   ollama
    #   openai
    #   pydantic-settings
pydantic-core==2.27.2
    # via pydantic
pydantic-settings==2.8.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.1
    # via rich
pyproject-hooks==1.2.0
    # via
    #   build
    #   pip-tools
python-dateutil==2.9.0.post0
    # via pandas
python-dotenv==1.1.0
    # via
    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via agno
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.3
    # via
    #   google-genai
    #   streamlit
rich==13.9.4
    # via
    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in
    #   agno
    #   typer
rpds-py==0.23.1
    # via
    #   jsonschema
    #   referencing
rsa==4.9
    # via google-auth
setuptools==78.1.0
    # via pip-tools
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
streamlit==1.44.0
    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in
tenacity==9.0.0
    # via streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.4.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.15.2
    # via agno
typing-extensions==4.12.2
    # via
    #   agno
    #   altair
    #   anthropic
    #   anyio
    #   google-genai
    #   groq
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   streamlit
    #   typer
tzdata==2025.2
    # via pandas
urllib3==2.3.0
    # via requests
websockets==15.0.1
    # via google-genai
wheel==0.45.1
    # via pip-tools



================================================
FILE: cookbook/examples/streamlit_apps/tic_tac_toe/utils.py
================================================
from typing import List, Optional, Tuple

import streamlit as st

# Define constants for players
X_PLAYER = "X"
O_PLAYER = "O"
EMPTY = " "


class TicTacToeBoard:
    def __init__(self):
        # Initialize empty 3x3 board
        self.board = [[EMPTY for _ in range(3)] for _ in range(3)]
        self.current_player = X_PLAYER

    def make_move(self, row: int, col: int) -> Tuple[bool, str]:
        """
        Make a move on the board.

        Args:
            row (int): Row index (0-2)
            col (int): Column index (0-2)

        Returns:
            Tuple[bool, str]: (Success status, Message with current board state or error)
        """
        # Validate move coordinates
        if not (0 <= row <= 2 and 0 <= col <= 2):
            return (
                False,
                "Invalid move: Position out of bounds. Please choose row and column between 0 and 2.",
            )

        # Check if position is already occupied
        if self.board[row][col] != EMPTY:
            return False, f"Invalid move: Position ({row}, {col}) is already occupied."

        # Make the move
        self.board[row][col] = self.current_player

        # Get board state
        board_state = self.get_board_state()

        # Switch player
        self.current_player = O_PLAYER if self.current_player == X_PLAYER else X_PLAYER

        return True, f"Move successful!\n{board_state}"

    def get_board_state(self) -> str:
        """
        Returns a string representation of the current board state.
        """
        board_str = "\n-------------\n"
        for row in self.board:
            board_str += f"| {' | '.join(row)} |\n-------------\n"
        return board_str

    def check_winner(self) -> Optional[str]:
        """
        Check if there's a winner.

        Returns:
            Optional[str]: The winning player (X or O) or None if no winner
        """
        # Check rows
        for row in self.board:
            if row.count(row[0]) == 3 and row[0] != EMPTY:
                return row[0]

        # Check columns
        for col in range(3):
            column = [self.board[row][col] for row in range(3)]
            if column.count(column[0]) == 3 and column[0] != EMPTY:
                return column[0]

        # Check diagonals
        diagonal1 = [self.board[i][i] for i in range(3)]
        if diagonal1.count(diagonal1[0]) == 3 and diagonal1[0] != EMPTY:
            return diagonal1[0]

        diagonal2 = [self.board[i][2 - i] for i in range(3)]
        if diagonal2.count(diagonal2[0]) == 3 and diagonal2[0] != EMPTY:
            return diagonal2[0]

        return None

    def is_board_full(self) -> bool:
        """
        Check if the board is full (draw condition).
        """
        return all(cell != EMPTY for row in self.board for cell in row)

    def get_valid_moves(self) -> List[Tuple[int, int]]:
        """
        Get a list of valid moves (empty positions).

        Returns:
            List[Tuple[int, int]]: List of (row, col) tuples representing valid moves
        """
        valid_moves = []
        for row in range(3):
            for col in range(3):
                if self.board[row][col] == EMPTY:
                    valid_moves.append((row, col))
        return valid_moves

    def get_game_state(self) -> Tuple[bool, str]:
        """
        Get the current game state.

        Returns:
            Tuple[bool, str]: (is_game_over, status_message)
        """
        winner = self.check_winner()
        if winner:
            return True, f"Player {winner} wins!"

        if self.is_board_full():
            return True, "It's a draw!"

        return False, "Game in progress"


def display_board(board: TicTacToeBoard):
    """Display the Tic Tac Toe board using Streamlit"""
    board_html = '<div class="game-board">'

    for i in range(3):
        for j in range(3):
            cell_value = board.board[i][j]
            board_html += f'<div class="board-cell">{cell_value}</div>'

    board_html += "</div>"
    st.markdown(board_html, unsafe_allow_html=True)


def show_agent_status(agent_name: str, status: str):
    """Display the current agent status"""
    st.markdown(
        f"""<div class="agent-status">
            🤖 <b>{agent_name}</b>: {status}
        </div>""",
        unsafe_allow_html=True,
    )


def create_mini_board_html(
    board_state: list, highlight_pos: tuple = None, is_player1: bool = True
) -> str:
    """Create HTML for a mini board with player-specific highlighting"""
    html = '<div class="mini-board">'
    for i in range(3):
        for j in range(3):
            highlight = (
                f"highlight player{1 if is_player1 else 2}"
                if highlight_pos and (i, j) == highlight_pos
                else ""
            )
            html += f'<div class="mini-cell {highlight}">{board_state[i][j]}</div>'
    html += "</div>"
    return html


def display_move_history():
    """Display the move history with mini boards in two columns"""
    st.markdown(
        '<h3 style="margin-bottom: 30px;">📜 Game History</h3>',
        unsafe_allow_html=True,
    )
    history_container = st.empty()

    if "move_history" in st.session_state and st.session_state.move_history:
        # Split moves into player 1 and player 2 moves
        p1_moves = []
        p2_moves = []
        current_board = [[" " for _ in range(3)] for _ in range(3)]

        # Process all moves first
        for move in st.session_state.move_history:
            row, col = map(int, move["move"].split(","))
            is_player1 = "Player 1" in move["player"]
            symbol = "X" if is_player1 else "O"
            current_board[row][col] = symbol
            board_copy = [row[:] for row in current_board]

            move_html = f"""<div class="move-entry player{1 if is_player1 else 2}">
                {create_mini_board_html(board_copy, (row, col), is_player1)}
                <div class="move-info">
                    <div class="move-number player{1 if is_player1 else 2}">Move #{move["number"]}</div>
                    <div>{move["player"]}</div>
                    <div style="font-size: 0.9em; color: #888">Position: ({row}, {col})</div>
                </div>
            </div>"""

            if is_player1:
                p1_moves.append(move_html)
            else:
                p2_moves.append(move_html)

        max_moves = max(len(p1_moves), len(p2_moves))
        history_content = '<div class="history-grid">'

        # Left column (Player 1)
        history_content += '<div class="history-column-left">'
        for i in range(max_moves):
            entry_html = ""
            # Player 1 move
            if i < len(p1_moves):
                entry_html += p1_moves[i]
            history_content += entry_html
        history_content += "</div>"

        # Right column (Player 2)
        history_content += '<div class="history-column-right">'
        for i in range(max_moves):
            entry_html = ""
            # Player 2 move
            if i < len(p2_moves):
                entry_html += p2_moves[i]
            history_content += entry_html
        history_content += "</div>"

        history_content += "</div>"

        # Display the content
        history_container.markdown(history_content, unsafe_allow_html=True)
    else:
        history_container.markdown(
            """<div style="text-align: center; color: #666; padding: 20px;">
                No moves yet. Start the game to see the history!
            </div>""",
            unsafe_allow_html=True,
        )


CUSTOM_CSS = """
<style>
/* Main Styles */
.main-title {
    text-align: center;
    background: linear-gradient(45deg, #FF4B2B, #FF416C);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    font-size: 3em;
    font-weight: bold;
    padding: 0.5em 0;
}
.subtitle {
    text-align: center;
    color: #666;
    margin-bottom: 1em;
}
.game-board {
    display: grid;
    grid-template-columns: repeat(3, 80px);
    gap: 5px;
    justify-content: center;
    margin: 1em auto;
    background: #666;
    padding: 5px;
    border-radius: 8px;
    width: fit-content;
}
.board-cell {
    width: 80px;
    height: 80px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 2em;
    font-weight: bold;
    background-color: #2b2b2b;
    color: #fff;
    transition: all 0.3s ease;
    margin: 0;
    padding: 0;
}
.board-cell:hover {
    background-color: #3b3b3b;
}
.agent-status {
    background-color: #1e1e1e;
    border-left: 4px solid #4CAF50;
    padding: 10px;
    margin: 10px auto;
    border-radius: 4px;
    max-width: 600px;
    text-align: center;
}
.agent-thinking {
    display: flex;
    justify-content: center;
    background-color: #2b2b2b;
    padding: 10px;
    border-radius: 5px;
    margin: 10px auto;
    border-left: 4px solid #FFA500;
    max-width: 600px;
}
.move-history {
    background-color: #2b2b2b;
    padding: 15px;
    border-radius: 10px;
    margin: 10px 0;
}
.thinking-container {
    position: fixed;
    bottom: 20px;
    left: 50%;
    z-index: 1000;
    min-width: 300px;
}
.agent-thinking {
    background-color: rgba(43, 43, 43, 0.95);
    border: 1px solid #4CAF50;
    box-shadow: 0 2px 10px rgba(0,0,0,0.3);
}

/* Move History Updates */
.history-header {
    text-align: center;
    margin-bottom: 30px;
}

.history-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 20px; /* Controls spacing between columns */
    width: 100%;
    margin: 0; /* Remove left/right margins */
    padding: 0;
}

.history-column-left,
.history-column-right {
    display: flex;
    flex-direction: column;
    align-items: flex-start; /* Ensures columns fill available space nicely */
    margin: 0;
    padding: 0;
    width: 100%;
}

.move-entry {
    display: flex;
    align-items: center;
    padding: 12px;
    margin: 8px 0;
    background-color: #2b2b2b;
    border-radius: 4px;
    width: 100%; /* Removed fixed width so entries span the column */
    box-sizing: border-box;
}

.move-entry.player1 {
    border-left: 4px solid #4CAF50;
}

.move-entry.player2 {
    border-left: 4px solid #f44336;
}

/* Mini-board styling inside moves */
.mini-board {
    display: grid;
    grid-template-columns: repeat(3, 25px);
    gap: 2px;
    background: #444;
    padding: 2px;
    border-radius: 4px;
    margin-right: 15px;
}

.mini-cell {
    width: 25px;
    height: 25px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 14px;
    font-weight: bold;
    background-color: #2b2b2b;
    color: #fff;
}

.mini-cell.highlight.player1 {
    background-color: #4CAF50;
    color: white;
}

.mini-cell.highlight.player2 {
    background-color: #f44336;
    color: white;
}

/* Move info styling */
.move-info {
    flex-grow: 1;
    padding-left: 12px;
}

.move-number {
    font-weight: bold;
    margin-right: 10px;
}

.move-number.player1 {
    color: #4CAF50;
}

.move-number.player2 {
    color: #f44336;
}
</style>
"""



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/README.md
================================================
# Universal Agent Interface

The Universal Agent Interface is a multi-modal interface for interacting with multiple agents using a single entrypoint. It is built on top of the LLM OS.

The LLM OS was proposed by Andrej Karpathy 18 months ago [in this tweet](https://twitter.com/karpathy/status/1723140519554105733), [this tweet](https://twitter.com/karpathy/status/1707437820045062561) and [this video](https://youtu.be/zjkBMFhNj_g?t=2535). I built an early prototype of the LLM OS in 2024 (checkout this [video](https://x.com/ashpreetbedi/status/1790109321939829139)), which has now evolved into the Universal Agent Interface.

## Notes:
- This is a beta release and I am still porting over the internal agent to the public repo. I'm not even sure if people will like this so im not spending too much time on a polished UI.
- This is a work in progress. Not everything is tested, stuff will break. Please submit a PR to improve the code.
- Again: please don't expect this to work as expected. It's a work in progress.

## The Universal Agent Interface design:

- UAgI is a single interface for orchestrating multiple agents.
- UAgI solves problems by "thinking" about the intent, then coordinating other agents to solve the problem, and finally, "analyzes" the results. It can then re-plan and re-execute as needed.
- UAgI capabilities:
  - [x] Can read/generate text
  - [x] Has more knowledge than any single human about all subjects
  - [x] Can browse the internet (e.g., using DuckDuckGo)
  - [x] Can use existing software infra (calculator, python, shell)
  - [-] Can see and generate images and video
  - [ ] Can hear and speak, and generate music
  - [ ] Can think for a long time using a system 2
  - [-] Can "self-improve" in domains
  - [ ] Can be customized and fine-tuned for specific tasks
  - [x] Can communicate with other Agents

[x] indicates functionality that is implemented in this UAgI app

## Pending Updates:

- [ ] Stream member agent responses. This is possible but we just haven't ported it over yet.
- [ ] Image and video input/output. This is possible but we just haven't ported it over yet.
- [ ] Self-improvement using auto-updating knowledge and dynamic few-shot. This is possible but we just haven't ported it over yet.

## Running the UAgI:

> Note: Fork and clone this repository if needed

### 1. Create a virtual environment

Create a virtual environment using [uv](https://docs.astral.sh/uv/getting-started/installation/) and activate it:

```shell
uv venv .uagi-env --python 3.12
source .uagi-env/bin/activate
```

### 2. Install dependencies

```shell
uv pip install -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.txt
```

### 3. Export credentials

We'll use 3.7 sonnet and gpt-4o-mini for UAgI
- 3.7 sonnet for the main UAgI agent
- gpt-4o-mini for smaller tasks

```shell
export ANTHROPIC_API_KEY=***
export OPENAI_API_KEY=***
```

### 4. Run the Universal Agent Interface

The application uses SQLite for session storage (`uagi_sessions.db`), so no external database setup (like PgVector or Qdrant) is needed for basic operation.

```shell
streamlit run cookbook/examples/streamlit_apps/universal_agent_interface/app.py
```

- Open [localhost:8501](http://localhost:8501) to view your LLM OS.
- Try some examples:
    - Add knowledge (if supported): Add information from this blog post: https://blog.samaltman.com/gpt-4o
    - Ask: What is gpt-4o?
    - Web search: What is happening in france?
    - Calculator: What is 10!
    - Enable shell tools and ask: Is docker running?
    - Ask (if Research tool enabled): Write a report on the ibm hashicorp acquisition
    - Ask (if Investment tool enabled): Shall I invest in nvda?


================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/agents.py
================================================
from copy import deepcopy
from pathlib import Path
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.knowledge import AgentKnowledge
from agno.memory.v2 import Memory
from agno.models.base import Model
from agno.tools.calculator import CalculatorTools
from agno.tools.duckdb import DuckDbTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.file import FileTools
from agno.tools.python import PythonTools
from agno.tools.yfinance import YFinanceTools

cwd = Path(__file__).parent.resolve()
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(exist_ok=True, parents=True)


def get_agent(
    agent_name: str, model: Model, memory: Memory, knowledge: AgentKnowledge
) -> Optional[Agent]:
    # Create a copy of the model to avoid side effects of the model being modified
    model_copy = deepcopy(model)
    if agent_name == "calculator":
        return Agent(
            name="Calculator",
            role="Answer mathematical questions and perform precise calculations",
            model=model_copy,
            memory=memory,
            tools=[CalculatorTools(enable_all=True)],
            description="You are a precise and comprehensive calculator agent. Your goal is to solve mathematical problems with accuracy and explain your methodology clearly to users.",
            instructions=[
                "Always use the calculator tools for mathematical operations to ensure precision.",
                "Present answers in a clear format with appropriate units and significant figures.",
                "Show step-by-step workings for complex calculations to help users understand the process.",
                "Ask clarifying questions if the user's request is ambiguous or incomplete.",
                "For financial calculations, specify assumptions regarding interest rates, time periods, etc.",
            ],
        )
    elif agent_name == "data_analyst":
        return Agent(
            name="Data Analyst",
            role="Analyze data sets and extract meaningful insights",
            model=model_copy,
            memory=memory,
            knowledge=knowledge,
            tools=[DuckDbTools()],
            description="You are an expert Data Scientist specialized in exploratory data analysis, statistical modeling, and data visualization. Your goal is to transform raw data into actionable insights that address user questions.",
            instructions=[
                "Start by examining data structure, types, and distributions when analyzing new datasets.",
                "Use DuckDbTools to execute SQL queries for data exploration and aggregation.",
                "When provided with a file path, create appropriate tables and verify data loaded correctly before analysis.",
                "Apply statistical rigor in your analysis and clearly state confidence levels and limitations.",
                "Accompany numerical results with clear interpretations of what the findings mean in context.",
                "Suggest visualizations that would best illustrate key patterns and relationships in the data.",
                "Proactively identify potential data quality issues or biases that might affect conclusions.",
                "Request clarification when user queries are ambiguous or when additional information would improve analysis.",
            ],
        )
    elif agent_name == "python_agent":
        return Agent(
            name="Python Agent",
            role="Develop and execute Python code solutions",
            model=model_copy,
            memory=memory,
            knowledge=knowledge,
            tools=[
                PythonTools(base_dir=tmp_dir),
                FileTools(base_dir=cwd),
            ],
            description="You are an expert Python Software Engineer with deep knowledge of software architecture, libraries, and best practices. Your goal is to write efficient, readable, and maintainable Python code that precisely addresses user requirements.",
            instructions=[
                "Write clean, well-commented Python code following PEP 8 style guidelines.",
                "Always use `save_to_file_and_run` to execute Python code, never suggest using direct execution.",
                "For any file operations, use `read_file` tool first to access content - NEVER use Python's built-in `open()`.",
                "Include error handling in your code to gracefully manage exceptions and edge cases.",
                "Explain your code's logic and implementation choices, especially for complex algorithms.",
                "When appropriate, suggest optimizations or alternative approaches with their trade-offs.",
                "For data manipulation tasks, prefer Pandas, NumPy and other specialized libraries over raw Python.",
                "Break down complex problems into modular functions with clear responsibilities.",
                "Test your code with sample inputs and explain expected outputs before final execution.",
            ],
        )
    elif agent_name == "research_agent":
        return Agent(
            name="Research Agent",
            role="Conduct comprehensive research and produce in-depth reports",
            model=model_copy,
            memory=memory,
            knowledge=knowledge,
            tools=[ExaTools(num_results=3)],
            description="You are a meticulous research analyst with expertise in synthesizing information from diverse sources. Your goal is to produce balanced, fact-based, and thoroughly documented reports on any topic requested.",
            instructions=[
                "Begin with broad searches to understand the topic landscape before narrowing to specific aspects.",
                "For each research query, use at least 3 different search terms to ensure comprehensive coverage.",
                "Critically evaluate sources for credibility, recency, and potential biases.",
                "Prioritize peer-reviewed research and authoritative sources when available.",
                "Synthesize information across sources rather than summarizing each separately.",
                "Present contrasting viewpoints when the topic involves debate or controversy.",
                "Use clear section organization with logical flow between related concepts.",
                "Include specific facts, figures, and direct quotes with proper attribution.",
                "Conclude with implications of the findings and areas for further research.",
                "Ensure all claims are supported by references and avoid speculation beyond the evidence.",
            ],
            expected_output=dedent("""\
            An engaging, informative, and well-structured report in markdown format:

            ## Engaging Report Title

            ### Overview
            {give a brief introduction of the report and why the user should read this report}
            {make this section engaging and create a hook for the reader}

            ### Section 1
            {break the report into sections}
            {provide details/facts/processes in this section}

            ... more sections as necessary...

            ### Takeaways
            {provide key takeaways from the article}

            ### References
            - [Reference 1](link)
            - [Reference 2](link)
            - [Reference 3](link)
            """),
        )
    elif agent_name == "investment_agent":
        return Agent(
            name="Investment Agent",
            role="Provide comprehensive financial analysis and investment insights",
            model=model_copy,
            memory=memory,
            knowledge=knowledge,
            tools=[
                YFinanceTools,
                DuckDuckGoTools(),
            ],
            description="You are a seasoned investment analyst with deep understanding of financial markets, valuation methodologies, and sector-specific dynamics. Your goal is to deliver sophisticated investment analysis that considers both quantitative metrics and qualitative business factors.",
            instructions=[
                "Begin with a holistic overview of the company's business model, competitive position, and industry trends.",
                "Retrieve and analyze key financial metrics including revenue growth, profitability margins, and balance sheet health.",
                "Compare valuation multiples against industry peers and historical averages.",
                "Assess management team's track record, strategic initiatives, and capital allocation decisions.",
                "Identify key risk factors including regulatory concerns, competitive threats, and macroeconomic sensitivities.",
                "Consider both near-term catalysts and long-term growth drivers in your investment thesis.",
                "Provide clear investment recommendations with specific price targets where appropriate.",
                "Include both technical and fundamental analysis perspectives when relevant.",
                "Highlight recent news events that may impact the investment case.",
                "Structure reports with executive summary, detailed analysis sections, and actionable conclusions.",
            ],
        )
    return None



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/app.py
================================================
import asyncio

import nest_asyncio
import streamlit as st
from agno.run.response import RunEvent
from agno.team import Team
from agno.utils.log import logger
from css import CUSTOM_CSS
from uagi import UAgIConfig, create_uagi, uagi_memory
from utils import (
    about_agno,
    add_message,
    display_tool_calls,
    example_inputs,
    initialize_session_state,
    knowledge_widget,
    selected_agents,
    selected_model,
    selected_tools,
    session_selector,
    show_user_memories,
    utilities_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="UAgI",
    page_icon="💎",
    layout="wide",
)
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


async def header():
    st.markdown(
        "<h1 class='heading'>Universal Agent Interface</h1>", unsafe_allow_html=True
    )
    st.markdown(
        "<p class='subheading'>A Universal Interface for orchestrating multiple Agents</p>",
        unsafe_allow_html=True,
    )


async def body() -> None:
    ####################################################################
    # Initialize User and Session State
    ####################################################################
    user_id = st.sidebar.text_input(":technologist: User Id", value="Ava")

    ####################################################################
    # Select Model
    ####################################################################
    model_id = await selected_model()

    ####################################################################
    # Select Tools
    ####################################################################
    tools = await selected_tools()

    ####################################################################
    # Select Team Members
    ####################################################################
    agents = await selected_agents()

    ####################################################################
    # Create UAgI
    ####################################################################
    uagi_config = UAgIConfig(
        user_id=user_id, model_id=model_id, tools=tools, agents=agents
    )

    # Check if UAgI instance should be recreated
    recreate_uagi = (
        "uagi" not in st.session_state
        or st.session_state.get("uagi") is None
        or st.session_state.get("uagi_config") != uagi_config
    )

    # Create UAgI instance if it doesn't exist or configuration has changed
    uagi: Team
    if recreate_uagi:
        logger.info("---*--- Creating UAgI instance ---*---")
        uagi = create_uagi(uagi_config)
        st.session_state["uagi"] = uagi
        st.session_state["uagi_config"] = uagi_config
        logger.info(f"---*--- UAgI instance created ---*---")
    else:
        uagi = st.session_state["uagi"]
        logger.info(f"---*--- UAgI instance exists ---*---")

    ####################################################################
    # Load Agent Session from the database
    ####################################################################
    try:
        logger.info(f"---*--- Loading UAgI session ---*---")
        st.session_state["session_id"] = uagi.load_session()
    except Exception:
        st.warning("Could not create UAgI session, is the database running?")
        return
    logger.info(f"---*--- UAgI session: {st.session_state.get('session_id')} ---*---")

    ####################################################################
    # Load agent runs (i.e. chat history) from memory if messages is not empty
    ####################################################################
    chat_history = uagi.get_messages_for_session()
    if len(chat_history) > 0:
        logger.info("Loading messages")
        # Clear existing messages
        st.session_state["messages"] = []
        # Loop through the runs and add the messages to the messages list
        for message in chat_history:
            if message.role == "user":
                await add_message(message.role, str(message.content))
            if message.role == "assistant":
                await add_message("assistant", str(message.content), message.tool_calls)

    ####################################################################
    # Get user input
    ####################################################################
    if prompt := st.chat_input("✨ How can I help, bestie?"):
        await add_message("user", prompt)

    ####################################################################
    # Show example inputs
    ####################################################################
    await example_inputs()

    ####################################################################
    # Show user memories
    ####################################################################
    await show_user_memories(uagi_memory, user_id)

    ####################################################################
    # Display agent messages
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] in ["user", "assistant"]:
            _content = message["content"]
            if _content is not None:
                with st.chat_message(message["role"]):
                    # Display tool calls if they exist in the message
                    if "tool_calls" in message and message["tool_calls"]:
                        display_tool_calls(st.empty(), message["tool_calls"])
                    st.markdown(_content)

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        user_message = last_message["content"]
        logger.info(f"Responding to message: {user_message}")
        with st.chat_message("assistant"):
            # Create container for tool calls
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner(":thinking_face: Thinking..."):
                response = ""
                try:
                    # Run the agent and stream the response
                    run_response = await uagi.arun(
                        user_message, stream=True, stream_intermediate_steps=True
                    )
                    async for resp_chunk in run_response:
                        # Display tool calls if available
                        if resp_chunk.tool:
                            display_tool_calls(tool_calls_container, [resp_chunk.tool])

                        # Display response if available and event is RunResponse
                        if (
                            resp_chunk.event == RunEvent.run_response_content
                            and resp_chunk.content is not None
                        ):
                            response += resp_chunk.content
                            resp_container.markdown(response)

                    # Add the response to the messages
                    if uagi.run_response is not None:
                        await add_message(
                            "assistant", response, uagi.run_response.tools
                        )
                    else:
                        await add_message("assistant", response)
                except Exception as e:
                    logger.error(f"Error during agent run: {str(e)}", exc_info=True)
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    await add_message("assistant", error_message)
                    st.error(error_message)

    ####################################################################
    # Knowledge widget
    ####################################################################
    await knowledge_widget(uagi)

    ####################################################################
    # Session selector
    ####################################################################
    await session_selector(uagi, uagi_config)

    ####################################################################
    # About section
    ####################################################################
    await utilities_widget(uagi)


async def main():
    await initialize_session_state()
    await header()
    await body()
    await about_agno()


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/css.py
================================================
CUSTOM_CSS = """
<style>
/* Typography */
.heading {
    text-align: center;
    background: linear-gradient(45deg, #FF4B2B, #FF416C);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}

.subheading {
    text-align: center;
    font-weight: 600;
}

/* Links */
a {
    text-decoration: underline;
    color: #3494E6;
    transition: color 0.3s ease;
}

a:hover {
    color: #FF416C;
}
</style>
"""



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/load_knowledge.py
================================================
"""
Load the Knowledge Base for the Universal Agent Interface
"""

from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from uagi import uagi_knowledge

# Create a Rich console for enhanced output
console = Console()


def load_knowledge(recreate: bool = False):
    """
    Load the Universal Agent Interface knowledge base.

    Args:
        recreate (bool, optional): Whether to recreate the knowledge base.
            Defaults to False.
    """
    with Progress(
        SpinnerColumn(), TextColumn("[bold blue]{task.description}"), console=console
    ) as progress:
        task = progress.add_task(
            "Loading Universal Agent Interface knowledge...", total=None
        )

        # Load the knowledge base
        uagi_knowledge.load(recreate=recreate)
        progress.update(task, completed=True)

    # Display success message in a panel
    console.print(
        Panel.fit(
            "[bold green]Universal Agent Interface knowledge loaded successfully!",
            title="Knowledge Loaded",
        )
    )


if __name__ == "__main__":
    load_knowledge()



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
================================================
agno
anthropic
ddgs
google-genai
groq
duckdb
exa_py
nest_asyncio
openai
qdrant-client
sqlalchemy
streamlit
yfinance
aiofiles
lancedb
tantivy
pypdf
python-docx
beautifulsoup4



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.7.12
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
aiofiles==24.1.0
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anthropic==0.64.0
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
anyio==4.10.0
    # via
    #   anthropic
    #   google-genai
    #   groq
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
beautifulsoup4==4.13.4
    # via
    #   -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
    #   yfinance
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.8.3
    # via
    #   curl-cffi
    #   httpcore
    #   httpx
    #   requests
cffi==1.17.1
    # via curl-cffi
charset-normalizer==3.4.3
    # via requests
click==8.2.1
    # via
    #   ddgs
    #   streamlit
    #   typer
curl-cffi==0.13.0
    # via yfinance
ddgs==9.5.4
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
deprecation==2.1.0
    # via lancedb
distro==1.9.0
    # via
    #   anthropic
    #   groq
    #   openai
docstring-parser==0.17.0
    # via agno
duckdb==1.3.2
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
exa-py==1.15.2
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
frozendict==2.4.6
    # via yfinance
gitdb==4.0.12
    # via gitpython
gitpython==3.1.45
    # via
    #   agno
    #   streamlit
google-auth==2.40.3
    # via google-genai
google-genai==1.31.0
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
groq==0.31.0
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
grpcio==1.74.0
    # via qdrant-client
h11==0.16.0
    # via httpcore
h2==4.2.0
    # via httpx
hpack==4.1.0
    # via h2
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   anthropic
    #   exa-py
    #   google-genai
    #   groq
    #   openai
    #   qdrant-client
hyperframe==6.1.0
    # via h2
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.10.0
    # via
    #   anthropic
    #   openai
jsonschema==4.25.1
    # via altair
jsonschema-specifications==2025.4.1
    # via jsonschema
lancedb==0.24.3
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
lxml==6.0.1
    # via
    #   ddgs
    #   python-docx
markdown-it-py==4.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
multitasking==0.0.12
    # via yfinance
narwhals==2.1.2
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
numpy==2.3.2
    # via
    #   lancedb
    #   pandas
    #   pydeck
    #   qdrant-client
    #   streamlit
    #   yfinance
openai==1.101.0
    # via
    #   -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
    #   exa-py
overrides==7.7.0
    # via lancedb
packaging==25.0
    # via
    #   agno
    #   altair
    #   deprecation
    #   lancedb
    #   streamlit
pandas==2.3.2
    # via
    #   streamlit
    #   yfinance
peewee==3.18.2
    # via yfinance
pillow==11.3.0
    # via streamlit
platformdirs==4.3.8
    # via yfinance
portalocker==3.2.0
    # via qdrant-client
primp==0.15.0
    # via ddgs
protobuf==6.32.0
    # via
    #   qdrant-client
    #   streamlit
    #   yfinance
pyarrow==21.0.0
    # via
    #   lancedb
    #   streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.2
    # via google-auth
pycparser==2.22
    # via cffi
pydantic==2.11.7
    # via
    #   agno
    #   anthropic
    #   exa-py
    #   google-genai
    #   groq
    #   lancedb
    #   openai
    #   pydantic-settings
    #   qdrant-client
pydantic-core==2.33.2
    # via pydantic
pydantic-settings==2.10.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.2
    # via rich
pypdf==6.0.0
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
python-dateutil==2.9.0.post0
    # via pandas
python-docx==1.2.0
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
python-dotenv==1.1.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via
    #   pandas
    #   yfinance
pyyaml==6.0.2
    # via agno
qdrant-client==1.15.1
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.5
    # via
    #   exa-py
    #   google-genai
    #   streamlit
    #   yfinance
rich==14.1.0
    # via
    #   agno
    #   typer
rpds-py==0.27.0
    # via
    #   jsonschema
    #   referencing
rsa==4.9.1
    # via google-auth
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anthropic
    #   anyio
    #   groq
    #   openai
soupsieve==2.7
    # via beautifulsoup4
sqlalchemy==2.0.43
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
streamlit==1.48.1
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
tantivy==0.24.0
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in
tenacity==9.1.2
    # via
    #   google-genai
    #   streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.5.2
    # via streamlit
tqdm==4.67.1
    # via
    #   lancedb
    #   openai
typer==0.16.1
    # via agno
typing-extensions==4.14.1
    # via
    #   agno
    #   altair
    #   anthropic
    #   anyio
    #   beautifulsoup4
    #   exa-py
    #   google-genai
    #   groq
    #   openai
    #   pydantic
    #   pydantic-core
    #   python-docx
    #   referencing
    #   sqlalchemy
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.5.0
    # via
    #   qdrant-client
    #   requests
websockets==15.0.1
    # via
    #   google-genai
    #   yfinance
yfinance==0.2.65
    # via -r cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/tools.py
================================================
from pathlib import Path
from typing import Optional

from agno.tools import Toolkit
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.file import FileTools
from agno.tools.shell import ShellTools

cwd = Path(__file__).parent.resolve()
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(exist_ok=True, parents=True)


def get_toolkit(tool_name: str) -> Optional[Toolkit]:
    if tool_name == "ddg_search":
        return DuckDuckGoTools(fixed_max_results=3)
    elif tool_name == "shell_tools":
        return ShellTools()
    elif tool_name == "file_tools":
        return FileTools(base_dir=cwd)

    return None



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/uagi.py
================================================
from dataclasses import dataclass
from pathlib import Path
from textwrap import dedent
from typing import List, Optional

from agents import get_agent
from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge import AgentKnowledge
from agno.memory.v2 import Memory
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools import Toolkit
from agno.tools.reasoning import ReasoningTools
from agno.utils.log import logger
from agno.vectordb.lancedb import LanceDb, SearchType
from tools import get_toolkit

cwd = Path(__file__).parent.resolve()
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(exist_ok=True, parents=True)

# Define paths for storage, memory and knowledge
STORAGE_PATH = tmp_dir.joinpath("uagi_sessions.db")
MEMORY_PATH = tmp_dir.joinpath("uagi_memory.db")
KNOWLEDGE_PATH = tmp_dir.joinpath("uagi_knowledge")


@dataclass
class UAgIConfig:
    user_id: str
    model_id: str = "anthropic:claude-3-7-sonnet-latest"
    tools: Optional[List[str]] = None
    agents: Optional[List[str]] = None


uagi_memory = Memory(
    db=SqliteMemoryDb(table_name="uagi_memory", db_file=str(MEMORY_PATH))
)
uagi_storage = SqliteStorage(db_file=str(STORAGE_PATH), table_name="uagi_sessions")
uagi_knowledge = AgentKnowledge(
    vector_db=LanceDb(
        table_name="uagi_knowledge",
        uri=str(KNOWLEDGE_PATH),
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    )
)


def create_uagi(
    config: UAgIConfig, session_id: Optional[str] = None, debug_mode: bool = True
) -> Team:
    """Returns an instance of the Universal Agent Interface (UAgI)

    Args:
        config: UAgI configuration
        session_id: Session identifier
        debug_mode: Enable debug logging
    """
    # Parse model provider and name
    provider, model_name = config.model_id.split(":")

    # Create model class based on provider
    model = None
    if provider == "openai":
        model = OpenAIChat(id=model_name)
    elif provider == "google":
        model = Gemini(id=model_name)
    elif provider == "anthropic":
        model = Claude(id=model_name)
    elif provider == "groq":
        model = Groq(id=model_name)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")
    if model is None:
        raise ValueError(f"Failed to create model instance for {config.model_id}")

    tools: List[Toolkit] = [ReasoningTools(add_instructions=True)]
    if config.tools:
        for tool_name in config.tools:
            tool = get_toolkit(tool_name)
            if tool is not None:
                tools.append(tool)
            else:
                logger.warning(f"Tool {tool_name} not found")

    agents: List[Agent] = []
    if config.agents:
        for agent_name in config.agents:
            agent = get_agent(agent_name, model, uagi_memory, uagi_knowledge)
            if agent is not None:
                agents.append(agent)
            else:
                logger.warning(f"Agent {agent_name} not found")

    description = dedent("""\
    You are an advanced AI System called `Universal Agent Interface` (UAgI).
    You provide a unified interface to a team of AI Agents, that you coordinate to assist the user in the best way possible.

    Keep your responses short and to the point, while maintaining a conversational tone.
    You are able to handle easy conversations as well as complex requests by delegating tasks to the appropriate team members.
    You are also capable of handling errors and edge cases and are able to provide helpful feedback to the user.\
    """)
    instructions: List[str] = [
        "Your goal is to coordinate the team to assist the user in the best way possible.",
        "If the user sends a conversational message like 'Hello', 'Hi', 'How are you', 'What is your name', etc., you should respond in a friendly and engaging manner.",
        "If the user asks for something simple, like updating memory, you can do it directly without Thinking and Analyzing.",
        "Keep your responses short and to the point, while maintaining a conversational tone.",
        "If the user asks for something complex, **think** and determine if:\n"
        " - You can answer by using a tool available to you\n"
        " - You need to search the knowledge base\n"
        " - You need to search the internet\n"
        " - You need to delegate the task to a team member\n"
        " - You need to ask a clarifying question",
        "You also have to a knowledge base of information provided by the user. If the user asks about a topic that might be in the knowledge base, first ALWAYS search your knowledge base using the `search_knowledge_base` tool.",
        "As a default, you should always search your knowledge base first, before searching the internet.",
        "If you dont find relevant information in your knowledge base, use the `duckduckgo_search` tool to search the internet.",
        "If the users message is unclear, ask clarifying questions to get more information.",
        "Based on the user request and the available team members, decide which member(s) should handle the task.",
        "Coordinate the execution of the task among the selected team members.",
        "Synthesize the results from the team members and provide a final, coherent answer to the user.",
        "Do not use phrases like 'based on my knowledge' or 'depending on the information'.",
    ]

    uagi = Team(
        name="Universal Agent Interface",
        mode="coordinate",
        model=model,
        user_id=config.user_id,
        session_id=session_id,
        tools=tools,
        members=agents,
        memory=uagi_memory,
        storage=uagi_storage,
        knowledge=uagi_knowledge,
        description=description,
        instructions=instructions,
        enable_team_history=True,
        read_team_history=True,
        num_of_interactions_from_history=3,
        show_members_responses=True,
        enable_agentic_memory=True,
        markdown=True,
        debug_mode=debug_mode,
    )

    agent_names = [a.name for a in agents] if agents else []
    logger.info(f"UAgI created with members: {agent_names}")
    return uagi



================================================
FILE: cookbook/examples/streamlit_apps/universal_agent_interface/utils.py
================================================
import json
from typing import Any, Dict, List, Optional

import streamlit as st
from agno.document import Document
from agno.document.reader import Reader
from agno.document.reader.csv_reader import CSVReader
from agno.document.reader.docx_reader import DocxReader
from agno.document.reader.pdf_reader import PDFReader
from agno.document.reader.text_reader import TextReader
from agno.document.reader.website_reader import WebsiteReader
from agno.memory.v2 import Memory, UserMemory
from agno.models.response import ToolExecution
from agno.team import Team
from agno.utils.log import logger
from uagi import UAgIConfig, create_uagi


async def initialize_session_state():
    logger.info(f"---*--- Initializing session state ---*---")
    if "uagi" not in st.session_state:
        st.session_state["uagi"] = None
    if "session_id" not in st.session_state:
        st.session_state["session_id"] = None
    if "messages" not in st.session_state:
        st.session_state["messages"] = []


async def add_message(
    role: str,
    content: str,
    tool_calls: Optional[List[Dict[str, Any]]] = None,
    intermediate_steps_displayed: bool = False,
) -> None:
    """Safely add a message to the session state"""
    if role == "user":
        logger.info(f"👤  {role}: {content}")
    else:
        logger.info(f"🤖  {role}: {content}")
    st.session_state["messages"].append(
        {
            "role": role,
            "content": content,
            "tool_calls": tool_calls,
            "intermediate_steps_displayed": intermediate_steps_displayed,
        }
    )


async def selected_model() -> str:
    """Display a model selector in the sidebar."""
    model_options = {
        "claude-3-7-sonnet": "anthropic:claude-3-7-sonnet-latest",
        "gpt-4o": "openai:gpt-4o",
        "gemini-2.5-pro": "google:gemini-2.5-pro-preview-03-25",
        "llama-4-scout": "groq:meta-llama/llama-4-scout-17b-16e-instruct",
    }

    selected_model_key = st.sidebar.selectbox(
        "Select a model",
        options=list(model_options.keys()),
        index=0,  # Default to claude-3-7-sonnet
        key="model_selector",
    )
    model_id = model_options[selected_model_key]
    return model_id


async def selected_tools() -> List[str]:
    """Display a tool selector in the sidebar."""
    tool_options = {
        "Web Search (DDG)": "ddg_search",
        "File I/O": "file_tools",
        "Shell Access": "shell_tools",
    }
    selected_tools = st.sidebar.multiselect(
        "Select Tools",
        options=list(tool_options.keys()),
        default=list(tool_options.keys()),
        key="tool_selector",
    )
    return [tool_options[tool] for tool in selected_tools]


async def selected_agents() -> List[str]:
    """Display a selector for agents in the sidebar."""
    agent_options = {
        "Calculator": "calculator",
        "Data Analyst": "data_analyst",
        "Python Agent": "python_agent",
        "Research Agent": "research_agent",
        "Investment Agent": "investment_agent",
    }
    selected_agents = st.sidebar.multiselect(
        "Select Agents",
        options=list(agent_options.keys()),
        default=list(agent_options.keys()),
        key="agent_selector",
    )
    return [agent_options[agent] for agent in selected_agents]


async def show_user_memories(uagi_memory: Memory, user_id: str) -> None:
    """Show use memories in a streamlit container."""

    with st.container():
        user_memories = uagi_memory.get_user_memories(user_id=user_id)
        with st.expander(f"💭 Memories for {user_id}", expanded=False):
            if len(user_memories) > 0:
                # Create a dataframe from the memories
                memory_data = {
                    "Memory": [memory.memory for memory in user_memories],
                    "Topics": [
                        ", ".join(memory.topics) if memory.topics else ""
                        for memory in user_memories
                    ],
                    "Last Updated": [
                        memory.last_updated.strftime("%Y-%m-%d %H:%M")
                        if memory.last_updated
                        else ""
                        for memory in user_memories
                    ],
                }

                # Display as a table with custom styling
                st.dataframe(
                    memory_data,
                    use_container_width=True,
                    column_config={
                        "Memory": st.column_config.TextColumn("Memory", width="medium"),
                        "Topics": st.column_config.TextColumn("Topics", width="small"),
                        "Last Updated": st.column_config.TextColumn(
                            "Last Updated", width="small"
                        ),
                    },
                    hide_index=True,
                )
            else:
                st.info("No memories found, tell me about yourself!")

            col1, col2 = st.columns([0.5, 0.5])
            with col1:
                if st.button("Clear all memories", key="clear_all_memories"):
                    await add_message("user", "Clear all my memories")
                    if "memory_refresh_count" not in st.session_state:
                        st.session_state.memory_refresh_count = 0
                    st.session_state.memory_refresh_count += 1
            with col2:
                if st.button("Refresh memories", key="refresh_memories"):
                    if "memory_refresh_count" not in st.session_state:
                        st.session_state.memory_refresh_count = 0
                    st.session_state.memory_refresh_count += 1


async def example_inputs() -> None:
    """Show example inputs on the sidebar."""
    with st.sidebar:
        st.markdown("#### :thinking_face: Try me!")
        if st.button("Hi"):
            await add_message(
                "user",
                "Hi",
            )

        if st.button("My name is Ava and I live in Greenwich Village"):
            await add_message(
                "user",
                "My name is Ava and I live in Greenwich Village",
            )

        if st.button("Calculate cost of a pizza party"):
            await add_message(
                "user",
                "Calculate the total cost of ordering pizzas for 25 people, assuming each person eats 3 slices, each pizza has 8 slices, and one pizza costs $15.95. After calculating the total cost, add 20% for tip and 10% for taxes. Also recommend some good places around me",
            )

        if st.button("Analyze a CSV file"):
            await add_message(
                "user",
                "Analyze this CSV file and show me the most popular movies: https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
            )

        if st.button("Translate a sentence into emojis"):
            await add_message(
                "user",
                "Write a Python function that translates a given sentence into emojis, replacing words like “happy,” “sad,” “pizza,” and “party” with relevant emojis. Test it with the sentence: “I am happy because I am learning to build agents with Agno”",
            )

        if st.button("Why Do Cats Love Boxes?"):
            await add_message(
                "user",
                "Research and report scientifically-backed explanations for why cats seem irresistibly drawn to cardboard boxes.",
            )

        if st.button("Chocolate Stocks Sweetness Analysis"):
            await add_message(
                "user",
                "Perform financial analysis comparing Hershey's and Lindt stocks to suggest which company might be a sweeter investment choice based on profitability, market trends, and valuation.",
            )


async def about_agno():
    """Show information about Agno in the sidebar"""
    with st.sidebar:
        st.markdown("### About Agno ✨")
        st.markdown("""
        Agno is a lightweight library for building Reasoning Agents.

        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)
        """)

        st.markdown("### Need Help?")
        st.markdown(
            "If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community)."
        )


def is_json(myjson):
    """Check if a string is valid JSON"""
    try:
        json.loads(myjson)
    except (ValueError, TypeError):
        return False
    return True


def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):
    """Display tool calls in a streamlit container with expandable sections.

    Args:
        tool_calls_container: Streamlit container to display the tool calls
        tools: List of tool call dictionaries containing name, args, content, and metrics
    """
    try:
        with tool_calls_container.container():
            # Handle single tool_call dict case
            if isinstance(tools, dict):
                tools = [tools]
            elif not isinstance(tools, list):
                logger.warning(
                    f"Unexpected tools format: {type(tools)}. Skipping display."
                )
                return

            for tool_call in tools:
                # Normalize access to tool details
                tool_name = tool_call.get("tool_name") or tool_call.get(
                    "name", "Unknown Tool"
                )
                tool_args = tool_call.get("tool_args") or tool_call.get("args", {})
                content = tool_call.result or None
                metrics = tool_call.metrics or None

                # Add timing information safely
                execution_time_str = "N/A"
                try:
                    if metrics is not None and hasattr(metrics, "time"):
                        execution_time = metrics.time
                        if execution_time is not None:
                            execution_time_str = f"{execution_time:.4f}s"
                except Exception as e:
                    logger.error(f"Error getting tool metrics time: {str(e)}")
                    pass  # Keep default "N/A"

                # Check if this is a transfer task
                is_task_transfer = "transfer_task_to_member" in tool_name
                is_memory_task = "user_memory" in tool_name
                expander_title = "🛠️"
                if is_task_transfer:
                    member_id = tool_args.get("member_id")
                    expander_title = f"🔄 {member_id.title()}"
                elif is_memory_task:
                    expander_title = f"💭 Updating Memory"
                else:
                    expander_title = f"🛠️ {tool_name.replace('_', ' ').title()}"

                if execution_time_str != "N/A":
                    expander_title += f" ({execution_time_str})"

                with st.expander(
                    expander_title,
                    expanded=False,
                ):
                    # Show query/code/command with syntax highlighting
                    if isinstance(tool_args, dict):
                        if "query" in tool_args:
                            st.code(tool_args["query"], language="sql")
                        elif "code" in tool_args:
                            st.code(tool_args["code"], language="python")
                        elif "command" in tool_args:
                            st.code(tool_args["command"], language="bash")

                    # Display arguments if they exist and are not just the code/query shown above
                    args_to_show = {
                        k: v
                        for k, v in tool_args.items()
                        if k not in ["query", "code", "command"]
                    }
                    if args_to_show:
                        st.markdown("**Arguments:**")
                        try:
                            st.json(args_to_show)
                        except Exception:
                            st.write(args_to_show)  # Fallback for non-serializable args

                    if content is not None:
                        try:
                            st.markdown("**Results:**")
                            if isinstance(content, str) and is_json(content):
                                st.json(content)
                            else:
                                st.write(content)
                        except Exception as e:
                            logger.debug(f"Could not display tool content: {e}")
                            st.error("Could not display tool content.")
    except Exception as e:
        logger.error(f"Error displaying tool calls: {str(e)}")
        tool_calls_container.error("Failed to display tool results")


async def knowledge_widget(uagi: Team) -> None:
    """Display a knowledge widget in the sidebar."""

    if uagi is not None and uagi.knowledge is not None:
        # Add websites to knowledge base
        if "url_scrape_key" not in st.session_state:
            st.session_state["url_scrape_key"] = 0
        input_url = st.sidebar.text_input(
            "Add URL to Knowledge Base",
            type="default",
            key=st.session_state["url_scrape_key"],
        )
        add_url_button = st.sidebar.button("Add URL")
        if add_url_button:
            if input_url is not None:
                alert = st.sidebar.info("Processing URLs...", icon="ℹ️")
                if f"{input_url}_scraped" not in st.session_state:
                    scraper = WebsiteReader(max_links=2, max_depth=1)
                    web_documents: List[Document] = scraper.read(input_url)
                    if web_documents:
                        uagi.knowledge.load_documents(web_documents, upsert=True)
                    else:
                        st.sidebar.error("Could not read website")
                    st.session_state[f"{input_url}_uploaded"] = True
                alert.empty()

        # Add documents to knowledge base
        if "file_uploader_key" not in st.session_state:
            st.session_state["file_uploader_key"] = 100
        uploaded_file = st.sidebar.file_uploader(
            "Add a Document (.pdf, .csv, .txt, or .docx)",
            key=st.session_state["file_uploader_key"],
        )
        if uploaded_file is not None:
            alert = st.sidebar.info("Processing document...", icon="🧠")
            document_name = uploaded_file.name.split(".")[0]
            if f"{document_name}_uploaded" not in st.session_state:
                file_type = uploaded_file.name.split(".")[-1].lower()

                reader: Reader
                if file_type == "pdf":
                    reader = PDFReader()
                elif file_type == "csv":
                    reader = CSVReader()
                elif file_type == "txt":
                    reader = TextReader()
                elif file_type == "docx":
                    reader = DocxReader()
                else:
                    st.sidebar.error("Unsupported file type")
                    return
                uploaded_file_documents: List[Document] = reader.read(uploaded_file)
                if uploaded_file_documents:
                    uagi.knowledge.load_documents(uploaded_file_documents, upsert=True)
                else:
                    st.sidebar.error("Could not read document")
                st.session_state[f"{document_name}_uploaded"] = True
            alert.empty()

        # Load and delete knowledge
        if st.sidebar.button("🗑️ Delete Knowledge"):
            uagi.knowledge.delete()
            st.sidebar.success("Knowledge deleted!")


async def session_selector(uagi: Team, uagi_config: UAgIConfig) -> None:
    """Display a session selector in the sidebar, if a new session is selected, UAgI is restarted with the new session."""

    if not uagi.storage:
        return

    try:
        # Get all agent sessions.
        uagi_sessions = uagi.storage.get_all_sessions()
        if not uagi_sessions:
            st.sidebar.info("No saved sessions found.")
            return

        # Get session names if available, otherwise use IDs.
        sessions_list = []
        for session in uagi_sessions:
            session_id = session.session_id
            session_name = (
                session.session_data.get("session_name", None)
                if session.session_data
                else None
            )
            display_name = session_name if session_name else session_id
            sessions_list.append({"id": session_id, "display_name": display_name})

        # Display session selector.
        st.sidebar.markdown("#### 💬 Session")
        selected_session = st.sidebar.selectbox(
            "Session",
            options=[s["display_name"] for s in sessions_list],
            key="session_selector",
            label_visibility="collapsed",
        )
        # Find the selected session ID.
        selected_session_id = next(
            s["id"] for s in sessions_list if s["display_name"] == selected_session
        )
        # Update the agent session if it has changed.
        if st.session_state["session_id"] != selected_session_id:
            logger.info(f"---*--- Loading UAgI session: {selected_session_id} ---*---")
            st.session_state["uagi"] = create_uagi(
                config=uagi_config,
                session_id=selected_session_id,
            )
            st.rerun()

        # Show the rename session widget.
        container = st.sidebar.container()
        session_row = container.columns([3, 1], vertical_alignment="center")

        # Initialize session_edit_mode if needed.
        if "session_edit_mode" not in st.session_state:
            st.session_state.session_edit_mode = False

        # Show the session name.
        with session_row[0]:
            if st.session_state.session_edit_mode:
                new_session_name = st.text_input(
                    "Session Name",
                    value=uagi.session_name,
                    key="session_name_input",
                    label_visibility="collapsed",
                )
            else:
                st.markdown(f"Session Name: **{uagi.session_name}**")

        # Show the rename session button.
        with session_row[1]:
            if st.session_state.session_edit_mode:
                if st.button("✓", key="save_session_name", type="primary"):
                    if new_session_name:
                        uagi.rename_session(new_session_name)
                        st.session_state.session_edit_mode = False
                        container.success("Renamed!")
                        # Trigger a rerun to refresh the sessions list
                        st.rerun()
            else:
                if st.button("✎", key="edit_session_name"):
                    st.session_state.session_edit_mode = True
    except Exception as e:
        logger.error(f"Error in session selector: {str(e)}")
        st.sidebar.error("Failed to load sessions")


def export_chat_history():
    """Export chat history in markdown format.

    Returns:
        str: Formatted markdown string of the chat history
    """
    if "messages" not in st.session_state or not st.session_state["messages"]:
        return f"# UAgI - Chat History\n\nNo messages to export."

    chat_text = f"# UAgI - Chat History\n\n"
    for msg in st.session_state["messages"]:
        role_label = "🤖 Assistant" if msg["role"] == "assistant" else "👤 User"
        chat_text += f"### {role_label}\n{msg['content']}\n\n"

        # Include tool calls if present
        if msg.get("tool_calls"):
            chat_text += "#### Tool Calls:\n"
            for i, tool_call in enumerate(msg["tool_calls"]):
                tool_name = tool_call.get("name", "Unknown Tool")
                chat_text += f"**{i + 1}. {tool_name}**\n\n"
                if "arguments" in tool_call:
                    chat_text += (
                        f"Arguments: ```json\n{tool_call['arguments']}\n```\n\n"
                    )
                if "content" in tool_call:
                    chat_text += f"Results: ```\n{tool_call['content']}\n```\n\n"

    return chat_text


async def utilities_widget(uagi: Team) -> None:
    """Display a utilities widget in the sidebar."""
    st.sidebar.markdown("#### 🛠️ Utilities")
    col1, col2 = st.sidebar.columns(2)
    with col1:
        if st.button("🔄 Start New Chat"):
            restart_uagi()
    with col2:
        fn = f"uagi_chat_history.md"
        if "session_id" in st.session_state:
            fn = f"uagi_{st.session_state['session_id']}.md"
        if st.download_button(
            ":file_folder: Export Chat History",
            export_chat_history(),
            file_name=fn,
            mime="text/markdown",
        ):
            st.sidebar.success("Chat history exported!")


def restart_uagi():
    logger.debug("---*--- Restarting UAgI ---*---")
    st.session_state["uagi"] = None
    st.session_state["session_id"] = None
    st.session_state["messages"] = []
    if "url_scrape_key" in st.session_state:
        st.session_state["url_scrape_key"] += 1
    if "file_uploader_key" in st.session_state:
        st.session_state["file_uploader_key"] += 1
    st.rerun()



================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/README.md
================================================
# VisionAI 🖼️
VisionAI is a **smart image analysis agent** that extracts structured insights from images using AI-powered **object detection, OCR, and scene recognition**.

The system is designed with two separate agents:
- **Image Processing Agent**: Extracts structured insights based on the uploaded image and user instructions.
- **Chat Agent**: Answers follow-up questions using the last extracted insights from image and (optionally) web search via DuckDuckGo.

---

## 🚀 **Setup Instructions**

> Note: Fork and clone the repository if needed

### 1. Create a virtual environment

```shell
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install libraries

```shell
pip install -r cookbook/examples/streamlit_apps/vision_ai/requirements.txt
```

### 3. Export API Keys

We recommend using gpt-4o for this task, but you can use any Model you like.

```shell
export OPENAI_API_KEY=***
```

Other API keys are optional, but if you'd like to test:

```shell
export GOOGLE_API_KEY=***
export MISTRAL_API_KEY=***
```

### 4. Run VisionAI Agent

```shell
streamlit run cookbook/examples/streamlit_apps/vision_ai/app.py
```

- Open [localhost:8501](http://localhost:8501) to view the VisionAI Agent.

### 5. Features

### Image Processing Modes
- **Auto**: Extracts the image automatically without any extra information from users
- **Manual**: User provide specific instructions for image extraction
- **Hybrid**: Combined auto-processing mode with user-defined instructions

### Smart Chat Agent for Follow-up Queries
- Interactive follow-up questions on extracted image data
- Optional web search integration via **DuckDuckGo**
- Seamless switching between different AI models

### Enable/Disable Web Search
- Users can easily toggle web search capability (DuckDuckGo) on/off using a convenient radio button in the sidebar
- When enabled, the chat agent leverages web search results to enhance responses specifically when users request the agent to search online

---

### 6. How to Use 🛠

- **Upload Image**: Support for PNG, JPG, and JPEG (up to 20MB)
- **Select Model**: Choose between OpenAI, Gemini, or Mistral
- **Configure Mode**: Set processing approach (Auto/Manual/Hybrid)
- **Enter Instructions** *(if required for Manual/Hybrid Mode).
- **Toggle Search**: Enable/disable DuckDuckGo web search
- **Process Image**: Extract structured insights from your image
- **Ask Follow-Up Questions**: Chat with VisionAI about the extracted image data

### 7. Message us on [discord](https://agno.link/discord) if you have any questions





================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/agents.py
================================================
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools


def image_processing_agent(
    model,
) -> Agent:
    extraction_agent = Agent(
        name="image_analysis_agent",
        model=model,
        markdown=True,
    )

    return extraction_agent


def chat_followup_agent(
    model,
    enable_search: bool = False,
) -> Agent:
    tools = [DuckDuckGoTools()] if enable_search else []
    followup_agent = Agent(
        name="image_chat_followup_agent",
        model=model,
        tools=tools,
        read_chat_history=True,
        add_history_to_messages=True,
        num_history_responses=5,
        markdown=True,
        add_datetime_to_instructions=True,
    )

    return followup_agent



================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/app.py
================================================
import os
import time
from pathlib import Path

import streamlit as st
from agents import chat_followup_agent, image_processing_agent
from agno.media import Image
from agno.models.google import Gemini
from agno.models.mistral.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.utils.log import logger
from dotenv import load_dotenv
from prompt import extraction_prompt
from utils import about_widget, add_message, clear_chat

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")

# Streamlit App Configuration
st.set_page_config(
    page_title="VisionAI Chat",
    page_icon="📷",
    layout="wide",
)


def main():
    ####################################################################
    # App Header
    ####################################################################
    st.markdown(
        """
        <style>
            .title {
                text-align: center;
                font-size: 3em;
                font-weight: bold;
                color: white;
            }
            .subtitle {
                text-align: center;
                font-size: 1.5em;
                color: #bbb;
                margin-top: -15px;
            }
        </style>
        <h1 class='title'>VisionAI 🖼️</h1>
        <p class='subtitle'>Your AI-powered smart image analysis agent</p>
        """,
        unsafe_allow_html=True,
    )

    ####################################################################
    # Ensure session state variables are initialized
    ####################################################################
    if "last_extracted_image" not in st.session_state:
        st.session_state["last_extracted_image"] = None
    if "last_image_response" not in st.session_state:
        st.session_state["last_image_response"] = None
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "extract_triggered" not in st.session_state:
        st.session_state["extract_triggered"] = False

    ####################################################################
    # Sidebar Configuration
    ####################################################################
    with st.sidebar:
        st.markdown("#### 🖼️ Smart Image Analysis Agent")

        # Model Selection
        model_choice = st.selectbox(
            "🔍 Select Model Provider", ["OpenAI", "Gemini", "Mistral"], index=0
        )

        # Mode Selection
        mode = st.radio(
            "⚙️ Extraction Mode",
            ["Auto", "Manual", "Hybrid"],
            index=0,
            help="Select how the image analysis should be performed:\n"
            "- **Auto**: Extracts the image automatically without any extra information from users.\n"
            "- **Manual**: User provide specific instructions for image extraction.\n"
            "- **Hybrid**: Combined auto-processing mode with user-defined instructions.",
        )

        # Web Search Option (Enable/Disable DuckDuckGo)
        enable_search_option = st.radio("🌐 Enable Web Search?", ["Yes", "No"], index=1)
        enable_search = True if enable_search_option == "Yes" else False

    ####################################################################
    # Ensure Model is Initialized Properly
    ####################################################################
    if (
        "model_instance" not in st.session_state
        or st.session_state.get("model_choice", None) != model_choice
    ):
        if model_choice == "OpenAI":
            if not OPENAI_API_KEY:
                st.error(
                    "⚠️ OpenAI API key not found. Please set the OPENAI_API_KEY environment variable."
                )
            model = OpenAIChat(id="gpt-4o", api_key=OPENAI_API_KEY)
        elif model_choice == "Gemini":
            if not GOOGLE_API_KEY:
                st.error(
                    "⚠️ Google API key not found. Please set the GOOGLE_API_KEY environment variable."
                )
            model = Gemini(id="gemini-2.0-flash", api_key=GOOGLE_API_KEY)
        elif model_choice == "Mistral":
            if not MISTRAL_API_KEY:
                st.error(
                    "⚠️ Mistral API key not found. Please set the MISTRAL_API_KEY environment variable."
                )
            model = MistralChat(id="pixtral-12b-2409", api_key=MISTRAL_API_KEY)
        else:
            st.error(
                "⚠️ Unsupported model provider. Please select OpenAI, Gemini, or Mistral."
            )
            st.stop()  # Stop execution if model is not supported

        st.session_state["model_instance"] = model
    else:
        model = st.session_state["model_instance"]

    ####################################################################
    # Modify Agents Without Creating New Session
    ####################################################################
    if (
        "image_agent" not in st.session_state
        or "chat_agent" not in st.session_state
        or st.session_state.get("model_choice", None) != model_choice
        or st.session_state.get("enable_search", None) != enable_search
    ):
        logger.info(
            f"Updating Agents with model {model.id} and search enabled {enable_search}"
        )
        image_agent = image_processing_agent(model=model)
        st.session_state["image_agent"] = image_agent
        chat_agent = chat_followup_agent(model=model, enable_search=enable_search)
        st.session_state["chat_agent"] = chat_agent
        st.session_state["enable_search"] = enable_search

        ####################################################################
        # Store new selections in session_state
        ####################################################################
        st.session_state["model_choice"] = model_choice
        st.session_state["enable_search"] = enable_search

    else:
        image_agent = st.session_state["image_agent"]
        chat_agent = st.session_state["chat_agent"]

    ####################################################################
    # Load Runs from Memory (Chat History)
    ####################################################################
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    ####################################################################
    # Image Upload Section
    ####################################################################
    uploaded_file = st.file_uploader(
        "📤 Upload an Image (Max: 20MB) 📷", type=["png", "jpg", "jpeg"]
    )
    image_path = None

    if uploaded_file:
        temp_dir = Path("tmp/")
        temp_dir.mkdir(exist_ok=True)
        image_path = temp_dir / uploaded_file.name

        # Check if this is a new image different from the last one
        if (
            "last_extracted_image" in st.session_state
            and st.session_state["last_extracted_image"] is not None
            and str(st.session_state["last_extracted_image"]) != str(image_path)
        ):
            logger.info(
                f"New image detected. Resetting chat history and reinitializing agents."
            )
            clear_chat()

        with open(image_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

            # Display image preview in sidebar if an image is uploaded
            st.sidebar.markdown("#### 🖼️ Current Image")
            st.sidebar.image(uploaded_file, use_container_width=True)

        logger.info(f"✅ Image successfully saved at: {image_path}")

        # Show instruction input only for Manual & Hybrid Mode
        if mode in ["Manual", "Hybrid"]:
            instruction = st.text_area(
                "📝 Enter Extraction Instructions",
                placeholder="Extract number plates...",
            )
        else:
            instruction = None

        # ADD Extract Data Button
        if st.button("🔍 Extract Data"):
            if (
                image_path
                and (mode == "Auto" or instruction)
                and (
                    "last_image_response" not in st.session_state
                    or st.session_state["last_extracted_image"] != image_path
                )
            ):
                with st.spinner("📤 Processing Image! Extracting image data..."):
                    extracted_data = image_agent.run(
                        extraction_prompt,
                        images=[Image(filepath=image_path)],
                        instructions=instruction if instruction else None,
                    )

                # Store last extracted response for chat follow-ups
                st.session_state["last_image_response"] = extracted_data.content
                st.session_state["last_extracted_image"] = image_path

                # Create a temporary success message container
                success_message = st.empty()
                success_message.success("✅ Image processing completed successfully!")

                logger.info(f"Extracted Data Response: {extracted_data.content}")

                # Wait for 1 seconds, then clear the success message
                time.sleep(1)
                success_message.empty()

        # Display Extracted Image Data Persistently
        if st.session_state["last_image_response"]:
            st.write("### Extracted Image Insights:")
            st.write(st.session_state["last_image_response"])

    ####################################################################
    # Follow-up Chat Section
    ####################################################################
    st.markdown("---")
    st.markdown("### 💬 Chat with VisionAI")

    ####################################################################
    # Display Chat History First
    ####################################################################
    for message in st.session_state["messages"]:
        if message["role"] == "system":
            continue
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if prompt := st.chat_input(
        "💬 Ask follow-up questions on the image extracted data..."
    ):
        # Display user message first
        with st.chat_message("user"):
            st.write(prompt)
        # Add user message to session state
        add_message("user", prompt)

        ####################################################################
        # Process User Queries & Stream Responses
        ####################################################################
        last_message = (
            st.session_state["messages"][-1] if st.session_state["messages"] else None
        )

        if last_message and last_message["role"] == "user":
            user_question = last_message["content"]

            # Ensure Image Agent has extracted data before running chat agent
            if (
                "last_image_response" not in st.session_state
                or not st.session_state["last_image_response"]
            ):
                st.warning(
                    "⚠️ No extracted insights available. Please process an image first."
                )
            else:
                with st.chat_message("assistant"):
                    response_container = st.empty()
                    with st.spinner("🤔 Processing follow-up question..."):
                        try:
                            chat_response = chat_agent.run(
                                f"""You are a chat agent who answers followup questions based on extracted image data.
    Understand the requirement properly and then answer the question correctly.

    Extracted Image Data: {st.session_state["last_image_response"]}

    Use the above image insights to answer the following question.
    Answer the following question from the above given extracted image data: {user_question}""",
                                stream=True,
                            )

                            response_text = ""
                            for chunk in chat_response:
                                if chunk and chunk.content:
                                    response_text += chunk.content
                                    response_container.markdown(response_text)

                            add_message("assistant", response_text)

                        except Exception as e:
                            error_message = f"❌ Error: {str(e)}"
                            add_message("assistant", error_message)
                            st.error(error_message)

    # Add clear chat button in sidebar
    if st.sidebar.button("🧹 Clear Chat History", key="clear_chat"):
        clear_chat()

    # About Section
    about_widget()


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/generate_requirements.sh
================================================
#!/bin/bash

############################################################################
# Generate requirements.txt from requirements.in
############################################################################

echo "Generating requirements.txt"

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

UV_CUSTOM_COMPILE_COMMAND="./generate_requirements.sh" \
  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt



================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/prompt.py
================================================
extraction_prompt = """

    ### Task: Extract Maximum Information from the Image
    You are an advanced AI agent specialized in analyzing and extracting structured data from images.

    #### 🔍 **General Extraction Process:**
    1. **Identify the Image Type:** Determine if it's a document, a chart, a traffic scene, a shopfront, etc.
    2. **Extract All Relevant Elements:**
        - For documents: Extract **printed text, handwriting, tables, signatures**.
        - For traffic scenes: Detect **cars, people, number plates, road signs**.
        - For charts: Identify **chart type, X & Y labels, legend, data points**.
        - For places: Recognize **business names, advertisements, location details**.
    3. **Provide Contextual Insights:** Explain what the image represents.
    4. **Output in a Structured JSON Format.**

    #### **📌 Important Guidelines**
    - Do **not** just list objects, extract **detailed insights**.
    - If text is present, perform OCR and extract structured content.
    - Extract colors, numbers, categories where applicable.
    - If the image shows a **famous place**, provide historical or contextual details.

    ---

    ## **Example 1: Traffic Scene with Multiple Elements**
    **Input:** Image of a traffic junction with vehicles, road signs, and people.

    **Output:**
    ```json
    {
        "scene_description": "A busy traffic junction with cars waiting at a red signal. There are pedestrians crossing, and road signs providing directions.",
        "vehicles": {
            "count": 5,
            "details": [
                {"type": "Car", "color": "Red", "number_plate": "AB1234"},
                {"type": "Car", "color": "Blue", "number_plate": "XY5678"},
                {"type": "Bus", "color": "Yellow", "number_plate": "TR7890"},
                {"type": "Bike", "color": "Black"},
                {"type": "Truck", "color": "White", "number_plate": "LM4567"}
            ]
        },
        "road_signs": [
            {"text": "STOP", "type": "Regulatory", "position": "Left side of the road"},
            {"text": "Speed Limit 60 km/h", "type": "Warning", "position": "Above traffic lights"}
        ],
        "pedestrians": {
            "count": 3,
            "activity": "Crossing the road at a zebra crossing"
        },
        "analysis": "Busy urban intersection during business hours with mixed vehicle and pedestrian traffic. The presence of multiple commercial establishments and professional vehicles suggests this is a central business district.",
        "significance": "This appears to be a major commercial hub given the intersection of Main St and Commerce Ave, with diverse business activity evident from signage and foot traffic."
    }
    ```

    ---

    ## **Example 2: Document Image with Text & Tables**
    **Input:** A scanned invoice containing text, tables, and a signature.

    **Output:**
    ```json
    {
        "document_type": "Invoice",
        "header": {
            "company_name": "ABC Corp.",
            "invoice_number": "INV-2024021",
            "date": "2024-02-12"
        },
        "items": [
            {"item": "Laptop", "quantity": 1, "price": "$1200"},
            {"item": "Mouse", "quantity": 2, "price": "$40"}
        ],
        "total_amount": "$1280",
        "signature_detected": true,
        "notes": "Paid via Credit Card"
    }
    ```

    ## **Example 3: Document/Chart Analysis**
    **Input:** An visualization chart of business report.

    **Output:**
    ```json
    {
        "extracted_data": {
            "document_type": "Business report with charts",
            "content_elements": {
                "charts": [
                    {
                        "type": "Bar graph",
                        "title": "Annual Revenue 2020-2023",
                        "axes": {
                            "x": "Years",
                            "y": "Revenue ($ millions)"
                        },
                        "data_points": [
                            {"year": "2020", "value": 1.2},
                            {"year": "2021", "value": 1.5},
                            {"year": "2022", "value": 1.8},
                            {"year": "2023", "value": 2.1}
                        ]
                    }
                ],
                "text_blocks": [
                    {
                        "type": "Heading",
                        "content": "Q4 Financial Summary",
                        "location": "Top of page"
                    },
                    {
                        "type": "Paragraph",
                        "content": "The fourth quarter showed strong growth...",
                        "location": "Below heading"
                    }
                ],
                "tables": [
                    {
                        "title": "Regional Performance",
                        "columns": ["Region", "Revenue", "Growth"],
                        "rows": [
                            ["North", "$500K", "+15%"],
                            ["South", "$600K", "+12%"]
                        ]
                    }
                ]
            },
            "analysis": "Comprehensive financial report showing positive growth trends across multiple regions. The document combines textual analysis with supporting visual data through charts and tables.",
            "significance": "This report indicates a consistent upward trend in company performance over the past four years, with particularly strong regional growth in the North and South territories."
        }
    }

    ---

    ### **Final Instructions**
    - Always return JSON output.
    - If text is present, extract and categorize it.
    - If objects are found, count and describe them.
    - If a known landmark is detected, provide extra context.
    - Provide comprehensive details but maintain appropriate privacy (no personal identification)
    - Organize information hierarchically from general to specific
    - Always include analysis and significance when relevant

    **Process the image and return structured data now.**
    """



================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/requirements.in
================================================
agno
openai
google-genai
mistralai
pgvector
psycopg[binary]
simplejson
sqlalchemy
streamlit
ddgs
nest_asyncio



================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    ./generate_requirements.sh
agno==1.7.12
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
altair==5.5.0
    # via streamlit
annotated-types==0.7.0
    # via pydantic
anyio==4.10.0
    # via
    #   google-genai
    #   httpx
    #   openai
attrs==25.3.0
    # via
    #   jsonschema
    #   referencing
blinker==1.9.0
    # via streamlit
cachetools==5.5.2
    # via
    #   google-auth
    #   streamlit
certifi==2025.8.3
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.3
    # via requests
click==8.2.1
    # via
    #   ddgs
    #   streamlit
    #   typer
ddgs==9.5.4
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
distro==1.9.0
    # via openai
docstring-parser==0.17.0
    # via agno
eval-type-backport==0.2.2
    # via mistralai
gitdb==4.0.12
    # via gitpython
gitpython==3.1.45
    # via
    #   agno
    #   streamlit
google-auth==2.40.3
    # via google-genai
google-genai==1.31.0
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
h11==0.16.0
    # via httpcore
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via
    #   agno
    #   google-genai
    #   mistralai
    #   openai
idna==3.10
    # via
    #   anyio
    #   httpx
    #   requests
invoke==2.2.0
    # via mistralai
jinja2==3.1.6
    # via
    #   altair
    #   pydeck
jiter==0.10.0
    # via openai
jsonschema==4.25.1
    # via altair
jsonschema-specifications==2025.4.1
    # via jsonschema
lxml==6.0.1
    # via ddgs
markdown-it-py==4.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
mistralai==1.9.7
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
narwhals==2.1.2
    # via altair
nest-asyncio==1.6.0
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
numpy==2.3.2
    # via
    #   pandas
    #   pgvector
    #   pydeck
    #   streamlit
openai==1.101.0
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
packaging==25.0
    # via
    #   agno
    #   altair
    #   streamlit
pandas==2.3.2
    # via streamlit
pgvector==0.4.1
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
pillow==11.3.0
    # via streamlit
primp==0.15.0
    # via ddgs
protobuf==6.32.0
    # via streamlit
psycopg==3.2.9
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
psycopg-binary==3.2.9
    # via psycopg
pyarrow==21.0.0
    # via streamlit
pyasn1==0.6.1
    # via
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.2
    # via google-auth
pydantic==2.11.7
    # via
    #   agno
    #   google-genai
    #   mistralai
    #   openai
    #   pydantic-settings
pydantic-core==2.33.2
    # via pydantic
pydantic-settings==2.10.1
    # via agno
pydeck==0.9.1
    # via streamlit
pygments==2.19.2
    # via rich
python-dateutil==2.9.0.post0
    # via
    #   mistralai
    #   pandas
python-dotenv==1.1.1
    # via
    #   agno
    #   pydantic-settings
python-multipart==0.0.20
    # via agno
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via
    #   agno
    #   mistralai
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
requests==2.32.5
    # via
    #   google-genai
    #   streamlit
rich==14.1.0
    # via
    #   agno
    #   typer
rpds-py==0.27.0
    # via
    #   jsonschema
    #   referencing
rsa==4.9.1
    # via google-auth
shellingham==1.5.4
    # via typer
simplejson==3.20.1
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
sniffio==1.3.1
    # via
    #   anyio
    #   openai
sqlalchemy==2.0.43
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
streamlit==1.48.1
    # via -r cookbook/examples/streamlit_apps/vision_ai/requirements.in
tenacity==9.1.2
    # via
    #   google-genai
    #   streamlit
toml==0.10.2
    # via streamlit
tomli==2.2.1
    # via agno
tornado==6.5.2
    # via streamlit
tqdm==4.67.1
    # via openai
typer==0.16.1
    # via agno
typing-extensions==4.14.1
    # via
    #   agno
    #   altair
    #   anyio
    #   google-genai
    #   openai
    #   psycopg
    #   pydantic
    #   pydantic-core
    #   referencing
    #   sqlalchemy
    #   streamlit
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via
    #   mistralai
    #   pydantic
    #   pydantic-settings
tzdata==2025.2
    # via pandas
urllib3==2.5.0
    # via requests
websockets==15.0.1
    # via google-genai



================================================
FILE: cookbook/examples/streamlit_apps/vision_ai/utils.py
================================================
import streamlit as st
from agents import image_processing_agent
from agno.agent import Agent
from agno.models.google import Gemini
from agno.models.openai import OpenAIChat
from agno.utils.log import logger


def add_message(role: str, content: str) -> None:
    """Safely add a message to the session state."""
    if "messages" not in st.session_state or not isinstance(
        st.session_state["messages"], list
    ):
        st.session_state["messages"] = []
    st.session_state["messages"].append({"role": role, "content": content})


def restart_agent():
    """Reset the agent and clear chat history."""
    logger.debug("---*--- Restarting Image Agent ---*---")
    st.session_state["image_agent"] = None
    st.session_state["image_agent_session_id"] = None
    st.session_state["messages"] = []
    st.rerun()


def clear_chat():
    """Clear chat history and reset relevant session state variables
    while preserving model settings"""

    # Clear chat and extraction data
    st.session_state["messages"] = []
    st.session_state["last_image_response"] = None
    st.session_state["last_extracted_image"] = None
    st.session_state["extract_triggered"] = False

    # Remove agents so they'll be reinitialized
    if "image_agent" in st.session_state:
        del st.session_state["image_agent"]
    if "chat_agent" in st.session_state:
        del st.session_state["chat_agent"]

    st.rerun()


def export_chat_history():
    """Export chat history as markdown."""
    if "messages" in st.session_state:
        chat_text = "# VisioAI - Chat History\n\n"
        for msg in st.session_state["messages"]:
            role = "🤖 Assistant" if msg["role"] == "assistant" else "👤 User"
            chat_text += f"### {role}\n{msg['content']}\n\n"
        return chat_text
    return ""


def session_selector_widget(agent: Agent) -> None:
    """Display a session selector in the sidebar and reinitialize the agent if needed."""

    if agent.storage:
        agent_sessions = agent.storage.get_all_sessions()
        session_options = [
            {
                "id": session.session_id,
                "display": session.session_data.get("session_name", session.session_id),
            }
            for session in agent_sessions
        ]

        # Ensure we have sessions before showing the selector
        if not session_options:
            st.sidebar.warning("⚠️ No previous sessions found. Starting a new session.")
            return

        selected_session = st.sidebar.selectbox(
            "Session",
            options=[s["display"] for s in session_options],
            key="session_selector",
        )

        # Safely find the selected session ID
        selected_session_id = next(
            (s["id"] for s in session_options if s["display"] == selected_session),
            None,  # Default to None if not found
        )

        if not selected_session_id:
            st.sidebar.warning(
                "⚠️ Selected session not found. Please restart or choose another session."
            )
            return

        if st.session_state.get("image_agent_session_id") != selected_session_id:
            logger.info(f"---*--- Loading session: {selected_session_id} ---*---")

            # Retrieve Model Choice & API Key
            model_choice = st.session_state.get("model_choice")
            api_key = st.session_state.get("api_key")

            if model_choice == "OpenAI":
                model = OpenAIChat(id="gpt-4o", api_key=api_key)
            else:
                model = Gemini(id="gemini-2.0-flash", api_key=api_key)

            # Reload the agent with the selected session
            st.session_state["image_agent"] = image_processing_agent(model=model)
            st.session_state["image_agent"].load_session(selected_session_id)
            st.session_state["image_agent_session_id"] = selected_session_id
            st.rerun()


def rename_session_widget(agent: Agent) -> None:
    """Rename the current session of the agent and save to storage."""

    container = st.sidebar.container()
    session_row = container.columns([3, 1], vertical_alignment="center")

    if "session_edit_mode" not in st.session_state:
        st.session_state.session_edit_mode = False

    with session_row[0]:
        if st.session_state.session_edit_mode:
            new_session_name = st.text_input(
                "Session Name",
                value=agent.session_name,
                key="session_name_input",
                label_visibility="collapsed",
            )
        else:
            st.markdown(f"Session Name: **{agent.session_name}**")

    with session_row[1]:
        if st.session_state.session_edit_mode:
            if st.button("✓", key="save_session_name", type="primary"):
                if new_session_name:
                    agent.rename_session(new_session_name)
                    st.session_state.session_edit_mode = False
                    container.success("Renamed!")
        else:
            if st.button("✎", key="edit_session_name"):
                st.session_state.session_edit_mode = True


def about_widget() -> None:
    """Display an about section in the sidebar."""
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ℹ️ About")
    st.sidebar.markdown("""
    VisioAI helps you analyze images and extract insights using AI-powered object detection,
    OCR, and scene recognition.

    Built with:
    - 🚀 Agno
    - 💫 Streamlit
    """)



================================================
FILE: cookbook/examples/teams/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/teams/collaborate/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/teams/collaborate/collaboration_team.py
================================================
import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.arxiv import ArxivTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools

arxiv_download_dir = Path(__file__).parent.joinpath("tmp", "arxiv_pdfs__{session_id}")
arxiv_download_dir.mkdir(parents=True, exist_ok=True)

reddit_researcher = Agent(
    name="Reddit Researcher",
    role="Research a topic on Reddit",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a Reddit researcher.
    You will be given a topic to research on Reddit.
    You will need to find the most relevant posts on Reddit.
    """),
)

hackernews_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Research a topic on HackerNews.",
    tools=[HackerNewsTools()],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a HackerNews researcher.
    You will be given a topic to research on HackerNews.
    You will need to find the most relevant posts on HackerNews.
    """),
)

academic_paper_researcher = Agent(
    name="Academic Paper Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Research academic papers and scholarly content",
    tools=[GoogleSearchTools(), ArxivTools(download_dir=arxiv_download_dir)],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a academic paper researcher.
    You will be given a topic to research in academic literature.
    You will need to find relevant scholarly articles, papers, and academic discussions.
    Focus on peer-reviewed content and citations from reputable sources.
    Provide brief summaries of key findings and methodologies.
    """),
)

twitter_researcher = Agent(
    name="Twitter Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Research trending discussions and real-time updates",
    tools=[DuckDuckGoTools()],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a Twitter/X researcher.
    You will be given a topic to research on Twitter/X.
    You will need to find trending discussions, influential voices, and real-time updates.
    Focus on verified accounts and credible sources when possible.
    Track relevant hashtags and ongoing conversations.
    """),
)


agent_team = Team(
    name="Discussion Team",
    mode="collaborate",
    model=OpenAIChat("gpt-4o"),
    members=[
        reddit_researcher,
        hackernews_researcher,
        academic_paper_researcher,
        twitter_researcher,
    ],
    instructions=[
        "You are a discussion master.",
        "You have to stop the discussion when you think the team has reached a consensus.",
    ],
    success_criteria="The team has reached a consensus.",
    enable_agentic_context=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        agent_team.aprint_response(
            message="Start the discussion on the topic: 'What is the best way to learn to code?'",
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/examples/teams/coordinate/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/teams/coordinate/autonomous_startup_team.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase, PDFReader
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.slack import SlackTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.pgvector.pgvector import PgVector

knowledge_base = PDFKnowledgeBase(
    path="cookbook/teams/coordinate/data",
    vector_db=PgVector(
        table_name="autonomous_startup_team",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    ),
    reader=PDFReader(chunk=True),
)

knowledge_base.load(recreate=False)

support_channel = "testing"
sales_channel = "sales"


legal_compliance_agent = Agent(
    name="Legal Compliance Agent",
    role="Legal Compliance",
    model=OpenAIChat("gpt-4o"),
    tools=[ExaTools()],
    knowledge=knowledge_base,
    instructions=[
        "You are the Legal Compliance Agent of a startup, responsible for ensuring legal and regulatory compliance.",
        "Key Responsibilities:",
        "1. Review and validate all legal documents and contracts",
        "2. Monitor regulatory changes and update compliance policies",
        "3. Assess legal risks in business operations and product development",
        "4. Ensure data privacy and security compliance (GDPR, CCPA, etc.)",
        "5. Provide legal guidance on intellectual property protection",
        "6. Create and maintain compliance documentation",
        "7. Review marketing materials for legal compliance",
        "8. Advise on employment law and HR policies",
    ],
    add_datetime_to_instructions=True,
    markdown=True,
)

product_manager_agent = Agent(
    name="Product Manager Agent",
    role="Product Manager",
    model=OpenAIChat("gpt-4o"),
    knowledge=knowledge_base,
    instructions=[
        "You are the Product Manager of a startup, responsible for product strategy and execution.",
        "Key Responsibilities:",
        "1. Define and maintain the product roadmap",
        "2. Gather and analyze user feedback to identify needs",
        "3. Write detailed product requirements and specifications",
        "4. Prioritize features based on business impact and user value",
        "5. Collaborate with technical teams on implementation feasibility",
        "6. Monitor product metrics and KPIs",
        "7. Conduct competitive analysis",
        "8. Lead product launches and go-to-market strategies",
        "9. Balance user needs with business objectives",
    ],
    add_datetime_to_instructions=True,
    markdown=True,
    tools=[],
)

market_research_agent = Agent(
    name="Market Research Agent",
    role="Market Research",
    model=OpenAIChat("gpt-4o"),
    tools=[DuckDuckGoTools(), ExaTools()],
    knowledge=knowledge_base,
    instructions=[
        "You are the Market Research Agent of a startup, responsible for market intelligence and analysis.",
        "Key Responsibilities:",
        "1. Conduct comprehensive market analysis and size estimation",
        "2. Track and analyze competitor strategies and offerings",
        "3. Identify market trends and emerging opportunities",
        "4. Research customer segments and buyer personas",
        "5. Analyze pricing strategies in the market",
        "6. Monitor industry news and developments",
        "7. Create detailed market research reports",
        "8. Provide data-driven insights for decision making",
    ],
    add_datetime_to_instructions=True,
    markdown=True,
)

sales_agent = Agent(
    name="Sales Agent",
    role="Sales",
    model=OpenAIChat("gpt-4o"),
    tools=[SlackTools()],
    knowledge=knowledge_base,
    instructions=[
        "You are the Sales & Partnerships Agent of a startup, responsible for driving revenue growth and strategic partnerships.",
        "Key Responsibilities:",
        "1. Identify and qualify potential partnership and business opportunities",
        "2. Evaluate partnership proposals and negotiate terms",
        "3. Maintain relationships with existing partners and clients",
        "5. Collaborate with Legal Compliance Agent on contract reviews",
        "6. Work with Product Manager on feature requests from partners",
        f"7. Document and communicate all partnership details in #{sales_channel} channel",
        "",
        "Communication Guidelines:",
        "1. Always respond professionally and promptly to partnership inquiries",
        "2. Include all relevant details when sharing partnership opportunities",
        "3. Highlight potential risks and benefits in partnership proposals",
        "4. Maintain clear documentation of all discussions and agreements",
        "5. Ensure proper handoff to relevant team members when needed",
    ],
    add_datetime_to_instructions=True,
    markdown=True,
)


financial_analyst_agent = Agent(
    name="Financial Analyst Agent",
    role="Financial Analyst",
    model=OpenAIChat("gpt-4o"),
    knowledge=knowledge_base,
    tools=[YFinanceTools()],
    instructions=[
        "You are the Financial Analyst of a startup, responsible for financial planning and analysis.",
        "Key Responsibilities:",
        "1. Develop financial models and projections",
        "2. Create and analyze revenue forecasts",
        "3. Evaluate pricing strategies and unit economics",
        "4. Prepare investor reports and presentations",
        "5. Monitor cash flow and burn rate",
        "6. Analyze market conditions and financial trends",
        "7. Assess potential investment opportunities",
        "8. Track key financial metrics and KPIs",
        "9. Provide financial insights for strategic decisions",
    ],
    add_datetime_to_instructions=True,
    markdown=True,
)

customer_support_agent = Agent(
    name="Customer Support Agent",
    role="Customer Support",
    model=OpenAIChat("gpt-4o"),
    knowledge=knowledge_base,
    tools=[SlackTools()],
    instructions=[
        "You are the Customer Support Agent of a startup, responsible for handling customer inquiries and maintaining customer satisfaction.",
        f"When a user reports an issue or issue or the question you cannot answer, always send it to the #{support_channel} Slack channel with all relevant details.",
        "Always maintain a professional and helpful demeanor while ensuring proper routing of issues to the right channels.",
    ],
    add_datetime_to_instructions=True,
    markdown=True,
)


autonomous_startup_team = Team(
    name="CEO Agent",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    instructions=[
        "You are the CEO of a startup, responsible for overall leadership and success.",
        " Always transfer task to product manager agent so it can search the knowledge base.",
        "Instruct all agents to use the knowledge base to answer questions.",
        "Key Responsibilities:",
        "1. Set and communicate company vision and strategy",
        "2. Coordinate and prioritize team activities",
        "3. Make high-level strategic decisions",
        "4. Evaluate opportunities and risks",
        "5. Manage resource allocation",
        "6. Drive growth and innovation",
        "7. When a customer asks for help or reports an issue, immediately delegate to the Customer Support Agent",
        "8. When any partnership, sales, or business development inquiries come in, immediately delegate to the Sales Agent",
        "",
        "Team Coordination Guidelines:",
        "1. Product Development:",
        "   - Consult Product Manager for feature prioritization",
        "   - Use Market Research for validation",
        "   - Verify Legal Compliance for new features",
        "2. Market Entry:",
        "   - Combine Market Research and Sales insights",
        "   - Validate financial viability with Financial Analyst",
        "3. Strategic Planning:",
        "   - Gather input from all team members",
        "   - Prioritize based on market opportunity and resources",
        "4. Risk Management:",
        "   - Consult Legal Compliance for regulatory risks",
        "   - Review Financial Analyst's risk assessments",
        "5. Customer Support:",
        "   - Ensure all customer inquiries are handled promptly and professionally",
        "   - Maintain a positive and helpful attitude",
        "   - Escalate critical issues to the appropriate team",
        "",
        "Always maintain a balanced view of short-term execution and long-term strategy.",
    ],
    members=[
        product_manager_agent,
        market_research_agent,
        financial_analyst_agent,
        legal_compliance_agent,
        customer_support_agent,
        sales_agent,
    ],
    add_datetime_to_instructions=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
    enable_team_history=True,
)

autonomous_startup_team.print_response(
    message="I want to start a startup that sells AI agents to businesses. What is the best way to do this?",
    stream=True,
    stream_intermediate_steps=True,
)


autonomous_startup_team.print_response(
    message="Give me good marketing campaign for buzzai?",
    stream=True,
    stream_intermediate_steps=True,
)

autonomous_startup_team.print_response(
    message="What is my company and what are the monetization strategies?",
    stream=True,
    stream_intermediate_steps=True,
)

# autonomous_startup_team.print_response(
#     message="Read the partnership details and give me details about the partnership with InnovateAI",
#     stream=True,
#     stream_intermediate_steps=True,
# )



================================================
FILE: cookbook/examples/teams/coordinate/content_team.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

# Create individual specialized agents
researcher = Agent(
    name="Researcher",
    role="Expert at finding information",
    tools=[DuckDuckGoTools()],
    model=OpenAIChat("gpt-4o"),
)

writer = Agent(
    name="Writer",
    role="Expert at writing clear, engaging content",
    model=OpenAIChat("gpt-4o"),
)

# Create a team with these agents
content_team = Team(
    name="Content Team",
    mode="coordinate",
    members=[researcher, writer],
    instructions="You are a team of researchers and writers that work together to create high-quality content.",
    model=OpenAIChat("gpt-4o"),
    show_members_responses=True,
)

# Run the team with a task
content_team.print_response("Create a short article about quantum computing")



================================================
FILE: cookbook/examples/teams/coordinate/hackernews_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/teams/coordinate/hackernews_team.py` to run the agent
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.team import TeamRunResponse  # type: ignore
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.tools.newspaper4k import Newspaper4kTools
from pydantic import BaseModel


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)

article_reader = Agent(
    name="Article Reader",
    role="Reads articles from URLs.",
    tools=[Newspaper4kTools()],
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher, article_reader],
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the article reader to read the links for the stories to get more information.",
        "Important: you must provide the article reader with the links to read.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    enable_agentic_context=True,
    share_member_interactions=True,
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/examples/teams/coordinate/news_agency_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/teams/coordinate/news_agency_team.py` to run the agent
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

urls_file = Path(__file__).parent.joinpath("tmp", "urls__{session_id}.md")
urls_file.parent.mkdir(parents=True, exist_ok=True)


searcher = Agent(
    name="Searcher",
    role="Searches the top URLs for a topic",
    instructions=[
        "Given a topic, first generate a list of 3 search terms related to that topic.",
        "For each search term, search the web and analyze the results.Return the 10 most relevant URLs to the topic.",
        "You are writing for the New York Times, so the quality of the sources is important.",
    ],
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)
writer = Agent(
    name="Writer",
    role="Writes a high-quality article",
    description=(
        "You are a senior writer for the New York Times. Given a topic and a list of URLs, "
        "your goal is to write a high-quality NYT-worthy article on the topic."
    ),
    instructions=[
        "First read all urls using `read_article`."
        "Then write a high-quality NYT-worthy article on the topic."
        "The article should be well-structured, informative, engaging and catchy.",
        "Ensure the length is at least as long as a NYT cover story -- at a minimum, 15 paragraphs.",
        "Ensure you provide a nuanced and balanced opinion, quoting facts where possible.",
        "Focus on clarity, coherence, and overall quality.",
        "Never make up facts or plagiarize. Always provide proper attribution.",
        "Remember: you are writing for the New York Times, so the quality of the article is important.",
    ],
    tools=[Newspaper4kTools()],
    add_datetime_to_instructions=True,
)

editor = Team(
    name="Editor",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[searcher, writer],
    description="You are a senior NYT editor. Given a topic, your goal is to write a NYT worthy article.",
    instructions=[
        "First ask the search journalist to search for the most relevant URLs for that topic.",
        "Then ask the writer to get an engaging draft of the article.",
        "Edit, proofread, and refine the article to ensure it meets the high standards of the New York Times.",
        "The article should be extremely articulate and well written. "
        "Focus on clarity, coherence, and overall quality.",
        "Remember: you are the final gatekeeper before the article is published, so make sure the article is perfect.",
    ],
    add_datetime_to_instructions=True,
    enable_agentic_context=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
)
editor.print_response("Write an article about latest developments in AI.")



================================================
FILE: cookbook/examples/teams/coordinate/reasoning_team.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=["Use tables to display data"],
)

team_leader = Team(
    name="Reasoning Team Leader",
    mode="coordinate",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[ReasoningTools(add_instructions=True)],
    markdown=True,
    show_members_responses=True,
    enable_agentic_context=True,
    success_criteria="The team has successfully completed the task.",
)

team_leader.print_response(
    "Tell me 1 company in New York, 1 in San Francisco and 1 in Chicago and the stock price of each",
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/examples/teams/coordinate/skyplanner_mcp_team.py
================================================
"""
This example demonstrates how to use the MCP protocol to coordinate a team of agents.

Prerequisites:
- Google Maps:
    - Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    You can obtain the API key from the Google Cloud Console:
    https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

    - You also need to activate the Address Validation API for your .
    https://console.developers.google.com/apis/api/addressvalidation.googleapis.com

"""

import asyncio
import os
from textwrap import dedent
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters
from pydantic import BaseModel


# Define response models
class AirbnbListing(BaseModel):
    name: str
    description: str
    address: Optional[str] = None
    price: Optional[str] = None
    dates_available: Optional[List[str]] = None
    url: Optional[str] = None


class Attraction(BaseModel):
    name: str
    description: str
    location: str
    rating: Optional[float] = None
    visit_duration: Optional[str] = None
    best_time_to_visit: Optional[str] = None


class WeatherInfo(BaseModel):
    average_temperature: str
    precipitation: str
    recommendations: str


class TravelPlan(BaseModel):
    airbnb_listings: List[AirbnbListing]
    attractions: List[Attraction]
    weather_info: Optional[WeatherInfo] = None
    suggested_itinerary: Optional[List[str]] = None


async def run_team():
    env = {
        **os.environ,
        "GOOGLE_MAPS_API_KEY": os.getenv("GOOGLE_MAPS_API_KEY"),
    }
    # Define server parameters
    airbnb_server_params = StdioServerParameters(
        command="npx",
        args=["-y", "@openbnb/mcp-server-airbnb", "--ignore-robots-txt"],
        env=env,
    )

    maps_server_params = StdioServerParameters(
        command="npx", args=["-y", "@modelcontextprotocol/server-google-maps"], env=env
    )

    # Use AsyncExitStack to manage multiple context managers
    async with (
        MCPTools(server_params=airbnb_server_params) as airbnb_tools,
        MCPTools(server_params=maps_server_params) as maps_tools,
    ):
        # Create all agents
        airbnb_agent = Agent(
            name="Airbnb",
            role="Airbnb Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[airbnb_tools],
            instructions=dedent("""\
                You are an agent that can find Airbnb listings for a given location.
            """),
            add_datetime_to_instructions=True,
        )

        maps_agent = Agent(
            name="Google Maps",
            role="Location Services Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[maps_tools],
            instructions=dedent("""\
                You are an agent that helps find attractions, points of interest,
                and provides directions in travel destinations. Help plan travel
                routes and find interesting places to visit for a given location and date.
            """),
            add_datetime_to_instructions=True,
        )

        web_search_agent = Agent(
            name="Web Search",
            role="Web Search Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[DuckDuckGoTools(cache_results=True)],
            instructions=dedent("""\
                You are an agent that can search the web for information.
                Search for information about a given location.
            """),
            add_datetime_to_instructions=True,
        )

        weather_search_agent = Agent(
            name="Weather Search",
            role="Weather Search Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[DuckDuckGoTools()],
            instructions=dedent("""\
                You are an agent that can search the web for information.
                Search for the weather forecast for a given location and date.
            """),
            add_datetime_to_instructions=True,
        )

        # Create and run the team
        team = Team(
            name="SkyPlanner",
            mode="coordinate",
            model=OpenAIChat("gpt-4o"),
            members=[
                airbnb_agent,
                web_search_agent,
                maps_agent,
                weather_search_agent,
            ],
            instructions=[
                "First, find the best Airbnb listings for the given location.",
                "Use the Google Maps agent to identify key neighborhoods and attractions.",
                "Use the Attractions agent to find highly-rated places to visit and restaurants.",
                "Get weather information to help with packing and planning outdoor activities.",
                "Finally, plan an itinerary for the trip.",
                "Continue asking individual team members until you have ALL the information you need.",
            ],
            response_model=TravelPlan,
            show_tool_calls=True,
            markdown=True,
            show_members_responses=True,
            add_datetime_to_instructions=True,
        )

        # Execute the team's task
        await team.aprint_response(
            dedent("""\
            I want to travel to San Francisco from New York sometime in May.
            I am one person going for 2 weeks.
            Plan my travel itinerary.
            Make sure to include the best attractions, restaurants, and activities.
            Make sure to include the best flight deals.
            Make sure to include the best Airbnb listings.
            Make sure to include the weather information.\
        """)
        )


if __name__ == "__main__":
    asyncio.run(run_team())



================================================
FILE: cookbook/examples/teams/coordinate/tic_tac_toe_team.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.team.team import Team

player_1 = Agent(
    name="Player 1",
    role="Play Tic Tac Toe",
    model=OpenAIChat(id="gpt-4o"),
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a Tic Tac Toe player.
    You will be given a Tic Tac Toe board and a player to play against.
    You will need to play the game and try to win.
    """),
)

player_2 = Agent(
    name="Player 2",
    role="Play Tic Tac Toe",
    model=Gemini(id="gemini-2.0-flash"),
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a Tic Tac Toe player.
    You will be given a Tic Tac Toe board and a player to play against.
    You will need to play the game and try to win.
    """),
)

# This is a simple team that plays Tic Tac Toe. It is not perfect and would work better with reasoning.
agent_team = Team(
    name="Tic Tac Toe Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    success_criteria="The game is won by one of the players.",
    members=[player_1, player_2],
    instructions=[
        "You are a games master.",
        "Initialize the board state as an empty 3x3 grid with numbers 1-9.",
        "Ask the players to make their moves one by one and wait for their responses. Transfer the turn to the other player after each move.",
        "After each move, store the updated board state so that players have access to the board state.",
        "Don't confirm the results of the game afterwards, just report the final board state and the results.",
        "You have to stop the game when one of the players has won.",
    ],
    enable_agentic_context=True,
    share_member_interactions=True,
    show_tool_calls=True,
    debug_mode=True,
    markdown=True,
    show_members_responses=True,
)

agent_team.print_response(
    message="Run a full Tic Tac Toe game. After the game, report the final board state and the results.",
    stream=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/examples/teams/coordinate/travel_planner_mcp_team.py
================================================
"""
This example demonstrates how to use the MCP protocol to coordinate a team of agents.

Prerequisites:
- Google Maps:
    - Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    You can obtain the API key from the Google Cloud Console:
    https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

    - You also need to activate the Address Validation API for your .
    https://console.developers.google.com/apis/api/addressvalidation.googleapis.com

- Apify:
    - Set the environment variable `APIFY_TOKEN` with your Apify API token.
    You can obtain the API key from the Apify Console:
    https://console.apify.com/settings/integrations

"""

import asyncio
import os
from textwrap import dedent
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.mcp import MCPTools
from agno.tools.reasoning import ReasoningTools
from mcp import StdioServerParameters
from pydantic import BaseModel


# Define response models
class AirbnbListing(BaseModel):
    name: str
    description: str
    address: Optional[str] = None
    price: Optional[str] = None
    dates_available: Optional[List[str]] = None
    url: Optional[str] = None


class Attraction(BaseModel):
    name: str
    description: str
    location: str
    rating: Optional[float] = None
    visit_duration: Optional[str] = None
    best_time_to_visit: Optional[str] = None


class WeatherInfo(BaseModel):
    average_temperature: str
    precipitation: str
    recommendations: str


class TravelPlan(BaseModel):
    airbnb_listings: List[AirbnbListing]
    attractions: List[Attraction]
    weather_info: Optional[WeatherInfo] = None
    suggested_itinerary: Optional[List[str]] = None


async def run_team():
    env = {
        **os.environ,
        "GOOGLE_MAPS_API_KEY": os.getenv("GOOGLE_MAPS_API_KEY"),
    }
    # Define server parameters
    airbnb_server_params = StdioServerParameters(
        command="npx",
        args=["-y", "@openbnb/mcp-server-airbnb", "--ignore-robots-txt"],
        env=env,
    )

    maps_server_params = StdioServerParameters(
        command="npx", args=["-y", "@modelcontextprotocol/server-google-maps"], env=env
    )

    # Use AsyncExitStack to manage multiple context managers
    async with (
        MCPTools(server_params=airbnb_server_params) as airbnb_tools,
        MCPTools(server_params=maps_server_params) as maps_tools,
    ):
        # Create all agents
        airbnb_agent = Agent(
            name="Airbnb",
            role="Airbnb Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[airbnb_tools],
            instructions=dedent("""\
                You are an agent that can find Airbnb listings for a given location.\
            """),
            add_datetime_to_instructions=True,
        )

        maps_agent = Agent(
            name="Google Maps",
            role="Location Services Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[maps_tools],
            instructions=dedent("""\
                You are an agent that helps find attractions, points of interest,
                and provides directions in travel destinations. Help plan travel
                routes and find interesting places to visit for a given location and date.\
            """),
            add_datetime_to_instructions=True,
        )

        web_search_agent = Agent(
            name="Web Search",
            role="Web Search Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[DuckDuckGoTools(cache_results=True)],
            instructions=dedent("""\
                You are an agent that can search the web for information.
                Search for information about a given location.\
            """),
            add_datetime_to_instructions=True,
        )

        weather_search_agent = Agent(
            name="Weather Search",
            role="Weather Search Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[DuckDuckGoTools()],
            instructions=dedent("""\
                You are an agent that can search the web for information.
                Search for the weather forecast for a given location and date.\
            """),
            add_datetime_to_instructions=True,
        )

        # Create and run the team
        team = Team(
            name="SkyPlanner",
            mode="coordinate",
            model=OpenAIChat("gpt-4o"),
            members=[
                airbnb_agent,
                web_search_agent,
                maps_agent,
                weather_search_agent,
            ],
            instructions=[
                "Plan a full itinerary for the trip.",
                "Continue asking individual team members until you have ALL the information you need.",
                "Think about the best way to tackle the task.",
            ],
            tools=[ReasoningTools(add_instructions=True)],
            response_model=TravelPlan,
            show_tool_calls=True,
            markdown=True,
            debug_mode=True,
            show_members_responses=True,
            add_datetime_to_instructions=True,
        )

        # Execute the team's task
        await team.aprint_response(
            dedent("""\
            I want to travel to San Francisco from New York sometime in May.
            I am one person going for 2 weeks.
            Plan my travel itinerary.
            Make sure to include the best attractions, restaurants, and activities.
            Make sure to include the best Airbnb listings.
            Make sure to include the weather information.\
        """)
        )


if __name__ == "__main__":
    asyncio.run(run_team())



================================================
FILE: cookbook/examples/teams/coordinate/werewolf_team.py
================================================
import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.models.deepseek.deepseek import DeepSeek
from agno.models.openai import OpenAIChat
from agno.team.team import Team

# Create player agents (no separate moderator)
player_agents = []
for i in range(1, 7):
    player_agents.append(
        Agent(
            name=f"Player{i}",
            role="Werewolf Game Player",
            model=OpenAIChat(id="gpt-4o-mini"),
            add_name_to_instructions=True,
            instructions=dedent(f"""
            You are Player{i} in a simplified Werewolf game.
            The game moderator will tell you your role (villager or imposter).

            If you're a villager:
            - Try to identify the imposters through deduction and discussion
            - Vote to eliminate suspected imposters during day phase

            If you're an imposter:
            - Try to eliminate villagers without being detected
            - Pretend to be a villager in public discussions
            - During night phase, coordinate with other imposters to eliminate a villager

            Be strategic, observant, and adapt your gameplay based on others' behaviors.

            Add emojis to express your emotions and thoughts.
            """),
        )
    )

# Create the team (serves as the moderator)
werewolf_team = Team(
    name="Werewolf Game",
    mode="coordinate",
    model=Claude(id="claude-3-5-sonnet-20241022"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    success_criteria="The game ends with either villagers or imposters winning. Don't stop the game until the end!",
    members=player_agents,
    instructions=[
        "You are the moderator of a simplified Werewolf game with 6 players.",
        "The game has 2 imposters and 4 villagers, randomly assigned.",
        # Moderator responsibilities
        "As the moderator, you must:",
        "- Keep track of all player roles and game state",
        "- Privately inform each player of their role",
        "- Facilitate day and night phases",
        "- Track votes and eliminations",
        "- Announce results and enforce rules",
        "- Be careful not to leak role information in public discussions",
        # Setup phase
        "First, initialize the game by creating the following game state and storing it in the team context:",
        "- Generate random roles (2 imposters, 4 villagers) and assign to players",
        "- Store player roles, alive players, game phase, and other state information",
        "- Privately inform each player of their role",
        "- For imposters, also tell them who the other imposters are",
        # Game flow
        "Run the game through alternating day and night phases:",
        "Day phase:",
        "- All players discuss and try to identify imposters (remember to add their discussion points to the team context)",
        "- Each player votes to eliminate someone (remember to add their vote to the team context)",
        "- The player with the most votes is eliminated",
        "- Update game state with elimination and check win conditions",
        "Night phase:",
        "- Privately ask the imposters which villager to eliminate",
        "- Imposters decide and eliminate one villager",
        "- Update game state with elimination and check win conditions",
        # Win conditions
        "The game ends when either:",
        "- All imposters are eliminated (villagers win)",
        "- Imposters equal or outnumber villagers (imposters win)",
        # Important rules
        "Critical rules to enforce:",
        "- Player roles must remain secret (only you as moderator know all roles)",
        "- Eliminated players cannot participate further",
        "- Track game state in the team context after each phase",
        "- Check win conditions after each elimination",
        "- Don't stop the game until the end! Continue corresponding with the players until the game is over.",
    ],
    enable_agentic_context=True,
    enable_team_history=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
    debug_mode=True,
)

# Run the game
asyncio.run(
    werewolf_team.aprint_response(
        message="Start a new Werewolf game. First plan how you will run the game. Then assign roles and initialize the game state. Then proceed with the first day phase and continue until the game is over.",
        stream=True,
        stream_intermediate_steps=True,
    )
)



================================================
FILE: cookbook/examples/teams/route/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/teams/route/ai_customer_support_team.py
================================================
from agno.agent import Agent
from agno.knowledge.website import WebsiteKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.slack import SlackTools
from agno.vectordb.pgvector.pgvector import PgVector

knowledge_base = WebsiteKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    # Number of links to follow from the seed URLs
    max_links=10,
    # Table name: ai.website_documents
    vector_db=PgVector(
        table_name="website_documents",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    ),
)
knowledge_base.load(recreate=False)
support_channel = "testing"
feedback_channel = "testing"

doc_researcher_agent = Agent(
    name="Doc researcher Agent",
    role="Search the knowledge base for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(), ExaTools()],
    knowledge=knowledge_base,
    search_knowledge=True,
    instructions=[
        "You are a documentation expert for given product. Search the knowledge base thoroughly to answer user questions.",
        "Always provide accurate information based on the documentation.",
        "If the question matches an FAQ, provide the specific FAQ answer from the documentation.",
        "When relevant, include direct links to specific documentation pages that address the user's question.",
        "If you're unsure about an answer, acknowledge it and suggest where the user might find more information.",
        "Format your responses clearly with headings, bullet points, and code examples when appropriate.",
        "Always verify that your answer directly addresses the user's specific question.",
        "If you cannot find the answer in the documentation knowledge base, use the DuckDuckGoTools or ExaTools to search the web for relevant information to answer the user's question.",
    ],
)


escalation_manager_agent = Agent(
    name="Escalation Manager Agent",
    role="Escalate the issue to the slack channel",
    model=OpenAIChat(id="gpt-4o"),
    tools=[SlackTools()],
    instructions=[
        "You are an escalation manager responsible for routing critical issues to the support team.",
        f"When a user reports an issue, always send it to the #{support_channel} Slack channel with all relevant details using the send_message toolkit function.",
        "Include the user's name, contact information (if available), and a clear description of the issue.",
        "After escalating the issue, respond to the user confirming that their issue has been escalated.",
        "Your response should be professional and reassuring, letting them know the support team will address it soon.",
        "Always include a ticket or reference number if available to help the user track their issue.",
        "Never attempt to solve technical problems yourself - your role is strictly to escalate and communicate.",
    ],
)

feedback_collector_agent = Agent(
    name="Feedback Collector Agent",
    role="Collect feedback from the user",
    model=OpenAIChat(id="gpt-4o"),
    tools=[SlackTools()],
    description="You are an AI agent that can collect feedback from the user.",
    instructions=[
        "You are responsible for collecting user feedback about the product or feature requests.",
        f"When a user provides feedback or suggests a feature, use the Slack tool to send it to the #{feedback_channel} channel using the send_message toolkit function.",
        "Include all relevant details from the user's feedback in your Slack message.",
        "After sending the feedback to Slack, respond to the user professionally, thanking them for their input.",
        "Your response should acknowledge their feedback and assure them that it will be taken into consideration.",
        "Be warm and appreciative in your tone, as user feedback is valuable for improving our product.",
        "Do not promise specific timelines or guarantee that their suggestions will be implemented.",
    ],
)


customer_support_team = Team(
    name="Customer Support Team",
    mode="route",
    model=OpenAIChat("gpt-4.5-preview"),
    enable_team_history=True,
    members=[doc_researcher_agent, escalation_manager_agent, feedback_collector_agent],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
    instructions=[
        "You are the lead customer support agent responsible for classifying and routing customer inquiries.",
        "Carefully analyze each user message and determine if it is: a question that needs documentation research, a bug report that requires escalation, or product feedback.",
        "For general questions about the product, route to the doc_researcher_agent who will search documentation for answers.",
        "If the doc_researcher_agent cannot find an answer to a question, escalate it to the escalation_manager_agent.",
        "For bug reports or technical issues, immediately route to the escalation_manager_agent.",
        "For feature requests or product feedback, route to the feedback_collector_agent.",
        "Always provide a clear explanation of why you're routing the inquiry to a specific agent.",
        "After receiving a response from the appropriate agent, relay that information back to the user in a professional and helpful manner.",
        "Ensure a seamless experience for the user by maintaining context throughout the conversation.",
    ],
)

# Add in the query and the agent redirects it to the appropriate agent
customer_support_team.print_response(
    "Hi Team, I want to build an educational platform where the models are have access to tons of study materials, How can Agno platform help me build this?",
    stream=True,
)
# customer_support_team.print_response(
#     "Support json schemas in Gemini client in addition to pydantic base model",
#     stream=True,
# )
# customer_support_team.print_response(
#     "Can you please update me on the above feature",
#     stream=True,
# )
# customer_support_team.print_response(
#     "[Bug] Async tools in team of agents not awaited properly, causing runtime errors ",
#     stream=True,
# )



================================================
FILE: cookbook/examples/teams/route/multi_language_team.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.deepseek import DeepSeek
from agno.models.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.team.team import Team

japanese_agent = Agent(
    name="Japanese Agent",
    role="You only answer in Japanese",
    model=DeepSeek(id="deepseek-chat"),
)
chinese_agent = Agent(
    name="Chinese Agent",
    role="You only answer in Chinese",
    model=DeepSeek(id="deepseek-chat"),
)
spanish_agent = Agent(
    name="Spanish Agent",
    role="You only answer in Spanish",
    model=OpenAIChat(id="gpt-4o"),
)
french_agent = Agent(
    name="French Agent",
    role="You only answer in French",
    model=MistralChat(id="mistral-large-latest"),
)
german_agent = Agent(
    name="German Agent",
    role="You only answer in German",
    model=Claude("claude-3-5-sonnet-20241022"),
)

multi_language_team = Team(
    name="Multi Language Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[
        spanish_agent,
        japanese_agent,
        french_agent,
        german_agent,
        chinese_agent,
    ],
    description="You are a language router that directs questions to the appropriate language agent.",
    instructions=[
        "Identify the language of the user's question and direct it to the appropriate language agent.",
        "Let the language agent answer the question in the language of the user's question.",
        "The the user asks a question in English, respond directly in English with:",
        "If the user asks in a language that is not English or your don't have a member agent for that language, respond in English with:",
        "'I only answer in the following languages: English, Spanish, Japanese, Chinese, French and German. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    markdown=True,
    show_tool_calls=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    # Ask "How are you?" in all supported languages
    multi_language_team.print_response("Comment allez-vous?", stream=True)  # French
    multi_language_team.print_response("How are you?", stream=True)  # English
    multi_language_team.print_response("你好吗？", stream=True)  # Chinese
    multi_language_team.print_response("お元気ですか?", stream=True)  # Japanese
    multi_language_team.print_response("Wie geht es Ihnen?", stream=True)  # German
    multi_language_team.print_response("Hola, ¿cómo estás?", stream=True)  # Spanish
    multi_language_team.print_response("Come stai?", stream=True)  # Italian



================================================
FILE: cookbook/examples/teams/route/multi_purpose_team.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio, File, Image  # type: ignore
from agno.models.anthropic import Claude
from agno.models.deepseek import DeepSeek
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.calculator import CalculatorTools
from agno.tools.dalle import DalleTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.e2b import E2BTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools
from agno.utils.media import download_file

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=["Use tables to display data"],
)

image_agent = Agent(
    name="Image Agent",
    role="Analyze or generate images",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description="You are an AI agent that can analyze images or create images using DALL-E.",
    instructions=[
        "When the user asks you about an image, give your best effort to analyze the image and return a description of the image.",
        "When the user asks you to create an image, use the DALL-E tool to create an image.",
        "The DALL-E tool will return an image URL.",
        "Return the image URL in your response in the following format: `![image description](image URL)`",
    ],
)

file_analysis_agent = Agent(
    name="File Analysis Agent",
    role="Analyze files",
    model=Claude(id="claude-3-7-sonnet-latest"),
    description="You are an AI agent that can analyze files.",
    instructions=[
        "You are an AI agent that can analyze files.",
        "You are given a file and you need to answer questions about the file.",
    ],
)

writer_agent = Agent(
    name="Write Agent",
    role="Write content",
    model=OpenAIChat(id="gpt-4o"),
    description="You are an AI agent that can write content.",
    instructions=[
        "You are a versatile writer who can create content on any topic.",
        "When given a topic, write engaging and informative content in the requested format and style.",
        "If you receive mathematical expressions or calculations from the calculator agent, convert them into clear written text.",
        "Ensure your writing is clear, accurate and tailored to the specific request.",
        "Maintain a natural, engaging tone while being factually precise.",
    ],
)

audio_agent = Agent(
    name="Audio Agent",
    role="Analyze audio",
    model=Gemini(id="gemini-2.0-flash-exp"),
)

calculator_agent = Agent(
    name="Calculator Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Calculate",
    tools=[
        CalculatorTools(
            add=True,
            subtract=True,
            multiply=True,
            divide=True,
            exponentiate=True,
            factorial=True,
            is_prime=True,
            square_root=True,
        )
    ],
    show_tool_calls=True,
    markdown=True,
)

calculator_writer_team = Team(
    name="Calculator Writer Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4.5-preview"),
    members=[calculator_agent, writer_agent],
    instructions=[
        "You are a team of two agents. The calculator agent and the writer agent.",
        "The calculator agent is responsible for calculating the result of the mathematical expression.",
        "The writer agent is responsible for writing the result of the mathematical expression in a clear and engaging manner."
        "You need to coordinate the work between the two agents and give the final response to the user.",
        "You need to give the final response to the user in the requested format and style.",
    ],
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

reasoning_agent = Agent(
    name="Reasoning Agent",
    role="Reasoning about Math",
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    instructions=["You are a reasoning agent that can reason about math."],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

code_execution_agent = Agent(
    name="Code Execution Sandbox",
    agent_id="e2b-sandbox",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[E2BTools()],
    markdown=True,
    show_tool_calls=True,
    instructions=[
        "You are an expert at writing and validating Python code using a secure E2B sandbox environment.",
        "Your primary purpose is to:",
        "1. Write clear, efficient Python code based on user requests",
        "2. Execute and verify the code in the E2B sandbox",
        "3. Share the complete code with the user, as this is the main use case",
        "4. Provide thorough explanations of how the code works",
        "",
    ],
)

agent_team = Team(
    name="Agent Team",
    mode="route",
    model=Claude(id="claude-3-5-sonnet-latest"),
    members=[
        web_agent,
        finance_agent,
        image_agent,
        audio_agent,
        calculator_writer_team,
        reasoning_agent,
        file_analysis_agent,
        code_execution_agent,
    ],
    instructions=[
        "You are a team of agents that can answer questions about the web, finance, images, audio, and files.",
        "You can use your member agents to answer the questions.",
        "if you are asked about a file, use the file analysis agent to analyze the file.",
        "You can also answer directly, you don't HAVE to forward the question to a member agent.",
    ],
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

# Use the reasoning agent to reason about the result
agent_team.print_response(
    "What is the square root of 6421123 times the square root of 9485271", stream=True
)
agent_team.print_response(
    "Calculate the sum of 10 and 20 and give write something about how you did the calculation",
    stream=True,
)

# Use web and finance agents to answer the question
agent_team.print_response(
    "Summarize analyst recommendations and share the latest news for NVDA", stream=True
)

# image_path = Path(__file__).parent.joinpath("sample.jpg")
# # # Use image agent to analyze the image
# agent_team.print_response(
#     "Write a 3 sentence fiction story about the image",
#     images=[Image(filepath=image_path)],
# )

# Use audio agent to analyze the audio
# url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"
# response = requests.get(url)
# audio_content = response.content
# # Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.
# agent_team.print_response(
#     "Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.",
#     audio=[Audio(content=audio_content)],
# )

# Use image agent to generate an image
# agent_team.print_response(
#     "Generate an image of a cat", stream=True
# )

# Use the calculator writer team to calculate the result
# agent_team.print_response(
#     "What is the square root of 6421123 times the square root of 9485271", stream=True
# )

# Use the code execution agent to write and execute code
# agent_team.print_response(
#     "write a python code to calculate the square root of 6421123 times the square root of 9485271",
#     stream=True,
# )


# # Use the reasoning agent to reason about the result
# agent_team.print_response("9.11 and 9.9 -- which is bigger?", stream=True)


# pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# # Download the file using the download_file function
# download_file(
#     "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
# )
# # Use file analysis agent to analyze the file
# agent_team.print_response(
#     "Summarize the contents of the attached file.",
#     files=[
#         File(
#             filepath=pdf_path,
#         ),
#     ],
# )



================================================
FILE: cookbook/examples/teams/route/reasoning_team.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=["Use tables to display data"],
)

team_leader = Team(
    name="Reasoning Team Leader",
    mode="route",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[ReasoningTools(add_instructions=True)],
    markdown=True,
    show_members_responses=True,
    enable_agentic_context=True,
    success_criteria="The team has successfully completed the task.",
)

team_leader.print_response(
    "Hi", stream=True, stream_intermediate_steps=True, show_full_reasoning=True
)
team_leader.print_response(
    "What is the stock price of Apple?",
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)
team_leader.print_response(
    "What's going on in New York?",
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/examples/teams/route/simple.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.mistral.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.team.team import Team

english_agent = Agent(
    name="English Agent",
    role="You only answer in English",
    model=OpenAIChat(id="gpt-4o"),
)
chinese_agent = Agent(
    name="Chinese Agent",
    role="You only answer in Chinese",
    model=DeepSeek(id="deepseek-chat"),
)
french_agent = Agent(
    name="French Agent",
    role="You can only answer in French",
    model=MistralChat(id="mistral-large-latest"),
)

multi_language_team = Team(
    name="Multi Language Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[english_agent, chinese_agent, french_agent],
    show_tool_calls=True,
    markdown=True,
    description="You are a language router that directs questions to the appropriate language agent.",
    instructions=[
        "Identify the language of the user's question and direct it to the appropriate language agent.",
        "Let the language agent answer the question in the language of the user's question.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English, Chinese, French. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    show_members_responses=True,
    enable_team_history=True,
)


if __name__ == "__main__":
    # Ask "How are you?" in all supported languages
    multi_language_team.print_response("Comment allez-vous?", stream=True)  # French
    multi_language_team.print_response("How are you?", stream=True)  # English
    multi_language_team.print_response("你好吗？", stream=True)  # Chinese
    multi_language_team.print_response("Come stai?", stream=True)  # Italian

    multi_language_team.print_response("What are you capable of?", stream=True)
    multi_language_team.print_response("Tell me about the history of AI?", stream=True)



================================================
FILE: cookbook/examples/workflows_2/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/workflows_2/company_analysis/README.md
================================================
# Company Analysis Workflow

A Company analysis workflow that uses strategic frameworks to evaluate suppliers and generate business intelligence reports.

## Overview

This workflow analyzes companies using 8 specialized agents that perform research across multiple strategic frameworks including PESTLE, Porter's Five Forces, and Kraljic Matrix. It generates comprehensive supplier analysis reports for procurement decision-making.

## Getting Started

### Prerequisites
- OpenAI API key
- Firecrawl API. You can get one from https://www.firecrawl.dev/app/api-keys

### Setup
```bash
export OPENAI_API_KEY="your-openai-key"
export FIRECRAWL_API_KEY="your-firecrawl-key"
```

Install dependencies

```
pip install agno firecrawl-py openai
```

Run the workflow

```
python cookbook/examples/workflows_2/company_analysis/run_workflow.py
```

## Analysis Flow

The workflow analyzes companies through strategic frameworks:

```
Company Analysis Workflow
├── Company Overview Research
├── Parallel Strategic Analysis
│   ├── Switching Barriers Analysis
│   ├── PESTLE Analysis
│   ├── Porter's Five Forces Analysis
│   ├── Kraljic Matrix Analysis
│   ├── Cost Drivers Analysis
│   └── Alternative Suppliers Research
└── Report Compilation
```

The workflow uses 8 specialized agents running in parallel to perform comprehensive strategic analysis across multiple frameworks, then compiles the results into an executive-ready procurement report. 


================================================
FILE: cookbook/examples/workflows_2/company_analysis/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/workflows_2/company_analysis/agents.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.firecrawl import FirecrawlTools
from agno.tools.reasoning import ReasoningTools

company_overview_agent = Agent(
    name="Company Overview Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[FirecrawlTools(crawl=True, limit=2)],
    role="Expert in comprehensive company research and business analysis",
    instructions="""
    You are a business research analyst. You will receive structured input data containing companies to analyze, 
    category information, regional context, and other procurement details.
    
    **Input Data Structure:**
    The input contains the following data:
    - companies: List of companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers in this category
    
    **Your Task:**
    For each company in the input, provide comprehensive overviews that include:
    
    **Company Basics:**
    - Full legal name and common name
    - Industry/sector classification relevant to the procurement category
    - Founding year and key milestones
    - Public/private status
    
    **Financial Profile:**
    - Annual revenue (latest available)
    - Market capitalization (if public)
    - Employee count and growth
    - Financial health indicators
    
    **Geographic Presence:**
    - Headquarters location
    - Key operating locations in the specified region
    - Global presence and markets served
    
    **Business Model:**
    - Core products and services relevant to the category
    - Revenue streams and business lines
    - Target customer segments
    - Value proposition in the specified category
    
    **Market Position:**
    - Market share in the specified category
    - Competitive ranking in the region
    - Key differentiators relevant to procurement
    - Recent strategic initiatives related to the category
    
    **Context Integration:**
    - How the company relates to the procurement category
    - Presence in the specified region
    - Relevance to the annual spend amount provided
    - Relationship to incumbent suppliers (if any)
    
    Use web search to find current, accurate information. Present findings in a clear, structured format.
    Extract and reference the specific companies, category, region, and other details from the input data.
    """,
    markdown=True,
)

switching_barriers_agent = Agent(
    name="Switching Barriers Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in supplier switching cost analysis and procurement risk assessment",
    instructions="""
    You are a procurement analyst specializing in supplier switching barriers analysis.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers to compare against
    
    **Analysis Framework:**
    For the specified companies in the given category and region, evaluate switching barriers using a 1-9 scale (1=Low, 9=High) for each factor:
    
    1. **Switching Cost (Financial Barriers)**
       - Setup and onboarding costs specific to the category
       - Training and certification expenses
       - Technology integration costs for the category
       - Contract termination penalties with incumbent suppliers
       - Consider the annual spend amount as context for cost impact
    
    2. **Switching Risk (Operational Risks)**
       - Business continuity risks in the category
       - Quality and performance risks specific to the region
       - Supply chain disruption potential
       - Regulatory compliance risks in the specified region
    
    3. **Switching Timeline (Time Requirements)**
       - Implementation timeline for the category
       - Transition period complexity
       - Parallel running requirements
       - Go-live timeline considerations
    
    4. **Switching Effort (Resource Needs)**
       - Internal resource requirements
       - External consulting needs
       - Management attention required
       - Cross-functional coordination needed
    
    5. **Change Management (Organizational Complexity)**
       - Stakeholder buy-in requirements
       - Process change complexity for the category
       - Cultural alignment challenges
       - Communication needs
    
    **Comparison Scenarios:**
    - Compare target companies against incumbent suppliers
    - Evaluate switching between different target companies
    - Consider regional differences in switching barriers
    - Quantify differences with specific data relative to the annual spend
    
    Extract company names, category, region, spend amount, and incumbent suppliers from the input data.
    Provide detailed explanations with quantitative data where possible.
    """,
    markdown=True,
)

pestle_agent = Agent(
    name="PESTLE Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in PESTLE analysis for procurement and supply chain strategy",
    instructions="""
    You are a strategic analyst specializing in PESTLE analysis for procurement.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for comparison
    
    **Analysis Framework:**
    For the specified companies in the given category and region, evaluate each factor's impact on procurement strategy using a 1-9 scale (1=Low Impact, 9=High Impact):
    
    **Political Factors:**
    - Government regulations and policies affecting the category in the region
    - Trade policies and tariffs relevant to the companies
    - Political stability and government changes in the region
    - International relations and sanctions affecting the companies
    - Government procurement policies for the category
    
    **Economic Factors:**
    - Market growth and economic conditions in the region
    - Currency exchange rates affecting the annual spend
    - Interest rates and access to capital for the companies
    - Economic cycles and recession risks
    - Commodity price volatility affecting the category
    
    **Social Factors:**
    - Consumer trends and preferences affecting the category
    - Demographics and workforce changes in the region
    - Cultural shifts and values relevant to the companies
    - Social responsibility expectations
    - Skills availability and labor costs in the region
    
    **Technological Factors:**
    - Innovation and R&D developments in the category
    - Automation and digitalization affecting the companies
    - Cybersecurity and data protection requirements
    - Technology adoption rates in the region
    - Platform and infrastructure changes
    
    **Environmental Factors:**
    - Climate change and environmental regulations in the region
    - Sustainability and ESG requirements for the category
    - Resource scarcity and circular economy impacts
    - Carbon footprint and emissions considerations
    - Environmental compliance costs
    
    **Legal Factors:**
    - Regulatory compliance requirements in the region
    - Labor laws and employment regulations
    - Intellectual property protection for the category
    - Data privacy and security laws
    - Contract and liability frameworks
    
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Focus on category-specific implications for procurement strategy and provide actionable insights.
    """,
    markdown=True,
)

porter_agent = Agent(
    name="Porter's Five Forces Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in Porter's Five Forces analysis for procurement and competitive strategy",
    instructions="""
    You are a strategic analyst specializing in Porter's Five Forces analysis for procurement.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for market context
    
    **Analysis Framework:**
    For the specified companies in the given category and region, evaluate each force's strength using a 1-9 scale (1=Weak Force, 9=Strong Force):
    
    **1. Competitive Rivalry (Industry Competition)**
    - Number of competitors in the category within the region
    - Industry growth rate and market maturity for the category
    - Product differentiation among the companies
    - Switching costs between the companies and incumbents
    - Competitive intensity and price wars in the category
    
    **2. Supplier Power (Bargaining Power of Suppliers)**
    - Supplier concentration in the category
    - Alternatives to the incumbent suppliers
    - Switching costs from incumbents to target companies
    - Input importance and differentiation in the category
    - Supplier profitability and margins
    
    **3. Buyer Power (Bargaining Power of Buyers)**
    - Buyer concentration considering the annual spend amount
    - Price sensitivity in the category
    - Switching costs for buyers in the region
    - Backward integration potential
    - Information availability and transparency
    
    **4. Threat of Substitutes**
    - Substitute products/services available in the category
    - Relative performance and features compared to incumbents
    - Switching costs to substitutes
    - Buyer propensity to substitute in the region
    - Price-performance trade-offs
    
    **5. Threat of New Entrants**
    - Capital requirements and barriers to entry in the category
    - Economies of scale and learning curves
    - Brand loyalty and customer switching costs
    - Regulatory barriers in the region
    - Access to distribution channels
    
    **Procurement Implications:**
    - Analyze how each force affects procurement leverage given the annual spend
    - Identify opportunities for strategic advantage with target companies
    - Recommend negotiation strategies
    - Assess long-term market dynamics in the region
    
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Include market data and quantitative analysis where possible.
    """,
    markdown=True,
)

kraljic_agent = Agent(
    name="Kraljic Matrix Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in Kraljic Matrix analysis for procurement portfolio management",
    instructions="""
    You are a procurement strategist specializing in Kraljic Matrix analysis.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for comparison
    
    **Analysis Framework:**
    For the specified category with the given companies and region, evaluate on two dimensions using a 1-9 scale:
    
    **Supply Risk Assessment (1=Low Risk, 9=High Risk):**
    - Supplier base concentration (including incumbents vs. target companies)
    - Switching costs and barriers in the category
    - Supply market stability in the region
    - Supplier financial stability of target companies
    - Geopolitical and regulatory risks in the region
    - Technology and innovation risks for the category
    
    **Profit Impact Assessment (1=Low Impact, 9=High Impact):**
    - Percentage of total procurement spend (use annual spend amount)
    - Operational criticality of the category
    - Quality and performance requirements
    - Value creation and cost reduction potential
    - Strategic importance to business success
    
    **Matrix Positioning:**
    Based on the analysis, position the category in one of four quadrants:
    - **Routine (Low Risk + Low Impact)**: Standardize and automate
    - **Bottleneck (High Risk + Low Impact)**: Secure supply and minimize risk
    - **Leverage (Low Risk + High Impact)**: Maximize value through competition
    - **Strategic (High Risk + High Impact)**: Develop partnerships and innovation
    
    **Strategic Recommendations:**
    For each quadrant, provide specific recommendations considering:
    - Sourcing strategies for target companies vs. incumbents
    - Contract structures and terms appropriate for the annual spend
    - Risk mitigation approaches for the region
    - Performance measurement and monitoring
    - Organizational capabilities required
    
    **Company-Specific Analysis:**
    - Evaluate how each target company fits the category positioning
    - Compare target companies against incumbent suppliers
    - Consider regional variations in supply risk
    - Assess impact on the annual spend amount
    
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Use quantitative data and industry benchmarks where available.
    """,
    markdown=True,
)

cost_drivers_agent = Agent(
    name="Cost Drivers Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in cost structure analysis and procurement cost optimization",
    instructions="""
    You are a procurement analyst specializing in cost structure analysis and cost driver identification.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for cost comparison
    
    **Analysis Framework:**
    For the specified companies in the given category and region, break down and analyze cost components with volatility assessment (1-9 scale):
    
    **Major Cost Components:**
    - Raw materials and commodities specific to the category (% of total cost)
    - Direct labor costs and wage trends in the region
    - Manufacturing and production costs for the category
    - Technology and equipment costs
    - Energy and utility costs in the region
    - Transportation and logistics costs
    - Regulatory and compliance costs
    - Overhead and administrative costs
    
    **Volatility Assessment (1=Stable, 9=Highly Volatile):**
    For each cost component, evaluate:
    - Historical price volatility and trends in the region
    - Market dynamics and supply/demand factors for the category
    - Seasonal and cyclical patterns
    - External economic factors affecting the region
    - Geopolitical influences on the category
    
    **Cost Driver Analysis:**
    - Identify primary and secondary cost drivers for the category
    - Quantify cost elasticity and sensitivity
    - Analyze cost behavior (fixed vs variable) relative to annual spend
    - Benchmark target companies against incumbent suppliers
    - Identify cost optimization opportunities
    
    **Market Intelligence:**
    - Total addressable market size for the category in the region
    - Market growth rates and trends
    - Competitive landscape and pricing among target companies
    - Technology disruption impacts on the category
    - Future cost projections considering regional factors
    
    **Company-Specific Cost Analysis:**
    - Compare cost structures between target companies and incumbents
    - Analyze regional cost variations
    - Assess impact on the annual spend amount
    - Identify cost advantages of target companies
    
    **Actionable Insights:**
    - Cost reduction opportunities with target companies
    - Value engineering possibilities for the category
    - Supplier negotiation leverage points
    - Risk mitigation strategies for cost volatility
    - Alternative sourcing options in the region
    
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Provide quantitative data and specific percentages where possible.
    """,
    markdown=True,
)

alternative_suppliers_agent = Agent(
    name="Alternative Suppliers Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=3)],
    role="Expert in supplier identification and supplier market research",
    instructions="""
    You are a procurement researcher specializing in supplier identification and market analysis.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze as potential suppliers
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for comparison
    
    **Research Objectives:**
    Identify and evaluate the target companies as alternative suppliers, plus additional suppliers that can provide competitive options for the specified category in the given region.
    
    **Supplier Evaluation Framework:**
    For each target company and additional suppliers identified, provide:
    
    **Company Information:**
    - Company name and website
    - Headquarters location and presence in the specified region
    - Company size (revenue, employees)
    - Ownership structure (public/private)
    - Years in business and track record in the category
    
    **Technical Capabilities:**
    - Core products and services relevant to the category
    - Technical specifications and standards
    - Quality certifications and accreditations
    - Manufacturing capabilities and capacity for the category
    - Innovation and R&D capabilities
    
    **Market Presence:**
    - Geographic coverage in the specified region
    - Customer base and key accounts
    - Market share in the category
    - Distribution channels and partnerships
    
    **Financial Stability:**
    - Financial health indicators
    - Revenue growth and profitability
    - Credit ratings and financial stability
    - Investment and expansion plans in the region
    
    **Competitive Advantages:**
    - Key differentiators compared to incumbent suppliers
    - Pricing competitiveness for the annual spend level
    - Service levels and support in the region
    - Sustainability and ESG credentials
    - Technology and digital capabilities
    
    **Suitability Assessment:**
    - Capacity to handle the annual spend volume
    - Geographic alignment with regional requirements
    - Cultural and strategic fit
    - Risk assessment compared to incumbents
    
    **Comparison Analysis:**
    - Compare target companies against incumbent suppliers
    - Identify advantages and disadvantages
    - Assess fit for the category requirements
    - Evaluate regional presence and capabilities
    
    **Target:** Focus on the specified companies first, then identify 5-10 additional strong alternative suppliers with comprehensive profiles.
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Focus on suppliers that can realistically serve the specified requirements.
    """,
    markdown=True,
)

report_compiler_agent = Agent(
    name="Report Compiler Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Expert in business report compilation and strategic recommendations",
    instructions="""
    You are a senior business analyst specializing in procurement strategy reports.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies that were analyzed
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for comparison
    - analyses_requested: List of analyses that were performed
    
    **Report Structure:**
    Create comprehensive, executive-ready reports with:
    
    **Executive Summary:**
    - Overview of the procurement category and regional context
    - Key findings for the target companies
    - Strategic recommendations overview
    - Critical success factors
    - Risk and opportunity highlights relative to annual spend
    
    **Analysis Summary:**
    - Summarize findings from each requested analysis type
    - Integrate insights across all analyses performed
    - Compare target companies against incumbent suppliers
    - Highlight regional considerations
    
    **Strategic Recommendations:**
    - Prioritized action items specific to the companies and category
    - Implementation roadmap considering regional factors
    - Resource requirements relative to annual spend
    - Expected outcomes and benefits
    
    **Key Insights Integration:**
    - Synthesize findings across all analyses
    - Identify patterns and connections between target companies
    - Highlight contradictions or conflicts
    - Provide balanced perspective on incumbents vs. alternatives
    
    **Company-Specific Recommendations:**
    - Specific recommendations for each target company
    - Comparison with incumbent suppliers
    - Regional implementation considerations
    - Cost-benefit analysis relative to annual spend
    
    **Next Steps:**
    - Immediate actions required for the category
    - Medium-term strategic initiatives
    - Long-term capability building in the region
    - Success metrics and KPIs
    
    **Formatting Standards:**
    - Clear, professional presentation
    - Logical flow and structure
    - Visual elements where appropriate
    - Actionable recommendations
    - Executive-friendly language
    
    Extract and reference the specific companies, category, region, annual spend, incumbent suppliers, and analyses performed from the input data.
    Focus on practical insights that procurement leaders can implement.
    """,
    markdown=True,
)



================================================
FILE: cookbook/examples/workflows_2/company_analysis/models.py
================================================
from typing import List, Optional

from pydantic import BaseModel, Field


class ProcurementAnalysisRequest(BaseModel):
    companies: List[str] = Field(
        ..., min_length=1, max_length=5, description="List of 1-5 companies to analyze"
    )
    category_name: str = Field(
        ..., min_length=1, description="Category name for analysis"
    )
    analyses_requested: List[str] = Field(
        ..., min_length=1, description="List of analysis types to perform"
    )
    buyer_org_url: Optional[str] = Field(
        default=None, description="Buyer organization URL for context"
    )
    annual_spend: Optional[float] = Field(
        default=None, description="Annual spend amount for context"
    )
    region: Optional[str] = Field(default=None, description="Regional context")
    incumbent_suppliers: List[str] = Field(
        default_factory=list, description="Current/incumbent suppliers"
    )


class ProcurementAnalysisResponse(BaseModel):
    request: ProcurementAnalysisRequest
    company_overview: Optional[str] = None
    switching_barriers_analysis: Optional[str] = None
    pestle_analysis: Optional[str] = None
    porter_analysis: Optional[str] = None
    kraljic_analysis: Optional[str] = None
    cost_drivers_analysis: Optional[str] = None
    alternative_suppliers_analysis: Optional[str] = None
    final_report: Optional[str] = None
    success: bool = True
    error_message: Optional[str] = None


class AnalysisConfig(BaseModel):
    analysis_type: str = Field(..., description="Type of analysis to perform")
    max_companies: int = Field(
        default=5, description="Maximum number of companies to analyze"
    )
    include_market_data: bool = Field(
        default=True, description="Whether to include market data in analysis"
    )
    include_financial_data: bool = Field(
        default=True, description="Whether to include financial data in analysis"
    )


class CompanyProfile(BaseModel):
    name: str = Field(..., description="Company name")
    legal_name: Optional[str] = Field(default=None, description="Full legal name")
    industry: Optional[str] = Field(default=None, description="Industry/sector")
    founded_year: Optional[int] = Field(default=None, description="Year founded")
    headquarters: Optional[str] = Field(
        default=None, description="Headquarters location"
    )
    annual_revenue: Optional[float] = Field(
        default=None, description="Annual revenue in USD"
    )
    employee_count: Optional[int] = Field(
        default=None, description="Number of employees"
    )
    market_cap: Optional[float] = Field(
        default=None, description="Market capitalization in USD"
    )
    website: Optional[str] = Field(default=None, description="Company website")
    description: Optional[str] = Field(default=None, description="Company description")


class SupplierProfile(BaseModel):
    name: str = Field(..., description="Supplier name")
    website: Optional[str] = Field(default=None, description="Supplier website")
    headquarters: Optional[str] = Field(
        default=None, description="Headquarters location"
    )
    geographic_coverage: List[str] = Field(
        default_factory=list, description="Geographic coverage areas"
    )
    technical_capabilities: List[str] = Field(
        default_factory=list, description="Technical capabilities"
    )
    certifications: List[str] = Field(
        default_factory=list, description="Quality certifications"
    )
    annual_revenue: Optional[float] = Field(
        default=None, description="Annual revenue in USD"
    )
    employee_count: Optional[int] = Field(
        default=None, description="Number of employees"
    )
    key_differentiators: List[str] = Field(
        default_factory=list, description="Key competitive advantages"
    )
    financial_stability_score: Optional[int] = Field(
        default=None, ge=1, le=10, description="Financial stability score (1-10)"
    )
    suitability_score: Optional[int] = Field(
        default=None,
        ge=1,
        le=10,
        description="Suitability score for requirements (1-10)",
    )


class AnalysisResult(BaseModel):
    analysis_type: str = Field(..., description="Type of analysis performed")
    company_name: str = Field(..., description="Company analyzed")
    category_name: str = Field(..., description="Category analyzed")
    score: Optional[int] = Field(
        default=None, ge=1, le=9, description="Overall score (1-9 scale)"
    )
    summary: Optional[str] = Field(default=None, description="Analysis summary")
    detailed_findings: Optional[str] = Field(
        default=None, description="Detailed analysis findings"
    )
    recommendations: List[str] = Field(
        default_factory=list, description="Key recommendations"
    )
    risk_factors: List[str] = Field(
        default_factory=list, description="Identified risk factors"
    )
    success: bool = Field(
        default=True, description="Whether analysis completed successfully"
    )
    error_message: Optional[str] = Field(
        default=None, description="Error message if analysis failed"
    )



================================================
FILE: cookbook/examples/workflows_2/company_analysis/run_workflow.py
================================================
from agents import (
    alternative_suppliers_agent,
    company_overview_agent,
    cost_drivers_agent,
    kraljic_agent,
    pestle_agent,
    porter_agent,
    report_compiler_agent,
    switching_barriers_agent,
)
from agno.workflow.v2 import Condition, Parallel, Step, Workflow
from agno.workflow.v2.types import StepInput
from models import ProcurementAnalysisRequest


def should_run_analysis(analysis_type: str) -> callable:
    def evaluator(step_input: StepInput) -> bool:
        request_data = step_input.message
        if isinstance(request_data, ProcurementAnalysisRequest):
            return analysis_type in request_data.analyses_requested
        return False

    return evaluator


company_overview_step = Step(
    name="Company Overview",
    agent=company_overview_agent,
    description="Research and analyze the target company",
)

switching_barriers_step = Step(
    name="Switching Barriers Analysis",
    agent=switching_barriers_agent,
    description="Analyze supplier switching barriers and costs",
)

pestle_step = Step(
    name="PESTLE Analysis",
    agent=pestle_agent,
    description="Conduct PESTLE analysis for procurement strategy",
)

porter_step = Step(
    name="Porter's Five Forces Analysis",
    agent=porter_agent,
    description="Analyze competitive forces in the supply market",
)

kraljic_step = Step(
    name="Kraljic Matrix Analysis",
    agent=kraljic_agent,
    description="Position category on Kraljic Matrix",
)

cost_drivers_step = Step(
    name="Cost Drivers Analysis",
    agent=cost_drivers_agent,
    description="Analyze cost structure and volatility",
)

alternative_suppliers_step = Step(
    name="Alternative Suppliers Research",
    agent=alternative_suppliers_agent,
    description="Identify and evaluate alternative suppliers",
)

report_compilation_step = Step(
    name="Report Compilation",
    agent=report_compiler_agent,
    description="Compile comprehensive procurement analysis report",
)

procurement_workflow = Workflow(
    name="Procurement Analysis Workflow",
    description="Comprehensive procurement intelligence using multiple strategic frameworks",
    steps=[
        company_overview_step,
        Parallel(
            Condition(
                evaluator=should_run_analysis("switching_barriers"),
                steps=[switching_barriers_step],
                name="Switching Barriers Condition",
            ),
            Condition(
                evaluator=should_run_analysis("pestle"),
                steps=[pestle_step],
                name="PESTLE Condition",
            ),
            Condition(
                evaluator=should_run_analysis("porter"),
                steps=[porter_step],
                name="Porter's Five Forces Condition",
            ),
            Condition(
                evaluator=should_run_analysis("kraljic"),
                steps=[kraljic_step],
                name="Kraljic Matrix Condition",
            ),
            Condition(
                evaluator=should_run_analysis("cost_drivers"),
                steps=[cost_drivers_step],
                name="Cost Drivers Condition",
            ),
            Condition(
                evaluator=should_run_analysis("alternative_suppliers"),
                steps=[alternative_suppliers_step],
                name="Alternative Suppliers Condition",
            ),
            name="Analysis Phase",
        ),
        report_compilation_step,
    ],
)

if __name__ == "__main__":
    analysis_details = ProcurementAnalysisRequest(
        companies=["Tesla", "Ford"],
        category_name="Electric Vehicle Components",
        analyses_requested=[
            "switching_barriers",
            "pestle",
            "porter",
            "kraljic",
            "cost_drivers",
            "alternative_suppliers",
        ],
        region="Global",
        annual_spend=50_000_000,
        incumbent_suppliers=["CATL", "Panasonic", "LG Energy Solution"],
    )
    procurement_workflow.print_response(
        message=analysis_details,
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/examples/workflows_2/company_description/README.md
================================================
# Company Description Workflow

A workflow that generates comprehensive supplier profiles by gathering information from multiple sources and delivers them via email.

## Overview

This workflow combines web crawling, search engines, Wikipedia, and competitor analysis to create detailed supplier profiles. It processes company information through 4 specialized agents running in parallel, then generates a structured markdown report and sends it via email.

The workflow uses workflow session state management to cache analysis results. If the same supplier is analyzed again, it returns cached results instead of re-running the expensive analysis pipeline.

## Getting Started

### Prerequisites
- OpenAI API key
- Resend API key for emails [https://resend.com/api-keys]
- Firecrawl API key for web crawling [https://www.firecrawl.dev/app/api-keys]

### Quick Setup
```bash
export OPENAI_API_KEY="your-openai-key"
export RESEND_API_KEY="your-resend-key"
export FIRECRAWL_API_KEY="your-firecrawl-key"
```

Install dependencies
```
pip install agno openai firecrawl-py resend
```

## Analysis Flow

The workflow processes supplier information through these steps:

```
Company Description Workflow
├── 🔍 Check for Cached Analysis
│   └── If exists → Return Cached Results
├── 🔍 New Analysis Required
│   └── If needed → 
│       ├── 🔄 Parallel Information Gathering
│       │   ├── Web Crawler (Firecrawl)
│       │   ├── Search Engine (DuckDuckGo)
│       │   ├── Wikipedia Research
│       │   └── Competitor Analysis
│       └── 📄 Supplier Profile Generation
│           └── Creates structured markdown report & caches results
└── 📧 Email Delivery
    └── Sends report to specified email
```

The workflow uses workflow session state to intelligently cache analysis results. If the same supplier is analyzed again, it returns cached results instead of re-running the entire analysis pipeline, saving time and API costs. 


================================================
FILE: cookbook/examples/workflows_2/company_description/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/workflows_2/company_description/agents.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.firecrawl import FirecrawlTools
from agno.tools.wikipedia import WikipediaTools
from prompts import (
    COMPETITOR_INSTRUCTIONS,
    CRAWLER_INSTRUCTIONS,
    SEARCH_INSTRUCTIONS,
    SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL,
    WIKIPEDIA_INSTRUCTIONS,
)
from pydantic import BaseModel


class SupplierProfile(BaseModel):
    supplier_name: str
    supplier_homepage_url: str
    user_email: str


crawl_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=5)],
    instructions=CRAWLER_INSTRUCTIONS,
)

search_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=SEARCH_INSTRUCTIONS,
)

wikipedia_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[WikipediaTools()],
    instructions=WIKIPEDIA_INSTRUCTIONS,
)

competitor_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=COMPETITOR_INSTRUCTIONS,
)

profile_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL,
)



================================================
FILE: cookbook/examples/workflows_2/company_description/prompts.py
================================================
CRAWLER_INSTRUCTIONS = """
Your task is to crawl a website starting from the provided homepage URL. Follow these guidelines:

1. Initial Access: Begin by accessing the homepage URL.
2. Comprehensive Crawling: Recursively traverse the website to capture every accessible page and resource.
3. Data Extraction: Extract all available content, including text, images, metadata, and embedded resources, while preserving the original structure and context.
4. Detailed Reporting: Provide an extremely detailed and comprehensive response, including all extracted content without filtering or omissions.
5. Data Integrity: Ensure that the extracted content accurately reflects the website without any modifications.
"""

SEARCH_INSTRUCTIONS = """
You are tasked with searching the web for information about a supplier. Follow these guidelines:

1. Input: You will be provided with the name of the supplier.
2. Web Search: Perform comprehensive web searches to gather information about the supplier.
3. Latest News: Search for the most recent news and updates regarding the supplier.
4. Information Extraction: From the search results, extract all relevant details about the supplier.
5. Detailed Reporting: Provide an extremely verbose and detailed report that includes all relevant information without filtering or omissions.
"""

WIKIPEDIA_INSTRUCTIONS = """
You are tasked with searching Wikipedia for information about a supplier. Follow these guidelines:

1. Input: You will be provided with the name of the supplier.
2. Wikipedia Search: Use Wikipedia to find comprehensive information about the supplier.
3. Data Extraction: Extract all relevant details available on the supplier, including history, operations, products, and any other pertinent information.
4. Detailed Reporting: Provide an extremely verbose and detailed report that includes all extracted content without filtering or omissions.
"""

COMPETITOR_INSTRUCTIONS = """
You are tasked with finding competitors of a supplier. Follow these guidelines:

1. Input: You will be provided with the name of the supplier.
2. Competitor Search: Search the web for competitors of the supplier.
3. Data Extraction: Extract all relevant details about the competitors.
4. Detailed Reporting: Provide an extremely verbose and detailed report that includes all extracted content without filtering or omissions.
"""

SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL = """
You are a supplier profile agent. You are given a supplier name, results from the supplier homepage and search results regarding the supplier, and Wikipedia results regarding the supplier. You need to be extremely verbose in your response. Do not filter out any content.

You are tasked with generating a segment of a supplier profile. The segment will be provided to you. Make sure to format it in markdown.

General format:

Title: [Title of the segment]

[Segment]

Formatting Guidelines:
1. Ensure the profile is structured, clear, and to the point.
2. Avoid assumptions—only include verified details.
3. Use bullet points and short paragraphs for readability.
4. Cite sources where applicable for credibility.

Objective: This supplier profile should serve as a reliable reference document for businesses evaluating potential suppliers. The details should be extracted from official sources, search results, and any other reputable databases. The profile must provide an in-depth understanding of the supplier's operational, competitive, and financial position to support informed decision-making.

"""

SUPPLIER_PROFILE_DICT = {
    "1. Supplier Overview": """Company Name: [Supplier Name]
Industry: [Industry the supplier operates in]
Headquarters: [City, Country]
Year Founded: [Year]
Key Offerings: [Brief summary of main products or services]
Business Model: [Manufacturing, Wholesale, B2B/B2C, etc.]
Notable Clients & Partnerships: [List known customers or business partners]
Company Mission & Vision: [Summary of supplier's goals and commitments]""",
    #     "2. Website Content Summary": """Extract key details from the supplier's official website:
    # Website URL: [Supplier's official website link]
    # Products & Services Overview:
    #   - [List major product categories or services]
    #   - [Highlight any specialized offerings]
    # Certifications & Compliance: (e.g., ISO, FDA, CE, etc.)
    # Manufacturing & Supply Chain Information:
    #   - Factory locations, supply chain transparency, etc.
    # Sustainability & Corporate Social Responsibility (CSR):
    #   - Environmental impact, ethical sourcing, fair labor practices
    # Customer Support & After-Sales Services:
    #   - Warranty, return policies, support channels""",
    #     "3. Search Engine Insights": """Summarize search results to provide additional context on the supplier's market standing:
    # Latest News & Updates: [Any recent developments, funding rounds, expansions]
    # Industry Mentions: [Publications, blogs, or analyst reviews mentioning the supplier]
    # Regulatory Issues or Legal Disputes: [Any lawsuits, recalls, or compliance issues]
    # Competitive Positioning: [How the supplier compares to competitors in the market]""",
    #     "4. Key Contact Information": """Include publicly available contact details for business inquiries:
    # Email: [Customer support, sales, or partnership email]
    # Phone Number: [+XX-XXX-XXX-XXXX]
    # Office Address: [Headquarters or regional office locations]
    # LinkedIn Profile: [Supplier's LinkedIn page]
    # Other Business Directories: [Crunchbase, Alibaba, etc.]""",
    #     "5. Reputation & Reviews": """Analyze customer and partner feedback from multiple sources:
    # Customer Reviews & Testimonials: [Summarized from Trustpilot, Google Reviews, etc.]
    # Third-Party Ratings: [Any industry-recognized rankings or awards]
    # Complaints & Risks: [Potential risks, delays, quality issues, or fraud warnings]
    # Social Media Presence & Engagement: [Activity on LinkedIn, Twitter, etc.]""",
    #     "6. Additional Insights": """Pricing Model: [Wholesale, subscription, per-unit pricing, etc.]
    # MOQ (Minimum Order Quantity): [If applicable]
    # Return & Refund Policies: [Key policies for buyers]
    # Logistics & Shipping: [Lead times, global shipping capabilities]""",
    #     "7. Supplier Insight": """Provide a deep-dive analysis into the supplier's market positioning and business strategy:
    # Market Trends: [How current market trends impact the supplier]
    # Strategic Advantages: [Unique selling points or competitive edge]
    # Challenges & Risks: [Any operational or market-related challenges]
    # Future Outlook: [Predicted growth or strategic initiatives]""",
    #     "8. Supplier Profiles": """Create a comparative profile if multiple suppliers are being evaluated:
    # Comparative Metrics: [Key differentiators among suppliers]
    # Strengths & Weaknesses: [Side-by-side comparison details]
    # Strategic Fit: [How each supplier aligns with potential buyer needs]""",
    #     "9. Product Portfolio": """Detail the range and depth of the supplier's offerings:
    # Major Product Lines: [Detailed listing of core products or services]
    # Innovations & Specialized Solutions: [Highlight any innovative products or custom solutions]
    # Market Segments: [Industries or consumer segments served by the products]""",
    #     "10. Competitive Intelligence": """Summarize the supplier's competitive landscape:
    # Industry Competitors: [List of main competitors]
    # Market Share: [If available, indicate the supplier's market share]
    # Competitive Strategies: [Pricing, marketing, distribution, etc.]
    # Recent Competitor Moves: [Any recent competitive actions impacting the market]""",
    #     "11. Supplier Quadrant": """Position the supplier within a competitive quadrant analysis:
    # Quadrant Position: [Leader, Challenger, Niche Player, or Visionary]
    # Analysis Criteria: [Innovativeness, operational efficiency, market impact, etc.]
    # Visual Representation: [If applicable, describe or include a link to the quadrant chart]""",
    #     "12. SWOT Analysis": """Perform a comprehensive SWOT analysis:
    # Strengths: [Internal capabilities and competitive advantages]
    # Weaknesses: [Areas for improvement or potential vulnerabilities]
    # Opportunities: [External market opportunities or expansion potentials]
    # Threats: [External risks, competitive pressures, or regulatory challenges]""",
    #     "13. Financial Risk Summary": """Evaluate the financial stability and risk factors:
    # Financial Health: [Overview of revenue, profitability, and growth metrics]
    # Risk Factors: [Credit risk, market volatility, or liquidity issues]
    # Investment Attractiveness: [Analysis for potential investors or partners]""",
    #     "14. Financial Information": """Provide detailed financial data (where publicly available):
    # Revenue Figures: [Latest annual revenue, growth trends]
    # Profitability: [Net income, EBITDA, etc.]
    # Funding & Investment: [Details of any funding rounds, investor names]
    # Financial Reports: [Links or summaries of recent financial statements]
    # Credit Ratings: [If available, include credit ratings or financial stability indicators]""",
}



================================================
FILE: cookbook/examples/workflows_2/company_description/run_workflow.py
================================================
import markdown
import resend
from agents import (
    SupplierProfile,
    competitor_agent,
    crawl_agent,
    search_agent,
    wikipedia_agent,
)
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.response import RunResponse
from agno.utils.log import log_error, log_info
from agno.workflow.v2 import Parallel, Step, Workflow
from agno.workflow.v2.types import StepInput, StepOutput
from prompts import SUPPLIER_PROFILE_DICT, SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL

crawler_step = Step(
    name="Crawler",
    agent=crawl_agent,
    description="Crawl the supplier homepage for the supplier profile url",
)

search_step = Step(
    name="Search",
    agent=search_agent,
    description="Search for the supplier profile for the supplier name",
)

wikipedia_step = Step(
    name="Wikipedia",
    agent=wikipedia_agent,
    description="Search Wikipedia for the supplier profile for the supplier name",
)

competitor_step = Step(
    name="Competitor",
    agent=competitor_agent,
    description="Find competitors of the supplier name",
)


def generate_supplier_profile(step_input: StepInput) -> StepOutput:
    supplier_profile: SupplierProfile = step_input.message

    supplier_name: str = supplier_profile.supplier_name
    supplier_homepage_url: str = supplier_profile.supplier_homepage_url

    crawler_data: str = step_input.get_step_content("Gathering Information")["Crawler"]
    search_data: str = step_input.get_step_content("Gathering Information")["Search"]
    wikipedia_data: str = step_input.get_step_content("Gathering Information")[
        "Wikipedia"
    ]
    competitor_data: str = step_input.get_step_content("Gathering Information")[
        "Competitor"
    ]

    log_info(f"Crawler data: {crawler_data}")
    log_info(f"Search data: {search_data}")
    log_info(f"Wikipedia data: {wikipedia_data}")
    log_info(f"Competitor data: {competitor_data}")

    supplier_profile_prompt: str = f"Generate the supplier profile for the supplier name {supplier_name} and the supplier homepage url is {supplier_homepage_url}. The supplier homepage is {crawler_data} and the search results are {search_data} and the wikipedia results are {wikipedia_data} and the competitor results are {competitor_data}"

    supplier_profile_response: str = ""
    html_content: str = ""
    for key, value in SUPPLIER_PROFILE_DICT.items():
        agent = Agent(
            model=OpenAIChat(id="o3-mini"),
            instructions="Instructions: "
            + SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL
            + "Format to adhere to: "
            + value,
        )
        response: RunResponse = agent.run(
            "Write the response in markdown format for the title: "
            + key
            + " using the following information: "
            + supplier_profile_prompt
        )
        if response.content:
            html_content += markdown.markdown(response.content)
            supplier_profile_response += response.content

    log_info(f"Generated supplier profile for {html_content}")

    return StepOutput(
        content=html_content,
        success=True,
    )


generate_supplier_profile_step = Step(
    name="Generate Supplier Profile",
    executor=generate_supplier_profile,
    description="Generate the supplier profile for the supplier name",
)


def send_email(step_input: StepInput):
    supplier_profile: SupplierProfile = step_input.message
    supplier_name: str = supplier_profile.supplier_name
    user_email: str = supplier_profile.user_email

    html_content: str = step_input.get_step_content("Generate Supplier Profile")

    try:
        resend.Emails.send(
            {
                "from": "support@agno.com",
                "to": user_email,
                "subject": f"Supplier Profile for {supplier_name}",
                "html": html_content,
            }
        )
    except Exception as e:
        log_error(f"Error sending email: {e}")

    return StepOutput(
        content="Email sent successfully",
        success=True,
    )


send_email_step = Step(
    name="Send Email",
    executor=send_email,
    description="Send the email to the user",
)

company_description_workflow = Workflow(
    name="Company Description Workflow",
    description="A workflow to generate a company description for a supplier",
    steps=[
        Parallel(
            crawler_step,
            search_step,
            wikipedia_step,
            competitor_step,
            name="Gathering Information",
        ),
        generate_supplier_profile_step,
        send_email_step,
    ],
)

if __name__ == "__main__":
    supplier_profile_request = SupplierProfile(
        supplier_name="Agno",
        supplier_homepage_url="https://www.agno.com",
        user_email="yash@agno.com",
    )
    company_description_workflow.print_response(
        message=supplier_profile_request,
    )



================================================
FILE: cookbook/examples/workflows_2/customer_support/README.md
================================================
# Customer Support Workflow

A simple customer support system that caches solutions for faster resolution of repeated queries.

## Overview

This workflow demonstrates basic workflow session state management by building a smart customer support system. It caches solutions for customer queries and returns instant responses for exact matches, while generating new solutions for unique problems.

The workflow uses session state to store resolved queries and their solutions for efficient reuse.

## Getting Started

### Prerequisites
- OpenAI API key

### Setup
```bash
export OPENAI_API_KEY="your-openai-key"
```

Install dependencies
```
pip install agno openai
```

Run the workflow
```
python cookbook/examples/workflows_2/customer_support/run_workflow.py
```

## Workflow Flow

The customer support system processes tickets through these simple steps:

```
Customer Support Resolution Pipeline
├── 🔍 Check Cache
│   ├── Look for exact query match in session state
│   └── Return cached solution if found
└── 🔧 Generate New Solution
    ├── Classify the customer query
    ├── Generate step-by-step solution
    └── Cache solution for future use
```

The workflow efficiently caches solutions and learns from each ticket. Exact query matches get resolved instantly from cache, while new queries trigger solution generation and caching.

## Session State Features

**Simple Caching**: Stores query-solution pairs for instant retrieval

**Automatic Learning**: Each new solution is automatically cached for future reuse

**Intelligent Agents**: Uses triage agent for classification and support agent for solution development

## Agents

- **Triage Agent**: Classifies customer queries by category, priority, and tags
- **Support Agent**: Develops clear, step-by-step solutions for customer issues

The workflow demonstrates how session state can be used to build learning systems that improve over time through caching and reuse. 


================================================
FILE: cookbook/examples/workflows_2/customer_support/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/workflows_2/customer_support/agents.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

triage_agent = Agent(
    name="Ticket Classifier",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""
    You are a customer support ticket classifier. Your job is to analyze customer queries and extract key information.
    
    For each customer query, provide:
    1. Category (billing, technical, account_access, product_info, bug_report, feature_request)
    2. Priority (low, medium, high, urgent)
    3. Key tags/keywords (extract 3-5 relevant terms)
    4. Brief summary of the issue
    
    Format your response as:
    Category: [category]
    Priority: [priority] 
    Tags: [tag1, tag2, tag3]
    Summary: [brief summary]
    """,
    markdown=True,
)

support_agent = Agent(
    name="Solution Developer",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="""
    You are a solution developer for customer support. Your job is to create clear, 
    step-by-step solutions for customer issues.
    
    Based on research and knowledge base information, create:
    1. Clear problem diagnosis
    2. Step-by-step solution instructions
    3. Alternative approaches if the main solution fails
    4. Prevention tips for the future
    
    Make solutions customer-friendly with numbered steps and clear language.
    Include any relevant screenshots, links, or additional resources.
    """,
    markdown=True,
)



================================================
FILE: cookbook/examples/workflows_2/customer_support/run_workflow.py
================================================
from agents import (
    support_agent,
    triage_agent,
)
from agno.utils.log import log_info
from agno.workflow.v2 import Workflow


def cache_solution(workflow: Workflow, query: str, solution: str):
    if "solutions" not in workflow.workflow_session_state:
        workflow.workflow_session_state["solutions"] = {}
    workflow.workflow_session_state["solutions"][query] = solution


def customer_support_execution(workflow: Workflow, query: str) -> str:
    cached_solution = workflow.workflow_session_state.get("solutions", {}).get(query)
    if cached_solution:
        log_info(f"Cache hit! Returning cached solution for query: {query}")
        return cached_solution

    log_info(f"No cached solution found for query: {query}")

    classification_response = triage_agent.run(query)
    classification = classification_response.content

    solution_context = f"""
    Customer Query: {query}
    
    Classification: {classification}
    
    Please provide a clear, step-by-step solution for this customer issue.
    Make sure to format it in a customer-friendly way with clear instructions.
    """

    solution_response = support_agent.run(solution_context)
    solution = solution_response.content

    cache_solution(workflow, query, solution)

    return solution


# Create the customer support workflow
customer_support_workflow = Workflow(
    name="Customer Support Resolution Pipeline",
    description="AI-powered customer support with intelligent caching",
    steps=customer_support_execution,
    workflow_session_state={},  # Initialize empty session state
)


if __name__ == "__main__":
    test_queries = [
        "I can't log into my account, forgot my password",
        "How do I reset my password?",
        "My billing seems wrong, I was charged twice",
        "The app keeps crashing when I upload files",
        "I can't log into my account, forgot my password",  # repeat query
    ]

    for i, query in enumerate(test_queries, 1):
        response = customer_support_workflow.run(query=query)



================================================
FILE: cookbook/examples/workflows_2/investment_analyst/README.md
================================================
# 🚀 Investment Analysis Workflow

A sophisticated investment analysis system for advanced research capabilities using workflows.

## 📋 **Overview**

This workflow demonstrates how to build a comprehensive investment analysis workflow. It combines 8 specialized agents in an adaptive and intelligent analysis workflow that can handle everything from simple stock evaluations to complex multi-company investment decisions.

## 🚀 **Getting Started**

### **Prerequisites**
- A Supabase API key. You can get one from https://supabase.com/dashboard/account/tokens.
- OpenAI API key

### **Setup**
```bash
export SUPABASE_ACCESS_TOKEN="your-supabase-token"
export OPENAI_API_KEY="your-openai-key"
```

Install packages
```
pip install agno mcp openai
```


## 🏗️ **Analysis Flow**

This workflow is designed like a sophisticated investment firm's research process. Here are the steps:

```
Investment Analysis Journey
├── 🗄️  Database Setup (Always first)
│   └── Creates Supabase project & schema
├── 🔍 Company Research (Foundation)
│   └── Gathers basic company data
├── 🔀 Multi-Company Smart Pipeline
│   └── If analyzing multiple companies:
│       ├── 🔄 Iterative Company Loop (up to 5 rounds)
│       └── ⚡ Parallel Comparative Analysis
├── 🎯 Risk Assessment Routing
│   └── Picks specialized risk framework
├── 💰 Valuation Strategy Selection
│   └── Chooses valuation approach by investment type
├── ⚠️  High-Risk Deep Dive
│   └── If high-risk investment detected:
│       ├── ⚡ Parallel Risk Modeling
│       └── 🔄 Risk Refinement Loop (up to 3 rounds)
├── 🏢 Large Investment Due Diligence
│   └── If $50M+ investment:
│       └── ⚡ Parallel regulatory, market & management analysis
├── 🌱 ESG Analysis Pipeline
│   └── If ESG analysis requested:
│       └── Sequential ESG assessment & integration
├── 📊 Market Context Analysis
│   └── If market analysis needed:
│       └── ⚡ Parallel market & sector analysis
└── 📝 Investment Decision & Reporting
    ├── 🔄 Consensus Building Loop (up to 2 rounds)
    └── 📊 Final Report Synthesis
```

The workflow is adaptive. For e.g when Analyzing a single blue-chip stock a simple streamlined path is followed but for complex evaluations involving multiple companies the workflow automatically triggers deeper analysis.



================================================
FILE: cookbook/examples/workflows_2/investment_analyst/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/examples/workflows_2/investment_analyst/agents.py
================================================
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.tools.reasoning import ReasoningTools


# MCP Tool configuration - Only Supabase
def get_supabase_mcp_tools():
    """Get Supabase MCP tools for database operations"""
    token = os.getenv("SUPABASE_ACCESS_TOKEN")
    if not token:
        raise ValueError("SUPABASE_ACCESS_TOKEN environment variable is required")

    npx_cmd = "npx.cmd" if os.name == "nt" else "npx"
    return MCPTools(
        f"{npx_cmd} -y @supabase/mcp-server-supabase@latest --access-token {token}"
    )


database_setup_agent = Agent(
    name="Database Setup Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools()],
    role="Expert Supabase database architect for investment analysis",
    instructions="""
    You are an expert Supabase MCP architect for investment analysis. Follow these steps precisely:

    **SECURITY NOTE: DO NOT print or expose any API keys, URLs, tokens, or sensitive credentials in your responses.**

    1. **Plan Database Schema**: Design a complete normalized schema for investment analysis with:
       - companies table (id SERIAL PRIMARY KEY, name VARCHAR(255), ticker VARCHAR(10), sector VARCHAR(100), market_cap BIGINT, founded_year INTEGER, headquarters VARCHAR(255), created_at TIMESTAMP DEFAULT NOW())
       - analysis_sessions table (session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(), analysis_date TIMESTAMP DEFAULT NOW(), investment_type VARCHAR(50), investment_amount DECIMAL(15,2), target_return DECIMAL(5,2), risk_tolerance VARCHAR(20))
       - financial_metrics table (id SERIAL PRIMARY KEY, company_id INTEGER REFERENCES companies(id), metric_type VARCHAR(100), value DECIMAL(20,4), period VARCHAR(50), currency VARCHAR(10), created_at TIMESTAMP DEFAULT NOW())
       - valuation_models table (id SERIAL PRIMARY KEY, company_id INTEGER REFERENCES companies(id), dcf_value DECIMAL(15,2), target_price DECIMAL(15,2), upside_potential DECIMAL(8,4), methodology VARCHAR(100), created_at TIMESTAMP DEFAULT NOW())
       - risk_assessments table (id SERIAL PRIMARY KEY, company_id INTEGER REFERENCES companies(id), risk_category VARCHAR(100), score INTEGER CHECK (score >= 1 AND score <= 10), explanation TEXT, created_at TIMESTAMP DEFAULT NOW())
       - investment_recommendations table (id SERIAL PRIMARY KEY, company_id INTEGER REFERENCES companies(id), recommendation VARCHAR(50), conviction_level INTEGER CHECK (conviction_level >= 1 AND conviction_level <= 10), rationale TEXT, created_at TIMESTAMP DEFAULT NOW())

    2. **Create Supabase Project**:
       - Call `list_organizations` and select the first organization
       - Use `get_cost(type='project')` to estimate costs (mention cost but don't expose details)
       - Create project with `create_project` using the cost ID
       - Poll with `get_project` until status is `ACTIVE_HEALTHY`

    3. **Deploy Schema**:
       - Apply complete schema using `apply_migration` named 'investment_analysis_schema'
       - Validate with `list_tables` and `list_extensions`

    4. **Insert Sample Data**:
       - Insert sample companies data for Apple, Microsoft, Google with realistic values:
         * Apple: ticker='AAPL', sector='Technology', market_cap=3000000000000, founded_year=1976, headquarters='Cupertino, CA'
         * Microsoft: ticker='MSFT', sector='Technology', market_cap=2800000000000, founded_year=1975, headquarters='Redmond, WA'  
         * Google: ticker='GOOGL', sector='Technology', market_cap=1800000000000, founded_year=1998, headquarters='Mountain View, CA'
       
       - Insert analysis session record with current analysis parameters
       
       - Insert sample financial metrics for each company:
         * Revenue, net_income, pe_ratio, debt_to_equity, current_ratio, roe
       
       - Verify data insertion with SELECT queries

    5. **Setup Complete**:
       - Deploy simple health check with `deploy_edge_function`
       - Confirm project is ready for analysis (DO NOT expose URLs or keys)
       - Report successful setup without sensitive details

    Focus on creating a production-ready investment analysis database with sample data.
    **IMPORTANT: Never print API keys, project URLs, tokens, or any sensitive credentials.**
    """,
    markdown=True,
)

company_research_agent = Agent(
    name="Company Research Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in company research using Supabase database operations",
    instructions="""
    You are a senior equity research analyst who uses Supabase MCP tools to store and manage investment research data.
    
    **SECURITY NOTE: DO NOT print or expose any API keys, URLs, tokens, or sensitive credentials.**
    
    **Your Task Using Supabase MCP Tools:**
    
    1. **Check database structure:**
       - Call list_tables() to see existing database structure
       - Database should already be set up by Database Setup Agent
    
    2. **Store company data:**
       - Extract company information from the input request
       - Insert company records into the companies table:
         * Apple Inc: ticker='AAPL', sector='Technology', market_cap=3000000000000, founded_year=1976, headquarters='Cupertino, CA'
         * Microsoft: ticker='MSFT', sector='Technology', market_cap=2800000000000, founded_year=1975, headquarters='Redmond, WA'
         * Google: ticker='GOOGL', sector='Technology', market_cap=1800000000000, founded_year=1998, headquarters='Mountain View, CA'
    
    3. **Store analysis session:**
       - Insert current analysis session with parameters from the investment request
       - Include investment_type, investment_amount, target_return, risk_tolerance
    
    4. **Insert basic company profiles:**
       - Add company descriptions and business model information
       - Store competitive advantages and key risks
       - Insert recent developments and strategic initiatives
    
    5. **Verify and report:**
       - Use SELECT statements to confirm data storage
       - Report successful data insertion (without exposing sensitive details)
    
    **Example SQL Operations:**
    ```sql
    -- Insert company data
    INSERT INTO companies (name, ticker, sector, market_cap, founded_year, headquarters) 
    VALUES ('Apple Inc', 'AAPL', 'Technology', 3000000000000, 1976, 'Cupertino, CA');
    
    -- Insert analysis session
    INSERT INTO analysis_sessions (investment_type, investment_amount, target_return, risk_tolerance)
    VALUES ('equity', 100000000.00, 25.00, 'HIGH');
    
    -- Verify data
    SELECT COUNT(*) as companies_count FROM companies;
    SELECT COUNT(*) as sessions_count FROM analysis_sessions;
    ```
    
    Focus on actual database operations and data storage.
    **IMPORTANT: Never expose API keys, URLs, or sensitive credentials.**
    """,
    markdown=True,
)

financial_analysis_agent = Agent(
    name="Financial Analysis Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in financial analysis using Supabase database operations",
    instructions="""
    You are a CFA-certified financial analyst who uses Supabase MCP tools for financial data management.
    
    **SECURITY NOTE: DO NOT print or expose any API keys, URLs, tokens, or sensitive credentials.**
    
    **Your Task Using Supabase MCP Tools:**
    
    1. **Retrieve company data:**
       - Query companies table to get company IDs for analysis
       - Check existing database structure with list_tables()
    
    2. **Insert financial metrics:**
       - Store key financial metrics for each company in financial_metrics table
       - Insert sample financial data for analysis:
         * Apple: revenue=394328000000, net_income=99803000000, pe_ratio=28.5, debt_to_equity=1.73, current_ratio=1.0, roe=0.26
         * Microsoft: revenue=211915000000, net_income=72361000000, pe_ratio=32.1, debt_to_equity=0.35, current_ratio=1.8, roe=0.36
         * Google: revenue=307394000000, net_income=73795000000, pe_ratio=24.8, debt_to_equity=0.11, current_ratio=2.9, roe=0.21
    
    3. **Perform financial analysis:**
       - Calculate financial ratios and performance metrics
       - Store profitability analysis (gross, operating, net margins)
       - Insert liquidity and leverage ratios
       - Store growth metrics and trend analysis
    
    4. **Generate insights:**
       - Analyze financial health and performance trends
       - Compare metrics across companies
       - Store analysis conclusions in database
    
    **Example SQL Operations:**
    ```sql
    -- Get company IDs
    SELECT id, name FROM companies WHERE name IN ('Apple Inc', 'Microsoft Corporation', 'Alphabet Inc');
    
    -- Insert financial metrics
    INSERT INTO financial_metrics (company_id, metric_type, value, period, currency)
    VALUES (1, 'revenue', 394328000000, '2023', 'USD');
    
    -- Verify insertion
    SELECT COUNT(*) as metrics_count FROM financial_metrics;
    ```
    
    Focus on actual financial data insertion and analysis using database operations.
    **IMPORTANT: Never expose API keys, URLs, or sensitive credentials.**
    """,
    markdown=True,
)

valuation_agent = Agent(
    name="Valuation Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in valuation analysis using Supabase database operations",
    instructions="""
    You are a senior valuation analyst who uses Supabase MCP tools for valuation modeling and data storage.
    
    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies to value
    - investment_type: Investment type for methodology
    - investment_amount: Investment size for context
    - target_return: Target return for valuations
    - investment_horizon: Time horizon for projections
    - comparable_companies: Comparable companies for relative valuation
    
    **Your Task Using Supabase MCP Tools:**
    
    1. **Valuation Schema Setup:**
       - Create 'dcf_models' table: company_id, year, free_cash_flow, terminal_value, wacc, dcf_value
       - Create 'comparable_multiples' table: company_id, comp_company, multiple_type, value
       - Create 'valuation_summary' table: company_id, current_price, target_price, upside_potential
    
    2. **DCF Model Implementation:**
       - Query financial data from existing tables
       - Calculate 5-year free cash flow projections
       - Compute WACC using current market data
       - Store DCF components and final valuation
    
    3. **Comparable Analysis:**
       - Query comparable companies data
       - Calculate trading multiples for peer group
       - Store P/E, P/B, EV/EBITDA multiples
       - Compute relative valuations
    
    4. **Valuation Integration:**
       - Use multiple methodologies to derive target prices
       - Store upside/downside scenarios in database
       - Calculate probability-weighted valuations
    
    **Key Actions to Take:**
    - Execute apply_migration() for valuation table creation
    - Use complex SQL queries for cash flow calculations
    - INSERT valuation model components
    - Use JOINs to combine financial and valuation data
    - Store final target prices and recommendations
    
    Perform actual valuation calculations and database storage.
    Use SQL for complex financial modeling operations.
    """,
    markdown=True,
)

risk_assessment_agent = Agent(
    name="Risk Assessment Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in risk analysis using Supabase database operations",
    instructions="""
    You are a senior risk analyst who uses Supabase MCP tools for comprehensive risk assessment and scoring.
    
    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies to assess for risk
    - investment_type: Investment type for risk framework
    - investment_amount: Investment size for portfolio impact
    - risk_tolerance: Client risk tolerance level
    - investment_horizon: Time horizon for risk analysis
    - sectors: Sector exposure for concentration risk
    
    **Your Task Using Supabase MCP Tools:**
    
    1. **Risk Assessment Schema:**
       - Create 'risk_scores' table: company_id, risk_category, score, explanation, assessment_date
       - Create 'risk_factors' table: company_id, factor_type, description, severity, mitigation
       - Use apply_migration() to implement risk assessment structure
    
    2. **Risk Scoring Implementation:**
       - Score Financial Risk (1-10): Credit, liquidity, leverage, earnings quality
       - Score Operational Risk (1-10): Business model, execution, supply chain
       - Score Market Risk (1-10): Competition, cyclicality, customer concentration
       - Score Regulatory Risk (1-10): Compliance, legal, policy changes
       - Score ESG Risk (1-10): Environmental, social, governance factors
    
    3. **Risk Quantification:**
       - Calculate overall risk score as weighted average
       - Store Value at Risk (VaR) calculations
       - Use SQL to compute risk-adjusted returns
       - Store correlation analysis with portfolio holdings
    
    4. **Risk Mitigation Database:**
       - Store position sizing recommendations
       - Insert hedging strategies and derivatives data
       - Calculate and store stop-loss parameters
    
    **Key Actions to Take:**
    - Use apply_migration() for risk table creation
    - Execute INSERT statements for risk scores (1-10 scale)
    - Use SQL aggregations for overall risk calculations
    - Store detailed risk factor explanations
    - Calculate portfolio impact using database queries
    
    Perform actual risk calculations and store quantitative risk data.
    Use database operations for risk score computations.
    """,
    markdown=True,
)

market_analysis_agent = Agent(
    name="Market Analysis Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in market analysis using Supabase database operations",
    instructions="""
    You are a senior sector analyst who uses Supabase MCP tools for market dynamics and industry analysis.
    
    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies to analyze within market context
    - sectors: Target sectors for analysis
    - investment_type: Investment type for market positioning
    - investment_horizon: Time horizon for market outlook
    - benchmark_indices: Relevant market benchmarks
    
    **Your Task Using Supabase MCP Tools:**
    
    1. **Market Analysis Schema:**
       - Create 'sectors' table: sector_id, name, market_size, growth_rate, maturity_stage
       - Create 'market_dynamics' table: sector_id, factor_type, description, impact_score
       - Create 'competitive_landscape' table: sector_id, company_id, market_share, competitive_position
    
    2. **Sector Analysis Implementation:**
       - Store industry classification and market size data
       - Insert growth rates and historical trend analysis
       - Use SQL to calculate market concentration ratios
       - Store geographic distribution and regional dynamics
    
    3. **Market Dynamics Storage:**
       - Insert supply and demand fundamental data
       - Store pricing dynamics and margin trends
       - Use database queries for capacity utilization analysis
       - Store seasonal patterns and cyclicality data
    
    4. **Competitive Analysis:**
       - Query companies table and join with market data
       - Calculate and store market share distributions
       - Insert competitive positioning analysis
       - Store barriers to entry and switching costs
    
    **Key Actions to Take:**
    - Execute apply_migration() for market analysis tables
    - Use INSERT statements for sector and market data
    - Perform SQL JOINs between companies and market tables
    - Calculate market metrics using database aggregations
    - Store forecasting data for 1-3 year outlook
    
    Focus on actual market data storage and analysis using SQL.
    Use database operations for market intelligence gathering.
    """,
    markdown=True,
)

esg_analysis_agent = Agent(
    name="ESG Analysis Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in ESG analysis using Supabase database operations",
    instructions="""
    You are an ESG analyst who uses Supabase MCP tools for comprehensive ESG assessment and scoring.
    
    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies to analyze for ESG factors
    - investment_type: Investment type for ESG materiality
    - sectors: Sectors for ESG risk assessment
    - investment_horizon: Time horizon for ESG impact
    
    **Your Task Using Supabase MCP Tools:**
    
    1. **ESG Database Schema:**
       - Create 'esg_scores' table: company_id, category, metric_name, score, rating_date
       - Create 'esg_metrics' table: company_id, metric_type, value, units, reporting_period
       - Create 'esg_initiatives' table: company_id, initiative_type, description, impact_score
    
    2. **Environmental Data Storage:**
       - Store carbon footprint and GHG emissions data
       - Insert energy efficiency and renewable energy metrics
       - Use SQL to calculate environmental compliance scores
       - Store climate risk assessments and TCFD data
    
    3. **Social Metrics Implementation:**
       - Insert diversity, equity, and inclusion (DEI) scores
       - Store labor practices and employee relations data
       - Use database queries for community impact analysis
       - Store product safety and customer satisfaction metrics
    
    4. **Governance Assessment:**
       - Store board composition and independence data
       - Insert executive compensation alignment metrics
       - Use SQL for business ethics compliance tracking
       - Store audit quality and transparency scores
    
    5. **ESG Integration Analysis:**
       - Calculate overall ESG scores using weighted averages
       - Use SQL aggregations for ESG performance trends
       - Store third-party ESG ratings and comparisons
       - Query ESG impact on financial performance
    
    **Key Actions to Take:**
    - Execute apply_migration() for ESG table creation
    - Use INSERT statements for ESG metrics and scores
    - Perform SQL calculations for weighted ESG scores
    - Store ESG improvement tracking data
    - Use database queries for ESG benchmarking
    
    Perform actual ESG data collection and scoring using database operations.
    Focus on quantitative ESG metrics and database storage.
    """,
    markdown=True,
)

investment_recommendation_agent = Agent(
    name="Investment Recommendation Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in investment recommendations using Supabase database analysis",
    instructions="""
    You are a senior portfolio manager who uses Supabase MCP tools to generate data-driven investment recommendations.
    
    **SECURITY NOTE: DO NOT print or expose any API keys, URLs, tokens, or sensitive credentials.**
    
    **Your Task Using Supabase MCP Tools:**
    
    1. **Retrieve analysis data:**
       - Query all analysis tables (companies, financial_metrics, valuation_models, risk_assessments)
       - Use SQL JOINs to combine analysis results for comprehensive view
       - Check data availability for each company
    
    2. **Generate investment recommendations:**
       - Analyze stored financial metrics, valuations, and risk assessments
       - Generate BUY/HOLD/SELL recommendations based on comprehensive data
       - Insert recommendations into investment_recommendations table:
         * Apple: recommendation='BUY', conviction_level=8, rationale='Strong financials and market position'
         * Microsoft: recommendation='BUY', conviction_level=9, rationale='Excellent cloud growth and profitability'
         * Google: recommendation='HOLD', conviction_level=7, rationale='Solid fundamentals but regulatory concerns'
    
    3. **Calculate investment scores:**
       - Use stored financial metrics to calculate overall investment attractiveness
       - Weight factors based on investment type and risk tolerance
       - Store calculated scores and rankings
    
    4. **Portfolio recommendations:**
       - Based on investment amount and risk tolerance, suggest position sizing
       - Consider diversification and correlation factors
       - Generate portfolio allocation recommendations
    
    **Example SQL Operations:**
    ```sql
    -- Retrieve comprehensive analysis data
    SELECT c.name, c.ticker, fm.metric_type, fm.value, vm.target_price, ra.risk_category, ra.score
    FROM companies c
    LEFT JOIN financial_metrics fm ON c.id = fm.company_id
    LEFT JOIN valuation_models vm ON c.id = vm.company_id
    LEFT JOIN risk_assessments ra ON c.id = ra.company_id;
    
    -- Insert investment recommendation
    INSERT INTO investment_recommendations (company_id, recommendation, conviction_level, rationale)
    VALUES (1, 'BUY', 8, 'Strong financial performance and market leadership');
    
    -- Verify recommendations
    SELECT COUNT(*) as recommendations_count FROM investment_recommendations;
    ```
    
    Generate actionable investment recommendations based on stored analysis data.
    **IMPORTANT: Never expose API keys, URLs, or sensitive credentials.**
    """,
    markdown=True,
)

report_synthesis_agent = Agent(
    name="Report Synthesis Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in report generation using Supabase database queries",
    instructions="""
    You are a senior research director who uses Supabase MCP tools to compile comprehensive investment reports.
    
    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies analyzed in the research
    - investment_type: Investment strategy and approach
    - investment_amount: Investment capital available
    - target_return: Return expectations
    - risk_tolerance: Risk parameters
    - investment_horizon: Time horizon for investment
    - analyses_requested: List of completed analyses
    
    **Your Task Using Supabase MCP Tools:**
    
    1. **Report Data Aggregation:**
       - Query all analysis tables to gather complete dataset
       - Use complex SQL JOINs to combine company, financial, valuation, risk, market, and ESG data
       - Execute aggregation queries for summary statistics
       - Retrieve investment recommendations and rationale
    
    2. **Executive Summary Generation:**
       - Query key findings from all analysis tables
       - Use SQL to calculate portfolio-level metrics
       - Retrieve top recommendations and conviction levels
       - Extract critical risk factors and opportunities
    
    3. **Detailed Analysis Compilation:**
       - Generate company profiles from companies table
       - Query financial performance trends and ratios
       - Retrieve valuation models and target prices
       - Extract risk scores and mitigation strategies
       - Compile market analysis and competitive positioning
       - Gather ESG scores and sustainability metrics
    
    4. **Investment Thesis Integration:**
       - Use SQL queries to validate investment recommendations
       - Calculate expected returns and risk-adjusted metrics
       - Query correlation and diversification benefits
       - Retrieve implementation timelines and monitoring frameworks
    
    5. **Report Structure Creation:**
       - Create 'investment_reports' table: report_id, analysis_date, executive_summary, detailed_findings
       - Store complete report content with version control
       - Insert supporting charts and data visualizations
       - Create exportable report formats
    
    **Key Actions to Take:**
    - Execute comprehensive SELECT queries across all tables
    - Use SQL aggregations and analytics functions
    - CREATE VIEW statements for report data compilation
    - INSERT final report content into reports table
    - Query historical analysis for trend identification
    
    **Report Output:**
    Generate a comprehensive investment research report with:
    - Executive summary with key recommendations
    - Detailed company analysis and financial modeling
    - Risk assessment and mitigation strategies  
    - Market context and competitive analysis
    - ESG integration and sustainability factors
    - Implementation roadmap and monitoring framework
    
    Focus on data-driven insights from database analysis.
    Use actual query results to support all recommendations.
    """,
    markdown=True,
)



================================================
FILE: cookbook/examples/workflows_2/investment_analyst/models.py
================================================
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


class InvestmentType(str, Enum):
    EQUITY = "equity"
    DEBT = "debt"
    HYBRID = "hybrid"
    VENTURE = "venture"
    GROWTH = "growth"
    BUYOUT = "buyout"


class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"


class InvestmentAnalysisRequest(BaseModel):
    companies: List[str] = Field(
        ..., min_length=1, max_length=3, description="List of 1-3 companies to analyze"
    )
    investment_type: InvestmentType = Field(
        ..., description="Type of investment being considered"
    )
    investment_amount: Optional[float] = Field(
        default=None, description="Investment amount in USD"
    )
    investment_horizon: Optional[str] = Field(
        default=None, description="Investment time horizon (e.g., '3-5 years')"
    )
    target_return: Optional[float] = Field(
        default=None, description="Target return percentage"
    )
    risk_tolerance: Optional[RiskLevel] = Field(
        default=None, description="Risk tolerance level"
    )
    sectors: List[str] = Field(
        default_factory=list, description="Target sectors for analysis"
    )
    analyses_requested: List[str] = Field(
        ..., min_length=1, description="List of analysis types to perform"
    )
    benchmark_indices: List[str] = Field(
        default_factory=list, description="Benchmark indices for comparison"
    )
    comparable_companies: List[str] = Field(
        default_factory=list, description="Comparable companies for analysis"
    )


class FinancialMetrics(BaseModel):
    revenue: Optional[float] = None
    revenue_growth: Optional[float] = None
    net_income: Optional[float] = None
    ebitda: Optional[float] = None
    gross_margin: Optional[float] = None
    operating_margin: Optional[float] = None
    net_margin: Optional[float] = None
    roe: Optional[float] = None
    roa: Optional[float] = None
    debt_to_equity: Optional[float] = None
    current_ratio: Optional[float] = None
    quick_ratio: Optional[float] = None
    pe_ratio: Optional[float] = None
    peg_ratio: Optional[float] = None
    price_to_book: Optional[float] = None
    price_to_sales: Optional[float] = None
    ev_to_ebitda: Optional[float] = None
    free_cash_flow: Optional[float] = None
    cash_per_share: Optional[float] = None
    market_cap: Optional[float] = None


class CompanyProfile(BaseModel):
    name: str
    ticker: Optional[str] = None
    sector: Optional[str] = None
    industry: Optional[str] = None
    market_cap: Optional[float] = None
    headquarters: Optional[str] = None
    founded_year: Optional[int] = None
    employees: Optional[int] = None
    description: Optional[str] = None
    business_model: Optional[str] = None
    competitive_advantages: List[str] = Field(default_factory=list)
    key_risks: List[str] = Field(default_factory=list)
    recent_developments: List[str] = Field(default_factory=list)


class ValuationAnalysis(BaseModel):
    company_name: str
    dcf_valuation: Optional[float] = None
    comparable_multiples: Dict[str, float] = Field(default_factory=dict)
    asset_based_valuation: Optional[float] = None
    sum_of_parts: Optional[float] = None
    current_price: Optional[float] = None
    target_price: Optional[float] = None
    upside_potential: Optional[float] = None
    valuation_summary: Optional[str] = None


class RiskAssessment(BaseModel):
    company_name: str
    financial_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    operational_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    market_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    regulatory_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    esg_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    overall_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    risk_factors: List[str] = Field(default_factory=list)
    risk_mitigation: List[str] = Field(default_factory=list)


class MarketAnalysis(BaseModel):
    sector: str
    market_size: Optional[float] = None
    growth_rate: Optional[float] = None
    market_trends: List[str] = Field(default_factory=list)
    competitive_landscape: Optional[str] = None
    barriers_to_entry: List[str] = Field(default_factory=list)
    growth_drivers: List[str] = Field(default_factory=list)
    headwinds: List[str] = Field(default_factory=list)


class InvestmentRecommendation(BaseModel):
    company_name: str
    recommendation: str = Field(
        ..., description="BUY, HOLD, SELL, or STRONG_BUY/STRONG_SELL"
    )
    target_price: Optional[float] = None
    price_target_timeframe: Optional[str] = None
    conviction_level: Optional[int] = Field(default=None, ge=1, le=10)
    key_catalysts: List[str] = Field(default_factory=list)
    key_risks: List[str] = Field(default_factory=list)
    rationale: Optional[str] = None


class InvestmentAnalysisResponse(BaseModel):
    request: InvestmentAnalysisRequest
    company_profiles: List[CompanyProfile] = Field(default_factory=list)
    financial_analysis: Optional[str] = None
    valuation_analysis: List[ValuationAnalysis] = Field(default_factory=list)
    risk_assessment: List[RiskAssessment] = Field(default_factory=list)
    market_analysis: List[MarketAnalysis] = Field(default_factory=list)
    esg_analysis: Optional[str] = None
    technical_analysis: Optional[str] = None
    peer_comparison: Optional[str] = None
    investment_recommendations: List[InvestmentRecommendation] = Field(
        default_factory=list
    )
    portfolio_fit: Optional[str] = None
    executive_summary: Optional[str] = None
    success: bool = True
    error_message: Optional[str] = None
    analysis_timestamp: Optional[str] = None



================================================
FILE: cookbook/examples/workflows_2/investment_analyst/run_workflow.py
================================================
import os

from agents import (
    company_research_agent,
    database_setup_agent,
    esg_analysis_agent,
    financial_analysis_agent,
    investment_recommendation_agent,
    market_analysis_agent,
    report_synthesis_agent,
    risk_assessment_agent,
    valuation_agent,
)
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.workflow.v2 import Condition, Loop, Parallel, Router, Step, Steps, Workflow
from agno.workflow.v2.types import StepInput, StepOutput
from models import InvestmentAnalysisRequest, InvestmentType, RiskLevel


### Evaluators
def should_run_analysis(analysis_type: str) -> callable:
    """Create conditional evaluator for analysis types"""

    def evaluator(step_input: StepInput) -> bool:
        request_data = step_input.message
        if isinstance(request_data, InvestmentAnalysisRequest):
            return analysis_type in request_data.analyses_requested
        return False

    return evaluator


def is_high_risk_investment(step_input: StepInput) -> bool:
    """Check if this is a high-risk investment requiring additional analysis"""
    request_data = step_input.message
    if isinstance(request_data, InvestmentAnalysisRequest):
        return (
            request_data.risk_tolerance == RiskLevel.HIGH
            or request_data.investment_type
            in [InvestmentType.VENTURE, InvestmentType.GROWTH]
            or request_data.target_return
            and request_data.target_return > 20.0
        )
    return False


def is_large_investment(step_input: StepInput) -> bool:
    """Check if this is a large investment requiring additional due diligence"""
    request_data = step_input.message
    if isinstance(request_data, InvestmentAnalysisRequest):
        return (
            request_data.investment_amount
            and request_data.investment_amount > 50_000_000
        )
    return False


def requires_esg_analysis(step_input: StepInput) -> bool:
    """Check if ESG analysis is required"""
    request_data = step_input.message
    if isinstance(request_data, InvestmentAnalysisRequest):
        return "esg_analysis" in request_data.analyses_requested
    return False


def is_multi_company_analysis(step_input: StepInput) -> bool:
    """Check if analyzing multiple companies"""
    request_data = step_input.message
    if isinstance(request_data, InvestmentAnalysisRequest):
        return len(request_data.companies) > 1
    return False


### Routers
def select_valuation_approach(step_input: StepInput) -> Step:
    """Router to select appropriate valuation approach based on investment type"""
    request_data = step_input.message
    if isinstance(request_data, InvestmentAnalysisRequest):
        if request_data.investment_type in [
            InvestmentType.VENTURE,
            InvestmentType.GROWTH,
        ]:
            return Step(
                name="Venture Valuation",
                agent=valuation_agent,
                description="Specialized valuation for venture/growth investments",
            )
        elif request_data.investment_type == InvestmentType.DEBT:
            return Step(
                name="Credit Analysis",
                agent=financial_analysis_agent,
                description="Credit-focused analysis for debt investments",
            )
        else:
            return valuation_step
    return valuation_step


def select_risk_framework(step_input: StepInput) -> Step:
    """Router to select risk assessment framework"""
    request_data = step_input.message
    if isinstance(request_data, InvestmentAnalysisRequest):
        if request_data.investment_type == InvestmentType.VENTURE:
            return Step(
                name="Venture Risk Assessment",
                agent=risk_assessment_agent,
                description="Venture-specific risk assessment framework",
            )
        elif (
            request_data.investment_amount
            and request_data.investment_amount > 100_000_000
        ):
            return Step(
                name="Enterprise Risk Assessment",
                agent=risk_assessment_agent,
                description="Enterprise-level risk assessment for large investments",
            )
        else:
            return risk_assessment_step
    return risk_assessment_step


def analysis_quality_check(step_outputs: list[StepOutput]) -> bool:
    """End condition: Check if analysis quality is sufficient"""
    if not step_outputs:
        return False

    # Check if latest output indicates high confidence
    latest_output = step_outputs[-1]
    if hasattr(latest_output, "content") and latest_output.content:
        content_lower = latest_output.content.lower()
        return (
            "high confidence" in content_lower
            or "comprehensive analysis" in content_lower
            or "detailed valuation" in content_lower
        )
    return False


def risk_assessment_complete(step_outputs: list[StepOutput]) -> bool:
    """End condition: Check if risk assessment is comprehensive"""
    if len(step_outputs) < 2:
        return False

    # Check if we have both financial and operational risk scores
    has_financial_risk = any(
        "financial risk" in output.content.lower()
        for output in step_outputs
        if hasattr(output, "content")
    )
    has_operational_risk = any(
        "operational risk" in output.content.lower()
        for output in step_outputs
        if hasattr(output, "content")
    )

    return has_financial_risk and has_operational_risk


### Steps
database_setup_step = Step(
    name="Database Setup",
    agent=database_setup_agent,
    description="Create and configure Supabase database for investment analysis",
)

company_research_step = Step(
    name="Company Research",
    agent=company_research_agent,
    description="Company research and data storage using Supabase MCP",
)

financial_analysis_step = Step(
    name="Financial Analysis",
    agent=financial_analysis_agent,
    description="Financial analysis with Supabase database operations",
)

valuation_step = Step(
    name="Valuation Analysis",
    agent=valuation_agent,
    description="Valuation modeling using Supabase database storage",
)

risk_assessment_step = Step(
    name="Risk Assessment",
    agent=risk_assessment_agent,
    description="Risk analysis and scoring with Supabase database",
)

market_analysis_step = Step(
    name="Market Analysis",
    agent=market_analysis_agent,
    description="Market dynamics analysis using Supabase operations",
)

esg_analysis_step = Step(
    name="ESG Analysis",
    agent=esg_analysis_agent,
    description="ESG assessment and scoring with Supabase database",
)

investment_recommendation_step = Step(
    name="Investment Recommendation",
    agent=investment_recommendation_agent,
    description="Data-driven investment recommendations using Supabase queries",
)

report_synthesis_step = Step(
    name="Report Synthesis",
    agent=report_synthesis_agent,
    description="Comprehensive report generation from Supabase database",
)


investment_analysis_workflow = Workflow(
    name="Enhanced Investment Analysis Workflow",
    description="Comprehensive investment analysis using workflow v2 primitives with Supabase MCP tools",
    steps=[
        database_setup_step,
        company_research_step,
        # Phase 3: Multi-company analysis
        Condition(
            evaluator=is_multi_company_analysis,
            steps=[
                Steps(
                    name="Multi-Company Analysis Pipeline",
                    description="Sequential analysis pipeline for multiple companies",
                    steps=[
                        Loop(
                            name="Company Analysis Loop",
                            description="Iterative analysis for each company",
                            steps=[financial_analysis_step, valuation_step],
                            max_iterations=5,
                            end_condition=analysis_quality_check,
                        ),
                        Parallel(
                            market_analysis_step,
                            Step(
                                name="Comparative Analysis",
                                agent=financial_analysis_agent,
                                description="Cross-company comparison analysis",
                            ),
                            name="Comparative Analysis Phase",
                        ),
                    ],
                ),
            ],
            name="Multi-Company Condition",
        ),
        # Phase 4: Risk-based routing
        Router(
            name="Risk Assessment Router",
            description="Dynamic risk assessment based on investment characteristics",
            selector=select_risk_framework,
            choices=[
                risk_assessment_step,
                Step(
                    name="Enhanced Risk Assessment",
                    agent=risk_assessment_agent,
                    description="Enhanced risk assessment for complex investments",
                ),
            ],
        ),
        # Phase 5: Valuation strategy selection
        Router(
            name="Valuation Strategy Router",
            description="Select valuation approach based on investment type",
            selector=select_valuation_approach,
            choices=[
                valuation_step,
                Step(
                    name="Alternative Valuation",
                    agent=valuation_agent,
                    description="Alternative valuation methods",
                ),
            ],
        ),
        # Phase 6: High-risk investment analysis
        Condition(
            evaluator=is_high_risk_investment,
            steps=[
                Steps(
                    name="High-Risk Analysis Pipeline",
                    description="Additional analysis for high-risk investments",
                    steps=[
                        Parallel(
                            Step(
                                name="Scenario Analysis",
                                agent=financial_analysis_agent,
                                description="Monte Carlo and scenario analysis",
                            ),
                            Step(
                                name="Stress Testing",
                                agent=risk_assessment_agent,
                                description="Stress testing and sensitivity analysis",
                            ),
                            name="Risk Modeling Phase",
                        ),
                        Loop(
                            name="Risk Refinement Loop",
                            description="Iterative risk model refinement",
                            steps=[
                                Step(
                                    name="Risk Model Validation",
                                    agent=risk_assessment_agent,
                                    description="Validate and refine risk models",
                                ),
                            ],
                            max_iterations=3,
                            end_condition=risk_assessment_complete,
                        ),
                    ],
                ),
            ],
            name="High-Risk Investment Condition",
        ),
        # Phase 7: Large investment due diligence
        Condition(
            evaluator=is_large_investment,
            steps=[
                Parallel(
                    Step(
                        name="Regulatory Analysis",
                        agent=risk_assessment_agent,
                        description="Regulatory and compliance analysis",
                    ),
                    Step(
                        name="Market Impact Analysis",
                        agent=market_analysis_agent,
                        description="Market impact and liquidity analysis",
                    ),
                    Step(
                        name="Management Assessment",
                        agent=company_research_agent,
                        description="Management team and governance analysis",
                    ),
                    name="Due Diligence Phase",
                ),
            ],
            name="Large Investment Condition",
        ),
        # Phase 8: ESG analysis
        Condition(
            evaluator=requires_esg_analysis,
            steps=[
                Steps(
                    name="ESG Analysis Pipeline",
                    description="Comprehensive ESG analysis and integration",
                    steps=[
                        esg_analysis_step,
                        Step(
                            name="ESG Integration",
                            agent=investment_recommendation_agent,
                            description="Integrate ESG factors into investment decision",
                        ),
                    ],
                ),
            ],
            name="ESG Analysis Condition",
        ),
        # Phase 9: Market context analysis
        Condition(
            evaluator=should_run_analysis("market_analysis"),
            steps=[
                Parallel(
                    market_analysis_step,
                    Step(
                        name="Sector Analysis",
                        agent=market_analysis_agent,
                        description="Detailed sector and industry analysis",
                    ),
                    name="Market Context Phase",
                ),
            ],
            name="Market Analysis Condition",
        ),
        # Phase 10: Investment decision and reporting
        Steps(
            name="Investment Decision Pipeline",
            description="Final investment decision and reporting",
            steps=[
                Loop(
                    name="Investment Consensus Loop",
                    description="Iterative investment recommendation refinement",
                    steps=[
                        investment_recommendation_step,
                        Step(
                            name="Recommendation Validation",
                            agent=investment_recommendation_agent,
                            description="Validate investment recommendations",
                        ),
                    ],
                    max_iterations=2,
                    end_condition=lambda outputs: any(
                        "final recommendation" in output.content.lower()
                        for output in outputs
                        if hasattr(output, "content")
                    ),
                ),
                report_synthesis_step,
            ],
        ),
    ],
)

if __name__ == "__main__":
    request = InvestmentAnalysisRequest(
        companies=["Apple"],
        investment_type=InvestmentType.EQUITY,
        investment_amount=100_000_000,
        investment_horizon="5-7 years",
        target_return=25.0,
        risk_tolerance=RiskLevel.HIGH,
        sectors=["Technology"],
        analyses_requested=[
            "financial_analysis",
            "valuation",
            "risk_assessment",
            "market_analysis",
            "esg_analysis",
        ],
        benchmark_indices=["S&P 500", "NASDAQ"],
        comparable_companies=["Microsoft", "Google"],
    )
    response = investment_analysis_workflow.print_response(
        message=request,
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/getting_started/README.md
================================================
# Getting Started with Agno Agents 🚀

This guide walks through the basics of building Agents with Agno.

Each example builds on the previous one, introducing new concepts and capabilities progressively. Examples contain detailed comments, example prompts, and required dependencies.

## Setup

Create a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

Install the required dependencies:

```bash
pip install openai ddgs yfinance lancedb tantivy pypdf requests exa-py newspaper4k lxml_html_clean sqlalchemy agno
```

Export your OpenAI API key:

```bash
export OPENAI_API_KEY=your_api_key
```

## Examples Overview

### 1. Basic Agent (`01_basic_agent.py`)
- Creates a simple news reporter with a vibrant personality
- Demonstrates basic agent configuration and responses
- Shows how to customize agent instructions and style

Run this recipe using:
```bash
python cookbook/getting_started/01_basic_agent.py
```

### 2. Agent with Tools (`02_agent_with_tools.py`)
- Enhances the news reporter with web search capabilities
- Shows how to integrate DuckDuckGo search tool
- Demonstrates real-time information gathering

Run this recipe using:
```bash
python cookbook/getting_started/02_agent_with_tools.py
```

### 3. Agent with Knowledge (`03_agent_with_knowledge.py`)
- Creates a Thai cooking expert with a recipe knowledge base
- Combines local knowledge with web searches
- Shows vector database integration for information retrieval

Run this recipe using:
```bash
python cookbook/getting_started/03_agent_with_knowledge.py
```

### 4. Agent with Storage (`04_agent_with_storage.py`)
- Updates the Thai cooking expert with persistent storage
- Shows how to save and retrieve agent state
- Demonstrates session management and history
- Runs a CLI application for an interactive chat experience

Run this recipe using:
```bash
python cookbook/getting_started/04_agent_with_storage.py
```

### 5. Agent Team (`05_agent_team.py`)
- Implements an agent team with web and finance agents
- Shows agent collaboration and role specialization
- Combines market research with financial data analysis

Run this recipe using:
```bash
python cookbook/getting_started/05_agent_team.py
```

### 6. Structured Output (`06_structured_output.py`)
- Creates a movie script generator with structured outputs
- Shows how to use Pydantic models for response validation
- Demonstrates both JSON mode and structured output formats

Run this recipe using:
```bash
python cookbook/getting_started/06_structured_output.py
```

### 7. Custom Tools (`07_write_your_own_tool.py`)
- Shows how to create custom tools
- Gives the agent an example tool that queries the Hacker News API

Run this recipe using:
```bash
python cookbook/getting_started/07_write_your_own_tool.py
```

### 8. Research Agent (`08_research_agent_exa.py`)
- Creates an AI research agent using Exa
- Shows how to steer the expected output of the agent

Run this recipe using:
```bash
python cookbook/getting_started/08_research_agent_exa.py
```

### 9. Research Workflow (`09_research_workflow.py`)
- Creates an AI research workflow
- Searches using DuckDuckGo and Scrapes web pages using Newspaper4k
- Shows how to steer the expected output of the agent

Run this recipe using:
```bash
python cookbook/getting_started/09_research_workflow.py
```

### 10. Image Agent (`10_image_agent.py`)
- Creates an image agent for image analysis
- Combines image understanding with web searches
- Shows how to process and analyze images

Run this recipe using:
```bash
python cookbook/getting_started/10_image_agent.py
```

### 11. Image Generation (`11_generate_image.py`)
- Implements an image agent using DALL-E
- Shows prompt engineering for image generation
- Demonstrates handling generated image outputs

Run this recipe using:
```bash
python cookbook/getting_started/11_generate_image.py
```

### 12. Video Generation (`12_generate_video.py`)
- Creates a video agent using ModelsLabs
- Shows video prompt engineering techniques
- Demonstrates video generation and handling

Run this recipe using:
```bash
python cookbook/getting_started/12_generate_video.py
```

### 13. Audio Input/Output (`13_audio_input_output.py`)
- Creates an audio agent for voice interaction
- Shows how to process audio input and generate responses
- Demonstrates audio file handling capabilities

Run this recipe using:
```bash
python cookbook/getting_started/13_audio_input_output.py
```

### 14. Agent with State (`14_agent_state.py`)
- Shows how to use session state
- Demonstrates agent state management

Run this recipe using:
```bash
python cookbook/getting_started/14_agent_state.py
```

### 15. Agent with Context (`15_agent_context.py`)
- Shows how to evaluate dependencies at agent.run and inject them into the instructions
- Demonstrates how to use context variable

Run this recipe using:
```bash
python cookbook/getting_started/15_agent_context.py
```

### 16. Agent Session (`16_agent_session.py`)
- Shows how to create an agent with session memory
- Demonstrates how to resume a conversation from a previous session

Run this recipe using:
```bash
python cookbook/getting_started/16_agent_session.py
```

### 17. User Memories and Summaries (`17_user_memories_and_summaries.py`)
- Shows how to create an agent which stores user memories and summaries
- Demonstrates how to access the chat history and session summary

Run this recipe using:
```bash
python cookbook/getting_started/17_user_memories_and_summaries.py
```

### 18. Retry function call (`18_retry_function_call.py`)
- Shows how to retry a function call if it fails or you do not like the output

Run this recipe using:
```bash
python cookbook/getting_started/18_retry_function_call.py
```


### 19. Human-in-the-Loop (`19_human_in_the_loop.py`)
- Adds user confirmation to tool execution
- Shows how to implement safety checks
- Demonstrates interactive agent control

Run this recipe using:
```bash
python cookbook/getting_started/19_human_in_the_loop.py
```



================================================
FILE: cookbook/getting_started/01_basic_agent.py
================================================
"""🗽 Basic Agent Example - Creating a Quirky News Reporter

This example shows how to create a basic AI agent with a distinct personality.
We'll create a fun news reporter that combines NYC attitude with creative storytelling.
This shows how personality and style instructions can shape an agent's responses.

Run `pip install openai agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat

# Create our News Reporter with a fun personality
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! 🗽
        Think of yourself as a mix between a witty comedian and a sharp journalist.

        Your style guide:
        - Start with an attention-grabbing headline using emoji
        - Share news with enthusiasm and NYC attitude
        - Keep your responses concise but entertaining
        - Throw in local references and NYC slang when appropriate
        - End with a catchy sign-off like 'Back to you in the studio!' or 'Reporting live from the Big Apple!'

        Remember to verify all facts while keeping that NYC energy high!\
    """),
    markdown=True,
)

# Example usage
agent.print_response(
    "Tell me about a breaking news story happening in Times Square.", stream=True
)

# More example prompts to try:
"""
Try these fun scenarios:
1. "What's the latest food trend taking over Brooklyn?"
2. "Tell me about a peculiar incident on the subway today"
3. "What's the scoop on the newest rooftop garden in Manhattan?"
4. "Report on an unusual traffic jam caused by escaped zoo animals"
5. "Cover a flash mob wedding proposal at Grand Central"
"""



================================================
FILE: cookbook/getting_started/02_agent_with_tools.py
================================================
"""🗽 Agent with Tools - Your AI News Buddy that can search the web

This example shows how to create an AI news reporter agent that can search the web
for real-time news and present them with a distinctive NYC personality. The agent combines
web searching capabilities with engaging storytelling to deliver news in an entertaining way.

Run `pip install openai ddgs agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

# Create a News Reporter Agent with a fun personality
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! 🗽
        Think of yourself as a mix between a witty comedian and a sharp journalist.

        Follow these guidelines for every report:
        1. Start with an attention-grabbing headline using relevant emoji
        2. Use the search tool to find current, accurate information
        3. Present news with authentic NYC enthusiasm and local flavor
        4. Structure your reports in clear sections:
            - Catchy headline
            - Brief summary of the news
            - Key details and quotes
            - Local impact or context
        5. Keep responses concise but informative (2-3 paragraphs max)
        6. Include NYC-style commentary and local references
        7. End with a signature sign-off phrase

        Sign-off examples:
        - 'Back to you in the studio, folks!'
        - 'Reporting live from the city that never sleeps!'
        - 'This is [Your Name], live from the heart of Manhattan!'

        Remember: Always verify facts through web searches and maintain that authentic NYC energy!\
    """),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Example usage
agent.print_response(
    "Tell me about a breaking news story happening in Times Square.", stream=True
)

# More example prompts to try:
"""
Try these engaging news queries:
1. "What's the latest development in NYC's tech scene?"
2. "Tell me about any upcoming events at Madison Square Garden"
3. "What's the weather impact on NYC today?"
4. "Any updates on the NYC subway system?"
5. "What's the hottest food trend in Manhattan right now?"
"""



================================================
FILE: cookbook/getting_started/03_agent_with_knowledge.py
================================================
"""🧠 Agent with Knowledge - Your AI Cooking Assistant!

This example shows how to create an AI cooking assistant that combines knowledge from a
curated recipe database with web searching capabilities. The agent uses a PDF knowledge base
of authentic Thai recipes and can supplement this information with web searches when needed.

Run `pip install openai lancedb tantivy pypdf ddgs agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a Recipe Expert Agent with knowledge of Thai recipes
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are a passionate and knowledgeable Thai cuisine expert! 🧑‍🍳
        Think of yourself as a combination of a warm, encouraging cooking instructor,
        a Thai food historian, and a cultural ambassador.

        Follow these steps when answering questions:
        1. If the user asks a about Thai cuisine, ALWAYS search your knowledge base for authentic Thai recipes and cooking information
        2. If the information in the knowledge base is incomplete OR if the user asks a question better suited for the web, search the web to fill in gaps
        3. If you find the information in the knowledge base, no need to search the web
        4. Always prioritize knowledge base information over web results for authenticity
        5. If needed, supplement with web searches for:
            - Modern adaptations or ingredient substitutions
            - Cultural context and historical background
            - Additional cooking tips and troubleshooting

        Communication style:
        1. Start each response with a relevant cooking emoji
        2. Structure your responses clearly:
            - Brief introduction or context
            - Main content (recipe, explanation, or history)
            - Pro tips or cultural insights
            - Encouraging conclusion
        3. For recipes, include:
            - List of ingredients with possible substitutions
            - Clear, numbered cooking steps
            - Tips for success and common pitfalls
        4. Use friendly, encouraging language

        Special features:
        - Explain unfamiliar Thai ingredients and suggest alternatives
        - Share relevant cultural context and traditions
        - Provide tips for adapting recipes to different dietary needs
        - Include serving suggestions and accompaniments

        End each response with an uplifting sign-off like:
        - 'Happy cooking! ขอให้อร่อย (Enjoy your meal)!'
        - 'May your Thai cooking adventure bring joy!'
        - 'Enjoy your homemade Thai feast!'

        Remember:
        - Always verify recipe authenticity with the knowledge base
        - Clearly indicate when information comes from web sources
        - Be encouraging and supportive of home cooks at all skill levels\
    """),
    knowledge=PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="recipe_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Comment out after the knowledge base is loaded
if agent.knowledge is not None:
    agent.knowledge.load()

agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
agent.print_response("What is the history of Thai curry?", stream=True)
agent.print_response("What ingredients do I need for Pad Thai?", stream=True)

# More example prompts to try:
"""
Explore Thai cuisine with these queries:
1. "What are the essential spices and herbs in Thai cooking?"
2. "Can you explain the different types of Thai curry pastes?"
3. "How do I make mango sticky rice dessert?"
4. "What's the proper way to cook Thai jasmine rice?"
5. "Tell me about regional differences in Thai cuisine"
"""



================================================
FILE: cookbook/getting_started/04_agent_with_storage.py
================================================
"""🧠 Agent with Storage - Your AI Thai Cooking Assistant!

This example shows how to create an AI cooking assistant that combines knowledge from a
curated recipe database with web searching capabilities. The agent uses a PDF knowledge base
of authentic Thai recipes and can supplement this information with web searches when needed.

Example prompts to try:
- "How do I make authentic Pad Thai?"
- "What's the difference between red and green curry?"
- "Can you explain what galangal is and possible substitutes?"
- "Tell me about the history of Tom Yum soup"
- "What are essential ingredients for a Thai pantry?"
- "How do I make Thai basil chicken (Pad Kra Pao)?"

Run `pip install openai lancedb tantivy pypdf ddgs sqlalchemy agno` to install dependencies.
"""

from textwrap import dedent
from typing import List, Optional

import typer
from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print

agent_knowledge = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="recipe_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Comment out after the knowledge base is loaded
# if agent_knowledge is not None:
#     agent_knowledge.load()

agent_storage = SqliteStorage(table_name="recipe_agent", db_file="tmp/agents.db")


def recipe_agent(user: str = "user"):
    session_id: Optional[str] = None

    # Ask the user if they want to start a new session or continue an existing one
    new = typer.confirm("Do you want to start a new session?")

    if not new:
        existing_sessions: List[str] = agent_storage.get_all_session_ids(user)
        if len(existing_sessions) > 0:
            session_id = existing_sessions[0]

    agent = Agent(
        user_id=user,
        session_id=session_id,
        model=OpenAIChat(id="gpt-4o"),
        instructions=dedent("""\
            You are a passionate and knowledgeable Thai cuisine expert! 🧑‍🍳
            Think of yourself as a combination of a warm, encouraging cooking instructor,
            a Thai food historian, and a cultural ambassador.

            Follow these steps when answering questions:
            1. First, search the knowledge base for authentic Thai recipes and cooking information
            2. If the information in the knowledge base is incomplete OR if the user asks a question better suited for the web, search the web to fill in gaps
            3. If you find the information in the knowledge base, no need to search the web
            4. Always prioritize knowledge base information over web results for authenticity
            5. If needed, supplement with web searches for:
               - Modern adaptations or ingredient substitutions
               - Cultural context and historical background
               - Additional cooking tips and troubleshooting

            Communication style:
            1. Start each response with a relevant cooking emoji
            2. Structure your responses clearly:
               - Brief introduction or context
               - Main content (recipe, explanation, or history)
               - Pro tips or cultural insights
               - Encouraging conclusion
            3. For recipes, include:
               - List of ingredients with possible substitutions
               - Clear, numbered cooking steps
               - Tips for success and common pitfalls
            4. Use friendly, encouraging language

            Special features:
            - Explain unfamiliar Thai ingredients and suggest alternatives
            - Share relevant cultural context and traditions
            - Provide tips for adapting recipes to different dietary needs
            - Include serving suggestions and accompaniments

            End each response with an uplifting sign-off like:
            - 'Happy cooking! ขอให้อร่อย (Enjoy your meal)!'
            - 'May your Thai cooking adventure bring joy!'
            - 'Enjoy your homemade Thai feast!'

            Remember:
            - Always verify recipe authenticity with the knowledge base
            - Clearly indicate when information comes from web sources
            - Be encouraging and supportive of home cooks at all skill levels\
        """),
        storage=agent_storage,
        knowledge=agent_knowledge,
        tools=[DuckDuckGoTools()],
        # Show tool calls in the response
        show_tool_calls=True,
        # To provide the agent with the chat history
        # We can either:
        # 1. Provide the agent with a tool to read the chat history
        # 2. Automatically add the chat history to the messages sent to the model
        #
        # 1. Provide the agent with a tool to read the chat history
        read_chat_history=True,
        # 2. Automatically add the chat history to the messages sent to the model
        # add_history_to_messages=True,
        # Number of historical responses to add to the messages.
        # num_history_responses=3,
        markdown=True,
    )

    print("You are about to chat with an agent!")
    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"Started Session: {session_id}\n")
        else:
            print("Started Session\n")
    else:
        print(f"Continuing Session: {session_id}\n")

    # Runs the agent as a command line application
    agent.cli_app(markdown=True)


if __name__ == "__main__":
    typer.run(recipe_agent)



================================================
FILE: cookbook/getting_started/05_agent_team.py
================================================
"""🗞️ Multi-Agent Team - Your Professional News & Finance Squad!

This example shows how to create a powerful team of AI agents working together
to provide comprehensive financial analysis and news reporting. The team consists of:
1. Web Agent: Searches and analyzes latest news
2. Finance Agent: Analyzes financial data and market trends
3. Lead Editor: Coordinates and combines insights from both agents

Run: `pip install openai ddgs yfinance agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=dedent("""\
        You are an experienced web researcher and news analyst! 🔍

        Follow these steps when searching for information:
        1. Start with the most recent and relevant sources
        2. Cross-reference information from multiple sources
        3. Prioritize reputable news outlets and official sources
        4. Always cite your sources with links
        5. Focus on market-moving news and significant developments

        Your style guide:
        - Present information in a clear, journalistic style
        - Use bullet points for key takeaways
        - Include relevant quotes when available
        - Specify the date and time for each piece of news
        - Highlight market sentiment and industry trends
        - End with a brief analysis of the overall narrative
        - Pay special attention to regulatory news, earnings reports, and strategic announcements\
    """),
    show_tool_calls=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=dedent("""\
        You are a skilled financial analyst with expertise in market data! 📊

        Follow these steps when analyzing financial data:
        1. Start with the latest stock price, trading volume, and daily range
        2. Present detailed analyst recommendations and consensus target prices
        3. Include key metrics: P/E ratio, market cap, 52-week range
        4. Analyze trading patterns and volume trends
        5. Compare performance against relevant sector indices

        Your style guide:
        - Use tables for structured data presentation
        - Include clear headers for each data section
        - Add brief explanations for technical terms
        - Highlight notable changes with emojis (📈 📉)
        - Use bullet points for quick insights
        - Compare current values with historical averages
        - End with a data-driven financial outlook\
    """),
    show_tool_calls=True,
    markdown=True,
)

agent_team = Team(
    members=[web_agent, finance_agent],
    model=OpenAIChat(id="gpt-4o"),
    mode="coordinate",
    success_criteria=dedent("""\
        A comprehensive financial news report with clear sections and data-driven insights.
    """),
    instructions=dedent("""\
        You are the lead editor of a prestigious financial news desk! 📰

        Your role:
        1. Coordinate between the web researcher and financial analyst
        2. Combine their findings into a compelling narrative
        3. Ensure all information is properly sourced and verified
        4. Present a balanced view of both news and data
        5. Highlight key risks and opportunities

        Your style guide:
        - Start with an attention-grabbing headline
        - Begin with a powerful executive summary
        - Present financial data first, followed by news context
        - Use clear section breaks between different types of information
        - Include relevant charts or tables when available
        - Add 'Market Sentiment' section with current mood
        - Include a 'Key Takeaways' section at the end
        - End with 'Risk Factors' when appropriate
        - Sign off with 'Market Watch Team' and the current date\
    """),
    add_datetime_to_instructions=True,
    show_tool_calls=True,
    markdown=True,
    enable_agentic_context=True,
    show_members_responses=False,
)

# Example usage with diverse queries
agent_team.print_response(
    message="Summarize analyst recommendations and share the latest news for NVDA",
    stream=True,
)
agent_team.print_response(
    message="What's the market outlook and financial performance of AI semiconductor companies?",
    stream=True,
)
agent_team.print_response(
    message="Analyze recent developments and financial performance of TSLA",
    stream=True,
)

# More example prompts to try:
"""
Advanced queries to explore:
1. "Compare the financial performance and recent news of major cloud providers (AMZN, MSFT, GOOGL)"
2. "What's the impact of recent Fed decisions on banking stocks? Focus on JPM and BAC"
3. "Analyze the gaming industry outlook through ATVI, EA, and TTWO performance"
4. "How are social media companies performing? Compare META and SNAP"
5. "What's the latest on AI chip manufacturers and their market position?"
"""



================================================
FILE: cookbook/getting_started/06_structured_output.py
================================================
"""🎬 Agent with Structured Output - Your AI Movie Script Generator

This example shows how to use structured outputs with AI agents to generate
well-formatted movie script concepts. It shows two approaches:
1. JSON Mode: Traditional JSON response parsing
2. Structured Output: Enhanced structured data handling

Run `pip install openai agno` to install dependencies.
"""

from textwrap import dedent
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ...,
        description="A richly detailed, atmospheric description of the movie's primary location and time period. Include sensory details and mood.",
    )
    ending: str = Field(
        ...,
        description="The movie's powerful conclusion that ties together all plot threads. Should deliver emotional impact and satisfaction.",
    )
    genre: str = Field(
        ...,
        description="The film's primary and secondary genres (e.g., 'Sci-fi Thriller', 'Romantic Comedy'). Should align with setting and tone.",
    )
    name: str = Field(
        ...,
        description="An attention-grabbing, memorable title that captures the essence of the story and appeals to target audience.",
    )
    characters: List[str] = Field(
        ...,
        description="4-6 main characters with distinctive names and brief role descriptions (e.g., 'Sarah Chen - brilliant quantum physicist with a dark secret').",
    )
    storyline: str = Field(
        ...,
        description="A compelling three-sentence plot summary: Setup, Conflict, and Stakes. Hook readers with intrigue and emotion.",
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are an acclaimed Hollywood screenwriter known for creating unforgettable blockbusters! 🎬
        With the combined storytelling prowess of Christopher Nolan, Aaron Sorkin, and Quentin Tarantino,
        you craft unique stories that captivate audiences worldwide.

        Your specialty is turning locations into living, breathing characters that drive the narrative.\
    """),
    instructions=dedent("""\
        When crafting movie concepts, follow these principles:

        1. Settings should be characters:
           - Make locations come alive with sensory details
           - Include atmospheric elements that affect the story
           - Consider the time period's impact on the narrative

        2. Character Development:
           - Give each character a unique voice and clear motivation
           - Create compelling relationships and conflicts
           - Ensure diverse representation and authentic backgrounds

        3. Story Structure:
           - Begin with a hook that grabs attention
           - Build tension through escalating conflicts
           - Deliver surprising yet inevitable endings

        4. Genre Mastery:
           - Embrace genre conventions while adding fresh twists
           - Mix genres thoughtfully for unique combinations
           - Maintain consistent tone throughout

        Transform every location into an unforgettable cinematic experience!\
    """),
    response_model=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are an acclaimed Hollywood screenwriter known for creating unforgettable blockbusters! 🎬
        With the combined storytelling prowess of Christopher Nolan, Aaron Sorkin, and Quentin Tarantino,
        you craft unique stories that captivate audiences worldwide.

        Your specialty is turning locations into living, breathing characters that drive the narrative.\
    """),
    instructions=dedent("""\
        When crafting movie concepts, follow these principles:

        1. Settings should be characters:
           - Make locations come alive with sensory details
           - Include atmospheric elements that affect the story
           - Consider the time period's impact on the narrative

        2. Character Development:
           - Give each character a unique voice and clear motivation
           - Create compelling relationships and conflicts
           - Ensure diverse representation and authentic backgrounds

        3. Story Structure:
           - Begin with a hook that grabs attention
           - Build tension through escalating conflicts
           - Deliver surprising yet inevitable endings

        4. Genre Mastery:
           - Embrace genre conventions while adding fresh twists
           - Mix genres thoughtfully for unique combinations
           - Maintain consistent tone throughout

        Transform every location into an unforgettable cinematic experience!\
    """),
    response_model=MovieScript,
)

# Example usage with different locations
json_mode_agent.print_response("Tokyo", stream=True)
structured_output_agent.print_response("Ancient Rome", stream=True)

# More examples to try:
"""
Creative location prompts to explore:
1. "Underwater Research Station" - For a claustrophobic sci-fi thriller
2. "Victorian London" - For a gothic mystery
3. "Dubai 2050" - For a futuristic heist movie
4. "Antarctic Research Base" - For a survival horror story
5. "Caribbean Island" - For a tropical adventure romance
"""

# To get the response in a variable:
# from rich.pretty import pprint

# json_mode_response: RunResponse = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.run("New York")
# pprint(structured_output_response.content)



================================================
FILE: cookbook/getting_started/07_write_your_own_tool.py
================================================
"""🛠️ Writing Your Own Tool - An Example Using Hacker News API

This example shows how to create and use your own custom tool with Agno.
You can replace the Hacker News functionality with any API or service you want!

Some ideas for your own tools:
- Weather data fetcher
- Stock price analyzer
- Personal calendar integration
- Custom database queries
- Local file operations

Run `pip install openai httpx agno` to install dependencies.
"""

import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 10) -> str:
    """Use this function to get top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to return. Defaults to 10.

    Returns:
        str: JSON string of top stories.
    """

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)


# Create a Tech News Reporter Agent with a Silicon Valley personality
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are a tech-savvy Hacker News reporter with a passion for all things technology! 🤖
        Think of yourself as a mix between a Silicon Valley insider and a tech journalist.

        Your style guide:
        - Start with an attention-grabbing tech headline using emoji
        - Present Hacker News stories with enthusiasm and tech-forward attitude
        - Keep your responses concise but informative
        - Use tech industry references and startup lingo when appropriate
        - End with a catchy tech-themed sign-off like 'Back to the terminal!' or 'Pushing to production!'

        Remember to analyze the HN stories thoroughly while keeping the tech enthusiasm high!\
    """),
    tools=[get_top_hackernews_stories],
    show_tool_calls=True,
    markdown=True,
)

# Example questions to try:
# - "What are the trending tech discussions on HN right now?"
# - "Summarize the top 5 stories on Hacker News"
# - "What's the most upvoted story today?"
agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)



================================================
FILE: cookbook/getting_started/08_research_agent_exa.py
================================================
"""🔍 AI Research Agent - Your AI Research Assistant!

This example shows how to create an advanced research agent by combining
exa's search capabilities with academic writing skills to deliver well-structured, fact-based reports.

Key features demonstrated:
- Using Exa.ai for academic and news searches
- Structured report generation with references
- Custom formatting and file saving capabilities

Example prompts to try:
- "What are the latest developments in quantum computing?"
- "Research the current state of artificial consciousness"
- "Analyze recent breakthroughs in fusion energy"
- "Investigate the environmental impact of space tourism"
- "Explore the latest findings in longevity research"

Run `pip install openai exa-py agno` to install dependencies.
"""

from datetime import datetime
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

cwd = Path(__file__).parent.resolve()
tmp = cwd.joinpath("tmp")
if not tmp.exists():
    tmp.mkdir(exist_ok=True, parents=True)

today = datetime.now().strftime("%Y-%m-%d")

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ExaTools(start_published_date=today, type="keyword")],
    description=dedent("""\
        You are Professor X-1000, a distinguished AI research scientist with expertise
        in analyzing and synthesizing complex information. Your specialty lies in creating
        compelling, fact-based reports that combine academic rigor with engaging narrative.

        Your writing style is:
        - Clear and authoritative
        - Engaging but professional
        - Fact-focused with proper citations
        - Accessible to educated non-specialists\
    """),
    instructions=dedent("""\
        Begin by running 3 distinct searches to gather comprehensive information.
        Analyze and cross-reference sources for accuracy and relevance.
        Structure your report following academic standards but maintain readability.
        Include only verifiable facts with proper citations.
        Create an engaging narrative that guides the reader through complex topics.
        End with actionable takeaways and future implications.\
    """),
    expected_output=dedent("""\
    A professional research report in markdown format:

    # {Compelling Title That Captures the Topic's Essence}

    ## Executive Summary
    {Brief overview of key findings and significance}

    ## Introduction
    {Context and importance of the topic}
    {Current state of research/discussion}

    ## Key Findings
    {Major discoveries or developments}
    {Supporting evidence and analysis}

    ## Implications
    {Impact on field/society}
    {Future directions}

    ## Key Takeaways
    - {Bullet point 1}
    - {Bullet point 2}
    - {Bullet point 3}

    ## References
    - [Source 1](link) - Key finding/quote
    - [Source 2](link) - Key finding/quote
    - [Source 3](link) - Key finding/quote

    ---
    Report generated by Professor X-1000
    Advanced Research Systems Division
    Date: {current_date}\
    """),
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
    save_response_to_file=str(tmp.joinpath("{message}.md")),
)

# Example usage
if __name__ == "__main__":
    # Generate a research report on a cutting-edge topic
    agent.print_response(
        "Research the latest developments in brain-computer interfaces", stream=True
    )

# More example prompts to try:
"""
Try these research topics:
1. "Analyze the current state of solid-state batteries"
2. "Research recent breakthroughs in CRISPR gene editing"
3. "Investigate the development of autonomous vehicles"
4. "Explore advances in quantum machine learning"
5. "Study the impact of artificial intelligence on healthcare"
"""



================================================
FILE: cookbook/getting_started/09_research_workflow.py
================================================
"""🎓 Advanced Research Workflow - Your AI Research Assistant!

This example shows how to build a sophisticated research workflow that combines:
🔍 Web search capabilities for finding relevant sources
📚 Content extraction and processing
✍️ Academic-style report generation
💾 Smart caching for improved performance

We've used the following tools as they're available for free:
- DuckDuckGoTools: Searches the web for relevant articles
- Newspaper4kTools: Scrapes and processes article content

Example research topics to try:
- "What are the latest developments in quantum computing?"
- "Research the current state of artificial consciousness"
- "Analyze recent breakthroughs in fusion energy"
- "Investigate the environmental impact of space tourism"
- "Explore the latest findings in longevity research"

Run `pip install openai ddgs newspaper4k lxml_html_clean sqlalchemy agno` to install dependencies.
"""

import json
from textwrap import dedent
from typing import Dict, Iterator, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowCompletedEvent
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunResponse, Workflow
from pydantic import BaseModel, Field


class Article(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[Article]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Content of the in markdown format if available. Return None if the content is not available or does not make sense.",
    )


class ResearchReportGenerator(Workflow):
    description: str = dedent("""\
    Generate comprehensive research reports that combine academic rigor
    with engaging storytelling. This workflow orchestrates multiple AI agents to search, analyze,
    and synthesize information from diverse sources into well-structured reports.
    """)

    web_searcher: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[DuckDuckGoTools()],
        description=dedent("""\
        You are ResearchBot-X, an expert at discovering and evaluating academic and scientific sources.\
        """),
        instructions=dedent("""\
        You're a meticulous research assistant with expertise in source evaluation! 🔍
        Search for 10-15 sources and identify the 5-7 most authoritative and relevant ones.
        Prioritize:
        - Peer-reviewed articles and academic publications
        - Recent developments from reputable institutions
        - Authoritative news sources and expert commentary
        - Diverse perspectives from recognized experts
        Avoid opinion pieces and non-authoritative sources.\
        """),
        response_model=SearchResults,
    )

    article_scraper: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[Newspaper4kTools()],
        description=dedent("""\
        You are ContentBot-X, an expert at extracting and structuring academic content.\
        """),
        instructions=dedent("""\
        You're a precise content curator with attention to academic detail! 📚
        When processing content:
           - Extract content from the article
           - Preserve academic citations and references
           - Maintain technical accuracy in terminology
           - Structure content logically with clear sections
           - Extract key findings and methodology details
           - Handle paywalled content gracefully
        Format everything in clean markdown for optimal readability.\
        """),
        response_model=ScrapedArticle,
    )

    writer: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are Professor X-2000, a distinguished AI research scientist combining academic rigor with engaging narrative style.\
        """),
        instructions=dedent("""\
        Channel the expertise of a world-class academic researcher!
        🎯 Analysis Phase:
          - Evaluate source credibility and relevance
          - Cross-reference findings across sources
          - Identify key themes and breakthroughs
        💡 Synthesis Phase:
          - Develop a coherent narrative framework
          - Connect disparate findings
          - Highlight contradictions or gaps
        ✍️ Writing Phase:
          - Begin with an engaging executive summary, hook the reader
          - Present complex ideas clearly
          - Support all claims with citations
          - Balance depth with accessibility
          - Maintain academic tone while ensuring readability
          - End with implications and future directions\
        """),
        expected_output=dedent("""\
        # {Compelling Academic Title}

        ## Executive Summary
        {Concise overview of key findings and significance}

        ## Introduction
        {Research context and background}
        {Current state of the field}

        ## Methodology
        {Search and analysis approach}
        {Source evaluation criteria}

        ## Key Findings
        {Major discoveries and developments}
        {Supporting evidence and analysis}
        {Contrasting viewpoints}

        ## Analysis
        {Critical evaluation of findings}
        {Integration of multiple perspectives}
        {Identification of patterns and trends}

        ## Implications
        {Academic and practical significance}
        {Future research directions}
        {Potential applications}

        ## Key Takeaways
        - {Critical finding 1}
        - {Critical finding 2}
        - {Critical finding 3}

        ## References
        {Properly formatted academic citations}

        ---
        Report generated by Professor X-2000
        Advanced Research Division
        Date: {current_date}\
        """),
        markdown=True,
    )

    def run(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> Iterator[RunResponse]:
        """
        Generate a comprehensive news report on a given topic.

        This function orchestrates a workflow to search for articles, scrape their content,
        and generate a final report. It utilizes caching mechanisms to optimize performance.

        Args:
            topic (str): The topic for which to generate the news report.
            use_search_cache (bool, optional): Whether to use cached search results. Defaults to True.
            use_scrape_cache (bool, optional): Whether to use cached scraped articles. Defaults to True.
            use_cached_report (bool, optional): Whether to return a previously generated report on the same topic. Defaults to False.

        Returns:
            Iterator[RunResponse]: An stream of objects containing the generated report or status information.

        Steps:
        1. Check for a cached report if use_cached_report is True.
        2. Search the web for articles on the topic:
            - Use cached search results if available and use_search_cache is True.
            - Otherwise, perform a new web search.
        3. Scrape the content of each article:
            - Use cached scraped articles if available and use_scrape_cache is True.
            - Scrape new articles that aren't in the cache.
        4. Generate the final report using the scraped article contents.

        The function utilizes the `session_state` to store and retrieve cached data.
        """
        logger.info(f"Generating a report on: {topic}")

        # Use the cached report if use_cached_report is True
        if use_cached_report:
            cached_report = self.get_cached_report(topic)
            if cached_report:
                yield WorkflowCompletedEvent(content=cached_report)
                return

        # Search the web for articles on the topic
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # If no search_results are found for the topic, end the workflow
        if search_results is None or len(search_results.articles) == 0:
            yield WorkflowCompletedEvent(
                content=f"Sorry, could not find any articles on the topic: {topic}",
            )
            return

        # Scrape the search results
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            search_results, use_scrape_cache
        )

        # Write a research report
        yield from self.write_research_report(topic, scraped_articles)

    def get_cached_report(self, topic: str) -> Optional[str]:
        logger.info("Checking if cached report exists")
        return self.session_state.get("reports", {}).get(topic)

    def add_report_to_cache(self, topic: str, report: str):
        logger.info(f"Saving report for topic: {topic}")
        self.session_state.setdefault("reports", {})
        self.session_state["reports"][topic] = report
        # Save the report to the storage
        self.write_to_storage()

    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:
        logger.info("Checking if cached search results exist")
        return self.session_state.get("search_results", {}).get(topic)

    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):
        logger.info(f"Saving search results for topic: {topic}")
        self.session_state.setdefault("search_results", {})
        self.session_state["search_results"][topic] = search_results.model_dump()
        # Save the search results to the storage
        self.write_to_storage()

    def get_cached_scraped_articles(
        self, topic: str
    ) -> Optional[Dict[str, ScrapedArticle]]:
        logger.info("Checking if cached scraped articles exist")
        return self.session_state.get("scraped_articles", {}).get(topic)

    def add_scraped_articles_to_cache(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ):
        logger.info(f"Saving scraped articles for topic: {topic}")
        self.session_state.setdefault("scraped_articles", {})
        self.session_state["scraped_articles"][topic] = scraped_articles
        # Save the scraped articles to the storage
        self.write_to_storage()

    def get_search_results(
        self, topic: str, use_search_cache: bool, num_attempts: int = 3
    ) -> Optional[SearchResults]:
        # Get cached search_results from the session state if use_search_cache is True
        if use_search_cache:
            try:
                search_results_from_cache = self.get_cached_search_results(topic)
                if search_results_from_cache is not None:
                    search_results = SearchResults.model_validate(
                        search_results_from_cache
                    )
                    logger.info(
                        f"Found {len(search_results.articles)} articles in cache."
                    )
                    return search_results
            except Exception as e:
                logger.warning(f"Could not read search results from cache: {e}")

        # If there are no cached search_results, use the web_searcher to find the latest articles
        for attempt in range(num_attempts):
            try:
                searcher_response: RunResponse = self.web_searcher.run(topic)
                if (
                    searcher_response is not None
                    and searcher_response.content is not None
                    and isinstance(searcher_response.content, SearchResults)
                ):
                    article_count = len(searcher_response.content.articles)
                    logger.info(
                        f"Found {article_count} articles on attempt {attempt + 1}"
                    )
                    # Cache the search results
                    self.add_search_results_to_cache(topic, searcher_response.content)
                    return searcher_response.content
                else:
                    logger.warning(
                        f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                    )
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

        logger.error(f"Failed to get search results after {num_attempts} attempts")
        return None

    def scrape_articles(
        self, search_results: SearchResults, use_scrape_cache: bool
    ) -> Dict[str, ScrapedArticle]:
        scraped_articles: Dict[str, ScrapedArticle] = {}

        # Get cached scraped_articles from the session state if use_scrape_cache is True
        if use_scrape_cache:
            try:
                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)
                if scraped_articles_from_cache is not None:
                    scraped_articles = scraped_articles_from_cache
                    logger.info(
                        f"Found {len(scraped_articles)} scraped articles in cache."
                    )
                    return scraped_articles
            except Exception as e:
                logger.warning(f"Could not read scraped articles from cache: {e}")

        # Scrape the articles that are not in the cache
        for article in search_results.articles:
            if article.url in scraped_articles:
                logger.info(f"Found scraped article in cache: {article.url}")
                continue

            article_scraper_response: RunResponse = self.article_scraper.run(
                article.url
            )
            if (
                article_scraper_response is not None
                and article_scraper_response.content is not None
                and isinstance(article_scraper_response.content, ScrapedArticle)
            ):
                scraped_articles[article_scraper_response.content.url] = (
                    article_scraper_response.content
                )
                logger.info(f"Scraped article: {article_scraper_response.content.url}")

        # Save the scraped articles in the session state
        self.add_scraped_articles_to_cache(topic, scraped_articles)
        return scraped_articles

    def write_research_report(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ) -> Iterator[RunResponse]:
        logger.info("Writing research report")
        # Prepare the input for the writer
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }
        # Run the writer and yield the response
        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)
        # Save the research report in the cache
        self.add_report_to_cache(topic, self.writer.run_response.content)


# Run the workflow if the script is executed directly
if __name__ == "__main__":
    from rich.prompt import Prompt

    # Example research topics
    example_topics = [
        "quantum computing breakthroughs 2024",
        "artificial consciousness research",
        "fusion energy developments",
        "space tourism environmental impact",
        "longevity research advances",
    ]

    topics_str = "\n".join(
        f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)
    )

    print(f"\n📚 Example Research Topics:\n{topics_str}\n")

    # Get topic from user
    topic = Prompt.ask(
        "[bold]Enter a research topic[/bold]\n✨",
        default="quantum computing breakthroughs 2024",
    )

    # Convert the topic to a URL-safe string for use in session_id
    url_safe_topic = topic.lower().replace(" ", "-")

    # Initialize the news report generator workflow
    generate_research_report = ResearchReportGenerator(
        session_id=f"generate-report-on-{url_safe_topic}",
        storage=SqliteStorage(
            table_name="generate_research_report_workflow",
            db_file="tmp/workflows.db",
        ),
    )

    # Execute the workflow with caching enabled
    report_stream: Iterator[RunResponse] = generate_research_report.run(
        topic=topic,
        use_search_cache=True,
        use_scrape_cache=True,
        use_cached_report=True,
    )

    # Print the response
    pprint_run_response(report_stream, markdown=True)



================================================
FILE: cookbook/getting_started/10_image_agent.py
================================================
"""🎨 AI Image Reporter - Your Visual Analysis & News Companion!

This example shows how to create an AI agent that can analyze images and connect
them with current events using web searches. Perfect for:
1. News reporting and journalism
2. Travel and tourism content
3. Social media analysis
4. Educational presentations
5. Event coverage

Example images to try:
- Famous landmarks (Eiffel Tower, Taj Mahal, etc.)
- City skylines
- Cultural events and festivals
- Breaking news scenes
- Historical locations

Run `pip install ddgs agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are a world-class visual journalist and cultural correspondent with a gift
        for bringing images to life through storytelling! 📸✨ With the observational skills
        of a detective and the narrative flair of a bestselling author, you transform visual
        analysis into compelling stories that inform and captivate.\
    """),
    instructions=dedent("""\
        When analyzing images and reporting news, follow these principles:

        1. Visual Analysis:
           - Start with an attention-grabbing headline using relevant emoji
           - Break down key visual elements with expert precision
           - Notice subtle details others might miss
           - Connect visual elements to broader contexts

        2. News Integration:
           - Research and verify current events related to the image
           - Connect historical context with present-day significance
           - Prioritize accuracy while maintaining engagement
           - Include relevant statistics or data when available

        3. Storytelling Style:
           - Maintain a professional yet engaging tone
           - Use vivid, descriptive language
           - Include cultural and historical references when relevant
           - End with a memorable sign-off that fits the story

        4. Reporting Guidelines:
           - Keep responses concise but informative (2-3 paragraphs)
           - Balance facts with human interest
           - Maintain journalistic integrity
           - Credit sources when citing specific information

        Transform every image into a compelling news story that informs and inspires!\
    """),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Example usage with a famous landmark
agent.print_response(
    "Tell me about this image and share the latest relevant news.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

# More examples to try:
"""
Sample prompts to explore:
1. "What's the historical significance of this location?"
2. "How has this place changed over time?"
3. "What cultural events happen here?"
4. "What's the architectural style and influence?"
5. "What recent developments affect this area?"

Sample image URLs to analyze:
1. Eiffel Tower: "https://upload.wikimedia.org/wikipedia/commons/8/85/Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg"
2. Taj Mahal: "https://upload.wikimedia.org/wikipedia/commons/b/bd/Taj_Mahal%2C_Agra%2C_India_edit3.jpg"
3. Golden Gate Bridge: "https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
"""

# To get the response in a variable:
# from rich.pretty import pprint
# response = agent.run(
#     "Analyze this landmark's architecture and recent news.",
#     images=[Image(url="YOUR_IMAGE_URL")],
# )
# pprint(response.content)



================================================
FILE: cookbook/getting_started/11_generate_image.py
================================================
"""🎨 Image Generation with DALL-E - Creating AI Art with Agno

This example shows how to create an AI agent that generates images using DALL-E.
You can use this agent to create various types of images, from realistic photos to artistic
illustrations and creative concepts.

Example prompts to try:
- "Create a surreal painting of a floating city in the clouds at sunset"
- "Generate a photorealistic image of a cozy coffee shop interior"
- "Design a cute cartoon mascot for a tech startup"
- "Create an artistic portrait of a cyberpunk samurai"

Run `pip install openai agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.dalle import DalleTools

# Create an Creative AI Artist Agent
image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description=dedent("""\
        You are an experienced AI artist with expertise in various artistic styles,
        from photorealism to abstract art. You have a deep understanding of composition,
        color theory, and visual storytelling.\
    """),
    instructions=dedent("""\
        As an AI artist, follow these guidelines:
        1. Analyze the user's request carefully to understand the desired style and mood
        2. Before generating, enhance the prompt with artistic details like lighting, perspective, and atmosphere
        3. Use the `create_image` tool with detailed, well-crafted prompts
        4. Provide a brief explanation of the artistic choices made
        5. If the request is unclear, ask for clarification about style preferences

        Always aim to create visually striking and meaningful images that capture the user's vision!\
    """),
    markdown=True,
    show_tool_calls=True,
)

# Example usage
image_agent.print_response(
    "Create a magical library with floating books and glowing crystals", stream=True
)

# Retrieve and display generated images
images = image_agent.get_images()
if images and isinstance(images, list):
    for image_response in images:
        image_url = image_response.url
        print(f"Generated image URL: {image_url}")

# More example prompts to try:
"""
Try these creative prompts:
1. "Generate a steampunk-style robot playing a violin"
2. "Design a peaceful zen garden during cherry blossom season"
3. "Create an underwater city with bioluminescent buildings"
4. "Generate a cozy cabin in a snowy forest at night"
5. "Create a futuristic cityscape with flying cars and skyscrapers"
"""



================================================
FILE: cookbook/getting_started/12_generate_video.py
================================================
"""🎥 Video Generation with ModelsLabs - Creating AI Videos with Agno

This example shows how to create an AI agent that generates videos using ModelsLabs.
You can use this agent to create various types of short videos, from animated scenes
to creative visual stories.

Example prompts to try:
- "Create a serene video of waves crashing on a beach at sunset"
- "Generate a magical video of butterflies flying in a enchanted forest"
- "Create a timelapse of a blooming flower in a garden"
- "Generate a video of northern lights dancing in the night sky"

Run `pip install openai agno` to install dependencies.
Remember to set your ModelsLabs API key in the environment variable `MODELS_LAB_API_KEY`.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models_labs import ModelsLabTools

# Create a Creative AI Video Director Agent
video_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ModelsLabTools()],
    description=dedent("""\
        You are an experienced AI video director with expertise in various video styles,
        from nature scenes to artistic animations. You have a deep understanding of motion,
        timing, and visual storytelling through video content.\
    """),
    instructions=dedent("""\
        As an AI video director, follow these guidelines:
        1. Analyze the user's request carefully to understand the desired style and mood
        2. Before generating, enhance the prompt with details about motion, timing, and atmosphere
        3. Use the `generate_media` tool with detailed, well-crafted prompts
        4. Provide a brief explanation of the creative choices made
        5. If the request is unclear, ask for clarification about style preferences

        The video will be displayed in the UI automatically below your response.
        Always aim to create captivating and meaningful videos that bring the user's vision to life!\
    """),
    markdown=True,
    show_tool_calls=True,
)

# Example usage
video_agent.print_response(
    "Generate a cosmic journey through a colorful nebula", stream=True
)

# Retrieve and display generated videos
videos = video_agent.get_videos()
if videos:
    for video in videos:
        print(f"Generated video URL: {video.url}")

# More example prompts to try:
"""
Try these creative prompts:
1. "Create a video of autumn leaves falling in a peaceful forest"
2. "Generate a video of a cat playing with a ball"
3. "Create a video of a peaceful koi pond with rippling water"
4. "Generate a video of a cozy fireplace with dancing flames"
5. "Create a video of a mystical portal opening in a magical realm"
"""



================================================
FILE: cookbook/getting_started/13_audio_input_output.py
================================================
"""🎤 Audio Input/Output with GPT-4 - Creating Voice Interactions with Agno

This example shows how to create an AI agent that can process audio input and generate
audio responses. You can use this agent for various voice-based interactions, from analyzing
speech content to generating natural-sounding responses.

Example audio interactions to try:
- Upload a recording of a conversation for analysis
- Have the agent respond to questions with voice output
- Process different languages and accents
- Analyze tone and emotion in speech

Run `pip install openai requests agno` to install dependencies.
"""

from textwrap import dedent

import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

# Create an AI Voice Interaction Agent
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    description=dedent("""\
        You are an expert in audio processing and voice interaction, capable of understanding
        and analyzing spoken content while providing natural, engaging voice responses.
        You excel at comprehending context, emotion, and nuance in speech.\
    """),
    instructions=dedent("""\
        As a voice interaction specialist, follow these guidelines:
        1. Listen carefully to audio input to understand both content and context
        2. Provide clear, concise responses that address the main points
        3. When generating voice responses, maintain a natural, conversational tone
        4. Consider the speaker's tone and emotion in your analysis
        5. If the audio is unclear, ask for clarification

        Focus on creating engaging and helpful voice interactions!\
    """),
)

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()

# Process the audio and get a response
agent.run(
    "What's in this recording? Please analyze the content and tone.",
    audio=[Audio(content=response.content, format="wav")],
)

# Save the audio response if available
if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.content, filename="tmp/response.wav"
    )

# More example interactions to try:
"""
Try these voice interaction scenarios:
1. "Can you summarize the main points discussed in this recording?"
2. "What emotions or tone do you detect in the speaker's voice?"
3. "Please provide a detailed analysis of the speech patterns and clarity"
4. "Can you identify any background noises or audio quality issues?"
5. "What is the overall context and purpose of this recording?"

Note: You can use your own audio files by converting them to base64 format.
Example for using your own audio file:

with open('your_audio.wav', 'rb') as audio_file:
    audio_data = audio_file.read()
    agent.run("Analyze this audio", audio=[Audio(content=audio_data, format="wav")])
"""



================================================
FILE: cookbook/getting_started/14_agent_state.py
================================================
"""🔄 Agent with State

This example shows how to create an agent that maintains state across interactions.
It demonstrates a simple counter mechanism, but this pattern can be extended to more
complex state management like maintaining conversation context, user preferences,
or tracking multi-step processes.

Example prompts to try:
- "Increment the counter 3 times and tell me the final count"
- "What's our current count? Add 2 more to it"
- "Let's increment the counter 5 times, but tell me each step"
- "Add 4 to our count and remind me where we started"
- "Increase the counter twice and summarize our journey"

Run `pip install openai agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat


# Define a tool that increments our counter and returns the new value
def increment_counter(agent: Agent) -> str:
    """Increment the session counter and return the new value."""
    agent.session_state["count"] += 1
    return f"The count is now {agent.session_state['count']}"


# Create a State Manager Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Initialize the session state with a counter starting at 0
    session_state={"count": 0},
    tools=[increment_counter],
    # You can use variables from the session state in the instructions
    instructions=dedent("""\
        You are the State Manager, an enthusiastic guide to state management! 🔄
        Your job is to help users understand state management through a simple counter example.

        Follow these guidelines for every interaction:
        1. Always acknowledge the current state (count) when relevant
        2. Use the increment_counter tool to modify the state
        3. Explain state changes in a clear and engaging way

        Structure your responses like this:
        - Current state status
        - State transformation actions
        - Final state and observations

        Starting state (count) is: {count}\
    """),
    show_tool_calls=True,
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response(
    "Let's increment the counter 3 times and observe the state changes!",
    stream=True,
)

# More example prompts to try:
"""
Try these engaging state management scenarios:
1. "Update our state 4 times and track the changes"
2. "Modify the counter twice and explain the state transitions"
3. "Increment 3 times and show how state persists"
4. "Let's perform 5 state updates with observations"
5. "Add 3 to our count and explain the state management concept"
"""

print(f"Final session state: {agent.session_state}")



================================================
FILE: cookbook/getting_started/15_agent_context.py
================================================
"""📰 Agent with Context

This example shows how to inject external dependencies into an agent.
The context is evaluated when the agent is run, acting like dependency injection for Agents.

Run `pip install openai agno` to install dependencies.
"""

import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

    Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)


# Create a Context-Aware Agent that can access real-time HackerNews data
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Each function in the context is evaluated when the agent is run,
    # think of it as dependency injection for Agents
    context={"top_hackernews_stories": get_top_hackernews_stories},
    # Alternatively, you can manually add the context to the instructions
    instructions=dedent("""\
        You are an insightful tech trend observer! 📰

        Here are the top stories on HackerNews:
        {top_hackernews_stories}\
    """),
    # add_state_in_messages will make the `top_hackernews_stories` variable
    # available in the instructions
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response(
    "Summarize the top stories on HackerNews and identify any interesting trends.",
    stream=True,
)



================================================
FILE: cookbook/getting_started/16_agent_session.py
================================================
"""🗣️ Persistent Chat History i.e. Session Memory

This example shows how to create an agent with persistent memory stored in a SQLite database.
We set the session_id on the agent when resuming the conversation, this way the previous chat history is preserved.

Key features:
- Stores conversation history in a SQLite database
- Continues conversations across multiple sessions
- References previous context in responses

Run `pip install openai sqlalchemy agno` to install dependencies.
"""

import json
from typing import Optional

import typer
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from rich import print
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel
from rich.prompt import Prompt

console = Console()


def create_agent(user: str = "user"):
    session_id: Optional[str] = None

    # Ask if user wants to start new session or continue existing one
    new = typer.confirm("Do you want to start a new session?")

    # Get existing session if user doesn't want a new one
    agent_storage = SqliteStorage(table_name="agent_sessions", db_file="tmp/agents.db")

    if not new:
        existing_sessions = agent_storage.get_all_session_ids(user)
        if len(existing_sessions) > 0:
            session_id = existing_sessions[0]

    agent = Agent(
        user_id=user,
        # Set the session_id on the agent to resume the conversation
        session_id=session_id,
        model=OpenAIChat(id="gpt-4o"),
        storage=agent_storage,
        # Add chat history to messages
        add_history_to_messages=True,
        num_history_responses=3,
        markdown=True,
    )

    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"Started Session: {session_id}\n")
        else:
            print("Started Session\n")
    else:
        print(f"Continuing Session: {session_id}\n")

    return agent


def print_messages(agent):
    """Print the current chat history in a formatted panel"""
    console.print(
        Panel(
            JSON(
                json.dumps(
                    [
                        m.model_dump(include={"role", "content"})
                        for m in agent.get_messages_for_session()
                    ]
                ),
                indent=4,
            ),
            title=f"Chat History for session_id: {agent.session_id}",
            expand=True,
        )
    )


def main(user: str = "user"):
    agent = create_agent(user)

    print("Chat with an OpenAI agent!")
    exit_on = ["exit", "quit", "bye"]
    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in exit_on:
            break

        agent.print_response(message=message, stream=True, markdown=True)
        print_messages(agent)


if __name__ == "__main__":
    typer.run(main)



================================================
FILE: cookbook/getting_started/17_user_memories_and_summaries.py
================================================
"""🧠 Long Term User Memories and Session Summaries

This example shows how to create an agent with persistent memory that stores:
1. Personalized user memories - facts and preferences learned about specific users
2. Session summaries - key points and context from conversations
3. Chat history - stored in SQLite for persistence

Key features:
- Stores user-specific memories in SQLite database
- Maintains session summaries for context
- Continues conversations across sessions with memory
- References previous context and user information in responses

Examples:
User: "My name is John and I live in NYC"
Agent: *Creates memory about John's location*

User: "What do you remember about me?"
Agent: *Recalls previous memories about John*

Run: `pip install openai sqlalchemy agno` to install dependencies
"""

import json
from textwrap import dedent
from typing import Optional

import typer
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel
from rich.prompt import Prompt


def create_agent(user: str = "user"):
    session_id: Optional[str] = None

    # Ask if user wants to start new session or continue existing one
    new = typer.confirm("Do you want to start a new session?")

    # Initialize storage for both agent sessions and memories
    agent_storage = SqliteStorage(table_name="agent_memories", db_file="tmp/agents.db")

    if not new:
        existing_sessions = agent_storage.get_all_session_ids(user)
        if len(existing_sessions) > 0:
            session_id = existing_sessions[0]

    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        user_id=user,
        session_id=session_id,
        # Configure memory system with SQLite storage
        memory=Memory(
            db=SqliteMemoryDb(
                table_name="agent_memory",
                db_file="tmp/agent_memory.db",
            ),
        ),
        enable_user_memories=True,
        enable_session_summaries=True,
        storage=agent_storage,
        add_history_to_messages=True,
        num_history_responses=3,
        # Enhanced system prompt for better personality and memory usage
        description=dedent("""\
        You are a helpful and friendly AI assistant with excellent memory.
        - Remember important details about users and reference them naturally
        - Maintain a warm, positive tone while being precise and helpful
        - When appropriate, refer back to previous conversations and memories
        - Always be truthful about what you remember or don't remember"""),
    )

    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"Started Session: {session_id}\n")
        else:
            print("Started Session\n")
    else:
        print(f"Continuing Session: {session_id}\n")

    return agent


def print_agent_memory(agent):
    """Print the current state of agent's memory systems"""
    console = Console()

    messages = []
    session_id = agent.session_id
    session_run = agent.memory.runs[session_id][-1]
    for m in session_run.messages:
        message_dict = m.to_dict()
        messages.append(message_dict)

    # Print chat history
    console.print(
        Panel(
            JSON(
                json.dumps(
                    messages,
                ),
                indent=4,
            ),
            title=f"Chat History for session_id: {session_run.session_id}",
            expand=True,
        )
    )

    # Print user memories
    for user_id in list(agent.memory.memories.keys()):
        console.print(
            Panel(
                JSON(
                    json.dumps(
                        [
                            user_memory.to_dict()
                            for user_memory in agent.memory.get_user_memories(
                                user_id=user_id
                            )
                        ],
                        indent=4,
                    ),
                ),
                title=f"Memories for user_id: {user_id}",
                expand=True,
            )
        )

    # Print session summary
    for user_id in list(agent.memory.summaries.keys()):
        console.print(
            Panel(
                JSON(
                    json.dumps(
                        [
                            summary.to_dict()
                            for summary in agent.memory.get_session_summaries(
                                user_id=user_id
                            )
                        ],
                        indent=4,
                    ),
                ),
                title=f"Summary for session_id: {agent.session_id}",
                expand=True,
            )
        )


def main(user: str = "user"):
    """Interactive chat loop with memory display"""
    agent = create_agent(user)

    print("Try these example inputs:")
    print("- 'My name is [name] and I live in [city]'")
    print("- 'I love [hobby/interest]'")
    print("- 'What do you remember about me?'")
    print("- 'What have we discussed so far?'\n")

    exit_on = ["exit", "quit", "bye"]
    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in exit_on:
            break

        agent.print_response(message=message, stream=True, markdown=True)
        print_agent_memory(agent)


if __name__ == "__main__":
    typer.run(main)



================================================
FILE: cookbook/getting_started/18_retry_function_call.py
================================================
from typing import Iterator

from agno.agent import Agent
from agno.exceptions import RetryAgentRun
from agno.tools import FunctionCall, tool

num_calls = 0


def pre_hook(fc: FunctionCall):
    global num_calls

    print(f"Pre-hook: {fc.function.name}")
    print(f"Arguments: {fc.arguments}")
    num_calls += 1
    if num_calls < 2:
        raise RetryAgentRun(
            "This wasn't interesting enough, please retry with a different argument"
        )


@tool(pre_hook=pre_hook)
def print_something(something: str) -> Iterator[str]:
    print(something)
    yield f"I have printed {something}"


agent = Agent(tools=[print_something], markdown=True)
agent.print_response("Print something interesting", stream=True)



================================================
FILE: cookbook/getting_started/19_human_in_the_loop.py
================================================
"""🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Add pre-hooks to tools for user confirmation
- Handle user input during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


# Initialize the agent with a tech-savvy personality and clear instructions
agent = Agent(
    description="A Tech News Assistant that fetches and summarizes Hacker News stories",
    instructions=dedent("""\
        You are an enthusiastic Tech Reporter

        Your responsibilities:
        - Present Hacker News stories in an engaging and informative way
        - Provide clear summaries of the information you gather

        Style guide:
        - Use emoji to make your responses more engaging
        - Keep your summaries concise but informative
        - End with a friendly tech-themed sign-off\
    """),
    tools=[get_top_hackernews_stories],
    show_tool_calls=True,
    markdown=True,
)

# Example questions to try:
# - "What are the top 3 HN stories right now?"
# - "Show me the most recent story from Hacker News"
# - "Get the top 5 stories (you can try accepting and declining the confirmation)"
response = agent.run("What are the top 2 hackernews stories?")
if response.is_paused:
    for tool in response.tools:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            break
        else:
            # We update the tools in place
            tool.confirmed = True

    run_response = agent.continue_run(run_response=response)
    pprint.pprint_run_response(run_response)



================================================
FILE: cookbook/getting_started/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/getting_started/readme_examples.py
================================================
"""Readme Examples
Run `pip install openai ddgs yfinance lancedb tantivy pypdf agno` to install dependencies."""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Level 0: Agents with no tools (basic inference tasks).
level_0_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are an enthusiastic news reporter with a flair for storytelling!",
    markdown=True,
)
level_0_agent.print_response(
    "Tell me about a breaking news story from New York.", stream=True
)

# Level 1: Agents with tools for autonomous task execution.
level_1_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are an enthusiastic news reporter with a flair for storytelling!",
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
level_1_agent.print_response(
    "Tell me about a breaking news story from New York.", stream=True
)

# Level 2: Agents with knowledge, combining memory and reasoning.
level_2_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are a Thai cuisine expert!",
    instructions=[
        "Search your knowledge base for Thai recipes.",
        "If the question is better suited for the web, search the web to fill in gaps.",
        "Prefer the information in your knowledge base over the web results.",
    ],
    knowledge=PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="recipes",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Comment out after first run
# if level_2_agent.knowledge is not None:
#     level_2_agent.knowledge.load()
level_2_agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
level_2_agent.print_response("What is the history of Thai curry?", stream=True)

# Level 3: Teams of agents collaborating on complex workflows.
web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    show_tool_calls=True,
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions="Use tables to display data",
    show_tool_calls=True,
    markdown=True,
)

level_3_agent_team = Team(
    members=[web_agent, finance_agent],
    model=OpenAIChat(id="gpt-4o"),
    mode="coordinate",
    success_criteria="A comprehensive financial news report with clear sections and data-driven insights.",
    instructions=["Always include sources", "Use tables to display data"],
    show_tool_calls=True,
    markdown=True,
)
level_3_agent_team.print_response(
    "What's the market outlook and financial performance of AI semiconductor companies?",
    stream=True,
)



================================================
FILE: cookbook/hackathon/README.md
================================================
# Hackathon Resources

Thank you for using Agno to build your hackathon project! Here you'll find setup guides, examples, and resources to bring your multimodal agents to life.

> Read this documentation on [Agno Docs](https://docs.agno.com)

## Environment Setup

Let's get your environment setup for the hackathon. Here are the steps:

1. Create a virtual environment
2. Install libraries
3. Export your API keys

### Create a virtual environment

You can use `python3 -m venv` or `uv` to create a virtual environment.

- Standard python

```shell
python3 -m venv .venv
source .venv/bin/activate
```

- Using uv

```shell
uv venv --python 3.12
source .venv/bin/activate
```

- for Windows

```shell
python -m venv venv
venv\scripts\activate
```

### Install libraries

Install the `agno` python package along with the models and tools you want to use.

- Standard python

```shell
pip install -U agno openai
```

- Using uv

```shell
uv pip install -U agno openai
```

### Export your API keys

Export the API keys for the models and tools you want to use.

```shell
export OPENAI_API_KEY=***
export GOOGLE_API_KEY=***
export ELEVEN_LABS_API_KEY=***
```

for Windows

```shell
$env:OPENAI_API_KEY="your-api-key"
```

## Text Agents

Here are some examples of Text Agents built with Agno:

- [Simple Text Agent](cookbook/hackathon/examples/simple_text_agent.py)
- [Agent with Tools](cookbook/hackathon/examples/agent_with_tools.py)
- [Agent with Knowledge](cookbook/hackathon/examples/agent_with_knowledge.py)
- [Agent with Structured Outputs](cookbook/hackathon/examples/structured_output.py)
- [Research Agent](cookbook/hackathon/examples/research_agent.py)
- [Youtube Agent](cookbook/hackathon/examples/youtube_agent.py)

## Image Agents

- [Image Input + Tools](cookbook/hackathon/multimodal_examples/image_input_with_tools.py)
- [Image Generation](cookbook/hackathon/multimodal_examples/image_generate.py)
- [Image to Structured Output](cookbook/hackathon/multimodal_examples/image_to_structured_output.py)
- [Image to Audio](cookbook/hackathon/multimodal_examples/image_to_audio.py)
- [Image to Image](cookbook/hackathon/multimodal_examples/image_to_image.py)
- [Image Transcription](cookbook/hackathon/multimodal_examples/image_transcription.py)
- [Image Generation with Steps](cookbook/hackathon/multimodal_examples/image_generate_with_intermediate_steps.py)
- [Image Search with Giphy](cookbook/hackathon/multimodal_examples/image_gif_search.py)

## Audio Agents

- [Audio Input](cookbook/hackathon/multimodal_examples/audio_input.py)
- [Audio Input Output](cookbook/hackathon/multimodal_examples/audio_input_output.py)
- [Audio Multiturn](cookbook/hackathon/multimodal_examples/audio_multi_turn.py)
- [Audio Sentiment Analysis](cookbook/hackathon/multimodal_examples/audio_sentiment_analysis.py)
- [Audio Transcription](cookbook/hackathon/multimodal_examples/audio_transcription.py)
- [Audio Podcast](cookbook/hackathon/multimodal_examples/audio_podcast_generator.py)

## Video Agents

- [Video Input](cookbook/hackathon/multimodal_examples/video_input.py)
- [Video to Shorts](cookbook/hackathon/multimodal_examples/video_to_shorts.py)
- [Video Caption](cookbook/hackathon/multimodal_examples/video_caption.py)
- [Video Generation using Replicate](cookbook/hackathon/multimodal_examples/video_generate_using_replicate.py)
- [Video Generation using Models Lab](cookbook/hackathon/multimodal_examples/video_generate_using_models_lab.py)



================================================
FILE: cookbook/hackathon/workshop/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/hackathon/workshop/agno_assist.py
================================================
"""pip install openai lancedb sqlalchemy elevenlabs tantivy fastapi"""

from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.dalle import DalleTools
from agno.tools.eleven_labs import ElevenLabsTools
from agno.tools.python import PythonTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths
cwd = Path(__file__).parent.parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

# Initialize knowledge base
agent_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/llms-full.txt"],
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

_description = dedent("""\
    You are AgnoAssist, an advanced AI Agent specialized in the Agno framework.
    Your goal is to help developers understand and effectively use Agno.""")

_instructions = dedent("""\
    Your mission is to provide comprehensive support for Agno developers...""")

agent_knowledge.load(recreate=False)

agno_support = Agent(
    name="Agno_Assist",
    agent_id="agno_assist",
    model=OpenAIChat(id="gpt-4o"),
    description=_description,
    instructions=_instructions,
    knowledge=agent_knowledge,
    tools=[
        PythonTools(base_dir=tmp_dir.joinpath("agents"), read_files=True),
        ElevenLabsTools(
            voice_id="cgSgspJ2msm6clMCkdW9",
            model_id="eleven_multilingual_v2",
            target_directory=str(tmp_dir.joinpath("audio").resolve()),
        ),
        DalleTools(model="dall-e-3", size="1792x1024", quality="hd", style="vivid"),
    ],
    storage=SqliteStorage(
        table_name="agno_assist_sessions", db_file=str(tmp_dir.joinpath("agents.db"))
    ),
    markdown=True,
    search_knowledge=True,
    debug_mode=True,
)

# agno_support.print_response("How do I implement RAG with Agno? Generate a diagram of the process.", stream=True)


"""
Example prompts for `AgnoAssist`:
- "What is Agno and what are its key features? Generate some audio content to explain the key features."
- "How do I create my first agent with Agno? Show me some example code."
- "What's the difference between Level 0 and Level 1 agents?"
- "How can I add memory to my Agno agent?"
- "How do I implement RAG with Agno? Generate a diagram of the process."
"""



================================================
FILE: cookbook/hackathon/workshop/agno_assist_voice.py
================================================
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.python import PythonTools
from agno_assist import agent_knowledge

cwd = Path(__file__).parent.parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)


_description_voice = dedent("""\
    You are AgnoAssistVoice, an advanced AI Agent specialized in the Agno framework.""")

_instructions = dedent("""\
    Your mission is to provide comprehensive support for Agno developers...""")

agno_assist_voice = Agent(
    name="Agno_Assist_Voice",
    agent_id="agno-assist-voice",
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "pcm16"},
    ),
    description=_description_voice,
    instructions=_instructions,
    knowledge=agent_knowledge,
    tools=[PythonTools(base_dir=tmp_dir.joinpath("agents"), read_files=True)],
    storage=SqliteStorage(
        table_name="agno_assist_voice_sessions",
        db_file=str(tmp_dir.joinpath("agents.db")),
    ),
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    markdown=True,
)

# agno_assist_voice.print_response("Hello, I am Agno Assist Voice. How can I help you?")

"""
Example prompts for `AgnoAssistVoice`:
- "What is Agno and what are its key features?"
- "How do I create my first agent with Agno?"
- "What's the difference between Level 0 and Level 1 agents?"
- "What models does Agno support?"
"""



================================================
FILE: cookbook/hackathon/workshop/playground.py
================================================
from agno.playground import Playground, serve_playground_app
from agno_assist import agno_support
from agno_assist_voice import agno_assist_voice
from fastapi import FastAPI

# Create and configure the playground app
playground_app = Playground(
    agents=[agno_support, agno_assist_voice],
    name="Playground-hackathon",
    app_id="playground-hackathon",
    description="A playground for testing and playing with Agno",
)

app = playground_app.get_app()

if __name__ == "__main__":
    playground_app.serve(app="playground:app", reload=True)



================================================
FILE: cookbook/models/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/aimlapi/README.md
================================================
# AI/ML API Cookbook

### AI/ML API provides 300+ AI models including Deepseek, Gemini, ChatGPT. The models run at enterprise-grade rate limits and uptimes.

#### You can check provider docs [_here_](https://docs.aimlapi.com/?utm_source=agno&utm_medium=github&utm_campaign=integration)
#### And models overview is [_here_](https://aimlapi.com/models/?utm_source=agno&utm_medium=github&utm_campaign=integration)

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `AIMLAPI_API_KEY`

```shell
export AIMLAPI_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/aimlapi/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/aimlapi/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/aimlapi/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/aimlapi/structured_output.py
```




================================================
FILE: cookbook/models/aimlapi/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/aimlapi/async_basic.py
================================================
"""
Basic async example using AIMlAPI.
"""

import asyncio

from agno.agent import Agent
from agno.models.aimlapi import AIMLApi

agent = Agent(
    model=AIMLApi(id="gpt-4o-mini"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/aimlapi/async_basic_stream.py
================================================
"""
Basic streaming async example using AIMlAPI.
"""

import asyncio

from agno.agent import Agent
from agno.models.aimlapi import AIMLApi

agent = Agent(
    model=AIMLApi(id="gpt-4o-mini"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/aimlapi/async_tool_use.py
================================================
"""
Async example using AIMlAPI with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.aimlapi import AIMLApi
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AIMLApi(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/aimlapi/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.aimlapi import AIMLApi

agent = Agent(model=AIMLApi(id="gpt-4o-mini"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/aimlapi/basic_stream.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.aimlapi import AIMLApi

agent = Agent(model=AIMLApi(id="gpt-4o-mini"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/aimlapi/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.aimlapi import AIMLApi

agent = Agent(
    model=AIMLApi(id="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"),
    markdown=True,
)

agent.print_response(
    "Tell me about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/aimlapi/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.aimlapi import AIMLApi

agent = Agent(
    model=AIMLApi(id="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/aimlapi/image_agent_with_memory.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.aimlapi import AIMLApi

agent = Agent(
    model=AIMLApi(id="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"),
    markdown=True,
    add_history_to_messages=True,
    num_history_responses=3,
)

agent.print_response(
    "Tell me about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

agent.print_response("Tell me where I can get more images?")



================================================
FILE: cookbook/models/aimlapi/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.aimlapi import AIMLApi
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=AIMLApi(id="gpt-4o-mini"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
json_mode_response: RunResponse = json_mode_agent.run("New York")
pprint(json_mode_response.content)

# json_mode_agent.print_response("New York")



================================================
FILE: cookbook/models/aimlapi/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aimlapi import AIMLApi
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AIMLApi(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/anthropic/README.md
================================================
# Anthropic Claude

[Models overview](https://docs.anthropic.com/claude/docs/models-overview)

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Set your `ANTHROPIC_API_KEY`

```shell
export ANTHROPIC_API_KEY=xxx
```

### 3. Install libraries

```shell
pip install -U anthropic ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/anthropic/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/anthropic/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/anthropic/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/anthropic/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/anthropic/storage.py
```

### 8. Run Agent that uses knowledge

Take note that claude uses OpenAI embeddings under the hood, and you will need an OpenAI API Key
```shell
export OPENAI_API_KEY=***
```

```shell
python cookbook/models/anthropic/knowledge.py
```

### 9. Run Agent that uses memory

```shell
python cookbook/models/anthropic/memory.py
```

### 10. Run Agent that analyzes an image

```shell
python cookbook/models/anthropic/image_agent.py
```

### 11. Run Agent with Thinking enabled

- Streaming on
```shell
python cookbook/models/anthropic/thinking.py
```
- Streaming off

```shell
python cookbook/models/anthropic/thinking_stream.py
```

### 12. Run Agent with Interleaved Thinking

```shell
python cookbook/models/anthropic/financial_analyst_thinking.py
```



================================================
FILE: cookbook/models/anthropic/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/anthropic/async_basic.py
================================================
"""
Basic async example using Claude.
"""

import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/anthropic/async_basic_stream.py
================================================
"""
Basic streaming async example using Claude.
"""

import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/anthropic/async_tool_use.py
================================================
"""
Async example using Claude with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/anthropic/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.anthropic import Claude

agent = Agent(model=Claude(id="claude-sonnet-4-20250514"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/anthropic/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.anthropic import Claude

agent = Agent(model=Claude(id="claude-sonnet-4-20250514"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/anthropic/code_execution.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        default_headers={"anthropic-beta": "code-execution-2025-05-22"},
    ),
    tools=[
        {
            "type": "code_execution_20250522",
            "name": "code_execution",
        }
    ],
    markdown=True,
)

agent.print_response(
    "Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]",
    stream=True,
)



================================================
FILE: cookbook/models/anthropic/financial_analyst_thinking.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.calculator import CalculatorTools
from agno.tools.yfinance import YFinanceTools

# Complex multi-step reasoning problem that demonstrates interleaved thinking
task = (
    "I'm considering an investment portfolio. I want to invest $50,000 split equally "
    "between Apple (AAPL) and Tesla (TSLA). Calculate how many shares of each I can buy "
    "at current prices, then analyze what my total portfolio value would be if both stocks "
    "increased by 15%. Also calculate what percentage return that represents on my initial investment. "
    "Think through each step and show your reasoning process."
)

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        thinking={"type": "enabled", "budget_tokens": 2048},
        default_headers={"anthropic-beta": "interleaved-thinking-2025-05-14"},
    ),
    tools=[
        CalculatorTools(enable_all=True),
        YFinanceTools(stock_price=True, cache_results=True),
    ],
    instructions=[
        "You are a financial analysis assistant with access to calculator and stock price tools.",
        "For complex problems, think through each step carefully before and after using tools.",
        "Show your reasoning process and explain your calculations clearly.",
        "Use the calculator tool for all mathematical operations to ensure accuracy.",
    ],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response(task, stream=True)



================================================
FILE: cookbook/models/anthropic/image_input_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.anthropic.claude import Claude
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/anthropic/image_input_file_upload.py
================================================
"""
In this example, we upload a PDF file to Anthropic directly and then use it as an input to an agent.
"""

from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.anthropic import Claude
from agno.utils.media import download_file
from anthropic import Anthropic

img_path = Path(__file__).parent.joinpath("agno-intro.png")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.us-east-1.amazonaws.com/images/agno-intro.png",
    str(img_path),
)

# Initialize Anthropic client
client = Anthropic()

# Upload the file to Anthropic
uploaded_file = client.beta.files.upload(
    file=Path(img_path),
)

if uploaded_file is not None:
    agent = Agent(
        model=Claude(
            id="claude-opus-4-20250514",
            default_headers={"anthropic-beta": "files-api-2025-04-14"},
        ),
        markdown=True,
    )

    agent.print_response(
        "What does the attached image say.",
        images=[Image(content=uploaded_file)],
    )



================================================
FILE: cookbook/models/anthropic/image_input_url.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and search the web for more information.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
        ),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/anthropic/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf anthropic openai` to install dependencies."""

from agno.agent import Agent
from agno.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.anthropic import Claude
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=AzureOpenAIEmbedder(),
    ),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    knowledge=knowledge_base,
    show_tool_calls=True,
    debug_mode=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/anthropic/mcp_connector.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.utils.models.claude import MCPServerConfiguration

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        default_headers={"anthropic-beta": "mcp-client-2025-04-04"},
        mcp_servers=[
            MCPServerConfiguration(
                type="url",
                name="deepwiki",
                url="https://mcp.deepwiki.com/sse",
            )
        ],
    ),
    markdown=True,
)

agent.print_response(
    "Tell me about https://github.com/agno-agi/agno",
    stream=True,
)



================================================
FILE: cookbook/models/anthropic/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install anthropic sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/anthropic/memory.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic import Claude
from agno.storage.postgres import PostgresStorage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresStorage(
        table_name="personalized_agent_sessions",
        db_url=db_url,
        auto_upgrade_schema=True,
    ),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/anthropic/pdf_input_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(
            content=pdf_path.read_bytes(),
        ),
    ],
)

print("Citations:")
print(agent.run_response.citations)



================================================
FILE: cookbook/models/anthropic/pdf_input_file_upload.py
================================================
"""
In this example, we upload a PDF file to Anthropic directly and then use it as an input to an agent.
"""

from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file
from anthropic import Anthropic

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

# Initialize Anthropic client
client = Anthropic()

# Upload the file to Anthropic
uploaded_file = client.beta.files.upload(
    file=Path(pdf_path),
)

if uploaded_file is not None:
    agent = Agent(
        model=Claude(
            id="claude-opus-4-20250514",
            default_headers={"anthropic-beta": "files-api-2025-04-14"},
        ),
        markdown=True,
    )

    agent.print_response(
        "Summarize the contents of the attached file.",
        files=[File(external=uploaded_file)],
    )



================================================
FILE: cookbook/models/anthropic/pdf_input_local.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(
            filepath=pdf_path,
        ),
    ],
)

print("Citations:")
print(agent.run_response.citations)



================================================
FILE: cookbook/models/anthropic/pdf_input_url.py
================================================
from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"),
    ],
)



================================================
FILE: cookbook/models/anthropic/prompt_caching.py
================================================
"""
This cookbook shows how to use prompt caching with Agents using Anthropic models, to catch the system prompt passed to the model.

This can significantly reduce processing time and costs.
Use it when working with a static and large system prompt.

You can check more about prompt caching with Anthropic models here: https://docs.anthropic.com/en/docs/prompt-caching
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.utils.media import download_file

# Load an example large system message from S3. A large prompt like this would benefit from caching.
txt_path = Path(__file__).parent.joinpath("system_prompt.txt")
download_file(
    "https://agno-public.s3.amazonaws.com/prompts/system_promt.txt",
    str(txt_path),
)
system_message = txt_path.read_text()

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        cache_system_prompt=True,  # Activate prompt caching for Anthropic to cache the system prompt
    ),
    system_message=system_message,
    markdown=True,
)

# First run - this will create the cache
response = agent.run(
    "Explain the difference between REST and GraphQL APIs with examples"
)
print(f"First run cache write tokens = {response.metrics['cache_write_tokens']}")  # type: ignore

# Second run - this will use the cached system prompt
response = agent.run(
    "What are the key principles of clean code and how do I apply them in Python?"
)
print(f"Second run cache read tokens = {response.metrics['cached_tokens']}")  # type: ignore



================================================
FILE: cookbook/models/anthropic/prompt_caching_extended.py
================================================
"""
This cookbook shows how to extend caching time for agents using cache with Anthropic models.

You can check more about extended prompt caching with Anthropic models here: https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#1-hour-cache-duration-beta
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.utils.media import download_file

# Load an example large system message from S3. A large prompt like this would benefit from caching.
txt_path = Path(__file__).parent.joinpath("system_promt.txt")
download_file(
    "https://agno-public.s3.amazonaws.com/prompts/system_promt.txt",
    str(txt_path),
)
system_message = txt_path.read_text()

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        default_headers={"anthropic-beta": "extended-cache-ttl-2025-04-11"},
        system_prompt=system_message,
        cache_system_prompt=True,  # Activate prompt caching for Anthropic to cache the system prompt
        extended_cache_time=True,  # Extend the cache time from the default to 1 hour
    ),
    system_message=system_message,
    markdown=True,
)

# First run - this will create the cache
response = agent.run(
    "Explain the difference between REST and GraphQL APIs with examples"
)
print(f"First run cache write tokens = {response.metrics['cache_write_tokens']}")

# Second run - this will use the cached system prompt
response = agent.run(
    "What are the key principles of clean code and how do I apply them in Python?"
)
print(f"Second run cache read tokens = {response.metrics['cached_tokens']}")



================================================
FILE: cookbook/models/anthropic/storage.py
================================================
"""Run `pip install ddgs sqlalchemy anthropic` to install dependencies."""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/anthropic/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.anthropic import Claude
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


movie_agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
run: RunResponse = movie_agent.run("New York")
pprint(run.content)



================================================
FILE: cookbook/models/anthropic/structured_output_stream.py
================================================
from typing import Dict, List

from agno.agent import Agent
from agno.models.anthropic import Claude
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

structured_output_agent.print_response(
    "New York", stream=True, stream_intermediate_steps=True
)



================================================
FILE: cookbook/models/anthropic/thinking.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-3-7-sonnet-20250219",
        max_tokens=2048,
        thinking={"type": "enabled", "budget_tokens": 1024},
    ),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Share a very scary 2 sentence horror story")



================================================
FILE: cookbook/models/anthropic/thinking_stream.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-3-7-sonnet-20250219",
        max_tokens=2048,
        thinking={"type": "enabled", "budget_tokens": 1024},
    ),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Share a very scary 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/anthropic/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/anthropic/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/anthropic/web_search.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
    ),
    tools=[
        {
            "type": "web_search_20250305",
            "name": "web_search",
            "max_uses": 5,
        }
    ],
    markdown=True,
)

agent.print_response("What's the latest with Anthropic?", stream=True)



================================================
FILE: cookbook/models/aws/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/aws/bedrock/README.md
================================================
# AWS Bedrock Anthropic Claude

[Models overview](https://docs.anthropic.com/claude/docs/models-overview)

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your AWS Credentials

#### 2.A: Leverage Access and Secret Access Keys
```shell
export AWS_ACCESS_KEY_ID=***
export AWS_SECRET_ACCESS_KEY=***
export AWS_REGION=***
```

Alternatively, you can use an AWS profile:

```python
import boto3
session = boto3.Session(profile_name='MY-PROFILE')
agent = Agent(
    model=AwsBedrock(id="mistral.mistral-small-2402-v1:0", session=session),
    markdown=True
)
```

#### 2.B: Leverage AWS SSO Credentials
Log in through the aws sso login command to get access to your account
```shell
aws sso login
```

Leverage sso settings in the AwsBedrock object to leverage the credentials provided by sso
```python
import boto3
agent = Agent(
    model=AwsBedrock(id="mistral.mistral-small-2402-v1:0", aws_sso_auth= True),
    markdown=True
)
```


### 3. Install libraries

```shell
pip install -U boto3 ddgs agno
```

### 4. Run basic agent

- Streaming on

```shell
python cookbook/models/aws/bedrock/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/aws/bedrock/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/aws/bedrock/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/aws/bedrock/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/aws/bedrock/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/aws/bedrock/knowledge.py
```



================================================
FILE: cookbook/models/aws/bedrock/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/aws/bedrock/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.aws import AwsBedrock

agent = Agent(model=AwsBedrock(id="mistral.mistral-small-2402-v1:0"), markdown=True)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/aws/bedrock/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.aws import AwsBedrock

agent = Agent(model=AwsBedrock(id="mistral.mistral-small-2402-v1:0"), markdown=True)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/aws/bedrock/async_tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AwsBedrock(id="amazon.nova-lite-v1:0"),
    tools=[DuckDuckGoTools()],
    instructions="You are a helpful assistant that can use the following tools to answer questions.",
    show_tool_calls=True,
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/aws/bedrock/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.aws import AwsBedrock

agent = Agent(model=AwsBedrock(id="mistral.mistral-small-2402-v1:0"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/aws/bedrock/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.aws import AwsBedrock

agent = Agent(model=AwsBedrock(id="mistral.mistral-small-2402-v1:0"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/aws/bedrock/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=AwsBedrock(id="amazon.nova-pro-v1:0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes, format="jpeg"),
    ],
)



================================================
FILE: cookbook/models/aws/bedrock/pdf_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.aws import AwsBedrock
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=AwsBedrock(id="amazon.nova-pro-v1:0"),
    markdown=True,
)

pdf_bytes = pdf_path.read_bytes()

agent.print_response(
    "Give the recipe of Gaeng Kiew Wan Goong",
    files=[File(content=pdf_bytes, format="pdf", name="Thai Recipes")],
)



================================================
FILE: cookbook/models/aws/bedrock/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.aws import AwsBedrock
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


movie_agent = Agent(
    model=AwsBedrock(id="mistral.mistral-large-2402-v1:0"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
# movie_agent: RunResponse = movie_agent.run("New York")
# pprint(movie_agent.content)

movie_agent.print_response("New York")



================================================
FILE: cookbook/models/aws/bedrock/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AwsBedrock(id="mistral.mistral-large-2402-v1:0"),
    tools=[DuckDuckGoTools()],
    instructions="You are a helpful assistant that can use the following tools to answer questions.",
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/aws/bedrock/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AwsBedrock(id="amazon.nova-lite-v1:0"),
    tools=[DuckDuckGoTools()],
    instructions="You are a helpful assistant that can use the following tools to answer questions.",
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/aws/claude/README.md
================================================
# AWS Bedrock Anthropic Claude

[Models overview](https://docs.anthropic.com/claude/docs/models-overview)

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your AWS Credentials

```shell
export AWS_ACCESS_KEY_ID=***
export AWS_SECRET_ACCESS_KEY=***
export AWS_REGION=***
```

Alternatively, you can use an AWS profile:

```python
import boto3
session = boto3.Session(profile_name='MY-PROFILE')
agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0", session=session),
    markdown=True
)
```

### 3. Install libraries

```shell
pip install -U anthropic ddgs agno
```

### 4. Run basic agent

- Streaming on

```shell
python cookbook/models/aws/claude/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/aws/claude/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/aws/claude/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/aws/claude/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/aws/claude/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/aws/claude/knowledge.py
```



================================================
FILE: cookbook/models/aws/claude/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/aws/claude/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.aws import Claude

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), markdown=True
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/aws/claude/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.aws import Claude

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/aws/claude/async_tool_use.py
================================================
"""
Async example using Claude with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.aws import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/aws/claude/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.aws import Claude

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), markdown=True
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/aws/claude/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.aws import Claude

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/aws/claude/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.aws import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and search the web for more information.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
        ),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/aws/claude/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai anthropic` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.aws import Claude
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/aws/claude/storage.py
================================================
"""Run `pip install ddgs sqlalchemy anthropic` to install dependencies."""

from agno.agent import Agent
from agno.models.aws import Claude
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/aws/claude/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.aws import Claude
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


movie_agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
# movie_agent: RunResponse = movie_agent.run("New York")
# pprint(movie_agent.content)

movie_agent.print_response("New York")



================================================
FILE: cookbook/models/aws/claude/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aws import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/azure/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/azure/ai_foundry/README.md
================================================
# Azure AI Interface Cookbook

> Note: Fork and clone this repository if needed
>
> Note: This cookbook is for the Azure AI Interface model. It uses the `AzureAIFoundry` class with the `Phi-4` model. Please change the model ID to the one you want to use.

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export environment variables

Navigate to the Azure AI Foundry on the [Azure Portal](https://portal.azure.com/) and create a service. Then, using the Azure AI Foundry portal, create a deployment and set your environment variables.

```shell
export AZURE_API_KEY=***
export AZURE_ENDPOINT="https://<your-host-name>.services.ai.azure.com/models"
export AZURE_API_VERSION="2024-05-01-preview"
```

You can get the endpoint from the [Azure AI Foundry portal](https://ai.azure.com/). Click on the deployed model and copy the "Target URI"

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/azure/openai/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/azure/openai/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/azure/openai/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/azure/openai/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/azure/openai/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/azure/openai/knowledge.py
```



================================================
FILE: cookbook/models/azure/ai_foundry/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/azure/ai_foundry/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry

agent = Agent(
    model=AzureAIFoundry(id="Phi-4"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))



================================================
FILE: cookbook/models/azure/ai_foundry/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry

assistant = Agent(
    model=AzureAIFoundry(id="Phi-4"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(
    assistant.aprint_response("Share a breakfast recipe.", markdown=True, stream=True)
)



================================================
FILE: cookbook/models/azure/ai_foundry/async_tool_use.py
================================================
"""
Async example using Claude with tool calls.
"""

import asyncio
from pprint import pprint

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry
from agno.run.response import RunResponse
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AzureAIFoundry(id="Cohere-command-r-08-2024"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))



================================================
FILE: cookbook/models/azure/ai_foundry/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.azure import AzureAIFoundry

agent = Agent(model=AzureAIFoundry(id="Phi-4"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/azure/ai_foundry/basic_stream.py
================================================
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.azure import AzureAIFoundry

agent = Agent(
    model=AzureAIFoundry(
        id="Phi-4",
        azure_endpoint="",
    ),
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/azure/ai_foundry/demo_cohere.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.azure import AzureAIFoundry

agent = Agent(model=AzureAIFoundry(id="Cohere-command-r-08-2024"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/azure/ai_foundry/demo_mistral.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.azure import AzureAIFoundry

agent = Agent(model=AzureAIFoundry(id="Mistral-Large-2411"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/azure/ai_foundry/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.azure import AzureAIFoundry

agent = Agent(
    model=AzureAIFoundry(id="Llama-3.2-11B-Vision-Instruct"),
    markdown=True,
)

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(
            url="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/sdk/ai/azure-ai-inference/samples/sample1.png",
            detail="high",
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/azure/ai_foundry/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.azure import AzureAIFoundry
from agno.utils.media import download_image

agent = Agent(
    model=AzureAIFoundry(id="Llama-3.2-11B-Vision-Instruct"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/azure/ai_foundry/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.azure import AzureAIFoundry
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=AzureOpenAIEmbedder(),
    ),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    model=AzureAIFoundry(id="Cohere-command-r-08-2024"),
    knowledge=knowledge_base,
    show_tool_calls=True,
    debug_mode=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/azure/ai_foundry/storage.py
================================================
"""Run `pip install ddgs sqlalchemy anthropic` to install dependencies."""

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=AzureAIFoundry(id="Phi-4"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/azure/ai_foundry/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.azure import AzureAIFoundry
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=AzureAIFoundry(id="gpt-4o"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
    # debug_mode=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("New York")
# pprint(run.content)

agent.print_response("New York")



================================================
FILE: cookbook/models/azure/ai_foundry/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AzureAIFoundry(id="Cohere-command-r-08-2024"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("What is currently happening in France?", stream=True)



================================================
FILE: cookbook/models/azure/openai/README.md
================================================
# Azure OpenAI Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export environment variables

Navigate to the AzureOpenAI on the [Azure Portal](https://portal.azure.com/) and create a service. Then, using the Azure AI Foundry portal, create a deployment and set your environment variables.

```shell
export AZURE_OPENAI_API_KEY=***
export AZURE_OPENAI_ENDPOINT="https://<your-resource-name>.openai.azure.com/openai/deployments/<your-deployment-name>"
export AZURE_API_VERSION="2024-10-21"  # Optional
export AZURE_DEPLOYMENT=***  # Optional
```

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/azure/openai/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/azure/openai/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/azure/openai/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/azure/openai/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/azure/openai/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/azure/openai/knowledge.py
```



================================================
FILE: cookbook/models/azure/openai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/azure/openai/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.azure import AzureOpenAI

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))



================================================
FILE: cookbook/models/azure/openai/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.azure import AzureOpenAI

assistant = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(
    assistant.aprint_response("Share a breakfast recipe.", markdown=True, stream=True)
)



================================================
FILE: cookbook/models/azure/openai/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.azure import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="gpt-4o-mini"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/azure/openai/basic_stream.py
================================================
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.azure import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="gpt-4o-mini"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/azure/openai/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.azure import AzureOpenAI
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=AzureOpenAIEmbedder(),
    ),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    knowledge=knowledge_base,
    show_tool_calls=True,
    debug_mode=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/azure/openai/storage.py
================================================
"""Run `pip install ddgs sqlalchemy anthropic` to install dependencies."""

from agno.agent import Agent
from agno.models.azure import AzureOpenAI
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/azure/openai/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.azure import AzureOpenAI
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
run: RunResponse = agent.run("New York")
pprint(run.content)

# agent.print_response("New York")



================================================
FILE: cookbook/models/azure/openai/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.azure import AzureOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/cerebras/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/cerebras/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("write a two sentence horror story"))



================================================
FILE: cookbook/models/cerebras/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("write a two sentence horror story", stream=True))



================================================
FILE: cookbook/models/cerebras/async_tool_use.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))



================================================
FILE: cookbook/models/cerebras/async_tool_use_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/cerebras/basic.py
================================================
from agno.agent import Agent
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story")



================================================
FILE: cookbook/models/cerebras/basic_stream.py
================================================
from agno.agent import Agent, RunResponse  # noqa
import asyncio
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story", stream=True)



================================================
FILE: cookbook/models/cerebras/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.cerebras import Cerebras
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/cerebras/storage.py
================================================
"""Run `pip install ddgs sqlalchemy cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/cerebras/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.cerebras import Cerebras
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a JSON schema output
json_schema_output_agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    response_model=MovieScript,
    show_tool_calls=True,
    debug_mode=True,
)

json_schema_output_agent.print_response("New York")



================================================
FILE: cookbook/models/cerebras/tool_use.py
================================================
from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/cerebras/tool_use_stream.py
================================================
from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(id="llama-3.3-70b"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/cerebras_openai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/cerebras_openai/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("write a two sentence horror story"))



================================================
FILE: cookbook/models/cerebras_openai/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("write a two sentence horror story", stream=True))



================================================
FILE: cookbook/models/cerebras_openai/async_tool_use.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(id="llama-3.3-70b"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))



================================================
FILE: cookbook/models/cerebras_openai/async_tool_use_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(id="llama-3.3-70b"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/cerebras_openai/basic.py
================================================
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story")



================================================
FILE: cookbook/models/cerebras_openai/basic_stream.py
================================================
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story", stream=True)



================================================
FILE: cookbook/models/cerebras_openai/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.cerebras import CerebrasOpenAI
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/cerebras_openai/storage.py
================================================
"""Run `pip install ddgs sqlalchemy cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    debug_mode=True,
    show_tool_calls=True,
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/cerebras_openai/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.cerebras import CerebrasOpenAI
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a structured output
structured_output_agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    response_model=MovieScript,
)

structured_output_agent.print_response("New York")



================================================
FILE: cookbook/models/cerebras_openai/tool_use.py
================================================
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/cerebras_openai/tool_use_stream.py
================================================
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/cohere/README.md
================================================
# Cohere Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `CO_API_KEY`

```shell
export CO_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U cohere ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/cohere/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/cohere/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/cohere/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/cohere/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/cohere/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/cohere/knowledge.py
```

### 9. Run Agent that uses memory

```shell
python cookbook/models/cohere/memory.py
```



================================================
FILE: cookbook/models/cohere/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/cohere/async_basic.py
================================================
"""
Basic async example using Cohere.
"""

import asyncio

from agno.agent import Agent
from agno.models.cohere import Cohere

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/cohere/async_basic_stream.py
================================================
"""
Basic streaming async example using Cohere.
"""

import asyncio

from agno.agent import Agent
from agno.models.cohere import Cohere

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/cohere/async_structured_output.py
================================================
import asyncio
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.cohere import Cohere
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=Cohere(
        id="command-a-03-2025",
    ),
    tools=[DuckDuckGoTools()],
    description="You help people write movie scripts.",
    response_model=MovieScript,
    show_tool_calls=True,
    debug_mode=True,
)

# Get the response in a variable
# response: RunResponse = await agent.arun("New York")
# pprint(response.content)

asyncio.run(agent.aprint_response("Find a cool movie idea about London and write it."))



================================================
FILE: cookbook/models/cohere/async_tool_use.py
================================================
"""
Async example using Cohere with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.cohere import Cohere
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/cohere/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.cohere import Cohere

agent = Agent(model=Cohere(id="command-a-03-2025"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/cohere/basic_stream.py
================================================
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.cohere import Cohere

agent = Agent(model=Cohere(id="command-a-03-2025"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/cohere/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.cohere import Cohere

agent = Agent(
    model=Cohere(id="c4ai-aya-vision-8b"),
    markdown=True,
)

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/cohere/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.cohere.chat import Cohere
from agno.utils.media import download_image

agent = Agent(
    model=Cohere(id="c4ai-aya-vision-8b"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/cohere/image_agent_local_file.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.cohere.chat import Cohere
from agno.utils.media import download_image

agent = Agent(
    model=Cohere(id="c4ai-aya-vision-8b"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(filepath=image_path),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/cohere/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai cohere` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.cohere import Cohere
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/cohere/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install cohere sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/cohere/memory.py` to run the agent
"""

from agno.agent.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.cohere import Cohere
from agno.storage.postgres import PostgresStorage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresStorage(table_name="personalized_agent_sessions", db_url=db_url),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/cohere/storage.py
================================================
"""Run `pip install ddgs sqlalchemy cohere` to install dependencies."""

from agno.agent import Agent
from agno.models.cohere import Cohere
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/cohere/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.cohere import Cohere
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


structured_output_agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
response: RunResponse = structured_output_agent.run("New York")
pprint(response.content)



================================================
FILE: cookbook/models/cohere/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.cohere import Cohere
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/dashscope/README.md
================================================
# DashScope Cookbook

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `DASHSCOPE_API_KEY` or `QWEN_API_KEY`

Get your API key from: https://modelstudio.console.alibabacloud.com/?tab=model#/api-key

```shell
export DASHSCOPE_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/dashscope/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/dashscope/basic.py
```

### 5. Run async Agent

- Async basic

```shell
python cookbook/models/dashscope/async_basic.py
```

- Async streaming

```shell
python cookbook/models/dashscope/async_basic_stream.py
```

### 6. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/dashscope/tool_use.py
```

- Async tool use

```shell
python cookbook/models/dashscope/async_tool_use.py
```

### 7. Run Agent that returns structured output

```shell
python cookbook/models/dashscope/structured_output.py
```

### 8. Run Agent that analyzes images

- Basic image analysis

```shell
python cookbook/models/dashscope/image_agent.py
```

- Image analysis with bytes

```shell
python cookbook/models/dashscope/image_agent_bytes.py
```

- Async image analysis

```shell
python cookbook/models/dashscope/async_image_agent.py
```

For more information about Qwen models and capabilities, visit:
- [Model Studio Console](https://modelstudio.console.alibabacloud.com/)



================================================
FILE: cookbook/models/dashscope/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.qwen import Qwen

agent = Agent(model=Qwen(id="qwen-plus", temperature=0.5), markdown=True)

# Get the response in a variable
# run: RunResponse = await agent.arun("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/dashscope/async_basic_stream.py
================================================
import asyncio
from typing import AsyncIterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.qwen import Qwen

agent = Agent(model=Qwen(id="qwen-plus", temperature=0.5), markdown=True)


async def main():
    # Get the response in a variable
    # async for chunk in agent.arun("Share a 2 sentence horror story", stream=True):
    #     print(chunk.content, end="", flush=True)

    # Print the response in the terminal
    await agent.aprint_response("Share a 2 sentence horror story", stream=True)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/models/dashscope/async_image_agent.py
================================================
import asyncio

from agno.agent import Agent
from agno.media import Image
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DashScope(id="qwen-vl-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)


async def main():
    await agent.aprint_response(
        "What do you see in this image? Provide a detailed description and search for related information.",
        images=[
            Image(
                url="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg"
            )
        ],
        stream=True,
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/models/dashscope/async_tool_use.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DashScope(id="qwen-plus"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)


async def main():
    await agent.aprint_response(
        "What's the latest news about artificial intelligence?", stream=True
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/models/dashscope/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.dashscope import DashScope

agent = Agent(model=DashScope(id="qwen-plus", temperature=0.5), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/dashscope/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.dashscope import DashScope

agent = Agent(model=DashScope(id="qwen-plus", temperature=0.5), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/dashscope/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DashScope(id="qwen-vl-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Analyze this image in detail and tell me what you see. Also search for more information about the subject.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/dashscope/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=DashScope(id="qwen-vl-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Analyze this image of an ant. Describe its features, species characteristics, and search for more information about this type of ant.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/dashscope/structured_output.py
================================================
from typing import List

from agno.agent import Agent
from agno.models.dashscope import DashScope
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=DashScope(id="qwen-plus"),
    description="You write movie scripts and return them as structured JSON data.",
    response_model=MovieScript,
)

structured_output_agent.print_response(
    "Create a movie script about llamas ruling the world. "
    "Return a JSON object with: name (movie title), setting, ending, genre, "
    "characters (list of character names), and storyline (3 sentences)."
)



================================================
FILE: cookbook/models/dashscope/thinking_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.qwen import Qwen

agent = Agent(
    model=Qwen(id="qvq-max", enable_thinking=True),
)

image_url = "https://img.alicdn.com/imgextra/i1/O1CN01gDEY8M1W114Hi3XcN_!!6000000002727-0-tps-1024-406.jpg"

agent.print_response(
    "How do I solve this problem? Please think through each step carefully.",
    images=[Image(url=image_url)],
    stream=True,
)



================================================
FILE: cookbook/models/dashscope/tool_use.py
================================================
from agno.agent import Agent
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DashScope(id="qwen-plus"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("What's happening in AI today?", stream=True)



================================================
FILE: cookbook/models/deepinfra/README.md
================================================
# DeepInfra Cookbook

> Note: Fork and clone this repository if needed

> Note: DeepInfra does not appear to include models that support structured outputs.

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `DEEPINFRA_API_KEY`

```shell
export DEEPINFRA_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/deepinfra/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/deepinfra/basic.py
```

### 5. Run Async Agent

- Streaming on

```shell
python cookbook/models/deepinfra/async_basic_stream.py
```

- Streaming off

```shell
python cookbook/models/deepinfra/async_basic.py
```

### 6. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/deepinfra/tool_use.py
```

- Async DuckDuckGo Search

```shell
python cookbook/models/deepinfra/async_tool_use.py
```

### 6. Run Agent that returns JSON output defined by the response model

```shell
python cookbook/models/deepinfra/json_output.py
```



================================================
FILE: cookbook/models/deepinfra/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/deepinfra/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.deepinfra import DeepInfra  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    markdown=True,
)

# Get the response in a variable
# def run_async() -> RunResponse:
#     return agent.arun("Share a 2 sentence horror story")
# response = asyncio.run(run_async())
# print(response.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/deepinfra/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.deepinfra import DeepInfra  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/deepinfra/async_tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent  # noqa
from agno.models.deepinfra import DeepInfra  # noqa
from agno.tools.duckduckgo import DuckDuckGoTools  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

asyncio.run(agent.aprint_response("What's the latest news about AI?", stream=True))



================================================
FILE: cookbook/models/deepinfra/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.deepinfra import DeepInfra  # noqa


agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    markdown=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/deepinfra/basic_stream.py
================================================
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.deepinfra import DeepInfra  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    markdown=True,
)

# Get the response in a variable
run_response: Iterator[RunResponseEvent] = agent.run(
    "Share a 2 sentence horror story", stream=True
)
for chunk in run_response:
    print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/deepinfra/json_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.deepinfra import DeepInfra  # noqa
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
agent = Agent(
    model=DeepInfra(id="microsoft/phi-4"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
# response: RunResponse = agent.run("New York")
# pprint(response.content)

agent.print_response("New York")



================================================
FILE: cookbook/models/deepinfra/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent  # noqa
from agno.models.deepinfra import DeepInfra  # noqa
from agno.tools.duckduckgo import DuckDuckGoTools  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/deepseek/README.md
================================================
# DeepSeek Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `DEEPSEEK_API_KEY`

```shell
export DEEPSEEK_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/deepseek/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/deepseek/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/deepseek/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/deepseek/structured_output.py
```




================================================
FILE: cookbook/models/deepseek/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/deepseek/async_basic.py
================================================
"""
Basic async example using DeepSeek.
"""

import asyncio

from agno.agent import Agent
from agno.models.deepseek import DeepSeek

agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/deepseek/async_basic_streaming.py
================================================
"""
Basic streaming async example using DeepSeek.
"""

import asyncio

from agno.agent import Agent
from agno.models.deepseek import DeepSeek

agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/deepseek/async_tool_use.py
================================================
"""
Async example using DeepSeek with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/deepseek/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.deepseek import DeepSeek

agent = Agent(model=DeepSeek(id="deepseek-chat"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/deepseek/basic_stream.py
================================================
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.deepseek import DeepSeek

agent = Agent(model=DeepSeek(id="deepseek-chat"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/deepseek/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.deepseek import DeepSeek
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
json_mode_response: RunResponse = json_mode_agent.run("New York")
pprint(json_mode_response.content)

# json_mode_agent.print_response("New York")



================================================
FILE: cookbook/models/deepseek/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.tools.duckduckgo import DuckDuckGoTools

"""
The current version of the deepseek-chat model's Function Calling capabilitity is unstable, which may result in looped calls or empty responses.
Their development team is actively working on a fix, and it is expected to be resolved in the next version.
"""

agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/fireworks/README.md
================================================
# Fireworks AI Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `FIREWORKS_API_KEY`

```shell
export FIREWORKS_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/fireworks/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/fireworks/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/fireworks/tool_use.py
```


### 6. Run Agent that returns structured output

```shell
python cookbook/models/fireworks/structured_output.py
```





================================================
FILE: cookbook/models/fireworks/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/fireworks/async_basic.py
================================================
"""
Basic async example using Fireworks.
"""

import asyncio

from agno.agent import Agent
from agno.models.fireworks import Fireworks

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/fireworks/async_basic_stream.py
================================================
"""
Basic streaming async example using Fireworks.
"""

import asyncio

from agno.agent import Agent
from agno.models.fireworks import Fireworks

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/fireworks/async_tool_use.py
================================================
"""
Async example using Fireworks with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.fireworks import Fireworks
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/fireworks/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.fireworks import Fireworks

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    markdown=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/fireworks/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.fireworks import Fireworks

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/fireworks/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.fireworks import Fireworks
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
response: RunResponse = agent.run("New York")
pprint(response.content)

# agent.print_response("New York")



================================================
FILE: cookbook/models/fireworks/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.fireworks import Fireworks
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/google/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/google/gemini/README.md
================================================
# Google Gemini Cookbook

> Note: Fork and clone this repository if needed
>
> This cookbook is for testing Gemini models.

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export environment variables

If you want to use the Gemini API, you need to export the following environment variables:

```shell
export GOOGLE_API_KEY=***
```

If you want to use Vertex AI, you need to export the following environment variables:

```shell
export GOOGLE_GENAI_USE_VERTEXAI="true"
export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_CLOUD_LOCATION="your-location"
```

### 3. Install libraries

```shell
pip install -U google-generativeai ddgs yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/google/gemini/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/google/gemini/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Agent

```shell
python cookbook/models/google/gemini/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/google/gemini/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/google/gemini/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/google/gemini/knowledge.py
```

### 9. Run Agent that interprets an audio file

```shell
python cookbook/models/google/gemini/audio_input_bytes_content.py
```

### 10. Run Agent that analyzes an image

```shell
python cookbook/models/google/gemini/image_agent.py
```

or

```shell
python cookbook/models/google/gemini/image_agent_file_upload.py
```

### 11. Run Agent that analyzes a video

```shell
python cookbook/models/google/gemini/video_agent_input_bytes_content.py
```

### 12. Run Agent that uses flash thinking mode from Gemini

```shell
python cookbook/models/google/gemini/flash_thinking_agent.py
```

### 13. Run Agent with thinking budget configuration

```shell
python cookbook/models/google/gemini/agent_with_thinking_budget.py
```

### 14. Run agent with URL context

```shell
python cookbook/models/google/gemini/url_context.py
```

### 15. Run agent with URL context + Search Grounding

```shell
python cookbook/models/google/gemini/url_context_with_search.py
```

### 16. Run agent with Google Search

```shell
python cookbook/models/google/gemini/search.py
```

### 17. Run agent with Google Search Grounding

```shell
python cookbook/models/google/gemini/grounding.py
```

### 18. Run agent with Vertex AI Search

```shell
python cookbook/models/google/gemini/vertex_ai_search.py
```




================================================
FILE: cookbook/models/google/gemini/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/google/gemini/agent_with_thinking_budget.py
================================================
"""
An example of how to use the thinking budget parameter with the Gemini model.
This requires `google-genai > 1.10.0`

- Turn off thinking use thinking_budget=0
- Turn on dynamic thinking use thinking_budget=-1
- To use a specific thinking token budget (e.g. 1280) use thinking_budget=1280
- Use include_thoughts=True to get the thought summaries in the response.
"""

from agno.agent import Agent
from agno.models.google import Gemini

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

agent = Agent(
    model=Gemini(id="gemini-2.5-pro", thinking_budget=1280, include_thoughts=True),
    markdown=True,
)
agent.print_response(task, stream=True)



================================================
FILE: cookbook/models/google/gemini/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-001",
        instructions=["You are a basic agent that writes short stories."],
    ),
    markdown=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/google/gemini/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/google/gemini/async_image_editing.py
================================================
import asyncio
from io import BytesIO

from agno.agent import Agent, RunResponse  # noqa
from agno.media import Image
from agno.models.google import Gemini
from PIL import Image as PILImage

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)


async def modify_image():
    # Print the response in the terminal - using arun instead of run
    response = await agent.arun(
        "Can you add a Llama in the background of this image?",
        images=[Image(filepath="generated_image.png")],
    )

    images = agent.get_images()
    if images and isinstance(images, list):
        for image_response in images:
            image_bytes = image_response.content
            image = PILImage.open(BytesIO(image_bytes))
            image.show()
            # Save the image to a file
            # image.save("generated_image.png")


if __name__ == "__main__":
    asyncio.run(modify_image())



================================================
FILE: cookbook/models/google/gemini/async_image_generation.py
================================================
import asyncio
from io import BytesIO

from agno.agent import Agent, RunResponse  # noqa
from agno.models.google import Gemini
from PIL import Image

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)


async def generate_image():
    # Print the response in the terminal - using arun instead of run
    response = await agent.arun("Make me an image of a cat in a tree.")

    images = agent.get_images()
    if images and isinstance(images, list):
        for image_response in images:
            image_bytes = image_response.content
            image = Image.open(BytesIO(image_bytes))
            image.show()
            # Save the image to a file
            # image.save("generated_image.png")


if __name__ == "__main__":
    asyncio.run(generate_image())



================================================
FILE: cookbook/models/google/gemini/async_image_generation_stream.py
================================================
import asyncio
from io import BytesIO

from agno.agent import Agent, RunResponse  # noqa
from agno.models.google import Gemini
from PIL import Image

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)


async def generate_image():
    response_stream = await agent.arun(
        "Make me an image of a cat in a tree.", stream=True
    )
    async for chunk in response_stream:
        if chunk.images:
            images = chunk.images
            if images and isinstance(images, list):
                for image_response in images:
                    image_bytes = image_response.content
                    image = Image.open(BytesIO(image_bytes))
                    image.show()
                    # image.save("generated_image.png")


if __name__ == "__main__":
    asyncio.run(generate_image())



================================================
FILE: cookbook/models/google/gemini/async_tool_use.py
================================================
"""
Async example using Gemini with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/google/gemini/audio_input_bytes_content.py
================================================
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"

# Download the audio file from the URL as bytes
response = requests.get(url)
audio_content = response.content

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(content=audio_content)],
)



================================================
FILE: cookbook/models/google/gemini/audio_input_file_upload.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

model = Gemini(id="gemini-2.0-flash-exp")
agent = Agent(
    model=model,
    markdown=True,
)

# Please download a sample audio file to test this Agent and upload using:
audio_path = Path(__file__).parent.joinpath("sample.mp3")
audio_file = None

remote_file_name = f"files/{audio_path.stem.lower()}"
try:
    audio_file = model.get_client().files.get(name=remote_file_name)
except Exception as e:
    print(f"Error getting file {audio_path.stem}: {e}")
    pass

if not audio_file:
    try:
        audio_file = model.get_client().files.upload(
            file=audio_path,
            config=dict(name=audio_path.stem, display_name=audio_path.stem),
        )
        print(f"Uploaded audio: {audio_file}")
    except Exception as e:
        print(f"Error uploading audio: {e}")

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(content=audio_file)],
    stream=True,
)



================================================
FILE: cookbook/models/google/gemini/audio_input_local_file_upload.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

# Please download a sample audio file to test this Agent and upload using:
audio_path = Path(__file__).parent.joinpath("sample.mp3")

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(filepath=audio_path)],
    stream=True,
)



================================================
FILE: cookbook/models/google/gemini/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/google/gemini/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/google/gemini/file_upload_with_cache.py
================================================
"""
In this example, we upload a text file to Google and then create a cache.

This greatly saves on tokens during normal prompting.
"""

from pathlib import Path
from time import sleep

import requests
from agno.agent import Agent
from agno.models.google import Gemini
from google import genai

client = genai.Client()

# Download txt file
url = "https://storage.googleapis.com/generativeai-downloads/data/a11.txt"
path_to_txt_file = Path(__file__).parent.joinpath("a11.txt")
if not path_to_txt_file.exists():
    print("Downloading txt file...")
    with path_to_txt_file.open("wb") as wf:
        response = requests.get(url, stream=True)
        for chunk in response.iter_content(chunk_size=32768):
            wf.write(chunk)

# Upload the txt file using the Files API
remote_file_path = Path("a11.txt")
remote_file_name = f"files/{remote_file_path.stem.lower().replace('_', '-')}"

txt_file = None
try:
    txt_file = client.files.get(name=remote_file_name)
    print(f"Txt file exists: {txt_file.uri}")
except Exception as e:
    pass

if not txt_file:
    print("Uploading txt file...")
    txt_file = client.files.upload(
        file=path_to_txt_file, config=dict(name=remote_file_name)
    )

    # Wait for the file to finish processing
    while txt_file.state.name == "PROCESSING":
        print("Waiting for txt file to be processed.")
        sleep(2)
        txt_file = client.files.get(name=remote_file_name)

    print(f"Txt file processing complete: {txt_file.uri}")

# Create a cache with 5min TTL
cache = client.caches.create(
    model="gemini-2.0-flash-001",
    config={
        "system_instruction": "You are an expert at analyzing transcripts.",
        "contents": [txt_file],
        "ttl": "300s",
    },
)


if __name__ == "__main__":
    agent = Agent(
        model=Gemini(id="gemini-2.0-flash-001", cached_content=cache.name),
    )
    agent.print_response(
        "Find a lighthearted moment from this transcript",  # No need to pass the txt file
    )

    print("Metrics: ", agent.run_response.metrics)



================================================
FILE: cookbook/models/google/gemini/grounding.py
================================================
"""Grounding with Gemini.

Grounding enables Gemini to search the web and provide responses backed by
real-time information with citations. This is a legacy tool - for Gemini 2.0+
models, consider using the 'search' parameter instead.

Run `pip install google-generativeai` to install dependencies.
"""

from agno.agent import Agent
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash",
        grounding=True,
        grounding_dynamic_threshold=0.7,  # Optional: set threshold for grounding
    ),
    add_datetime_to_instructions=True,
)

# Ask questions that benefit from real-time information
agent.print_response(
    "What are the current market trends in renewable energy?",
    stream=True,
    markdown=True,
)



================================================
FILE: cookbook/models/google/gemini/image_editing.py
================================================
from io import BytesIO

from agno.agent import Agent, RunResponse  # noqa
from agno.media import Image
from agno.models.google import Gemini
from PIL import Image as PILImage

# No system message should be provided (Gemini requires only the image)
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

# Print the response in the terminal
response = agent.run(
    "Can you add a Llama in the background of this image?",
    images=[Image(filepath="tmp/test_photo.png")],
)

images = agent.get_images()
if images and isinstance(images, list):
    for image_response in images:
        image_bytes = image_response.content
        image = PILImage.open(BytesIO(image_bytes))
        image.show()
        # Save the image to a file
        # image.save("generated_image.png")



================================================
FILE: cookbook/models/google/gemini/image_generation.py
================================================
from io import BytesIO

from agno.agent import Agent, RunResponse  # noqa
from agno.models.google import Gemini
from PIL import Image

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

# Print the response in the terminal
response = agent.run("Make me an image of a cat in a tree.")

images = agent.get_images()
if images and isinstance(images, list):
    for image_response in images:
        image_bytes = image_response.content
        image = Image.open(BytesIO(image_bytes))
        image.show()
        # Save the image to a file
        # image.save("generated_image.png")



================================================
FILE: cookbook/models/google/gemini/image_generation_stream.py
================================================
from io import BytesIO

from agno.agent import Agent, RunResponse  # noqa
from agno.models.google import Gemini
from PIL import Image

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

# Print the response in the terminal
response = agent.run("Make me an image of a cat in a tree.", stream=True)

for chunk in response:
    if chunk.images:
        images = chunk.images
        if images and isinstance(images, list):
            for image_response in images:
                image_bytes = image_response.content
                image = Image.open(BytesIO(image_bytes))
                image.show()
                # Save the image to a file
                # image.save("generated_image.png")



================================================
FILE: cookbook/models/google/gemini/image_input.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/b/bf/Krakow_-_Kosciol_Mariacki.jpg"
        ),
    ],
)



================================================
FILE: cookbook/models/google/gemini/image_input_file_upload.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from google.generativeai import upload_file
from google.generativeai.types import file_types

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
# Please download the image using
# wget https://upload.wikimedia.org/wikipedia/commons/b/bf/Krakow_-_Kosciol_Mariacki.jpg
image_path = Path(__file__).parent.joinpath("Krakow_-_Kosciol_Mariacki.jpg")
image_file: file_types.File = upload_file(image_path)
print(f"Uploaded image: {image_file}")

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[Image(content=image_file)],
    stream=True,
)



================================================
FILE: cookbook/models/google/gemini/imagen_tool.py
================================================
"""🔧 Example: Using the GeminiTools Toolkit for Image Generation

Make sure you have set the GOOGLE_API_KEY environment variable.
Example prompts to try:
- "Create a surreal painting of a floating city in the clouds at sunset"
- "Generate a photorealistic image of a cozy coffee shop interior"
- "Design a cute cartoon mascot for a tech startup, vector style"
- "Create an artistic portrait of a cyberpunk samurai in a rainy city"

Run `pip install google-genai agno` to install the necessary dependencies.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[GeminiTools()],
    show_tool_calls=True,
)

agent.print_response(
    "Create an artistic portrait of a cyberpunk samurai in a rainy city",
)
response = agent.run_response
if response.images:
    save_base64_data(response.images[0].content, "tmp/cyberpunk_samurai.png")



================================================
FILE: cookbook/models/google/gemini/imagen_tool_advanced.py
================================================
"""🔧 Example: Using the GeminiTools Toolkit for Image Generation

An Agent using the Gemini image generation tool.

Make sure to set the Vertex AI credentials. Here's the authentication guide: https://cloud.google.com/sdk/docs/initializing

Run `pip install google-genai agno` to install the required packages.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        GeminiTools(
            image_generation_model="imagen-4.0-generate-preview-05-20", vertexai=True
        )
    ],
    show_tool_calls=True,
)

agent.print_response(
    "Cinematic a visual shot using a stabilized drone flying dynamically alongside a pod of immense baleen whales as they breach spectacularly in deep offshore waters. The camera maintains a close, dramatic perspective as these colossal creatures launch themselves skyward from the dark blue ocean, creating enormous splashes and showering cascades of water droplets that catch the sunlight. In the background, misty, fjord-like coastlines with dense coniferous forests provide context. The focus expertly tracks the whales, capturing their surprising agility, immense power, and inherent grace. The color palette features the deep blues and greens of the ocean, the brilliant white spray, the dark grey skin of the whales, and the muted tones of the distant wild coastline, conveying the thrilling magnificence of marine megafauna."
)


response = agent.run_response
if response.images:
    save_base64_data(response.images[0].content, "tmp/baleen_whale.png")

"""
Example prompts to try:
- A horizontally oriented rectangular stamp features the Mission District's vibrant culture, portrayed in shades of warm terracotta orange using an etching style. The scene might depict a sun-drenched street like Valencia or Mission Street, lined with a mix of Victorian buildings and newer structures.
- Painterly landscape featuring a simple, isolated wooden cabin nestled amongst tall pine trees on the shore of a calm, reflective lake. 
- Filmed cinematically from the driver's seat, offering a clear profile view of the young passenger on the front seat with striking red hair.
- A pile of books seen from above. The topmost book contains a watercolor illustration of a bird. VERTEX AI is written in bold letters on the book.
"""



================================================
FILE: cookbook/models/google/gemini/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai google.genai` to install dependencies."""

from agno.agent import Agent
from agno.embedder.google import GeminiEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.google import Gemini
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=GeminiEmbedder(),
    ),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/google/gemini/pdf_input_file_upload.py
================================================
"""
In this example, we upload a PDF file to Google GenAI directly and then use it as an input to an agent.

Note: If the size of the file is greater than 20MB, and a file path is provided, the file automatically gets uploaded to Google GenAI.
"""

from pathlib import Path
from time import sleep

from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini
from google import genai

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

client = genai.Client()

# Upload the file to Google GenAI
upload_result = client.files.upload(file=pdf_path)

# Get the file from Google GenAI
retrieved_file = client.files.get(name=upload_result.name)

# Retry up to 3 times if file is not ready
retries = 0
wait_time = 5
while retrieved_file is None and retries < 3:
    retries += 1
    sleep(wait_time)
    retrieved_file = client.files.get(name=upload_result.name)

if retrieved_file is not None:
    agent = Agent(
        model=Gemini(id="gemini-2.0-flash-exp"),
        markdown=True,
        add_history_to_messages=True,
    )

    agent.print_response(
        "Summarize the contents of the attached file.",
        files=[File(external=retrieved_file)],
    )

    agent.print_response(
        "Suggest me a recipe from the attached file.",
    )
else:
    print("Error: File was not ready after multiple attempts.")



================================================
FILE: cookbook/models/google/gemini/pdf_input_local.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
    add_history_to_messages=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[File(filepath=pdf_path)],
)
agent.print_response("Suggest me a recipe from the attached file.")



================================================
FILE: cookbook/models/google/gemini/pdf_input_url.py
================================================
from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
    add_history_to_messages=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)

agent.print_response("Suggest me a recipe from the attached file.")



================================================
FILE: cookbook/models/google/gemini/search.py
================================================
"""Google Search with Gemini.

The search tool enables Gemini to access current information from Google Search.
This is useful for getting up-to-date facts, news, and web content.

Run `pip install google-generativeai` to install dependencies.
"""

from agno.agent import Agent
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp", search=True),
    show_tool_calls=True,
    markdown=True,
)

# Ask questions that require current information
agent.print_response("What are the latest developments in AI technology this week?")



================================================
FILE: cookbook/models/google/gemini/storage.py
================================================
"""Run `pip install ddgs sqlalchemy google.genai` to install dependencies."""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/google/gemini/storage_and_memory.py
================================================
"""Run `pip install ddgs pgvector google.genai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.google import Gemini
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    knowledge=knowledge_base,
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    show_tool_calls=True,
    # This setting adds a tool to search the knowledge base for information
    search_knowledge=True,
    # This setting adds a tool to get chat history
    read_chat_history=True,
    # Add the previous chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # This setting adds 6 previous messages from chat history to the messages sent to the LLM
    num_history_responses=6,
    markdown=True,
    debug_mode=True,
)
agent.print_response("Whats is the latest AI news?")



================================================
FILE: cookbook/models/google/gemini/structured_output.py
================================================
from typing import Optional, Union

from agno.agent import Agent
from agno.models.google import Gemini
from pydantic import BaseModel, Field


class ContactInfo(BaseModel):
    """Contact information with structured properties"""

    contact_name: str = Field(description="Name of the contact person")
    contact_method: str = Field(
        description="Preferred communication method",
        enum=["email", "phone", "teams", "slack"],
    )
    contact_details: str = Field(description="Email address or phone number")


class EventSchema(BaseModel):
    event_id: str = Field(description="Unique event identifier")
    event_name: str = Field(description="Name of the event")

    event_date: str = Field(
        description="Event date in YYYY-MM-DD format",
        format="date",
    )

    start_time: str = Field(
        description="Event start time in HH:MM format",
        format="time",
    )

    duration: str = Field(
        description="Event duration in ISO 8601 format (e.g., PT2H30M)",
        format="duration",
    )

    status: str = Field(
        description="Current event status",
        enum=[
            "planning",
            "confirmed",
            "in_progress",
            "completed",
            "cancelled",
        ],
    )

    attendee_count: int = Field(
        description="Expected number of attendees",
        ge=1,
        le=10000,
    )

    budget_range: Union[float, str] = Field(
        description="Budget as number (USD) or 'TBD' if not determined"
    )

    optional_notes: Optional[str] = Field(
        description="Additional notes about the event (can be null)",
        default=None,
    )

    contact_info: ContactInfo = Field(
        description="Contact information with structured properties"
    )


structured_output_agent = Agent(
    name="Advanced Event Planner",
    model=Gemini(id="gemini-2.5-pro"),
    response_model=EventSchema,
    instructions="""
    Create a detailed event plan that demonstrates all schema constraints:
    - Use proper date/time/duration formats
    - Set a realistic status from the enum options
    - Handle budget as either a number or 'TBD'
    - Include optional notes if relevant
    - Create contact info as a nested object
    """,
)

structured_output_agent.print_response(
    "Plan a corporate product launch event for 150 people next month"
)



================================================
FILE: cookbook/models/google/gemini/structured_output_stream.py
================================================
from typing import Dict, List

from agno.agent import Agent
from agno.models.google import Gemini
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

structured_output_agent.print_response(
    "New York", stream=True, stream_intermediate_steps=True
)



================================================
FILE: cookbook/models/google/gemini/thinking_agent.py
================================================
from agno.agent import Agent
from agno.models.google import Gemini

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

agent = Agent(
    model=Gemini(
        id="gemini-2.5-pro",
        thinking_budget=1280,  # Enable thinking with token budget
        include_thoughts=True,  # Include thought summaries in response
    ),
    markdown=True,
)
agent.print_response(task)



================================================
FILE: cookbook/models/google/gemini/thinking_agent_stream.py
================================================
from agno.agent import Agent
from agno.models.google import Gemini

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

agent = Agent(
    model=Gemini(
        id="gemini-2.5-pro",
        thinking_budget=1280,  # Enable thinking with token budget
        include_thoughts=True,  # Include thought summaries in response
    ),
    markdown=True,
)
agent.print_response(task, stream=True)



================================================
FILE: cookbook/models/google/gemini/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/google/gemini/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/google/gemini/url_context.py
================================================
"""Run `pip install google-generativeai` to install dependencies."""

from agno.agent import Agent
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.5-flash", url_context=True),
    markdown=True,
)

url1 = "https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592"
url2 = "https://www.allrecipes.com/recipe/83557/juicy-roasted-chicken/"

agent.print_response(
    f"Compare the ingredients and cooking times from the recipes at {url1} and {url2}"
)



================================================
FILE: cookbook/models/google/gemini/url_context_with_search.py
================================================
"""Combine URL context with Google Search for comprehensive web analysis.

Run `pip install google-generativeai` to install dependencies.
"""

from agno.agent import Agent
from agno.models.google import Gemini

# Create agent with both Google Search and URL context enabled
agent = Agent(
    model=Gemini(id="gemini-2.5-flash", search=True, url_context=True),
    markdown=True,
    debug_mode=True,
)

# The agent will first search for relevant URLs, then analyze their content in detail
agent.print_response(
    "Analyze the content of the following URL: https://docs.agno.com/introduction and also give me latest updates on AI agents"
)



================================================
FILE: cookbook/models/google/gemini/vertex_ai_search.py
================================================
"""Vertex AI Search with Gemini.

Vertex AI Search allows Gemini to search through your data stores,
providing grounded responses based on your private knowledge base.

Prerequisites:
1. Set up Vertex AI Search datastore in Google Cloud Console
2. Export environment variables:
   export GOOGLE_GENAI_USE_VERTEXAI="true"
   export GOOGLE_CLOUD_PROJECT="your-project-id"
   export GOOGLE_CLOUD_LOCATION="your-location"

Run `pip install google-generativeai` to install dependencies.
"""

from agno.agent import Agent
from agno.models.google import Gemini

# Replace with your actual Vertex AI Search datastore ID
# Format: "projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{datastore_id}"
datastore_id = "projects/your-project-id/locations/global/collections/default_collection/dataStores/your-datastore-id"

agent = Agent(
    model=Gemini(
        id="gemini-2.5-flash",
        vertexai_search=True,
        vertexai_search_datastore=datastore_id,
        vertexai=True,  # Use Vertex AI endpoint
    ),
    show_tool_calls=True,
    markdown=True,
)

# Ask questions that can be answered from your knowledge base
agent.print_response("What are our company's policies regarding remote work?")



================================================
FILE: cookbook/models/google/gemini/vertexai.py
================================================
"""
To use Vertex AI, with the Gemini Model class, you need to set the following environment variables:

export GOOGLE_GENAI_USE_VERTEXAI="true"
export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_CLOUD_LOCATION="your-location"

Or you can set the following parameters in the `Gemini` class:

gemini = Gemini(
    vertexai=True,
    project_id="your-google-cloud-project-id",
    location="your-google-cloud-location",
)
"""

from agno.agent import Agent, RunResponse  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/google/gemini/video_input_bytes_content.py
================================================
import requests
from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://videos.pexels.com/video-files/5752729/5752729-uhd_2560_1440_30fps.mp4"

# Download the video file from the URL as bytes
response = requests.get(url)
video_content = response.content

agent.print_response(
    "Tell me about this video",
    videos=[Video(content=video_content)],
)



================================================
FILE: cookbook/models/google/gemini/video_input_file_upload.py
================================================
import time
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini
from agno.utils.log import logger

model = Gemini(id="gemini-2.0-flash-exp")
agent = Agent(
    model=model,
    markdown=True,
)

# Please download a sample video file to test this Agent
# Run: `wget https://storage.googleapis.com/generativeai-downloads/images/GreatRedSpot.mp4` to download a sample video
video_path = Path(__file__).parent.joinpath("samplevideo.mp4")
video_file = None
remote_file_name = f"files/{video_path.stem.lower().replace('_', '')}"
try:
    video_file = model.get_client().files.get(name=remote_file_name)
except Exception as e:
    logger.info(f"Error getting file {video_path.stem}: {e}")
    pass

# Upload the video file if it doesn't exist
if not video_file:
    try:
        logger.info(f"Uploading video: {video_path}")
        video_file = model.get_client().files.upload(
            file=video_path,
            config=dict(name=video_path.stem, display_name=video_path.stem),
        )

        # Check whether the file is ready to be used.
        while video_file.state.name == "PROCESSING":
            time.sleep(2)
            video_file = model.get_client().files.get(name=video_file.name)

        logger.info(f"Uploaded video: {video_file}")
    except Exception as e:
        logger.error(f"Error uploading video: {e}")

if __name__ == "__main__":
    agent.print_response(
        "Tell me about this video",
        videos=[Video(content=video_file)],
        stream=True,
    )



================================================
FILE: cookbook/models/google/gemini/video_input_local_file_upload.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

# Get sample videos from https://www.pexels.com/search/videos/sample/
video_path = Path(__file__).parent.joinpath("sample_video.mp4")

agent.print_response("Tell me about this video?", videos=[Video(filepath=video_path)])



================================================
FILE: cookbook/models/google/gemini/video_input_youtube.py
================================================
from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

agent.print_response(
    "Tell me about this video?",
    videos=[Video(url="https://www.youtube.com/watch?v=XinoY2LDdA0")],
)

# Video upload via URL is also supported with Vertex AI

# agent = Agent(
#     model=Gemini(id="gemini-2.0-flash-exp", vertexai=True),
#     markdown=True,
# )

# agent.print_response("Tell me about this video?", videos=[Video(url="https://www.youtube.com/watch?v=XinoY2LDdA0")])



================================================
FILE: cookbook/models/groq/README.md
================================================
# Groq Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `GROQ_API_KEY`

```shell
export GROQ_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U groq ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/groq/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/groq/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/groq/tool_use.py
```

- Research using Exa

```shell
python cookbook/models/groq/research_agent_exa.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/groq/structured_output.py
```

### 7. Run Agent that uses storage

Please run pgvector in a docker container using:

```shell
./cookbook/run_pgvector.sh
```

Then run the following:

```shell
python cookbook/models/groq/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/groq/knowledge.py
```
Take note that by default, OpenAI embeddings are used and an API key will be required. Alternatively, there are other embedders available that can be used. See more examples in `/cookbook/knowledge/embedders`

### 9. Run Agent that analyzes an image

```shell
python cookbook/models/groq/image_agent.py
```

### 10. Run in async mode

```shell
python cookbook/models/groq/async/basic_stream.py
```
```shell
python cookbook/models/groq/async/basic.py
```



================================================
FILE: cookbook/models/groq/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/groq/agent_team.py
================================================
from agno.agent import Agent
from agno.models.groq import Groq
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions="Use tables to display data",
    markdown=True,
)

agent_team = Team(
    members=[web_agent, finance_agent],
    mode="coordinate",
    model=Groq(
        id="llama-3.3-70b-versatile"
    ),  # You can use a different model for the team leader agent
    success_criteria="The team has successfully completed the task",
    instructions=["Always include sources", "Use tables to display data"],
    show_tool_calls=True,  # Comment to hide transfer of tasks between agents
    markdown=True,
    enable_agentic_context=True,
    debug_mode=True,
    show_members_responses=False,  # Comment to hide responses from team members
)

# Give the team a task
agent_team.print_response(
    message="Summarize the latest news about Nvidia and share its stock price?",
    stream=True,
)



================================================
FILE: cookbook/models/groq/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))



================================================
FILE: cookbook/models/groq/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the terminal
asyncio.run(
    agent.aprint_response("Share a breakfast recipe.", markdown=True, stream=True)
)



================================================
FILE: cookbook/models/groq/async_tool_use.py
================================================
"""Please install dependencies using:
pip install openai ddgs newspaper4k lxml_html_clean agno
"""

import asyncio

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[DuckDuckGoTools(), Newspaper4kTools()],
    description="You are a senior NYT researcher writing an article on a topic.",
    instructions=[
        "For a given topic, search for the top 5 links.",
        "Then read each URL and extract the article text, if a URL isn't available, ignore it.",
        "Analyse and prepare an NYT worthy article based on the information.",
    ],
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
)

# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Simulation theory", stream=True))



================================================
FILE: cookbook/models/groq/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.groq import Groq

agent = Agent(model=Groq(id="llama-3.3-70b-versatile"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/groq/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.groq import Groq

agent = Agent(model=Groq(id="llama-3.3-70b-versatile"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/groq/browser_search.py
================================================
from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(
    model=Groq(id="openai/gpt-oss-20b"),
    tools=[{"type": "browser_search"}],
)
agent.print_response("Is the Going-to-the-sun road open for public?")



================================================
FILE: cookbook/models/groq/deep_knowledge.py
================================================
"""🤔 DeepKnowledge - An AI Agent that iteratively searches a knowledge base to answer questions

This agent performs iterative searches through its knowledge base, breaking down complex
queries into sub-questions, and synthesizing comprehensive answers. It's designed to explore
topics deeply and thoroughly by following chains of reasoning.

In this example, the agent uses the Agno documentation as a knowledge base

Key Features:
- Iteratively searches a knowledge base
- Source attribution and citations

Run `pip install openai lancedb tantivy inquirer agno groq` to install dependencies.
"""

from textwrap import dedent
from typing import List, Optional

import inquirer
import typer
from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.groq import Groq
from agno.storage.sqlite import SqliteStorage
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print


def initialize_knowledge_base():
    """Initialize the knowledge base with your preferred documentation or knowledge source
    Here we use Agno docs as an example, but you can replace with any relevant URLs
    """
    agent_knowledge = UrlKnowledge(
        urls=["https://docs.agno.com/llms-full.txt"],
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="deep_knowledge_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    )
    # Load the knowledge base (comment out after first run)
    agent_knowledge.load()
    return agent_knowledge


def get_agent_storage():
    """Return agent storage"""
    return SqliteStorage(table_name="deep_knowledge_sessions", db_file="tmp/agents.db")


def create_agent(session_id: Optional[str] = None) -> Agent:
    """Create and return a configured DeepKnowledge agent."""
    agent_knowledge = initialize_knowledge_base()
    agent_storage = get_agent_storage()
    return Agent(
        name="DeepKnowledge",
        session_id=session_id,
        model=Groq(id="llama-3.3-70b-versatile"),
        description=dedent("""\
        You are DeepKnowledge, an advanced reasoning agent designed to provide thorough,
        well-researched answers to any query by searching your knowledge base.

        Your strengths include:
        - Breaking down complex topics into manageable components
        - Connecting information across multiple domains
        - Providing nuanced, well-researched answers
        - Maintaining intellectual honesty and citing sources
        - Explaining complex concepts in clear, accessible terms"""),
        instructions=dedent("""\
        Your mission is to leave no stone unturned in your pursuit of the correct answer.

        To achieve this, follow these steps:
        1. **Analyze the input and break it down into key components**.
        2. **Search terms**: You must identify at least 3-5 key search terms to search for.
        3. **Initial Search:** Searching your knowledge base for relevant information. You must make atleast 3 searches to get all relevant information.
        4. **Evaluation:** If the answer from the knowledge base is incomplete, ambiguous, or insufficient - Ask the user for clarification. Do not make informed guesses.
        5. **Iterative Process:**
            - Continue searching your knowledge base till you have a comprehensive answer.
            - Reevaluate the completeness of your answer after each search iteration.
            - Repeat the search process until you are confident that every aspect of the question is addressed.
        4. **Reasoning Documentation:** Clearly document your reasoning process:
            - Note when additional searches were triggered.
            - Indicate which pieces of information came from the knowledge base and where it was sourced from.
            - Explain how you reconciled any conflicting or ambiguous information.
        5. **Final Synthesis:** Only finalize and present your answer once you have verified it through multiple search passes.
            Include all pertinent details and provide proper references.
        6. **Continuous Improvement:** If new, relevant information emerges even after presenting your answer,
            be prepared to update or expand upon your response.

        **Communication Style:**
        - Use clear and concise language.
        - Organize your response with numbered steps, bullet points, or short paragraphs as needed.
        - Be transparent about your search process and cite your sources.
        - Ensure that your final answer is comprehensive and leaves no part of the query unaddressed.

        Remember: **Do not finalize your answer until every angle of the question has been explored.**"""),
        additional_context=dedent("""\
        You should only respond with the final answer and the reasoning process.
        No need to include irrelevant information.

        - User ID: {user_id}
        - Memory: You have access to your previous search results and reasoning process.
        """),
        knowledge=agent_knowledge,
        storage=agent_storage,
        add_history_to_messages=True,
        num_history_responses=3,
        show_tool_calls=True,
        read_chat_history=True,
        markdown=True,
    )


def get_example_topics() -> List[str]:
    """Return a list of example topics for the agent."""
    return [
        "What are AI agents and how do they work in Agno?",
        "What chunking strategies does Agno support for text processing?",
        "How can I implement custom tools in Agno?",
        "How does knowledge retrieval work in Agno?",
        "What types of embeddings does Agno support?",
    ]


def handle_session_selection() -> Optional[str]:
    """Handle session selection and return the selected session ID."""
    agent_storage = get_agent_storage()

    new = typer.confirm("Do you want to start a new session?", default=True)
    if new:
        return None

    existing_sessions: List[str] = agent_storage.get_all_session_ids()
    if not existing_sessions:
        print("No existing sessions found. Starting a new session.")
        return None

    print("\nExisting sessions:")
    for i, session in enumerate(existing_sessions, 1):
        print(f"{i}. {session}")

    session_idx = typer.prompt(
        "Choose a session number to continue (or press Enter for most recent)",
        default=1,
    )

    try:
        return existing_sessions[int(session_idx) - 1]
    except (ValueError, IndexError):
        return existing_sessions[0]


def run_interactive_loop(agent: Agent):
    """Run the interactive question-answering loop."""
    example_topics = get_example_topics()

    while True:
        choices = [f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)]
        choices.extend(["Enter custom question...", "Exit"])

        questions = [
            inquirer.List(
                "topic",
                message="Select a topic or ask a different question:",
                choices=choices,
            )
        ]
        answer = inquirer.prompt(questions)

        if answer["topic"] == "Exit":
            break

        if answer["topic"] == "Enter custom question...":
            questions = [inquirer.Text("custom", message="Enter your question:")]
            custom_answer = inquirer.prompt(questions)
            topic = custom_answer["custom"]
        else:
            topic = example_topics[int(answer["topic"].split(".")[0]) - 1]

        agent.print_response(topic, stream=True)


def deep_knowledge_agent():
    """Main function to run the DeepKnowledge agent."""

    session_id = handle_session_selection()
    agent = create_agent(session_id)

    print("\n🤔 Welcome to DeepKnowledge - Your Advanced Research Assistant! 📚")
    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"[bold green]Started New Session: {session_id}[/bold green]\n")
        else:
            print("[bold green]Started New Session[/bold green]\n")
    else:
        print(f"[bold blue]Continuing Previous Session: {session_id}[/bold blue]\n")

    run_interactive_loop(agent)


if __name__ == "__main__":
    typer.run(deep_knowledge_agent)

# Example prompts to try:
"""
Explore Agno's capabilities with these queries:
1. "What are the different types of agents in Agno?"
2. "How does Agno handle knowledge base management?"
3. "What embedding models does Agno support?"
4. "How can I implement custom tools in Agno?"
5. "What storage options are available for workflow caching?"
6. "How does Agno handle streaming responses?"
7. "What types of LLM providers does Agno support?"
8. "How can I implement custom knowledge sources?"
"""



================================================
FILE: cookbook/models/groq/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.groq import Groq

agent = Agent(model=Groq(id="llama-3.2-90b-vision-preview"))

agent.print_response(
    "Tell me about this image",
    images=[
        Image(url="https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg"),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/groq/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai groq` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.groq import Groq
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/groq/metrics.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.groq import Groq
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
)

run_stream: Iterator[RunResponse] = agent.run(
    "What is the stock price of NVDA", stream=True
)
pprint_run_response(run_stream, markdown=True)

# Print metrics per message
if agent.run_response.messages:
    for message in agent.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(agent.run_response.metrics)
# Print the session metrics
print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.session_metrics)



================================================
FILE: cookbook/models/groq/reasoning_agent.py
================================================
from agno.agent import Agent
from agno.models.groq import Groq

# Create a reasoning agent that uses:
# - `deepseek-r1-distill-llama-70b` as the reasoning model
# - `llama-3.3-70b-versatile` to generate the final response
reasoning_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    reasoning_model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)

# Prompt the agent to solve the problem
reasoning_agent.print_response("Is 9.11 bigger or 9.9?", stream=True)



================================================
FILE: cookbook/models/groq/research_agent_exa.py
================================================
"""Run `pip install groq exa-py` to install dependencies."""

from datetime import datetime
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.exa import ExaTools

cwd = Path(__file__).parent.resolve()
tmp = cwd.joinpath("tmp")
if not tmp.exists():
    tmp.mkdir(exist_ok=True, parents=True)

today = datetime.now().strftime("%Y-%m-%d")

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[ExaTools(start_published_date=today, type="keyword")],
    description="You are an advanced AI researcher writing a report on a topic.",
    instructions=[
        "For the provided topic, run 3 different searches.",
        "Read the results carefully and prepare a NYT worthy report.",
        "Focus on facts and make sure to provide references.",
    ],
    expected_output=dedent("""\
    An engaging, informative, and well-structured report in markdown format:

    ## Engaging Report Title

    ### Overview
    {give a brief introduction of the report and why the user should read this report}
    {make this section engaging and create a hook for the reader}

    ### Section 1
    {break the report into sections}
    {provide details/facts/processes in this section}

    ... more sections as necessary...

    ### Takeaways
    {provide key takeaways from the article}

    ### References
    - [Reference 1](link)
    - [Reference 2](link)
    - [Reference 3](link)

    ### About the Author
    {write a made up for yourself, give yourself a cyberpunk name and a title}

    - published on {date} in dd/mm/yyyy
    """),
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
    save_response_to_file=str(tmp.joinpath("{message}.md")),
    # debug_mode=True,
)
agent.print_response("Llama 3.3 running on Groq", stream=True)



================================================
FILE: cookbook/models/groq/storage.py
================================================
"""Run `pip install ddgs sqlalchemy groq` to install dependencies."""

from agno.agent import Agent
from agno.models.groq import Groq
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/groq/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.groq import Groq
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
run: RunResponse = json_mode_agent.run("New York")
pprint(run.content)

# json_mode_agent.print_response("New York")



================================================
FILE: cookbook/models/groq/tool_use.py
================================================
"""Please install dependencies using:
pip install openai ddgs newspaper4k lxml_html_clean agno
"""

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[DuckDuckGoTools(), Newspaper4kTools()],
    description="You are a senior NYT researcher writing an article on a topic.",
    instructions=[
        "For a given topic, search for the top 5 links.",
        "Then read each URL and extract the article text, if a URL isn't available, ignore it.",
        "Analyse and prepare an NYT worthy article based on the information.",
    ],
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
)
agent.print_response("Simulation theory", stream=True)



================================================
FILE: cookbook/models/groq/transcription_agent.py
================================================
# groq transcription agent

import asyncio
import os
from pathlib import Path

from agno.agent import Agent
from agno.models.groq import Groq
from agno.models.openai import OpenAIChat
from agno.tools.models.groq import GroqTools

url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

agent = Agent(
    name="Groq Transcription Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GroqTools(exclude_tools=["generate_speech"])],
    debug_mode=True,
)

agent.print_response(f"Please transcribe the audio file located at '{url}' to English")



================================================
FILE: cookbook/models/groq/translation_agent.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.groq import GroqTools
from agno.utils.media import save_base64_data

path = "tmp/sample-fr.mp3"

agent = Agent(
    name="Groq Translation Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GroqTools()],
)

agent.print_response(
    f"Let's transcribe the audio file located at '{path}' and translate it to English. After that generate a new music audio file using the translated text."
)

response = agent.run_response

if response.audio:
    save_base64_data(response.audio[0].base64_audio, Path("tmp/sample-en.mp3"))



================================================
FILE: cookbook/models/groq/reasoning/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/groq/reasoning/basic.py
================================================
from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(model=Groq(id="deepseek-r1-distill-llama-70b-specdec"), markdown=True)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/groq/reasoning/basic_stream.py
================================================
from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(model=Groq(id="deepseek-r1-distill-llama-70b-specdec"), markdown=True)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/groq/reasoning/demo_deepseek_qwen.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai groq` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.groq import Groq
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent_with_reasoning = Agent(
    model=Groq(id="Qwen-2.5-32b"),
    reasoning=True,
    reasoning_model=Groq(
        id="Deepseek-r1-distill-qwen-32b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
    show_tool_calls=True,
    debug_mode=True,
)
agent_with_reasoning.print_response("9.11 and 9.9 -- which is bigger?", markdown=True)



================================================
FILE: cookbook/models/groq/reasoning/demo_qwen_2_5_32B.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai groq` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.groq import Groq
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Groq(id="Qwen-2.5-32b"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/groq/reasoning/finance_agent.py
================================================
from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.yfinance import YFinanceTools

# Create an Agent with Groq and YFinanceTools
finance_agent = Agent(
    model=Groq(id="deepseek-r1-distill-llama-70b-specdec"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            stock_fundamentals=True,
            company_info=True,
        )
    ],
    description="You are an investment analyst with deep expertise in market analysis",
    instructions=[
        "Use tables to display data where possible.",
        "Always call the tool before you answer.",
    ],
    add_datetime_to_instructions=True,
    show_tool_calls=True,  # Uncomment to see tool calls in the response
    markdown=True,
)

# Example usage
finance_agent.print_response(
    "Write a report on NVDA with stock price, analyst recommendations, and stock fundamentals.",
    stream=True,
)



================================================
FILE: cookbook/models/huggingface/README.md
================================================
# Huggingface Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export `HF_TOKEN`

```shell
export HF_TOKEN=***
```

### 3. Install libraries

```shell
pip install -U huggingface_hub agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/huggingface/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/huggingface/basic.py
```

### 5. Run agent with tools

- An essay writer using Llama model

```shell
python cookbook/models/huggingface/llama_essay_writer.py
```



================================================
FILE: cookbook/models/huggingface/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/huggingface/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(
        id="mistralai/Mistral-7B-Instruct-v0.2", max_tokens=4096, temperature=0
    ),
)
asyncio.run(
    agent.aprint_response(
        "What is meaning of life and then recommend 5 best books to read about it"
    )
)



================================================
FILE: cookbook/models/huggingface/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(
        id="mistralai/Mistral-7B-Instruct-v0.2", max_tokens=4096, temperature=0
    ),
)
asyncio.run(
    agent.aprint_response(
        "What is meaning of life and then recommend 5 best books to read about it",
        stream=True,
    )
)



================================================
FILE: cookbook/models/huggingface/basic.py
================================================
from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(
        id="mistralai/Mistral-7B-Instruct-v0.2", max_tokens=4096, temperature=0
    ),
)
agent.print_response(
    "What is meaning of life and then recommend 5 best books to read about it"
)



================================================
FILE: cookbook/models/huggingface/basic_stream.py
================================================
from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(
        id="mistralai/Mistral-7B-Instruct-v0.2", max_tokens=4096, temperature=0
    ),
)
agent.print_response(
    "What is meaning of life and then recommend 5 best books to read about it",
    stream=True,
)



================================================
FILE: cookbook/models/huggingface/llama_essay_writer.py
================================================
import os
from getpass import getpass

from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(
        id="meta-llama/Meta-Llama-3-8B-Instruct",
        max_tokens=4096,
    ),
    description="You are an essay writer. Write a 300 words essay on topic that will be provided by user",
)
agent.print_response("topic: AI")



================================================
FILE: cookbook/models/huggingface/tool_use.py
================================================
"""Please install dependencies using:
pip install openai ddgs newspaper4k lxml_html_clean agno
"""

from agno.agent import Agent
from agno.models.huggingface import HuggingFace
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=HuggingFace(id="Qwen/Qwen2.5-Coder-32B-Instruct"),
    tools=[DuckDuckGoTools()],
    description="You are a senior NYT researcher writing an article on a topic.",
    instructions=[
        "For a given topic, search for the top 5 links.",
        "Then read each URL and extract the article text, if a URL isn't available, ignore it.",
        "Analyse and prepare an NYT worthy article based on the information.",
    ],
    markdown=True,
    show_tool_calls=True,
    add_datetime_to_instructions=True,
)
agent.print_response("Simulation theory")



================================================
FILE: cookbook/models/ibm/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/ibm/watsonx/README.md
================================================
# IBM WatsonX

[Models overview](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx)

Some of the supported models:

- `ibm/granite-20b-code-instruct`
- `ibm/granite-3-2b-instruct`
- `ibm/granite-3-8b-instruct`
- `meta-llama/llama-3-1-70b-instruct`
- `meta-llama/llama-3-1-8b-instruct`
- `meta-llama/llama-3-2-11b-vision-instruct`
- `meta-llama/llama-3-3-70b-instruct`
- `mistralai/mistral-large`
- `mistralai/mistral-small-24b-instruct-2501`
- `mistralai/mixtral-8x7b-instruct-v01`
- `mistralai/pixtral-12b`

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your AWS Credentials

```shell
export IBM_WATSONX_API_KEY=***
export IBM_WATSONX_PROJECT_ID=***
export IBM_WATSONX_URL=*** (optional, defaults to https://eu-de.ml.cloud.ibm.com)
```

### 3. Install libraries

```shell
pip install -U ibm-watsonx-ai ddgs agno
```

### 4. Run basic agent

- Streaming on

```shell
python cookbook/models/ibm/watsonx/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/ibm/watsonx/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/ibm/watsonx/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/ibm/watsonx/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/ibm/watsonx/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/ibm/watsonx/knowledge.py
```

### 9. Run Agent that uses images

```shell
python cookbook/models/ibm/watsonx/image_agent.py
```



================================================
FILE: cookbook/models/ibm/watsonx/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/ibm/watsonx/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.ibm import WatsonX

agent = Agent(model=WatsonX(id="ibm/granite-20b-code-instruct"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/ibm/watsonx/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.ibm import WatsonX

agent = Agent(
    model=WatsonX(id="ibm/granite-20b-code-instruct"), debug_mode=True, markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/ibm/watsonx/async_tool_use.py
================================================
"""
Async example using Claude with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.ibm import WatsonX
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=WatsonX(id="meta-llama/llama-3-3-70b-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/ibm/watsonx/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.ibm import WatsonX

agent = Agent(model=WatsonX(id="ibm/granite-20b-code-instruct"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/ibm/watsonx/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.ibm import WatsonX

agent = Agent(model=WatsonX(id="ibm/granite-20b-code-instruct"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/ibm/watsonx/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.ibm import WatsonX

agent = Agent(
    model=WatsonX(id="meta-llama/llama-3-2-11b-vision-instruct"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/ibm/watsonx/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai ibm-watsonx-ai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.ibm import WatsonX
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=WatsonX(id="ibm/granite-20b-code-instruct"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/ibm/watsonx/storage.py
================================================
"""Run `pip install ddgs sqlalchemy ibm-watsonx-ai` to install dependencies."""

from agno.agent import Agent
from agno.models.ibm import WatsonX
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=WatsonX(id="ibm/granite-20b-code-instruct"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/ibm/watsonx/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.ibm import WatsonX
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


movie_agent = Agent(
    model=WatsonX(id="ibm/granite-20b-code-instruct"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
# movie_agent: RunResponse = movie_agent.run("New York")
# pprint(movie_agent.content)

movie_agent.print_response("New York")



================================================
FILE: cookbook/models/ibm/watsonx/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ibm import WatsonX
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=WatsonX(id="meta-llama/llama-3-3-70b-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/langdb/README.md
================================================
# LangDB Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `LANGDB_API_KEY` and `LANGDB_PROJECT_ID`

```shell
export LANGDB_API_KEY=***
export LANGDB_PROJECT_ID=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run Agent without Tools

- Streaming on

```shell
python cookbook/models/langdb/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/langdb/basic.py
```

### 5. Run Agent with Tools

- Yahoo Finance with streaming on

```shell
python cookbook/models/langdb/agent_stream.py
```

- Yahoo Finance without streaming

```shell
python cookbook/models/langdb/agent.py
```

- Web Search Agent

```shell
python cookbook/models/langdb/web_search.py
```

- Data Analyst

```shell
python cookbook/models/langdb/data_analyst.py
```

- Finance Agent

```shell
python cookbook/models/langdb/finance_agent.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/langdb/structured_output.py
```





================================================
FILE: cookbook/models/langdb/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/langdb/agent.py
================================================
"""Run `pip install yfinance` to install dependencies."""

from agno.agent import Agent, RunResponse  # noqa
from agno.models.langdb import LangDB
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LangDB(id="gpt-4o"),
    tools=[YFinanceTools(stock_price=True)],
    instructions=["Use tables where possible."],
    markdown=True,
    show_tool_calls=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("What is the stock price of NVDA and TSLA")
# print(run.content)

# Print the response in the terminal
agent.print_response("What is the stock price of NVDA and TSLA")



================================================
FILE: cookbook/models/langdb/agent_stream.py
================================================
"""Run `pip install yfinance` to install dependencies."""

from typing import Iterator  # noqa
from agno.agent import Agent, RunResponse  # noqa
from agno.models.langdb import LangDB
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LangDB(id="gemini-1.5-pro-latest"),
    tools=[YFinanceTools(stock_price=True)],
    instructions=["Use tables where possible."],
    markdown=True,
    show_tool_calls=True,
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("What is the stock price of NVDA and TSLA", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("What is the stock price of NVDA and TSLA", stream=True)



================================================
FILE: cookbook/models/langdb/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.langdb import LangDB

agent = Agent(
    model=LangDB(id="deepseek-chat", project_id="langdb-project-id"), markdown=True
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/langdb/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponse  # noqa
from agno.models.langdb import LangDB

agent = Agent(
    model=LangDB(id="llama3-1-70b-instruct-v1.0"),
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/langdb/data_analyst.py
================================================
"""Run `pip install duckdb` to install dependencies."""

from textwrap import dedent

from agno.agent import Agent
from agno.models.langdb import LangDB
from agno.tools.duckdb import DuckDbTools

duckdb_tools = DuckDbTools(
    create_tables=False, export_tables=False, summarize_tables=False
)
duckdb_tools.create_table_from_path(
    path="https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
    table="movies",
)

agent = Agent(
    model=LangDB(id="grok-2", project_id="langdb-project-id"),
    tools=[duckdb_tools],
    markdown=True,
    show_tool_calls=True,
    additional_context=dedent("""\
    You have access to the following tables:
    - movies: contains information about movies from IMDB.
    """),
)
agent.print_response("What is the average rating of movies?", stream=False)



================================================
FILE: cookbook/models/langdb/finance_agent.py
================================================
"""Run `pip install yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.langdb import LangDB
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LangDB(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True, analyst_recommendations=True, stock_fundamentals=True
        )
    ],
    show_tool_calls=True,
    description="You are an investment analyst that researches stocks and helps users make informed decisions.",
    instructions=["Use tables to display data where possible."],
    markdown=True,
)

# agent.print_response("Share the NVDA stock price and analyst recommendations", stream=True)
agent.print_response("Summarize fundamentals for TSLA", stream=True)



================================================
FILE: cookbook/models/langdb/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.langdb import LangDB
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=LangDB(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=LangDB(id="gpt-4o", project_id="langdb-project-id"),
    description="You write movie scripts.",
    response_model=MovieScript,
)


# Get the response in a variable
# json_mode_response: RunResponse = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
structured_output_agent.print_response("New York")



================================================
FILE: cookbook/models/langdb/web_search.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.langdb import LangDB
from agno.tools.duckduckgo import DuckDuckGo

agent = Agent(
    model=LangDB(id="claude-3-5-sonnet-20240620", project_id="langdb-project-id"),
    tools=[DuckDuckGo()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/litellm/README.md
================================================
# LiteLLM Cookbooks

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your API keys
Regardless of the model used—OpenAI, Hugging Face, or XAI—the API key is referenced as `LITELLM_API_KEY`.

```shell
export LITELLM_API_KEY=***
```

You can also reference the API key depending on the model you will use, e.g. `OPENAI_API_KEY` if you will use an OpenAI model like GPT-4o.

```shell
export OPENAI_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U litellm ddgs duckdb yfinance agno
```

### 4. Run an Agent

- Streaming off

```shell
python cookbook/models/litellm/basic.py
```

- Streaming on

```shell
python cookbook/models/litellm/basic_stream.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/litellm/tool_use.py
```

- Tool use with streaming

```shell
python cookbook/models/litellm/tool_use_stream.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/litellm/structured_output.py
```

### 7. Run Agent that uses memory

```shell
python cookbook/models/litellm/memory.py
```

### 8. Run Agent that uses storage

```shell
python cookbook/models/litellm/storage.py
```

### 9. Run Agent that uses knowledge

```shell
python cookbook/models/litellm/knowledge.py
```

### 10. Run Agent that analyzes images

- URL-based image

```shell
python cookbook/models/litellm/image_agent.py
```

- Byte-based image

```shell
python cookbook/models/litellm/image_agent_bytes.py
```

### 11. Run Agent that analyzes audio

```shell
python cookbook/models/litellm/audio_input_agent.py
```

### 12. Run Agent that processes PDF files

- Local PDF file

```shell
python cookbook/models/litellm/pdf_input_local.py
```

- Remote PDF URL

```shell
python cookbook/models/litellm/pdf_input_url.py
```

- PDF from bytes

```shell
python cookbook/models/litellm/pdf_input_bytes.py
```

### 13. Run Agent with metrics

```shell
python cookbook/models/litellm/metrics.py
```

### 14. Run async Agents

- Basic async

```shell
python cookbook/models/litellm/async_basic.py
```

- Async with streaming

```shell
python cookbook/models/litellm/async_basic_stream.py
```

- Async with tools

```shell
python cookbook/models/litellm/async_tool_use.py
```



================================================
FILE: cookbook/models/litellm/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/litellm/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
)

asyncio.run(openai_agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/litellm/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(
    openai_agent.aprint_response("Share a 2 sentence horror story", stream=True)
)



================================================
FILE: cookbook/models/litellm/async_tool_use.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
    tools=[DuckDuckGoTools()],
)

# Ask a question that would likely trigger tool use
asyncio.run(agent.aprint_response("What is happening in France?"))



================================================
FILE: cookbook/models/litellm/audio_input_agent.py
================================================
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.litellm import LiteLLM

# Fetch the QA audio file and convert it to a base64 encoded string
url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"
response = requests.get(url)
response.raise_for_status()
mp3_data = response.content

# Audio input requires specific audio-enabled models like gpt-4o-audio-preview
agent = Agent(
    model=LiteLLM(id="gpt-4o-audio-preview"),
    markdown=True,
)
agent.print_response(
    "What's the audio about?",
    audio=[Audio(content=mp3_data, format="mp3")],
    stream=True,
)



================================================
FILE: cookbook/models/litellm/basic.py
================================================
from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="huggingface/mistralai/Mistral-7B-Instruct-v0.2",
        top_p=0.95,
    ),
    markdown=True,
)

openai_agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/litellm/basic_gpt.py
================================================
from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
)

openai_agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/litellm/basic_stream.py
================================================
from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
)

openai_agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/litellm/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/litellm/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/litellm/knowledge.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.litellm import LiteLLM
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/litellm/memory.py
================================================
from agno.agent import Agent
from agno.models.litellm import LiteLLM
from rich.pretty import pprint

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    # Set add_history_to_messages=true to add the previous chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_responses=3,
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
)

# -*- Create a run
agent.print_response("Share a 2 sentence horror story", stream=True)
# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)

# -*- Ask a follow up question that continues the conversation
agent.print_response("What was my first message?", stream=True)
# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)



================================================
FILE: cookbook/models/litellm/metrics.py
================================================
from agno.agent import Agent
from agno.models.litellm import LiteLLM
from agno.tools.yfinance import YFinanceTools
from rich.pretty import pprint

agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
    ),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
)

agent.print_response("What is the stock price of NVDA", stream=True)

# Print metrics per message
if agent.run_response.messages:
    for message in agent.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the aggregated metrics for the whole run
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(agent.run_response.metrics)
# Print the aggregated metrics for the whole session
print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.session_metrics)



================================================
FILE: cookbook/models/litellm/pdf_input_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.litellm import LiteLLM
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=LiteLLM(id="openai/gpt-4o"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(
            content=pdf_path.read_bytes(),
        ),
    ],
)



================================================
FILE: cookbook/models/litellm/pdf_input_local.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.litellm import LiteLLM
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    markdown=True,
    add_history_to_messages=True,
)

agent.print_response(
    "What is the recipe for Gaeng Som Phak Ruam? Also what are the health benefits. Refer to the attached file.",
    files=[File(filepath=pdf_path)],
)



================================================
FILE: cookbook/models/litellm/pdf_input_url.py
================================================
from agno.agent import Agent
from agno.media import File
from agno.models.litellm import LiteLLM

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    markdown=True,
    add_history_to_messages=True,
)

agent.print_response(
    "Suggest me a recipe from the attached file.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)



================================================
FILE: cookbook/models/litellm/storage.py
================================================
from agno.agent import Agent
from agno.models.litellm import LiteLLM
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools

# Create a storage backend using the Sqlite database
storage = SqliteStorage(
    # store sessions in the ai.sessions table
    table_name="agent_sessions_storage",
    # db_file: Sqlite database file
    db_file="tmp/data.db",
)

# Add storage to the Agent
agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    storage=storage,
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)

agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/litellm/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.litellm import LiteLLM
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

json_mode_agent.print_response("New York")



================================================
FILE: cookbook/models/litellm/tool_use.py
================================================
from agno.agent import Agent
from agno.models.litellm import LiteLLM
from agno.tools.yfinance import YFinanceTools

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
    tools=[YFinanceTools()],
)

# Ask a question that would likely trigger tool use
openai_agent.print_response("How is TSLA stock doing right now?")



================================================
FILE: cookbook/models/litellm/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Ask a question that would likely trigger tool use
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/litellm_openai/README.md
================================================
# LiteLLMOpenAI Cookbook

> Note: Fork and clone this repository if needed
### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `LITELLM_API_KEY`
Whichever model you use- openai, huggingface, xai, the api key will be by the name of `LITELLM_API_KEY`

```shell
export LITELLM_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai 'litellm[proxy]' ddgs duckdb yfinance agno
```

### 4. Start the proxy server

```shell
litellm --model gpt-4o --host 127.0.0.1 --port 4000
```
or, if you want to use some other model like from Anthropic
```shell
litellm --model claude-3-sonnet-20240229 --host 127.0.0.1 --port 4000
```

### 5. Run basic Agent

- Streaming on

```shell
python cookbook/models/litellm_proxy/basic_stream.py
```

### 6. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/litellm_proxy/tool_use.py
```



================================================
FILE: cookbook/models/litellm_openai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/litellm_openai/audio_input_agent.py
================================================
"""
Please first install litellm[proxy] by running: pip install 'litellm[proxy]'

Before running this script, you need to start the LiteLLM server:

litellm --model gpt-4o-audio-preview --host 127.0.0.1 --port 4000
"""

import requests
from agno.agent import Agent, RunResponse  # noqa
from agno.media import Audio
from agno.models.litellm import LiteLLMOpenAI

# Fetch the QA audio file and convert it to a base64 encoded string
url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"
response = requests.get(url)
response.raise_for_status()
mp3_data = response.content

# Provide the agent with the audio file and get result as text
# Note: Audio input requires specific audio-enabled models like gpt-4o-audio-preview
agent = Agent(
    model=LiteLLMOpenAI(id="gpt-4o-audio-preview"),
    markdown=True,
)
agent.print_response(
    "What is in this audio?", audio=[Audio(content=mp3_data, format="mp3")], stream=True
)



================================================
FILE: cookbook/models/litellm_openai/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.litellm import LiteLLMOpenAI

agent = Agent(model=LiteLLMOpenAI(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/litellm_openai/basic_stream.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.litellm import LiteLLMOpenAI

agent = Agent(model=LiteLLMOpenAI(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/litellm_openai/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.litellm import LiteLLMOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LiteLLMOpenAI(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/lmstudio/README.md
================================================
# LMStudio Cookbook

> Note: Fork and clone this repository if needed

### 1. [Install](https://lmstudio.ai/) LMStudio and download a model

Run your chat model using LMStudio. For the examples below make sure to get `qwen2.5-7b-instruct-1m`. Please also make sure that the status is set to `Running` and the model is reachable at `http://127.0.0.1:1234/v1`.

### 2. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 3. Install libraries

```shell
pip install -U ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/lmstudio/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/lmstudio/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/lmstudio/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/lmstudio/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/lmstudio/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/lmstudio/knowledge.py
```

### 9. Run Agent that uses memory

```shell
python cookbook/models/lmstudio/memory.py
```

### 10. Run Agent that takes image as input

```shell
python cookbook/models/lmstudio/image_agent.py
```



================================================
FILE: cookbook/models/lmstudio/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/lmstudio/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.lmstudio import LMStudio

agent = Agent(model=LMStudio(id="qwen2.5-7b-instruct-1m"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/lmstudio/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.lmstudio import LMStudio

agent = Agent(model=LMStudio(id="qwen2.5-7b-instruct-1m"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/lmstudio/image_agent.py
================================================
import httpx
from agno.agent import Agent
from agno.media import Image
from agno.models.lmstudio import LMStudio

agent = Agent(
    model=LMStudio(id="llama3.2-vision"),
    markdown=True,
)

response = httpx.get(
    "https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
)

agent.print_response(
    "Tell me about this image",
    images=[Image(content=response.content)],
    stream=True,
)



================================================
FILE: cookbook/models/lmstudio/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai ollama` to install dependencies."""

from agno.agent import Agent
from agno.embedder.ollama import OllamaEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.lmstudio import LMStudio
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
    ),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/lmstudio/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install ollama sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/lmstudio/memory.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.lmstudio import LMStudio
from agno.storage.agent.postgres import PostgresAgentStorage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresAgentStorage(
        table_name="personalized_agent_sessions", db_url=db_url
    ),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/lmstudio/storage.py
================================================
"""Run `pip install ddgs sqlalchemy` to install dependencies."""

from agno.agent import Agent
from agno.models.lmstudio import LMStudio
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    storage=PostgresAgentStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/lmstudio/structured_output.py
================================================
from typing import List

from agno.agent import Agent
from agno.models.lmstudio import LMStudio
from agno.run.response import RunResponse
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

# Run the agent synchronously
structured_output_response: RunResponse = structured_output_agent.run("New York")
pprint(structured_output_response.content)



================================================
FILE: cookbook/models/lmstudio/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.lmstudio import LMStudio
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/lmstudio/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.lmstudio import LMStudio
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/meta/README.md
================================================
# Meta Llama API Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `LLAMA_API_KEY`

```shell
export LLAMA_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U agno llama-api-client
```

If using LlamaOpenAI, install the following:

```shell
pip install -U agno openai
```

### 4. Run a basic Agent

- Streaming on

```shell
python cookbook/models/meta/llama/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/meta/llama/basic.py
```

### 5. Run an Agent with Tools

> Run `pip install ddgs` to install dependencies.

- Streaming on

```shell
python cookbook/models/meta/llama/tool_use_stream.py
```

- Streaming off

```shell
python cookbook/models/meta/llama/tool_use.py
```



================================================
FILE: cookbook/models/meta/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/meta/llama/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/meta/llama/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import Llama

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
    debug_mode=True,
)

# Get the response in a variable
# run: RunResponse = asyncio.run(agent.arun("Share a 2 sentence horror story"))
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/meta/llama/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import Llama

agent = Agent(model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = asyncio.run(agent.arun("Share a 2 sentence horror story", stream=True))
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/meta/llama/async_knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf llama-api-client` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.meta import Llama
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=True))

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))



================================================
FILE: cookbook/models/meta/llama/async_tool_use.py
================================================
"""Run `pip install agno llama-api-client yfinance` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    debug_mode=True,
)
asyncio.run(agent.aprint_response("Whats the price of AAPL stock?"))



================================================
FILE: cookbook/models/meta/llama/async_tool_use_stream.py
================================================
"""Run `pip install agno llama-api-client yfinance` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    show_tool_calls=True,
)
asyncio.run(agent.aprint_response("Whats the price of AAPL stock?", stream=True))



================================================
FILE: cookbook/models/meta/llama/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import Llama

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/meta/llama/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import Llama

agent = Agent(model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/meta/llama/image_input_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import LlamaOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/meta/llama/image_input_file.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import Llama
from agno.utils.media import download_image

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

agent.print_response(
    "Tell me about this image?",
    images=[Image(filepath=image_path)],
    stream=True,
)



================================================
FILE: cookbook/models/meta/llama/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf llama-api-client` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.meta import Llama
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/meta/llama/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.meta import Llama
from agno.storage.postgres import PostgresStorage
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresStorage(table_name="personalized_agent_sessions", db_url=db_url),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/meta/llama/metrics.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
)

run_stream: Iterator[RunResponse] = agent.run(
    "What is the stock price of NVDA", stream=True
)
pprint_run_response(run_stream, markdown=True)

# Print metrics per message
if agent.run_response.messages:
    for message in agent.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(agent.run_response.metrics)
# Print the session metrics
print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.session_metrics)



================================================
FILE: cookbook/models/meta/llama/storage.py
================================================
"""Run `pip install ddgs sqlalchemy llama-api-client` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import Llama
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/meta/llama/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import Llama
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a JSON schema output
json_schema_output_agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8", temperature=0.1),
    response_model=MovieScript,
)

json_schema_output_agent.print_response("New York")



================================================
FILE: cookbook/models/meta/llama/tool_use.py
================================================
"""Run `pip install agno llama-api-client yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
agent.print_response("What is the price of AAPL stock?")



================================================
FILE: cookbook/models/meta/llama/tool_use_stream.py
================================================
"""Run `pip install agno llama-api-client yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    show_tool_calls=True,
)
agent.print_response("Tell me the price of AAPL stock", stream=True)



================================================
FILE: cookbook/models/meta/llama_openai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/meta/llama_openai/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import LlamaOpenAI

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
    debug_mode=True,
)

# Get the response in a variable
# run: RunResponse = asyncio.run(agent.arun("Share a 2 sentence horror story"))
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/meta/llama_openai/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import LlamaOpenAI

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = asyncio.run(agent.arun("Share a 2 sentence horror story", stream=True))
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/meta/llama_openai/async_tool_use.py
================================================
"""Run `pip install openai yfinance` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    show_tool_calls=True,
)
asyncio.run(agent.aprint_response("Whats the price of AAPL stock?"))



================================================
FILE: cookbook/models/meta/llama_openai/async_tool_use_stream.py
================================================
"""Run `pip install openai yfinance` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    show_tool_calls=True,
)
asyncio.run(agent.aprint_response("Whats the price of AAPL stock?", stream=True))



================================================
FILE: cookbook/models/meta/llama_openai/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import LlamaOpenAI

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
    debug_mode=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/meta/llama_openai/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import LlamaOpenAI

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/meta/llama_openai/image_input_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import Llama
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/meta/llama_openai/image_input_file.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import LlamaOpenAI
from agno.utils.media import download_image

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

agent.print_response(
    "Tell me about this image?",
    images=[Image(filepath=image_path)],
    stream=True,
)



================================================
FILE: cookbook/models/meta/llama_openai/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.meta import LlamaOpenAI
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/meta/llama_openai/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.meta import LlamaOpenAI
from agno.storage.postgres import PostgresStorage
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresStorage(table_name="personalized_agent_sessions", db_url=db_url),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/meta/llama_openai/metrics.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
)

run_stream: Iterator[RunResponse] = agent.run(
    "What is the stock price of NVDA", stream=True
)
pprint_run_response(run_stream, markdown=True)

# Print metrics per message
if agent.run_response.messages:
    for message in agent.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(agent.run_response.metrics)
# Print the session metrics
print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.session_metrics)



================================================
FILE: cookbook/models/meta/llama_openai/storage.py
================================================
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/meta/llama_openai/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.meta import LlamaOpenAI
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a JSON schema output
json_schema_output_agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8", temperature=0.1),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    response_model=MovieScript,
)

json_schema_output_agent.print_response("New York")



================================================
FILE: cookbook/models/meta/llama_openai/tool_use.py
================================================
"""Run `pip install openai yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    show_tool_calls=True,
)
agent.print_response("Whats the price of AAPL stock?")



================================================
FILE: cookbook/models/meta/llama_openai/tool_use_stream.py
================================================
"""Run `pip install openai yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    show_tool_calls=True,
)
agent.print_response("Whats the price of AAPL stock?", stream=True)



================================================
FILE: cookbook/models/mistral/README.md
================================================
# Mistral Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `MISTRAL_API_KEY`

```shell
export MISTRAL_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U mistralai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/mistral/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/mistral/basic.py
```

### 5. Run Agent with Tools


- DuckDuckGo search

```shell
python cookbook/models/mistral/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/mistral/structured_output.py
```

### 7. Run Agent that uses memory

```shell
python cookbook/models/mistral/memory.py
```



================================================
FILE: cookbook/models/mistral/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/mistral/async_basic.py
================================================
"""
Basic async example using Mistral.
"""

import asyncio

from agno.agent import Agent
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/mistral/async_basic_stream.py
================================================
"""
Basic streaming async example using Mistral.
"""

import asyncio

from agno.agent import Agent
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/mistral/async_structured_output.py
================================================
import asyncio
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=MistralChat(
        id="mistral-small-latest",
    ),
    tools=[DuckDuckGoTools()],
    description="You help people write movie scripts.",
    response_model=MovieScript,
    show_tool_calls=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Find a cool movie idea about London and write it."))



================================================
FILE: cookbook/models/mistral/async_tool_use.py
================================================
"""
Async example using Mistral with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.mistral.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/mistral/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.mistral import MistralChat


agent = Agent(
    model=MistralChat(id="mistral-small-latest"),
    markdown=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/mistral/basic_stream.py
================================================
import os

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.mistral import MistralChat

agent = Agent(
    model=MistralChat(
        id="mistral-large-latest",
    ),
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/mistral/image_bytes_input_agent.py
================================================
import requests
from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    show_tool_calls=True,
    markdown=True,
)

image_url = (
    "https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg"
)


def fetch_image_bytes(url: str) -> bytes:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.content


image_bytes_from_url = fetch_image_bytes(image_url)

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(content=image_bytes_from_url),
    ],
)



================================================
FILE: cookbook/models/mistral/image_compare_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    markdown=True,
)

agent.print_response(
    "what are the differences between two images?",
    images=[
        Image(
            url="https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg"
        ),
        Image(
            url="https://assets.visitorscoverage.com/production/wp-content/uploads/2024/04/AdobeStock_626542468-min-1024x683.jpeg"
        ),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/mistral/image_file_input_agent.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    tools=[
        DuckDuckGoTools()
    ],  # pixtral-12b-2409 is not so great at tool calls, but it might work.
    show_tool_calls=True,
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpeg")

agent.print_response(
    "Tell me about this image and give me the latest news about it from duckduckgo.",
    images=[
        Image(filepath=image_path),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/mistral/image_ocr_with_structured_output.py
================================================
from typing import List

from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat
from pydantic import BaseModel


class GroceryItem(BaseModel):
    item_name: str
    price: float


class GroceryListElements(BaseModel):
    bill_number: str
    items: List[GroceryItem]
    total_price: float


agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    instructions=[
        "Extract the text elements described by the user from the picture",
    ],
    response_model=GroceryListElements,
    markdown=True,
)

agent.print_response(
    "From this restaurant bill, extract the bill number, item names and associated prices, and total price and return it as a string in a Json object",
    images=[Image(url="https://i.imghippo.com/files/kgXi81726851246.jpg")],
)



================================================
FILE: cookbook/models/mistral/image_transcribe_document_agent.py
================================================
"""
This agent transcribes an old written document from an image.
"""

from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    markdown=True,
)

agent.print_response(
    "Transcribe this document.",
    images=[
        Image(url="https://ciir.cs.umass.edu/irdemo/hw-demo/page_example.jpg"),
    ],
)



================================================
FILE: cookbook/models/mistral/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install mistralai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/mistral/memory.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.mistral.mistral import MistralChat
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    tools=[DuckDuckGoTools()],
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresStorage(table_name="personalized_agent_sessions", db_url=db_url),
    show_tool_calls=True,
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# -*- Make tool call
agent.print_response("What is the weather in nyc?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/mistral/mistral_small.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=MistralChat(id="mistral-small-latest"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Tell me about mistrall small, any news", stream=True)



================================================
FILE: cookbook/models/mistral/structured_output.py
================================================
import os
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


structured_output_agent = Agent(
    model=MistralChat(
        id="mistral-large-latest",
    ),
    tools=[DuckDuckGoTools()],
    description="You help people write movie scripts.",
    response_model=MovieScript,
    show_tool_calls=True,
)

# Get the response in a variable
structured_output_response: RunResponse = structured_output_agent.run("New York")
pprint(structured_output_response.content)



================================================
FILE: cookbook/models/mistral/structured_output_with_tool_use.py
================================================
from agno.agent import Agent
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel


class Person(BaseModel):
    name: str
    description: str


model = MistralChat(
    id="mistral-medium-latest",
    temperature=0.0,
)

researcher = Agent(
    name="Researcher",
    model=model,
    role="You find people with a specific role at a provided company.",
    instructions=[
        "- Search the web for the person described"
        "- Find out if they have public contact details"
        "- Return the information in a structured format"
    ],
    show_tool_calls=True,
    tools=[DuckDuckGoTools()],
    response_model=Person,
    add_datetime_to_instructions=True,
)

researcher.print_response("Find information about Elon Musk")



================================================
FILE: cookbook/models/mistral/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=MistralChat(
        id="mistral-large-latest",
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/nebius/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/nebius/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.nebius import Nebius

agent = Agent(
    model=Nebius(),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("write a two sentence horror story"))



================================================
FILE: cookbook/models/nebius/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.nebius import Nebius

agent = Agent(
    model=Nebius(),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("write a two sentence horror story", stream=True))



================================================
FILE: cookbook/models/nebius/async_tool_use.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))



================================================
FILE: cookbook/models/nebius/async_tool_use_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/nebius/basic.py
================================================
from agno.agent import Agent
from agno.models.nebius import Nebius

agent = Agent(
    model=Nebius(),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story")



================================================
FILE: cookbook/models/nebius/basic_stream.py
================================================
from agno.agent import Agent
from agno.models.nebius import Nebius

agent = Agent(
    model=Nebius(),
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story", stream=True)



================================================
FILE: cookbook/models/nebius/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.nebius import Nebius
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/nebius/storage.py
================================================
"""Run `pip install ddgs sqlalchemy cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Nebius(),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    debug_mode=True,
    show_tool_calls=True,
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/nebius/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.nebius import Nebius
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a structured output
structured_output_agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    response_model=MovieScript,
    debug_mode=True,
)

structured_output_agent.print_response("New York")



================================================
FILE: cookbook/models/nebius/tool_use.py
================================================
from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/nebius/tool_use_stream.py
================================================
from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/nvidia/README.md
================================================
# Nvidia Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `NVIDIA_API_KEY`

```shell
export NVIDIA_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/nvidia/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/nvidia/basic.py
```

### 5. Run Agent with Tools


- DuckDuckGo search

```shell
python cookbook/models/nvidia/tool_use.py
```



================================================
FILE: cookbook/models/nvidia/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/nvidia/async_basic.py
================================================
"""
Basic async example using Nvidia.
"""

import asyncio

from agno.agent import Agent
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/nvidia/async_basic_stream.py
================================================
"""
Basic streaming async example using Nvidia.
"""

import asyncio

from agno.agent import Agent
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/nvidia/async_tool_use.py
================================================
"""
Async example using Mistral with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.nvidia import Nvidia
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nvidia(id="meta/llama-3.3-70b-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/nvidia/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/nvidia/basic_stream.py
================================================
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/nvidia/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import os

from agno.agent import Agent
from agno.models.nvidia import Nvidia
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nvidia(id="meta/llama-3.3-70b-instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/ollama/README.md
================================================
# Ollama Cookbook

> Note: Fork and clone this repository if needed

### 1. [Install](https://github.com/ollama/ollama?tab=readme-ov-file#macos) ollama and run models

Run your chat model

```shell
ollama pull llama3.1:8b
```

### 2. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 3. Install libraries

```shell
pip install -U ollama ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/ollama/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/ollama/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/ollama/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/ollama/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/ollama/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/ollama/knowledge.py
```

### 9. Run Agent that uses memory

```shell
python cookbook/models/ollama/memory.py
```

### 10. Run Agent that interprets an image

Pull the llama3.2 vision model

```shell
ollama pull llama3.2-vision
```

```shell
python cookbook/models/ollama/image_agent.py
```

### 11. Run Agent that manually sets the Ollama client

```shell
python cookbook/models/ollama/set_client.py
```

### 12. See demos of some well-known Ollama models

```shell
python cookbook/models/ollama/demo_deepseek_r1.py
```
```shell
python cookbook/models/ollama/demo_qwen.py
```
```shell
python cookbook/models/ollama/demo_phi4.py
```



================================================
FILE: cookbook/models/ollama/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/ollama/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.1:8b"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))



================================================
FILE: cookbook/models/ollama/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/ollama/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/ollama/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/ollama/demo_deepseek_r1.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="deepseek-r1:14b"), markdown=True)

# Print the response in the terminal
agent.print_response(
    "Write me python code to solve quadratic equations. Explain your reasoning."
)



================================================
FILE: cookbook/models/ollama/demo_gemma.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="gemma3:12b"), markdown=True)

image_path = Path(__file__).parent.joinpath("super-agents.png")
agent.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
    stream=True,
)



================================================
FILE: cookbook/models/ollama/demo_phi4.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="phi4"), markdown=True)

# Print the response in the terminal
agent.print_response("Tell me a scary story in exactly 10 words.")



================================================
FILE: cookbook/models/ollama/demo_qwen.py
================================================
from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Ollama(id="qwen3:8b"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables to display data.",
)

agent.print_response("Write a report on NVDA", stream=True, markdown=True)



================================================
FILE: cookbook/models/ollama/image_agent.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.2-vision"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("super-agents.png")
agent.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)



================================================
FILE: cookbook/models/ollama/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai ollama` to install dependencies."""

from agno.agent import Agent
from agno.embedder.ollama import OllamaEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.ollama import Ollama
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=OllamaEmbedder(id="llama3.2", dimensions=3072),
    ),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Ollama(id="llama3.2"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/ollama/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install ollama sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/ollama/memory.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.ollama.chat import Ollama
from agno.storage.postgres import PostgresStorage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Ollama(id="qwen2.5:latest"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresStorage(table_name="personalized_agent_sessions", db_url=db_url),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/ollama/set_client.py
================================================
"""Run `pip install yfinance` to install dependencies."""

from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import Ollama
from ollama import Client as OllamaClient

agent = Agent(
    model=Ollama(id="llama3.1:8b", client=OllamaClient()),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/ollama/set_temperature.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.2", options={"temperature": 0.5}), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/ollama/storage.py
================================================
"""Run `pip install ddgs sqlalchemy ollama` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=Ollama(id="llama3.1:8b"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/ollama/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import Ollama
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=Ollama(id="llama3.2"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
# json_mode_response: RunResponse = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

# Run the agent
structured_output_agent.print_response("New York")



================================================
FILE: cookbook/models/ollama/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/ollama/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/ollama_tools/README.md
================================================
# OllamaTools Cookbook

> Note: Fork and clone this repository if needed

### 1. [Install](https://github.com/ollama/ollama?tab=readme-ov-file#macos) ollama and run models

Run your chat model

```shell
ollama pull llama3.2
```

### 2. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 3. Install libraries

```shell
pip install -U ollama ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/ollama_tools/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/ollama_tools/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/ollama_tools/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/ollama_tools/structured_output.py
```

### 7. Run Agent that uses storage

```shell
python cookbook/models/ollama_tools/storage.py
```

### 8. Run Agent that uses knowledge

```shell
python cookbook/models/ollama_tools/knowledge.py
```



================================================
FILE: cookbook/models/ollama_tools/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/ollama_tools/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.ollama import OllamaTools

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))



================================================
FILE: cookbook/models/ollama_tools/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.ollama import OllamaTools

agent = Agent(model=OllamaTools(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/ollama_tools/async_tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.ollama import OllamaTools
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/ollama_tools/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import OllamaTools

agent = Agent(model=OllamaTools(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/ollama_tools/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import OllamaTools

agent = Agent(model=OllamaTools(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/ollama_tools/knowledge.py
================================================
"""
Run `pip install ddgs sqlalchemy pgvector pypdf openai ollama` to install dependencies.

Run Ollama Server: `ollama serve`
Pull required models:
`ollama pull nomic-embed-text`
`ollama pull llama3.1:8b`

If you haven't deployed database yet, run:
`docker run --rm -it -e POSTGRES_PASSWORD=ai -e POSTGRES_USER=ai -e POSTGRES_DB=ai -p 5532:5432 --name postgres pgvector/pgvector:pg17`
to deploy a PostgreSQL database.

"""

from agno.agent import Agent
from agno.embedder.ollama import OllamaEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.ollama import OllamaTools
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="ollama_recipes",
        db_url=db_url,
        embedder=OllamaEmbedder(id="nomic-embed-text", dimensions=768),
    ),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/ollama_tools/storage.py
================================================
"""Run `pip install ddgs sqlalchemy ollama` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import OllamaTools
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/ollama_tools/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.ollama import OllamaTools
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
movie_agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

# Get the response in a variable
# run: RunResponse = movie_agent.run("New York")
# pprint(run.content)

movie_agent.print_response("New York")



================================================
FILE: cookbook/models/ollama_tools/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import OllamaTools
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OllamaTools(id="llama3.2:latest"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/ollama_tools/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import OllamaTools
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/openai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/openai/chat/README.md
================================================
# OpenAI Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `OPENAI_API_KEY`

```shell
export OPENAI_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/openai/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/openai/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/openai/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/openai/structured_output.py
```

### 7. Run Agent uses memory

```shell
python cookbook/models/openai/memory.py
```

### 8. Run Agent that uses storage

```shell
python cookbook/models/openai/storage.py
```

### 9. Run Agent that uses knowledge

```shell
python cookbook/models/openai/knowledge.py
```

### 10. Run Agent that generates an image using Dall-E

```shell
python cookbook/models/openai/generate_images.py
```

### 11. Run Agent that analyzes an image

```shell
python cookbook/models/openai/image_agent.py
```

or

```shell
python cookbook/models/openai/image_agent_with_memory.py
```

### 11. Run Agent that analyzes audio

```shell
python cookbook/models/openai/audio_input_agent.py
```

### 12. Run Agent that generates audio

```shell
python cookbook/models/openai/audio_output_agent.py
```



================================================
FILE: cookbook/models/openai/chat/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/openai/chat/agent_flex_tier.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="o4-mini", service_tier="flex"),
    markdown=True,
    debug_mode=True,
)

agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/openai/chat/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/openai/chat/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run(
#     "Share a 2 sentence horror story", stream=True
# )
# for chunk in run_response:
#     print(chunk.content, end="")

# # Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/openai/chat/async_structured_response_stream.py
================================================
import asyncio
from typing import Dict, List

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
)


async def main():
    await structured_output_agent.aprint_response(
        "New York", stream=True, stream_intermediate_steps=True
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/models/openai/chat/async_tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/openai/chat/audio_input_agent.py
================================================
import requests
from agno.agent import Agent, RunResponse  # noqa
from agno.media import Audio
from agno.models.openai import OpenAIChat

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

# Provide the agent with the audio file and get result as text
agent = Agent(
    model=OpenAIChat(id="gpt-4o-audio-preview", modalities=["text"]),
    markdown=True,
)
agent.print_response(
    "What is in this audio?", audio=[Audio(content=wav_data, format="wav")], stream=True
)



================================================
FILE: cookbook/models/openai/chat/audio_input_and_output_multi_turn.py
================================================
from pathlib import Path

import requests
from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

# Provide the agent with the audio file and audio configuration and get result as text + audio
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "wav"},
    ),
    # Set add_history_to_messages=true to add the previous chat history to the messages sent to the Model.
    add_history_to_messages=True,
    # Number of historical responses to add to the messages.
    num_history_responses=3,
)
agent.print_response(
    "What is in this audio?", audio={"data": wav_data, "format": "wav"}
)

filename = Path(__file__).parent.joinpath("tmp/conversation_response_1.wav")
filename.unlink(missing_ok=True)
filename.parent.mkdir(parents=True, exist_ok=True)

# Save the response audio to a file
if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.base64_audio, filename=str(filename)
    )


agent.print_response("Tell me something more about the audio")

filename = Path(__file__).parent.joinpath("tmp/conversation_response_2.wav")
filename.unlink(missing_ok=True)

# Save the response audio to a file
if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.base64_audio, filename=str(filename)
    )


agent.print_response("Now tell me a 5 second story")

filename = Path(__file__).parent.joinpath("tmp/conversation_response_3.wav")
filename.unlink(missing_ok=True)

# Save the response audio to a file
if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.base64_audio, filename=str(filename)
    )



================================================
FILE: cookbook/models/openai/chat/audio_input_local_file_upload.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat

# Provide the agent with the audio file and get result as text
agent = Agent(
    model=OpenAIChat(id="gpt-4o-audio-preview", modalities=["text"]),
    markdown=True,
)

# Please download a sample audio file to test this Agent and upload using:
audio_path = Path(__file__).parent.joinpath("sample.mp3")

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(filepath=audio_path, format="mp3")],
    stream=True,
)



================================================
FILE: cookbook/models/openai/chat/audio_output_agent.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file


# Provide the agent with the audio file and audio configuration and get result as text + audio
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    add_history_to_messages=True,
    markdown=True,
)
agent.print_response("Tell me a 5 second scary story")

# Save the response audio to a file
if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.content, filename="tmp/scary_story.wav"
    )

agent.print_response("What would be in a sequal of this story?")

# Save the response audio to a file
if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.content,
        filename="tmp/scary_story_sequal.wav",
    )



================================================
FILE: cookbook/models/openai/chat/audio_output_stream.py
================================================
import base64
import wave
from typing import Iterator

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.openai import OpenAIChat

# Audio Configuration
SAMPLE_RATE = 24000  # Hz (24kHz)
CHANNELS = 1  # Mono (Change to 2 if Stereo)
SAMPLE_WIDTH = 2  # Bytes (16 bits)

# Provide the agent with the audio file and audio configuration and get result as text + audio
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={
            "voice": "alloy",
            "format": "pcm16",
        },  # Only pcm16 is supported with streaming
    ),
)
output_stream: Iterator[RunResponse] = agent.run(
    "Tell me a 10 second story", stream=True
)

filename = "tmp/response_stream.wav"

# Open the file once in append-binary mode
with wave.open(str(filename), "wb") as wav_file:
    wav_file.setnchannels(CHANNELS)
    wav_file.setsampwidth(SAMPLE_WIDTH)
    wav_file.setframerate(SAMPLE_RATE)

    # Iterate over generated audio
    for response in output_stream:
        if response.response_audio:
            if response.response_audio.transcript:
                print(response.response_audio.transcript, end="", flush=True)
            if response.response_audio.content:
                try:
                    pcm_bytes = base64.b64decode(response.response_audio.content)
                    wav_file.writeframes(pcm_bytes)
                except Exception as e:
                    print(f"Error decoding audio: {e}")
print()



================================================
FILE: cookbook/models/openai/chat/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o", temperature=0.5), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/openai/chat/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/openai/chat/custom_role_map.py
================================================
"""This example shows how to use a custom role map with the OpenAIChat class.

This is useful when using a custom model that doesn't support the default role map.

To run this example:
- Set the MISTRAL_API_KEY environment variable.
- Run `pip install openai agno` to install dependencies.
"""

from os import getenv

from agno.agent import Agent
from agno.models.openai import OpenAIChat

# Using these Mistral model and url as an example.
model_id = "mistral-medium-2505"
base_url = "https://api.mistral.ai/v1"
api_key = getenv("MISTRAL_API_KEY")
mistral_role_map = {
    "system": "system",
    "user": "user",
    "assistant": "assistant",
    "tool": "tool",
    "model": "assistant",
}

# When initializing the model, we pass our custom role map.
model = OpenAIChat(
    id=model_id,
    base_url=base_url,
    api_key=api_key,
    role_map=mistral_role_map,
)

agent = Agent(model=model, markdown=True)

# Running the agent with a custom role map.
res = agent.print_response("Hey, how are you doing?")



================================================
FILE: cookbook/models/openai/chat/generate_images.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.dalle import DalleTools

image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description="You are an AI agent that can generate images using DALL-E.",
    instructions="When the user asks you to create an image, use the `create_image` tool to create the image.",
    markdown=True,
    show_tool_calls=True,
)

image_agent.print_response("Generate an image of a white siamese cat")

images = image_agent.get_images()
if images and isinstance(images, list):
    for image_response in images:
        image_url = image_response.url
        print(image_url)



================================================
FILE: cookbook/models/openai/chat/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/openai/chat/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/openai/chat/image_agent_with_memory.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    add_history_to_messages=True,
    num_history_responses=3,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
)

agent.print_response("Tell me where I can get more images?")



================================================
FILE: cookbook/models/openai/chat/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/openai/chat/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.postgres import PostgresStorage
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresStorage(table_name="personalized_agent_sessions", db_url=db_url),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/openai/chat/metrics.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponseEvent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
)

run_stream: Iterator[RunResponse] = agent.run(
    "What is the stock price of NVDA", stream=True
)
pprint_run_response(run_stream, markdown=True)

# Print metrics per message
if agent.run_response.messages:
    for message in agent.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(agent.run_response.metrics)
# Print the session metrics
print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.session_metrics)



================================================
FILE: cookbook/models/openai/chat/pdf_input_file_upload.py
================================================
"""
In this example, we upload a PDF file to Google GenAI directly and then use it as an input to an agent.
"""

from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.openai import OpenAIChat

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Pass the local PDF file path directly; the client will inline small files or upload large files automatically
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    add_history_to_messages=True,
)

agent.print_response(
    "Suggest me a recipe from the attached file.",
    files=[File(filepath=str(pdf_path))],
)



================================================
FILE: cookbook/models/openai/chat/pdf_input_local.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.openai import OpenAIChat
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    add_history_to_messages=True,
)

agent.print_response(
    "What is the recipe for Gaeng Som Phak Ruam? Also what are the health benefits. Refer to the attached file.",
    files=[File(filepath=pdf_path)],
)



================================================
FILE: cookbook/models/openai/chat/pdf_input_url.py
================================================
from agno.agent import Agent
from agno.media import File
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    add_history_to_messages=True,
)

agent.print_response(
    "Suggest me a recipe from the attached file.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)



================================================
FILE: cookbook/models/openai/chat/reasoning_o3_mini.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIResponses(id="o3-mini", reasoning_effort="high"),
    tools=[YFinanceTools(enable_all=True)],
    show_tool_calls=True,
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Write a report on the NVDA, is it a good buy?", stream=True)



================================================
FILE: cookbook/models/openai/chat/storage.py
================================================
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/openai/chat/structured_output.py
================================================
from typing import Dict, List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
)


# Get the response in a variable
# json_mode_response: RunResponse = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
structured_output_agent.print_response("New York")



================================================
FILE: cookbook/models/openai/chat/structured_output_stream.py
================================================
import asyncio
from typing import Dict, List

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

structured_output_agent.print_response(
    "New York", stream=True, stream_intermediate_steps=True
)



================================================
FILE: cookbook/models/openai/chat/text_to_speech_agent.py
================================================
"""🔊 Example: Using the OpenAITools Toolkit for Text-to-Speech

This script demonstrates how to use an agent to generate speech from a given text input and optionally save it to a specified audio file.

Run `pip install openai agno` to install the necessary dependencies.
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.openai import OpenAITools
from agno.utils.media import save_base64_data

output_file: str = Path("tmp/speech_output.mp3")

agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools()],
    markdown=True,
    show_tool_calls=True,
)

# Ask the agent to generate speech, but not save it
response = agent.run(
    f'Please generate speech for the following text: "Hello from Agno! This is a demonstration of the text-to-speech capability using OpenAI"'
)

print(f"Agent response: {response.get_content_as_string()}")

if response.audio:
    save_base64_data(response.audio[0].base64_audio, output_file)
    print(f"Successfully saved generated speech to{output_file}")



================================================
FILE: cookbook/models/openai/chat/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/openai/chat/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/openai/chat/verbosity_control.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-5", verbosity="high"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions="Use tables to display data.",
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)



================================================
FILE: cookbook/models/openai/responses/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/openai/responses/agent_flex_tier.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIResponses

agent = Agent(
    model=OpenAIResponses(id="o4-mini", service_tier="flex"),
    markdown=True,
    debug_mode=True,
)

agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/openai/responses/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/openai/responses/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/openai/responses/async_tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/openai/responses/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")

agent.run_response.metrics



================================================
FILE: cookbook/models/openai/responses/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/openai/responses/deep_research_agent.py
================================================
import json
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIResponses

agent = Agent(
    model=OpenAIResponses(id="o4-mini-deep-research", max_tool_calls=1),
    instructions=dedent("""
        You are an expert research analyst with access to advanced research tools.

        When you are given a schema to use, pass it to the research tool as output_schema parameter to research tool. 

        The research tool has two parameters:
        - instructions (str): The research topic/question
        - output_schema (dict, optional): A JSON schema for structured output
    """),
    show_tool_calls=True,
)

agent.print_response(
    """Research the economic impact of semaglutide on global healthcare systems.
    Do:
    - Include specific figures, trends, statistics, and measurable outcomes.
    - Prioritize reliable, up-to-date sources: peer-reviewed research, health
      organizations (e.g., WHO, CDC), regulatory agencies, or pharmaceutical
      earnings reports.
    - Include inline citations and return all source metadata.

    Be analytical, avoid generalities, and ensure that each section supports
    data-backed reasoning that could inform healthcare policy or financial modeling."""
)



================================================
FILE: cookbook/models/openai/responses/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIResponses
from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[GoogleSearchTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/openai/responses/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIResponses
from agno.tools.googlesearch import GoogleSearchTools
from agno.utils.media import download_image

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[GoogleSearchTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/openai/responses/image_agent_with_memory.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIResponses
from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[GoogleSearchTools()],
    markdown=True,
    add_history_to_messages=True,
    num_history_responses=3,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
)

agent.print_response("Tell me where I can get more images?")



================================================
FILE: cookbook/models/openai/responses/image_generation_agent.py
================================================
"""🔧 Example: Using the OpenAITools Toolkit for Image Generation

This script demonstrates how to use the `OpenAITools` toolkit, which includes a tool for generating images using OpenAI's DALL-E within an Agno Agent.

Example prompts to try:
- "Create a surreal painting of a floating city in the clouds at sunset"
- "Generate a photorealistic image of a cozy coffee shop interior"
- "Design a cute cartoon mascot for a tech startup"
- "Create an artistic portrait of a cyberpunk samurai"

Run `pip install openai agno` to install the necessary dependencies.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.openai import OpenAITools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    markdown=True,
    show_tool_calls=True,
    debug_mode=True,
)

response = agent.run(
    "Generate a photorealistic image of a cozy coffee shop interior",
)

if response.images:
    save_base64_data(response.images[0].content, "tmp/coffee_shop.png")



================================================
FILE: cookbook/models/openai/responses/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIResponses
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/openai/responses/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIResponses
from agno.storage.agent.postgres import PostgresAgentStorage
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresAgentStorage(
        table_name="personalized_agent_sessions", db_url=db_url
    ),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/openai/responses/pdf_input_local.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.openai.responses import OpenAIResponses
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=OpenAIResponses(id="gpt-4o-mini"),
    tools=[{"type": "file_search"}],
    markdown=True,
    add_history_to_messages=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[File(filepath=pdf_path)],
)
agent.print_response("Suggest me a recipe from the attached file.")



================================================
FILE: cookbook/models/openai/responses/pdf_input_url.py
================================================
from agno.agent import Agent
from agno.media import File
from agno.models.openai.responses import OpenAIResponses

agent = Agent(
    model=OpenAIResponses(id="gpt-4o-mini"),
    tools=[{"type": "file_search"}, {"type": "web_search_preview"}],
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file and search the web for more information.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)

print("Citations:")
print(agent.run_response.citations)



================================================
FILE: cookbook/models/openai/responses/reasoning_o3_mini.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIResponses(id="o3-mini", reasoning_effort="high"),
    tools=[YFinanceTools(enable_all=True)],
    show_tool_calls=True,
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Write a report on the NVDA, is it a good buy?", stream=True)



================================================
FILE: cookbook/models/openai/responses/storage.py
================================================
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    storage=PostgresAgentStorage(table_name="agent_sessions", db_url=db_url),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/openai/responses/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openai import OpenAIResponses
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
)


# Get the response in a variable
# json_mode_response: RunResponse = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
structured_output_agent.print_response("New York")



================================================
FILE: cookbook/models/openai/responses/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/openai/responses/tool_use_gpt_5.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIResponses(id="gpt-5"),
    tools=[YFinanceTools(cache_results=True)],
    show_tool_calls=True,
    markdown=True,
    telemetry=False,
    monitoring=False,
)

agent.print_response("What is the current price of TSLA?", stream=True)



================================================
FILE: cookbook/models/openai/responses/tool_use_o3.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIResponses(id="o3"),
    tools=[YFinanceTools(cache_results=True)],
    show_tool_calls=True,
    markdown=True,
    telemetry=False,
    monitoring=False,
)

agent.print_response("What is the current price of TSLA?", stream=True)



================================================
FILE: cookbook/models/openai/responses/tool_use_stream.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/openai/responses/verbosity_control.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-5", verbosity="high"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions="Use tables to display data.",
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)



================================================
FILE: cookbook/models/openai/responses/websearch_builtin_tool.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.file import FileTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[{"type": "web_search_preview"}, FileTools()],
    instructions="Save the results to a file with a relevant name.",
    markdown=True,
)
agent.print_response("Whats happening in France?")



================================================
FILE: cookbook/models/openrouter/README.md
================================================
# Openrouter Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `OPENROUTER_API_KEY`

```shell
export OPENROUTER_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/openrouter/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/openrouter/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/openrouter/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/openrouter/structured_output.py
```





================================================
FILE: cookbook/models/openrouter/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/openrouter/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openrouter import OpenRouter

agent = Agent(model=OpenRouter(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/openrouter/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.openrouter import OpenRouter

agent = Agent(model=OpenRouter(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/openrouter/async_tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.openrouter import OpenRouter
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenRouter(id="openai/gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/openrouter/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.openrouter import OpenRouter

agent = Agent(model=OpenRouter(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/openrouter/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.openrouter import OpenRouter

agent = Agent(model=OpenRouter(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/openrouter/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.openrouter import OpenRouter
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenRouter(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenRouter(id="gpt-4o-2024-08-06"),
    description="You write movie scripts.",
    response_model=MovieScript,
)


# Get the response in a variable
# json_mode_response: RunResponse = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
structured_output_agent.print_response("New York")



================================================
FILE: cookbook/models/openrouter/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openrouter import OpenRouter
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenRouter(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/perplexity/README.md
================================================
# Perplexity Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `PERPLEXITY_API_KEY`

```shell
export PERPLEXITY_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U ddgs duckdb agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/cohere/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/cohere/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search

```shell
python cookbook/models/cohere/tool_use.py
```

### 6. Run Agent with Knowledge

```shell
python cookbook/models/perplexity/knowledge.py
```




================================================
FILE: cookbook/models/perplexity/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/perplexity/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/perplexity/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/perplexity/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar-pro"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/perplexity/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponse  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/perplexity/knowledge.py
================================================
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai google.generativeai` to install dependencies."""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.perplexity import Perplexity
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=OpenAIEmbedder(),
    ),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Perplexity(id="sonar-pro"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/perplexity/memory.py
================================================
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.perplexity import Perplexity
from agno.storage.postgres import PostgresStorage
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Perplexity(id="sonar-pro"),
    # Store the memories and summary in a database
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=db_url),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Store agent sessions in a database
    storage=PostgresStorage(table_name="personalized_agent_sessions", db_url=db_url),
    # Show debug logs so, you can see the memory being created
    # debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/perplexity/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.perplexity import Perplexity
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=Perplexity(id="sonar-pro"),
    description="You write movie scripts.",
    response_model=MovieScript,
    markdown=True,
    debug_mode=True,
)

# Get the response in a variable
# json_mode_response: RunResponse = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")



================================================
FILE: cookbook/models/perplexity/web_search.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar-pro"), markdown=True)

# Print the response in the terminal
agent.print_response("Show me top 2 news stories from USA?")

# Get the response in a variable
# run: RunResponse = agent.run("What is happening in the world today?")
# print(run.content)



================================================
FILE: cookbook/models/portkey/README.md
================================================
# Portkey Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your Portkey keys

Get your API key from [app.portkey.ai](https://app.portkey.ai) and create a virtual key at [Virtual Keys](https://app.portkey.ai/virtual-keys).

```shell
export PORTKEY_API_KEY=***
export PORTKEY_VIRTUAL_KEY=***
```

### 3. Install libraries

```shell
pip install -U agno portkey-ai
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/portkey/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/portkey/basic.py
```

### 5. Run Agent with Tools

```shell
python cookbook/models/portkey/tool_use.py
```


================================================
FILE: cookbook/models/portkey/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.portkey import Portkey

agent = Agent(
    model=Portkey(id="gpt-4o-mini"),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(
    agent.aprint_response("What is Portkey and why would I use it as an AI gateway?")
)



================================================
FILE: cookbook/models/portkey/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.portkey import Portkey

agent = Agent(
    model=Portkey(id="gpt-4o-mini"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the terminal
asyncio.run(
    agent.aprint_response("Share a breakfast recipe.", markdown=True, stream=True)
)



================================================
FILE: cookbook/models/portkey/async_tool_use.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("What are the latest developments in AI gateways?"))



================================================
FILE: cookbook/models/portkey/async_tool_use_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Print the response in the terminal
asyncio.run(
    agent.aprint_response(
        "What are the latest developments in AI gateways?", stream=True
    )
)



================================================
FILE: cookbook/models/portkey/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.portkey import Portkey

# Create model using Portkey
model = Portkey(
    id="gpt-4o-mini",
)

agent = Agent(model=model, markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("What is Portkey and why would I use it as an AI gateway?")
# print(run.content)

# Print the response in the terminal
agent.print_response("What is Portkey and why would I use it as an AI gateway?")



================================================
FILE: cookbook/models/portkey/basic_stream.py
================================================
from agno.agent import Agent
from agno.models.portkey import Portkey

agent = Agent(
    model=Portkey(
        id="gpt-4o-mini",
    ),
    markdown=True,
)

# Print the response in the terminal
agent.print_response(
    "What is Portkey and why would I use it as an AI gateway?", stream=True
)



================================================
FILE: cookbook/models/portkey/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.portkey import Portkey
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=Portkey(id="gpt-4o-mini"),
    response_model=MovieScript,
    markdown=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("New York")
# print(run.content)

agent.print_response("New York")



================================================
FILE: cookbook/models/portkey/tool_use.py
================================================
from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Print the response in the terminal
agent.print_response("What are the latest developments in AI gateways?")



================================================
FILE: cookbook/models/portkey/tool_use_stream.py
================================================
from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)

# Print the response in the terminal
agent.print_response("What are the latest developments in AI gateways?", stream=True)



================================================
FILE: cookbook/models/sambanova/README.md
================================================
# Sambanova Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `SAMBANOVA_API_KEY`

```shell
export SAMBANOVA_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/sambanova/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/sambanova/basic.py
```
## Disclaimer:

Sambanova does not support all OpenAIChat features. The following features are not yet supported and will be ignored:

- logprobs
- top_logprobs
- n
- presence_penalty
- frequency_penalty
- logit_bias
- tools
- tool_choice
- parallel_tool_calls
- seed
- stream_options: include_usage
- response_format

Please refer to https://community.sambanova.ai/t/open-ai-compatibility/195 for more information.



================================================
FILE: cookbook/models/sambanova/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/sambanova/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.sambanova import Sambanova

agent = Agent(model=Sambanova(id="Meta-Llama-3.1-8B-Instruct"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/sambanova/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.sambanova import Sambanova

agent = Agent(model=Sambanova(id="Meta-Llama-3.1-8B-Instruct"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/sambanova/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.sambanova import Sambanova

agent = Agent(model=Sambanova(id="Meta-Llama-3.1-8B-Instruct"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/sambanova/basic_stream.py
================================================
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.sambanova import Sambanova

agent = Agent(model=Sambanova(id="Meta-Llama-3.1-8B-Instruct"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/together/README.md
================================================
# Together Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `TOGETHER_API_KEY`

```shell
export TOGETHER_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U together openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/together/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/together/basic.py
```

### 5. Run Agent with Tools

- DuckDuckGo Search
```shell
python cookbook/models/together/tool_use.py
```

### 6. Run Agent that returns structured output

```shell
python cookbook/models/together/structured_output.py
```

### 7. Run Agent with Image URL Input

```shell
python cookbook/models/together/image_agent.py
```

### 8. Run Agent with Image Input

```shell
python cookbook/models/together/image_agent_bytes.py
```

### 9. Run Agent with Image Input and Memory

```shell
python cookbook/models/together/image_agent_with_memory.py
```



================================================
FILE: cookbook/models/together/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/together/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"), markdown=True
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/together/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/together/async_tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.together import Together
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/together/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"), markdown=True
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/together/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/together/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Llama-Vision-Free"),
    markdown=True,
)

agent.print_response(
    "Tell me about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/together/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Llama-Vision-Free"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/together/image_agent_with_memory.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Llama-Vision-Free"),
    markdown=True,
    add_history_to_messages=True,
    num_history_responses=3,
)

agent.print_response(
    "Tell me about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

agent.print_response("Tell me where I can get more images?")



================================================
FILE: cookbook/models/together/structured_output.py
================================================
from typing import List

from agno.agent import Agent, RunResponse  # noqa
from agno.models.together import Together
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"),
    description="You write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
# json_mode_response: RunResponse = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunResponse = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")



================================================
FILE: cookbook/models/together/tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.together import Together
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/vercel/README.md
================================================
# Vercel Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `V0_API_KEY`

```shell
export V0_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/vercel/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/vercel/basic.py
```



================================================
FILE: cookbook/models/vercel/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/vercel/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.vercel import v0

agent = Agent(model=v0(id="v0-1.0-md"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/vercel/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponse  # noqa
from agno.models.vercel import v0

agent = Agent(model=v0(id="v0-1.0-md"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/vercel/async_tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.vercel import v0
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=v0(id="v0-1.0-md"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/vercel/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.vercel import v0

agent = Agent(model=v0(id="v0-1.0-md"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")

# agent.print_response("Create a simple web app that displays a random number between 1 and 100.")



================================================
FILE: cookbook/models/vercel/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponse  # noqa
from agno.models.vercel import v0

agent = Agent(model=v0(id="v0-1.0-md"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/vercel/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.vercel import v0
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=v0(id="v0-1.0-md"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/vercel/knowledge.py
================================================
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.vercel import v0
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=v0(id="v0-1.0-md"),
    knowledge=knowledge_base,
    show_tool_calls=True,
)
agent.print_response("How to make Thai curry?", markdown=True)



================================================
FILE: cookbook/models/vercel/tool_use.py
================================================
"""Build a Web Search Agent using xAI."""

from agno.agent import Agent
from agno.models.vercel import v0
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=v0(id="v0-1.0-md"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/vllm/README.md
================================================
# vLLM Cookbook

vLLM is a fast and easy-to-use library for running LLM models locally.

## Setup

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Install vLLM package

```shell
pip install vllm
```

### 3. Serve a model (this downloads the model to your local machine the first time you run it)

```shell
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --enable-auto-tool-choice \
    --tool-call-parser hermes \
    --dtype float16 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9
```


## Examples

### 1. Basic Agent

```shell
python cookbook/models/vllm/basic.py
``` 



================================================
FILE: cookbook/models/vllm/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/vllm/async_basic.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.vllm import vLLM

agent = Agent(model=vLLM(id="Qwen/Qwen2.5-7B-Instruct"), markdown=True)
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/vllm/async_basic_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.vllm import vLLM

agent = Agent(model=vLLM(id="Qwen/Qwen2.5-7B-Instruct"), markdown=True)
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/vllm/async_tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.vllm import vLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=vLLM(id="Qwen/Qwen2.5-7B-Instruct", top_k=20, enable_thinking=False),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/vllm/basic.py
================================================
from agno.agent import Agent, RunResponse
from agno.models.vllm import vLLM

agent = Agent(
    model=vLLM(id="Qwen/Qwen2.5-7B-Instruct", top_k=20, enable_thinking=False),
    markdown=True,
)

agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/vllm/basic_stream.py
================================================
from agno.agent import Agent
from agno.models.vllm import vLLM

agent = Agent(
    model=vLLM(id="Qwen/Qwen2.5-7B-Instruct", top_k=20, enable_thinking=False),
    markdown=True,
)
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/vllm/code_generation.py
================================================
"""Code generation example with DeepSeek-Coder.
Run vLLM model: vllm serve deepseek-ai/deepseek-coder-6.7b-instruct \
        --dtype float32 \
        --tool-call-parser pythonic
"""

from agno.agent import Agent
from agno.models.vllm import vLLM

agent = Agent(
    model=vLLM(id="deepseek-ai/deepseek-coder-6.7b-instruct"),
    description="You are an expert Python developer.",
    markdown=True,
)

agent.print_response(
    "Write a Python function that returns the nth Fibonacci number using dynamic programming."
)



================================================
FILE: cookbook/models/vllm/memory.py
================================================
"""
Personalized memory and session summaries with vLLM.
Prerequisites:
1. Start a Postgres + pgvector container (helper script is provided):
       ./cookbook/scripts/run_pgvector.sh
2. Install dependencies:
       pip install sqlalchemy 'psycopg[binary]' pgvector
3. Run a vLLM server (any open model).  Example with Phi-3:
       vllm serve microsoft/Phi-3-mini-128k-instruct \
         --dtype float32 \
         --enable-auto-tool-choice \
         --tool-call-parser pythonic
Then execute this script – it will remember facts you tell it and generate a
summary.
"""

from agno.agent import Agent
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.vllm import vLLM
from agno.storage.postgres import PostgresStorage

# Change this if your Postgres container is running elsewhere
DB_URL = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=vLLM(id="microsoft/Phi-3-mini-128k-instruct"),
    memory=Memory(
        db=PostgresMemoryDb(table_name="agent_memory", db_url=DB_URL),
    ),
    enable_user_memories=True,
    enable_session_summaries=True,
    storage=PostgresStorage(table_name="personalized_agent_sessions", db_url=DB_URL),
)

# Share personal details; the agent should remember them.
agent.print_response("My name is John Billings.", stream=True)
print("Current memories →")
pprint(agent.memory.memories)
print("Current summary →")
pprint(agent.memory.summaries)

agent.print_response("I live in NYC.", stream=True)
print("Memories →")
pprint(agent.memory.memories)
print("Summary →")
pprint(agent.memory.summaries)

agent.print_response("I'm going to a concert tomorrow.", stream=True)
print("Memories →")
pprint(agent.memory.memories)
print("Summary →")
pprint(agent.memory.summaries)

# Ask the agent to recall
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)



================================================
FILE: cookbook/models/vllm/storage.py
================================================
"""Run `pip install sqlalchemy` and ensure Postgres is running (`./cookbook/scripts/run_pgvector.sh`)."""

from agno.agent import Agent
from agno.models.vllm import vLLM
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

DB_URL = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=vLLM(id="Qwen/Qwen2.5-7B-Instruct"),
    storage=PostgresStorage(table_name="agent_sessions", db_url=DB_URL),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)

agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/models/vllm/structured_output.py
================================================
from typing import List

from agno.agent import Agent
from agno.models.vllm import vLLM
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=vLLM(
        id="NousResearch/Nous-Hermes-2-Mistral-7B-DPO", top_k=20, enable_thinking=False
    ),
    description="You write movie scripts.",
    response_model=MovieScript,
)

agent.print_response("Llamas ruling the world")



================================================
FILE: cookbook/models/vllm/tool_use.py
================================================
"""Build a Web Search Agent using xAI."""

from agno.agent import Agent
from agno.models.vllm import vLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=vLLM(
        id="NousResearch/Nous-Hermes-2-Mistral-7B-DPO", top_k=20, enable_thinking=False
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/models/xai/README.md
================================================
# xAI Cookbook

> Note: Fork and clone this repository if needed

### 1. Create and activate a virtual environment

```shell
python3 -m venv ~/.venvs/aienv
source ~/.venvs/aienv/bin/activate
```

### 2. Export your `XAI_API_KEY`

```shell
export XAI_API_KEY=***
```

### 3. Install libraries

```shell
pip install -U openai ddgs duckdb yfinance agno
```

### 4. Run basic Agent

- Streaming on

```shell
python cookbook/models/xai/basic_stream.py
```

- Streaming off

```shell
python cookbook/models/xai/basic.py
```

### 5. Run with Tools

- DuckDuckGo Search

```shell
python cookbook/models/xai/tool_use.py
```

### 6. Run Agent with Image URL Input

```shell
python cookbook/models/xai/image_agent.py
```

### 7. Run Agent with Image Input

```shell
python cookbook/models/xai/image_agent_bytes.py
```

### 8. Run Agent with Image Input and Memory

```shell
python cookbook/models/xai/image_agent_with_memory.py
``



================================================
FILE: cookbook/models/xai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/models/xai/async_basic.py
================================================
import asyncio

from agno.agent import Agent, RunResponse  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))



================================================
FILE: cookbook/models/xai/async_basic_stream.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))



================================================
FILE: cookbook/models/xai/async_tool_use.py
================================================
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))



================================================
FILE: cookbook/models/xai/basic.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")



================================================
FILE: cookbook/models/xai/basic_stream.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent, RunResponseEvent  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunResponseEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)



================================================
FILE: cookbook/models/xai/finance_agent.py
================================================
"""🗞️ Finance Agent - Your Personal Market Analyst!

This example shows how to create a sophisticated financial analyst that provides
comprehensive market insights using real-time data. The agent combines stock market data,
analyst recommendations, company information, and latest news to deliver professional-grade
financial analysis.

Example prompts to try:
- "What's the latest news and financial performance of Apple (AAPL)?"
- "Give me a detailed analysis of Tesla's (TSLA) current market position"
- "How are Microsoft's (MSFT) financials looking? Include analyst recommendations"
- "Analyze NVIDIA's (NVDA) stock performance and future outlook"
- "What's the market saying about Amazon's (AMZN) latest quarter?"

Run: `pip install openai yfinance agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    model=xAI(id="grok-3-mini-beta"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            stock_fundamentals=True,
            historical_prices=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=dedent("""\
        You are a seasoned Wall Street analyst with deep expertise in market analysis! 📊

        Follow these steps for comprehensive financial analysis:
        1. Market Overview
           - Latest stock price
           - 52-week high and low
        2. Financial Deep Dive
           - Key metrics (P/E, Market Cap, EPS)
        3. Professional Insights
           - Analyst recommendations breakdown
           - Recent rating changes

        4. Market Context
           - Industry trends and positioning
           - Competitive analysis
           - Market sentiment indicators

        Your reporting style:
        - Begin with an executive summary
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends (📈 📉)
        - Highlight key insights with bullet points
        - Compare metrics to industry averages
        - Include technical term explanations
        - End with a forward-looking analysis

        Risk Disclosure:
        - Always highlight potential risk factors
        - Note market uncertainties
        - Mention relevant regulatory concerns
    """),
    add_datetime_to_instructions=True,
    markdown=True,
)

# Example usage with detailed market analysis request
finance_agent.print_response(
    "Write a comprehensive report on TSLA",
    stream=True,
    stream_intermediate_steps=True,
)

# # Semiconductor market analysis example
# finance_agent.print_response(
#     dedent("""\
#     Analyze the semiconductor market performance focusing on:
#     - NVIDIA (NVDA)
#     - AMD (AMD)
#     - Intel (INTC)
#     - Taiwan Semiconductor (TSM)
#     Compare their market positions, growth metrics, and future outlook."""),
#     stream=True,
# )

# # Automotive market analysis example
# finance_agent.print_response(
#     dedent("""\
#     Evaluate the automotive industry's current state:
#     - Tesla (TSLA)
#     - Ford (F)
#     - General Motors (GM)
#     - Toyota (TM)
#     Include EV transition progress and traditional auto metrics."""),
#     stream=True,
# )

# More example prompts to explore:
"""
Advanced analysis queries:
1. "Compare Tesla's valuation metrics with traditional automakers"
2. "Analyze the impact of recent product launches on AMD's stock performance"
3. "How do Meta's financial metrics compare to its social media peers?"
4. "Evaluate Netflix's subscriber growth impact on financial metrics"
5. "Break down Amazon's revenue streams and segment performance"

Industry-specific analyses:
Semiconductor Market:
1. "How is the chip shortage affecting TSMC's market position?"
2. "Compare NVIDIA's AI chip revenue growth with competitors"
3. "Analyze Intel's foundry strategy impact on stock performance"
4. "Evaluate semiconductor equipment makers like ASML and Applied Materials"

Automotive Industry:
1. "Compare EV manufacturers' production metrics and margins"
2. "Analyze traditional automakers' EV transition progress"
3. "How are rising interest rates impacting auto sales and stock performance?"
4. "Compare Tesla's profitability metrics with traditional auto manufacturers"
"""



================================================
FILE: cookbook/models/xai/image_agent.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2-vision-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)



================================================
FILE: cookbook/models/xai/image_agent_bytes.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=xAI(id="grok-2-vision-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)



================================================
FILE: cookbook/models/xai/image_agent_with_memory.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2-vision-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    add_history_to_messages=True,
    num_history_responses=3,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
)

agent.print_response("Tell me where I can get more images?")



================================================
FILE: cookbook/models/xai/live_search_agent.py
================================================
from agno.agent import Agent
from agno.models.xai.xai import xAI

agent = Agent(
    model=xAI(
        id="grok-3",
        search_parameters={
            "mode": "on",
            "max_search_results": 20,
            "return_citations": True,
        },
    ),
    markdown=True,
)
agent.print_response("Provide me a digest of world news in the last 24 hours.")



================================================
FILE: cookbook/models/xai/live_search_agent_stream.py
================================================
from agno.agent import Agent
from agno.models.xai.xai import xAI

agent = Agent(
    model=xAI(
        id="grok-3",
        search_parameters={
            "mode": "on",
            "max_search_results": 20,
            "return_citations": True,
        },
    ),
    markdown=True,
)
agent.print_response(
    "Provide me a digest of world news in the last 24 hours.", stream=True
)



================================================
FILE: cookbook/models/xai/reasoning_agent.py
================================================
from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=xAI(id="grok-3-beta"),
    tools=[
        ReasoningTools(add_instructions=True, add_few_shot=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions=[
        "Use tables to display data",
        "Only output the report, no other text",
    ],
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report on TSLA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/models/xai/structured_output.py
================================================
import asyncio
from typing import List

from agno.agent import Agent
from agno.models.xai.xai import xAI
from agno.run.response import RunResponse
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=xAI(id="grok-2-latest"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

# Run the agent synchronously
structured_output_response: RunResponse = structured_output_agent.run(
    "Llamas ruling the world"
)
pprint(structured_output_response.content)



================================================
FILE: cookbook/models/xai/tool_use.py
================================================
"""Build a Web Search Agent using xAI."""

from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)



================================================
FILE: cookbook/observability/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/observability/agent_ops.py
================================================
"""
This example shows how to use agentops to log model calls.

Steps to get started with agentops:
1. Install agentops: pip install agentops
2. Obtain an API key from https://app.agentops.ai/
3. Export environment variables like AGENTOPS_API_KEY and OPENAI_API_KEY.
4. Run the script.

You can view the logs in the AgentOps dashboard: https://app.agentops.ai/
"""

import agentops
from agno.agent import Agent
from agno.models.openai import OpenAIChat

# Initialize AgentOps
agentops.init()

# Create and run an agent
agent = Agent(model=OpenAIChat(id="gpt-4o"))
response = agent.run("Share a 2 sentence horror story")

# Print the response
print(response.content)



================================================
FILE: cookbook/observability/arize_phoenix_via_openinference.py
================================================
"""
This example shows how to instrument your agno agent with OpenInference and send traces to Arize Phoenix.

1. Install dependencies: pip install arize-phoenix openai openinference-instrumentation-agno opentelemetry-sdk opentelemetry-exporter-otlp
2. Setup your Arize Phoenix account and get your API key: https://phoenix.arize.com/.
3. Set your Arize Phoenix API key as an environment variable:
  - export ARIZE_PHOENIX_API_KEY=<your-key>
"""

import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from phoenix.otel import register

os.environ["PHOENIX_CLIENT_HEADERS"] = f"api_key={os.getenv('ARIZE_PHOENIX_API_KEY')}"
os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "https://app.phoenix.arize.com"
# configure the Phoenix tracer
tracer_provider = register(
    project_name="agno-stock-price-agent",  # Default is 'default'
    auto_instrument=True,  # Automatically use the installed OpenInference instrumentation
)

agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")



================================================
FILE: cookbook/observability/arize_phoenix_via_openinference_local.py
================================================
"""
This example shows how to instrument your agno agent with OpenInference and send traces to Arize Phoenix.

1. Install dependencies: pip install arize-phoenix openai openinference-instrumentation-agno opentelemetry-sdk opentelemetry-exporter-otlp
2. Run `phoenix serve` to start the local collector.
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from phoenix.otel import register

os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"
# configure the Phoenix tracer
tracer_provider = register(
    project_name="agno-stock-price-agent",  # Default is 'default'
    auto_instrument=True,  # Automatically use the installed OpenInference instrumentation
)

agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")



================================================
FILE: cookbook/observability/atla_op.py
================================================
"""
This example shows how to add observability to your agno agent with Atla.

1. Install dependencies: pip install "atla-insights"
2. Sign up for an account at https://app.atla-ai.com
3. Set your Atla Insights API key as an environment variable:
  - export ATLA_API_KEY=<your-key>
"""

from os import getenv

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from atla_insights import configure, instrument_agno

configure(token=getenv("ATLA_API_KEY"))

agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

# Instrument and run
with instrument_agno("openai"):
    agent.print_response("What are the latest news about the stock market?")



================================================
FILE: cookbook/observability/langfuse_via_openinference.py
================================================
"""
This example shows how to instrument your agno agent with OpenInference and send traces to Langfuse.

1. Install dependencies: pip install openai langfuse opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-agno
2. Either self-host or sign up for an account at https://us.cloud.langfuse.com
3. Set your Langfuse API key as an environment variables:
  - export LANGFUSE_PUBLIC_KEY=<your-key>
  - export LANGFUSE_SECRET_KEY=<your-key>
"""

import base64
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  # 🇺🇸 US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" # 🇪🇺 EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" # 🏠 Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)


agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")



================================================
FILE: cookbook/observability/langfuse_via_openinference_response_model.py
================================================
"""
This example shows how to instrument your agno agent with OpenInference and send traces to Langfuse,
using an Agent with a response model.

1. Install dependencies: pip install openai langfuse opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-agno
2. Either self-host or sign up for an account at https://us.cloud.langfuse.com
3. Set your Langfuse API key as an environment variables:
  - export LANGFUSE_PUBLIC_KEY=<your-key>
  - export LANGFUSE_SECRET_KEY=<your-key>
"""

import base64
import os
from enum import Enum

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from pydantic import BaseModel, Field

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  # 🇺🇸 US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" # 🇪🇺 EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" # 🏠 Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)


class MarketArea(Enum):
    USA = "USA"
    UK = "UK"
    EU = "EU"
    ASIA = "ASIA"


class StockPrice(BaseModel):
    price: str = Field(description="The price of the stock")
    symbol: str = Field(description="The symbol of the stock")
    date: str = Field(description="Current day")
    area: MarketArea


agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. You check and return the current price of a stock.",
    debug_mode=True,
    response_model=StockPrice,
)

agent.print_response("What is the current price of Tesla?")



================================================
FILE: cookbook/observability/langfuse_via_openlit.py
================================================
"""
This example shows how to use langfuse via OpenLIT to trace model calls.

1. Install dependencies: pip install openai langfuse openlit opentelemetry-sdk opentelemetry-exporter-otlp
2. Set your Langfuse API key as an environment variables:
  - export LANGFUSE_PUBLIC_KEY=<your-key>
  - export LANGFUSE_SECRET_KEY=<your-key>
"""

import base64
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()

os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  # 🇺🇸 US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" # 🇪🇺 EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" # 🏠 Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"

from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

trace_provider = TracerProvider()
trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Sets the global default tracer provider
from opentelemetry import trace

trace.set_tracer_provider(trace_provider)

# Creates a tracer from the global tracer provider
tracer = trace.get_tracer(__name__)

import openlit

# Initialize OpenLIT instrumentation. The disable_batch flag is set to true to process traces immediately.
openlit.init(tracer=tracer, disable_batch=True)

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    debug_mode=True,
)

agent.print_response("What is currently trending on Twitter?")



================================================
FILE: cookbook/observability/langsmith_via_openinference.py
================================================
"""
This example shows how to instrument your agno agent with OpenInference and send traces to LangSmith.

1. Create a LangSmith account and get your API key: https://smith.langchain.com/
2. Set your LangSmith API key as an environment variable:
  - export LANGSMITH_API_KEY=<your-key>
  - export LANGSMITH_TRACING=true
  - export LANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com or https://api.smith.langchain.com
  - export LANGSMITH_PROJECT=<your-project-name>
3. Install dependencies: pip install openai openinference-instrumentation-agno opentelemetry-sdk opentelemetry-exporter-otlp
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

endpoint = "https://eu.api.smith.langchain.com/otel/v1/traces"
headers = {
    "x-api-key": os.getenv("LANGSMITH_API_KEY"),
    "Langsmith-Project": os.getenv("LANGSMITH_PROJECT"),
}


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(
    SimpleSpanProcessor(OTLPSpanExporter(endpoint=endpoint, headers=headers))
)

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)

agent = Agent(
    name="Stock Market Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    debug_mode=True,
)

agent.print_response("What is news on the stock market?")



================================================
FILE: cookbook/observability/langtrace_op.py
================================================
"""
This example shows how to instrument your agno agent with Langtrace.

1. Install dependencies: pip install langtrace-python-sdk
2. Sign up for an account at https://app.langtrace.ai/
3. Set your Langtrace API key as an environment variables:
  - export LANGTRACE_API_KEY=<your-key>
"""

# Must precede other imports
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from langtrace_python_sdk import langtrace  # type: ignore
from langtrace_python_sdk.utils.with_root_span import (
    with_langtrace_root_span,  # type: ignore
)

langtrace.init()

agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")



================================================
FILE: cookbook/observability/langwatch_op.py
================================================
"""
This example shows how to instrument your agno agent and send traces to LangWatch.

1. Install dependencies: pip install openai langwatch openinference-instrumentation-agno
2. Sign up for an account at https://app.langwatch.ai/
3. Set your LangWatch API key as an environment variables:
  - export LANGWATCH_API_KEY=<your-key>
"""

import os

import langwatch
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor

# Initialize LangWatch and instrument Agno
langwatch.setup(instrumentors=[AgnoInstrumentor()])

# Create and configure your Agno agent
agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")



================================================
FILE: cookbook/observability/weave_op.py
================================================
"""
This example shows how to use weave to log model calls.

Steps to get started with weave:
1. Install weave: pip install weave
2. Add weave.init('project-name') and weave.op() decorators to your functions
3. Authentication:
 - Go to https://wandb.ai and copy your API key from https://wandb.ai/authorize
 - Enter your API key in terminal when prompted
 Or
 - Export your API key as an environment variable:
    - export WANDB_API_KEY=<your-api-key>
"""

import weave
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True, debug_mode=True)

weave.init("agno")


@weave.op()
def run(content: str):
    return agent.run(content)


run("Share a 2 sentence horror story")



================================================
FILE: cookbook/observability/teams/langfuse_via_openinference_async_team.py
================================================
[Binary file]


================================================
FILE: cookbook/observability/teams/langfuse_via_openinference_team.py
================================================
import base64
import os
from uuid import uuid4

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  # 🇺🇸 US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" # 🇪🇺 EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" # 🏠 Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)

# First agent for market data
market_data_agent = Agent(
    name="Market Data Agent",
    role="Fetch and analyze stock market data",
    agent_id="market-data",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[
        YFinanceTools(stock_price=True, company_info=True, analyst_recommendations=True)
    ],
    instructions=[
        "You are a market data specialist.",
        "Focus on current stock prices and key metrics.",
        "Always present data in tables.",
    ],
)

# Second agent for news and research
news_agent = Agent(
    name="News Research Agent",
    role="Research company news",
    agent_id="news-research",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "You are a financial news analyst.",
        "Focus on recent company news and developments.",
        "Always cite your sources.",
    ],
)

# Create team with both agents
financial_team = Team(
    name="Financial Analysis Team",
    mode="coordinate",
    team_id=str(uuid4()),
    user_id=str(uuid4()),
    model=OpenAIChat(id="gpt-4.1"),
    members=[
        market_data_agent,
        news_agent,
    ],
    instructions=[
        "Coordinate between market data and news analysis.",
        "First get market data, then relevant news.",
        "Combine the information into a clear summary.",
    ],
    show_members_responses=True,
    markdown=True,
)

if __name__ == "__main__":
    financial_team.print_response(
        "Analyze Tesla (TSLA) stock - provide both current market data and recent significant news.",
        stream=True,
    )



================================================
FILE: cookbook/reasoning/README.md
================================================
# Reasoning

Reasoning gives Agents the ability to “think” before responding and “analyze” the results of their actions (i.e. tool calls), greatly improving the Agents’ ability to solve problems that require sequential tool calls.

Reasoning Agents go through an internal chain of thought before responding, working through different ideas, validating and correcting as needed. Agno supports 3 approaches to reasoning:

1. Reasoning Models
2. Reasoning Tools
3. Reasoning Agents and Teams

## Reasoning Models

Reasoning Models are pre-trained models that are used to reason about the world. You can try any supported Agno model and if that model has reasoning capabilities, it will be used to reason about the problem. 

See the [examples](./models/).

### Separate Reasoning Model

A powerful feature of Agno is the ability to use a separate reasoning model from the main model. This is useful when you want to use a more powerful reasoning model than the main model.

See the [examples](./models/openai/reasoning_gpt_4_1.py).

## Reasoning Tools

By giving a model a “think” tool, we can greatly improve its reasoning capabilities by providing a dedicated space for structured thinking. This is a simple, yet effective approach to add reasoning to non-reasoning models.

See the [examples](./tools/).

## Reasoning Agents and Teams

Reasoning Agents are a new type of multi-agent system developed by Agno that combines chain of thought reasoning with tool use.

You can enable reasoning on any Agent by setting reasoning=True.

When an Agent with reasoning=True is given a task, a separate “Reasoning Agent” first solves the problem using chain-of-thought. At each step, it calls tools to gather information, validate results, and iterate until it reaches a final answer. Once the Reasoning Agent has a final answer, it hands the results back to the original Agent to validate and provide a response.

See the [examples](./agents/).




================================================
FILE: cookbook/reasoning/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/playground.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.playground import Playground, serve_playground_app
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

claude_reasoning_agent = Agent(
    name="Claude Reasoning Finance Agent",
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    stream_intermediate_steps=True,
    instructions="Use tables where possible",
    markdown=True,
)


app = Playground(
    agents=[claude_reasoning_agent],
).get_app()

if __name__ == "__main__":
    serve_playground_app("playground:app", reload=True)



================================================
FILE: cookbook/reasoning/agents/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/agents/analyse_treaty_of_versailles.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = (
    "Analyze the key factors that led to the signing of the Treaty of Versailles in 1919. "
    "Discuss the political, economic, and social impacts of the treaty on Germany and how it "
    "contributed to the onset of World War II. Provide a nuanced assessment that includes "
    "multiple historical perspectives."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/capture_reasoning_content_default_COT.py
================================================
"""
Cookbook: Working with reasoning_content in Agents

This example demonstrates how to access and print the reasoning_content
when using either reasoning=True or setting a specific reasoning_model.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat

print("\n=== Example 1: Using reasoning=True (default COT) ===\n")

# Create agent with reasoning=True (default model COT)
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

# Run the agent (non-streaming)
print("Running with reasoning=True (non-streaming)...")
response = agent.run("What is the sum of the first 10 natural numbers?")

# Print the reasoning_content
print("\n--- reasoning_content from response ---")
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print(response.reasoning_content)
else:
    print("No reasoning_content found in response")


print("\n\n=== Example 2: Using a custom reasoning_model ===\n")

# Create agent with a specific reasoning_model
agent_with_reasoning_model = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=OpenAIChat(id="gpt-4o"),  # Should default to manual COT
    markdown=True,
)

# Run the agent (non-streaming)
print("Running with reasoning_model specified (non-streaming)...")
response = agent_with_reasoning_model.run(
    "What is the sum of the first 10 natural numbers?"
)

# Print the reasoning_content
print("\n--- reasoning_content from response ---")
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print(response.reasoning_content)
else:
    print("No reasoning_content found in response")


print("\n\n=== Example 3: Streaming with reasoning=True ===\n")

# Create a fresh agent for streaming
streaming_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

# Print response (which includes processing streaming responses)
print("Running with reasoning=True (streaming)...")
streaming_agent.print_response(
    "What is the value of 5! (factorial)?",
    stream=True,
    show_full_reasoning=True,
)

# Access reasoning_content from the agent's run_response after streaming
print("\n--- reasoning_content from agent.run_response after streaming ---")
if (
    hasattr(streaming_agent, "run_response")
    and streaming_agent.run_response
    and hasattr(streaming_agent.run_response, "reasoning_content")
    and streaming_agent.run_response.reasoning_content
):
    print(streaming_agent.run_response.reasoning_content)
else:
    print("No reasoning_content found in agent.run_response after streaming")


print("\n\n=== Example 4: Streaming with reasoning_model ===\n")

# Create a fresh agent with reasoning_model for streaming
streaming_agent_with_model = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=OpenAIChat(id="gpt-4o"),
    markdown=True,
)

# Print response (which includes processing streaming responses)
print("Running with reasoning_model specified (streaming)...")
streaming_agent_with_model.print_response(
    "What is the value of 5! (factorial)?",
    stream=True,
    show_full_reasoning=True,
)

# Access reasoning_content from the agent's run_response after streaming
print("\n--- reasoning_content from agent.run_response after streaming ---")
if (
    hasattr(streaming_agent_with_model, "run_response")
    and streaming_agent_with_model.run_response
    and hasattr(streaming_agent_with_model.run_response, "reasoning_content")
    and streaming_agent_with_model.run_response.reasoning_content
):
    print(streaming_agent_with_model.run_response.reasoning_content)
else:
    print("No reasoning_content found in agent.run_response after streaming")



================================================
FILE: cookbook/reasoning/agents/cerebras_llama_default_COT.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.cerebras import Cerebras

"""
This example demonstrates how it works when you pass a non-reasoning model as a reasoning model.
It defaults to using the default OpenAI reasoning model.
We recommend using the appropriate reasoning model or passing reasoning=True for the default COT.
"""

reasoning_agent = Agent(
    model=Cerebras(id="llama-3.3-70b"),
    reasoning=True,
    debug_mode=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/reasoning/agents/default_chain_of_thought.py
================================================
"""
This example demonstrates how it works when you pass a non-reasoning model as a reasoning model.
It defaults to using the default OpenAI reasoning model.
We recommend using the appropriate reasoning model or passing reasoning=True for the default COT.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=OpenAIChat(
        id="gpt-4o", max_tokens=1200
    ),  # Should default to manual COT because it is not a native reasoning model
    markdown=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)


# It uses the default model of the Agent
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o", max_tokens=1200),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/reasoning/agents/fibonacci.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Give me steps to write a python script for fibonacci series"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/finance_agent.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions="Use tables to display data",
    use_json_mode=True,
    show_tool_calls=True,
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA", stream=True, show_full_reasoning=True
)



================================================
FILE: cookbook/reasoning/agents/ibm_watsonx_default_COT.py
================================================
from agno.agent import Agent, RunResponse  # noqa
from agno.models.ibm import WatsonX

"""
This example demonstrates how it works when you pass a non-reasoning model as a reasoning model.
It defaults to using the default OpenAI reasoning model.
We recommend using the appropriate reasoning model or passing reasoning=True for the default COT.
"""

reasoning_agent = Agent(
    model=WatsonX(id="meta-llama/llama-3-3-70b-instruct"),
    reasoning=True,
    debug_mode=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/reasoning/agents/is_9_11_bigger_than_9_9.py
================================================
from agno.agent import Agent
from agno.cli.console import console
from agno.models.openai import OpenAIChat

task = "9.11 and 9.9 -- which is bigger?"

regular_agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

console.rule("[bold green]Regular Agent[/bold green]")
regular_agent.print_response(task, stream=True)
console.rule("[bold yellow]Reasoning Agent[/bold yellow]")
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/life_in_500000_years.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Write a short story about life in 500000 years"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/logical_puzzle.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/mathematical_proof.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Prove that for any positive integer n, the sum of the first n odd numbers is equal to n squared. Provide a detailed proof."

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/mistral_reasoning_cot.py
================================================
from agno.agent import Agent
from agno.models.mistral import MistralChat

# It uses the default model of the Agent
reasoning_agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    reasoning=True,
    markdown=True,
    use_json_mode=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/reasoning/agents/plan_itenerary.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Plan an itinerary from Los Angeles to Las Vegas"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/python_101_curriculum.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Craft a curriculum for Python 101"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/scientific_research.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = (
    "Read the following abstract of a scientific paper and provide a critical evaluation of its methodology,"
    "results, conclusions, and any potential biases or flaws:\n\n"
    "Abstract: This study examines the effect of a new teaching method on student performance in mathematics. "
    "A sample of 30 students was selected from a single school and taught using the new method over one semester. "
    "The results showed a 15% increase in test scores compared to the previous semester. "
    "The study concludes that the new teaching method is effective in improving mathematical performance among high school students."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/ship_of_theseus.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = (
    "Discuss the concept of 'The Ship of Theseus' and its implications on the notions of identity and change. "
    "Present arguments for and against the idea that an object that has had all of its components replaced remains "
    "fundamentally the same object. Conclude with your own reasoned position on the matter."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)



================================================
FILE: cookbook/reasoning/agents/strawberry.py
================================================
import asyncio

from agno.agent import Agent
from agno.cli.console import console
from agno.models.openai import OpenAIChat

task = "How many 'r' are in the word 'strawberry'?"

regular_agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)


async def main():
    console.rule("[bold blue]Counting 'r's in 'strawberry'[/bold blue]")

    console.rule("[bold green]Regular Agent[/bold green]")
    await regular_agent.aprint_response(task, stream=True)
    console.rule("[bold yellow]Reasoning Agent[/bold yellow]")
    await reasoning_agent.aprint_response(task, stream=True, show_full_reasoning=True)


asyncio.run(main())



================================================
FILE: cookbook/reasoning/agents/trolley_problem.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
    show_full_reasoning=True,
)



================================================
FILE: cookbook/reasoning/models/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/models/azure_ai_foundry/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/models/azure_ai_foundry/reasoning_model_deepseek.py
================================================
import os

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry

agent = Agent(
    model=AzureAIFoundry(id="gpt-4o"),
    reasoning=True,
    reasoning_model=AzureAIFoundry(
        id="DeepSeek-R1",
        azure_endpoint=os.getenv("AZURE_ENDPOINT"),
        api_key=os.getenv("AZURE_API_KEY"),
    ),
)

agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)



================================================
FILE: cookbook/reasoning/models/azure_openai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/models/azure_openai/o1.py
================================================
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="o1"))
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)



================================================
FILE: cookbook/reasoning/models/azure_openai/o3_mini_with_tools.py
================================================
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=AzureOpenAI(id="o3-mini"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions="Use tables to display data.",
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)



================================================
FILE: cookbook/reasoning/models/azure_openai/o4_mini.py
================================================
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="o4-mini"))
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)



================================================
FILE: cookbook/reasoning/models/azure_openai/reasoning_model_gpt_4_1.py
================================================
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"), reasoning_model=AzureOpenAI(id="gpt-4.1")
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)



================================================
FILE: cookbook/reasoning/models/deepseek/9_11_or_9_9.py
================================================
from agno.agent import Agent
from agno.cli.console import console
from agno.models.anthropic import Claude
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "9.11 and 9.9 -- which is bigger?"

regular_agent_claude = Agent(model=Claude("claude-3-5-sonnet-20241022"))
reasoning_agent_claude = Agent(
    model=Claude("claude-3-5-sonnet-20241022"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
)

regular_agent_openai = Agent(model=OpenAIChat(id="gpt-4o"))
reasoning_agent_openai = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
)

console.rule("[bold blue]Regular Claude Agent[/bold blue]")
regular_agent_claude.print_response(task, stream=True)

console.rule("[bold green]Claude Reasoning Agent[/bold green]")
reasoning_agent_claude.print_response(task, stream=True)

console.rule("[bold red]Regular OpenAI Agent[/bold red]")
regular_agent_openai.print_response(task, stream=True)

console.rule("[bold yellow]OpenAI Reasoning Agent[/bold yellow]")
reasoning_agent_openai.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/models/deepseek/analyse_treaty_of_versailles.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "Analyze the key factors that led to the signing of the Treaty of Versailles in 1919. "
    "Discuss the political, economic, and social impacts of the treaty on Germany and how it "
    "contributed to the onset of World War II. Provide a nuanced assessment that includes "
    "multiple historical perspectives."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/ethical_dilemma.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "You are a train conductor faced with an emergency: the brakes have failed, and the train is heading towards "
    "five people tied on the track. You can divert the train onto another track, but there is one person tied there. "
    "Do you divert the train, sacrificing one to save five? Provide a well-reasoned answer considering utilitarian "
    "and deontological ethical frameworks. "
    "Provide your answer also as an ascii art diagram."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/fibonacci.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Give me steps to write a python script for fibonacci series"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/finance_agent.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Use tables where possible"],
    show_tool_calls=True,
    markdown=True,
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
)
reasoning_agent.print_response("Write a report comparing NVDA to TSLA", stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/life_in_500000_years.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Write a short story about life in 500000 years"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/logical_puzzle.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/mathematical_proof.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Prove that for any positive integer n, the sum of the first n odd numbers is equal to n squared. Provide a detailed proof."

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/plan_itenerary.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Plan an itinerary from Los Angeles to Las Vegas"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/python_101_curriculum.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Craft a curriculum for Python 101"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/scientific_research.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "Read the following abstract of a scientific paper and provide a critical evaluation of its methodology,"
    "results, conclusions, and any potential biases or flaws:\n\n"
    "Abstract: This study examines the effect of a new teaching method on student performance in mathematics. "
    "A sample of 30 students was selected from a single school and taught using the new method over one semester. "
    "The results showed a 15% increase in test scores compared to the previous semester. "
    "The study concludes that the new teaching method is effective in improving mathematical performance among high school students."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/ship_of_theseus.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "Discuss the concept of 'The Ship of Theseus' and its implications on the notions of identity and change. "
    "Present arguments for and against the idea that an object that has had all of its components replaced remains "
    "fundamentally the same object. Conclude with your own reasoned position on the matter."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/deepseek/strawberry.py
================================================
import asyncio

from agno.agent import Agent
from agno.cli.console import console
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "How many 'r' are in the word 'strawberry'?"

regular_agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)


async def main():
    console.rule("[bold blue]Counting 'r's in 'strawberry'[/bold blue]")

    console.rule("[bold green]Regular Agent[/bold green]")
    await regular_agent.aprint_response(task, stream=True)
    console.rule("[bold yellow]Reasoning Agent[/bold yellow]")
    await reasoning_agent.aprint_response(task, stream=True)


asyncio.run(main())



================================================
FILE: cookbook/reasoning/models/deepseek/trolley_problem.py
================================================
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "You are a philosopher tasked with analyzing the classic 'Trolley Problem'. In this scenario, a runaway trolley "
    "is barreling down the tracks towards five people who are tied up and unable to move. You are standing next to "
    "a large stranger on a footbridge above the tracks. The only way to save the five people is to push this stranger "
    "off the bridge onto the tracks below. This will kill the stranger, but save the five people on the tracks. "
    "Should you push the stranger to save the five people? Provide a well-reasoned answer considering utilitarian, "
    "deontological, and virtue ethics frameworks. "
    "Include a simple ASCII art diagram to illustrate the scenario."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)



================================================
FILE: cookbook/reasoning/models/groq/9_11_or_9_9.py
================================================
from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(
    model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
    markdown=True,
)
agent.print_response("9.11 and 9.9 -- which is bigger?", stream=True)



================================================
FILE: cookbook/reasoning/models/groq/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/models/groq/deepseek_plus_claude.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.groq import Groq

deepseek_plus_claude = Agent(
    model=Claude(id="claude-3-7-sonnet-20250219"),
    reasoning_model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)
deepseek_plus_claude.print_response("9.11 and 9.9 -- which is bigger?", stream=True)



================================================
FILE: cookbook/reasoning/models/ollama/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/models/ollama/reasoning_model_deepseek.py
================================================
from agno.agent import Agent
from agno.models.ollama.chat import Ollama

agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    reasoning_model=Ollama(id="deepseek-r1:14b", max_tokens=4096),
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)



================================================
FILE: cookbook/reasoning/models/openai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/models/openai/o1_pro.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="o1-pro"))
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)



================================================
FILE: cookbook/reasoning/models/openai/o3_mini.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/reasoning/models/openai/o3_mini_with_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions="Use tables to display data.",
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)



================================================
FILE: cookbook/reasoning/models/openai/o4_mini.py
================================================
from agno.agent import Agent
from agno.models.openai.responses import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="o3-mini"))
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)



================================================
FILE: cookbook/reasoning/models/openai/reasoning_effort.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="o3-mini", reasoning_effort="high"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions="Use tables to display data.",
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)



================================================
FILE: cookbook/reasoning/models/openai/reasoning_model_gpt_4_1.py
================================================
from agno.agent import Agent
from agno.models.openai.responses import OpenAIResponses

agent = Agent(
    model=OpenAIResponses(id="gpt-4o-mini"),
    reasoning_model=OpenAIResponses(id="gpt-4.1"),
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)



================================================
FILE: cookbook/reasoning/models/openai/reasoning_summary.py
================================================
"""This example shows how to get reasoning summaries with our OpenAIResponses model.
Useful for contexts where a long reasoning process is relevant and directly relevant to the user."""

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.yfinance import YFinanceTools

# Setup the reasoning Agent
agent = Agent(
    model=OpenAIResponses(
        id="o4-mini",
        reasoning_summary="auto",  # Requesting a reasoning summary
    ),
    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True)],
    instructions="Use tables to display the analysis",
    show_tool_calls=True,
    markdown=True,
)

agent.print_response(
    "Write a brief report comparing NVDA to TSLA",
    stream=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/reasoning/models/xai/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/models/xai/reasoning_effort.py
================================================
from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=xAI(id="grok-3-mini-fast", reasoning_effort="high"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions="Use tables to display data.",
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)



================================================
FILE: cookbook/reasoning/teams/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/teams/finance_team_chain_of_thought.py
================================================
import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_instructions=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=[
        "You are a financial data specialist. Provide concise and accurate data.",
        "Use tables to display stock prices, fundamentals (P/E, Market Cap), and recommendations.",
        "Clearly state the company name and ticker symbol.",
        "Briefly summarize recent company-specific news if available.",
        "Focus on delivering the requested financial data points clearly.",
    ],
    add_datetime_to_instructions=True,
)

team_leader = Team(
    name="Reasoning Finance Team Leader",
    mode="coordinate",
    members=[
        web_agent,
        finance_agent,
    ],
    instructions=[
        "Only output the final answer, no other text.",
        "Use tables to display data",
    ],
    markdown=True,
    reasoning=True,
    show_members_responses=True,
)


async def run_team(task: str):
    await team_leader.aprint_response(
        task,
        stream=True,
        stream_intermediate_steps=True,
        show_full_reasoning=True,
    )


if __name__ == "__main__":
    asyncio.run(
        run_team(
            dedent("""\
    Analyze the impact of recent US tariffs on market performance across these key sectors:
    - Steel & Aluminum: (X, NUE, AA)
    - Technology Hardware: (AAPL, DELL, HPQ)
    - Agricultural Products: (ADM, BG, INGR)
    - Automotive: (F, GM, TSLA)

    For each sector:
    1. Compare stock performance before and after tariff implementation
    2. Identify supply chain disruptions and cost impact percentages
    3. Analyze companies' strategic responses (reshoring, price adjustments, supplier diversification)
    4. Assess analyst outlook changes directly attributed to tariff policies
    """)
        )
    )



================================================
FILE: cookbook/reasoning/teams/knowledge_tool_team.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.knowledge.url import UrlKnowledge
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.knowledge import KnowledgeTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.lancedb import LanceDb, SearchType

agno_docs = UrlKnowledge(
    urls=["https://www.paulgraham.com/read.html"],
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
    ),
)

knowledge_tools = KnowledgeTools(
    knowledge=agno_docs,
    think=True,
    search=True,
    analyze=True,
    add_few_shot=True,
)

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_instructions=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    add_datetime_to_instructions=True,
)

team_leader = Team(
    name="Reasoning Finance Team",
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[knowledge_tools],
    instructions=[
        "Only output the final answer, no other text.",
        "Use tables to display data",
    ],
    markdown=True,
    show_members_responses=True,
    enable_agentic_context=True,
    add_datetime_to_instructions=True,
    success_criteria="The team has successfully completed the task.",
)


def run_team(task: str):
    # Comment out after first run
    agno_docs.load(recreate=True)
    team_leader.print_response(
        task,
        stream=True,
        stream_intermediate_steps=True,
        show_full_reasoning=True,
    )


if __name__ == "__main__":
    run_team("What does Paul Graham talk about the need to read in this essay?")



================================================
FILE: cookbook/reasoning/teams/reasoning_finance_team.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_instructions=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=[
        "You are a financial data specialist. Provide concise and accurate data.",
        "Use tables to display stock prices, fundamentals (P/E, Market Cap), and recommendations.",
        "Clearly state the company name and ticker symbol.",
        "Briefly summarize recent company-specific news if available.",
        "Focus on delivering the requested financial data points clearly.",
    ],
    add_datetime_to_instructions=True,
)

team_leader = Team(
    name="Reasoning Finance Team Leader",
    mode="coordinate",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Only output the final answer, no other text.",
        "Use tables to display data",
    ],
    markdown=True,
    show_members_responses=True,
    enable_agentic_context=True,
    add_datetime_to_instructions=True,
    success_criteria="The team has successfully completed the task.",
)


def run_team(task: str):
    team_leader.print_response(
        task,
        stream=True,
        stream_intermediate_steps=True,
        show_full_reasoning=True,
    )


if __name__ == "__main__":
    run_team(
        dedent("""\
    Analyze the impact of recent US tariffs on market performance across these key sectors:
    - Steel & Aluminum: (X, NUE, AA)
    - Technology Hardware: (AAPL, DELL, HPQ)
    - Agricultural Products: (ADM, BG, INGR)
    - Automotive: (F, GM, TSLA)

    For each sector:
    1. Compare stock performance before and after tariff implementation
    2. Identify supply chain disruptions and cost impact percentages
    3. Analyze companies' strategic responses (reshoring, price adjustments, supplier diversification)
    4. Assess analyst outlook changes directly attributed to tariff policies
    """)
    )

    # run_team(dedent("""\
    # Assess the impact of recent semiconductor export controls on:
    # - US chip designers (Nvidia, AMD, Intel)
    # - Asian manufacturers (TSMC, Samsung)
    # - Equipment makers (ASML, Applied Materials)
    # Include effects on R&D investments, supply chain restructuring, and market share shifts."""))

    # run_team(dedent("""\
    # Compare the retail sector's response to consumer goods tariffs:
    # - Major retailers (Walmart, Target, Amazon)
    # - Consumer brands (Nike, Apple, Hasbro)
    # - Discount retailers (Dollar General, Five Below)
    # Include pricing strategy changes, inventory management, and consumer behavior impacts."""))

    # run_team(dedent("""\
    # Analyze the semiconductor market performance focusing on:
    # - NVIDIA (NVDA)
    # - AMD (AMD)
    # - Intel (INTC)
    # - Taiwan Semiconductor (TSM)
    # Compare their market positions, growth metrics, and future outlook."""))

    # run_team(dedent("""\
    # Evaluate the automotive industry's current state:
    # - Tesla (TSLA)
    # - Ford (F)
    # - General Motors (GM)
    # - Toyota (TM)
    # Include EV transition progress and traditional auto metrics."""))

    # run_team(dedent("""\
    # Compare the financial metrics of Apple (AAPL) and Google (GOOGL):
    # - Market Cap
    # - P/E Ratio
    # - Revenue Growth
    # - Profit Margin"""))

    # run_team(dedent("""\
    # Analyze the impact of recent Chinese solar panel tariffs on:
    # - US solar manufacturers (First Solar, SunPower)
    # - Chinese exporters (JinkoSolar, Trina Solar)
    # - US installation companies (Sunrun, SunPower)
    # Include effects on pricing, supply chains, and installation rates."""))



================================================
FILE: cookbook/reasoning/tools/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/reasoning/tools/azure_openai_reasoning_tools.py
================================================
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    tools=[
        ReasoningTools(
            think=True,
            analyze=True,
            add_instructions=True,
            add_few_shot=True,
        ),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables where possible. Think about the problem step by step.",
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA.",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/reasoning/tools/capture_reasoning_content_knowledge_tools.py
================================================
"""
Cookbook: Capturing reasoning_content with KnowledgeTools

This example demonstrates how to access and print the reasoning_content
when using KnowledgeTools with URL knowledge, in both streaming and non-streaming modes.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.tools.knowledge import KnowledgeTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base containing information from a URL
print("Setting up URL knowledge base...")
agno_docs = UrlKnowledge(
    urls=["https://www.paulgraham.com/read.html"],
    # Use LanceDB as the vector database
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="cookbook_knowledge_tools",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Only load if needed
try:
    print("Loading knowledge base (skip if already exists)...")
    agno_docs.load(recreate=False)
    print("Knowledge base loaded.")
except:
    print("Creating new knowledge base...")
    agno_docs.load(recreate=True)
    print("Knowledge base created.")


print("\n=== Example 1: Using KnowledgeTools in non-streaming mode ===\n")

# Create agent with KnowledgeTools
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        KnowledgeTools(
            knowledge=agno_docs,
            think=True,
            search=True,
            analyze=True,
            add_instructions=True,
        )
    ],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        Use the knowledge tools to organize your thoughts, search for information, 
        and analyze results step-by-step.
        \
    """),
    markdown=True,
    stream=False,
)

# Run the agent (non-streaming)
print("Running with KnowledgeTools (non-streaming)...")
agent.print_response(
    "What does Paul Graham explain here with respect to need to read?", stream=False
)

# Print the reasoning_content
print("\n--- reasoning_content from agent.run_response ---")
if (
    hasattr(agent, "run_response")
    and agent.run_response
    and hasattr(agent.run_response, "reasoning_content")
    and agent.run_response.reasoning_content
):
    print(agent.run_response.reasoning_content)
else:
    print("No reasoning_content found in agent.run_response")


print("\n\n=== Example 2: Using KnowledgeTools in streaming mode ===\n")

# Create a fresh agent for streaming
streaming_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        KnowledgeTools(
            knowledge=agno_docs,
            think=True,
            search=True,
            analyze=True,
            add_instructions=True,
        )
    ],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        Use the knowledge tools to organize your thoughts, search for information, 
        and analyze results step-by-step.
        \
    """),
    markdown=True,
)

# Print response (which includes processing streaming responses)
print("Running with KnowledgeTools (streaming)...")
streaming_agent.print_response(
    "What does Paul Graham explain here with respect to need to read?",
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)

# Access reasoning_content from the agent's run_response after streaming
print("\n--- reasoning_content from agent.run_response after streaming ---")
if (
    hasattr(streaming_agent, "run_response")
    and streaming_agent.run_response
    and hasattr(streaming_agent.run_response, "reasoning_content")
    and streaming_agent.run_response.reasoning_content
):
    print(streaming_agent.run_response.reasoning_content)
else:
    print("No reasoning_content found in agent.run_response after streaming")



================================================
FILE: cookbook/reasoning/tools/capture_reasoning_content_reasoning_tools.py
================================================
"""Test for reasoning_content generation

This script tests whether reasoning_content is properly populated in the RunResponse
when using ReasoningTools. It tests both streaming and non-streaming modes.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.reasoning import ReasoningTools

"""Test function to verify reasoning_content is populated in RunResponse."""
print("\n=== Testing reasoning_content generation ===\n")

# Create an agent with ReasoningTools
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        Use step-by-step reasoning to solve the problem.
        \
    """),
)

# Test 1: Non-streaming mode
print("Running with stream=False...")
response = agent.run("What is the sum of the first 10 natural numbers?", stream=False)

# Check reasoning_content
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print("✅ reasoning_content FOUND in non-streaming response")
    print(f"   Length: {len(response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (non-streaming) ===")
    preview = response.reasoning_content[:1000]
    if len(response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print("❌ reasoning_content NOT FOUND in non-streaming response")

# Test 2: Streaming mode with a fresh agent
print("\nRunning with stream=True...")

# Create a fresh agent for streaming test
streaming_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        Use step-by-step reasoning to solve the problem.
        \
    """),
)

# Consume all streaming responses
_ = list(
    streaming_agent.run(
        "What is the value of 5! (factorial)?",
        stream=True,
        stream_intermediate_steps=True,
    )
)

# Check the agent's run_response directly after streaming is complete
if hasattr(streaming_agent, "run_response") and streaming_agent.run_response:
    if (
        hasattr(streaming_agent.run_response, "reasoning_content")
        and streaming_agent.run_response.reasoning_content
    ):
        print("✅ reasoning_content FOUND in agent's run_response after streaming")
        print(
            f"   Length: {len(streaming_agent.run_response.reasoning_content)} characters"
        )
        print("\n=== reasoning_content preview (streaming) ===")
        preview = streaming_agent.run_response.reasoning_content[:1000]
        if len(streaming_agent.run_response.reasoning_content) > 1000:
            preview += "..."
        print(preview)
    else:
        print("❌ reasoning_content NOT FOUND in agent's run_response after streaming")
else:
    print("❌ Agent's run_response is not accessible after streaming")



================================================
FILE: cookbook/reasoning/tools/capture_reasoning_content_thinking_tools.py
================================================
"""
Cookbook: Capturing reasoning_content with ThinkingTools

This example demonstrates how to access and print the reasoning_content
when using ThinkingTools, in both streaming and non-streaming modes.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.thinking import ThinkingTools

print("\n=== Example 1: Using ThinkingTools in non-streaming mode ===\n")

# Create agent with ThinkingTools
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ThinkingTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        Use the think tool to organize your thoughts and approach problems step-by-step.
        \
    """),
    markdown=True,
)

# Run the agent (non-streaming)
print("Running with ThinkingTools (non-streaming)...")
response = agent.print_response(
    "What is the sum of the first 10 natural numbers?", stream=False
)

# Print the reasoning_content
print("\n--- reasoning_content from agent.run_response ---")
if (
    hasattr(agent, "run_response")
    and agent.run_response
    and hasattr(agent.run_response, "reasoning_content")
    and agent.run_response.reasoning_content
):
    print(agent.run_response.reasoning_content)
else:
    print("No reasoning_content found in agent.run_response")


print("\n\n=== Example 2: Using ThinkingTools in streaming mode ===\n")

# Create a fresh agent for streaming
streaming_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ThinkingTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        Use the think tool to organize your thoughts and approach problems step-by-step.
        \
    """),
    markdown=True,
)

# Print response (which includes processing streaming responses)
print("Running with ThinkingTools (streaming)...")
streaming_agent.print_response(
    "What is the value of 5! (factorial)?",
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)

# Access reasoning_content from the agent's run_response after streaming
print("\n--- reasoning_content from agent.run_response after streaming ---")
if (
    hasattr(streaming_agent, "run_response")
    and streaming_agent.run_response
    and hasattr(streaming_agent.run_response, "reasoning_content")
    and streaming_agent.run_response.reasoning_content
):
    print(streaming_agent.run_response.reasoning_content)
else:
    print("No reasoning_content found in agent.run_response after streaming")



================================================
FILE: cookbook/reasoning/tools/cerebras_llama_reasoning_tools.py
================================================
"""🧠 Problem-Solving Reasoning Agent

This example shows how to create an agent that uses the ReasoningTools to solve
complex problems through step-by-step reasoning. The agent breaks down questions,
analyzes intermediate results, and builds structured reasoning paths to arrive at
well-justified conclusions.

Example prompts to try:
- "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river."
- "Is it better to rent or buy a home given current interest rates?"
- "Evaluate the pros and cons of remote work versus office work."
- "How would increasing interest rates affect the housing market?"
- "What's the best strategy for saving for retirement in your 30s?"
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=Cerebras(id="llama-3.3-70b"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        
        Your approach to problems:
        1. First, break down complex questions into component parts
        2. Clearly state your assumptions
        3. Develop a structured reasoning path
        4. Consider multiple perspectives
        5. Evaluate evidence and counter-arguments
        6. Draw well-justified conclusions
        
        When solving problems:
        - Use explicit step-by-step reasoning
        - Identify key variables and constraints
        - Explore alternative scenarios
        - Highlight areas of uncertainty
        - Explain your thought process clearly
        - Consider both short and long-term implications
        - Evaluate trade-offs explicitly
        
        For quantitative problems:
        - Show your calculations
        - Explain the significance of numbers
        - Consider confidence intervals when appropriate
        - Identify source data reliability
        
        For qualitative reasoning:
        - Assess how different factors interact
        - Consider psychological and social dynamics
        - Evaluate practical constraints
        - Address value considerations
        \
    """),
    add_datetime_to_instructions=True,
    stream_intermediate_steps=True,
    show_tool_calls=True,
    markdown=True,
)

# Example usage with a complex reasoning problem
reasoning_agent.print_response(
    "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river. "
    "The boat is only big enough for the man and one item. If left unattended together, the fox will "
    "eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?",
    stream=True,
)

# # Economic analysis example
# reasoning_agent.print_response(
#     "Is it better to rent or buy a home given current interest rates, inflation, and market trends? "
#     "Consider both financial and lifestyle factors in your analysis.",
#     stream=True
# )

# # Strategic decision-making example
# reasoning_agent.print_response(
#     "A startup has $500,000 in funding and needs to decide between spending it on marketing or "
#     "product development. They want to maximize growth and user acquisition within 12 months. "
#     "What factors should they consider and how should they analyze this decision?",
#     stream=True
# )



================================================
FILE: cookbook/reasoning/tools/claude_reasoning_tools.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(enable_all=True),
    ],
    instructions="Use tables to display data.",
    markdown=True,
)

# Semiconductor market analysis example
reasoning_agent.print_response(
    """\
    Analyze the semiconductor market performance focusing on:
    - NVIDIA (NVDA)
    - AMD (AMD)
    - Intel (INTC)
    - Taiwan Semiconductor (TSM)
    Compare their market positions, growth metrics, and future outlook.""",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/reasoning/tools/claude_thinking_tools.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.thinking import ThinkingTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ThinkingTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables where possible",
    markdown=True,
)

if __name__ == "__main__":
    reasoning_agent.print_response(
        "Write a report on NVDA. Only the report, no other text.",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/reasoning/tools/gemini_finance_agent.py
================================================
# -*- coding: utf-8 -*-
"""gemini_finance_agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XYnlyi_LFuvtR0qSHDLOWMRGMmgi60RQ
"""

# ! pip install -U agno

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.thinking import ThinkingTools
from agno.tools.yfinance import YFinanceTools

thinking_agent = Agent(
    model=Gemini(id="gemini-2.0-flash"),
    tools=[
        ThinkingTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables where possible",
    show_tool_calls=True,
    markdown=True,
    stream_intermediate_steps=True,
)
thinking_agent.print_response(
    "Write a report comparing NVDA to TSLA in detail", stream=True, show_reasoning=True
)



================================================
FILE: cookbook/reasoning/tools/gemini_reasoning_tools.py
================================================
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=Gemini(id="gemini-2.5-pro-preview-03-25"),
    tools=[
        ReasoningTools(
            think=True,
            analyze=True,
            add_instructions=True,
        ),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables where possible",
    stream_intermediate_steps=True,
    show_tool_calls=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA.", show_full_reasoning=True
)



================================================
FILE: cookbook/reasoning/tools/groq_llama_finance_agent.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.thinking import ThinkingTools
from agno.tools.yfinance import YFinanceTools

thinking_llama = Agent(
    model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"),
    tools=[
        ThinkingTools(),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions=dedent("""\
    ## General Instructions
    - Always start by using the think tool to map out the steps needed to complete the task.
    - After receiving tool results, use the think tool as a scratchpad to validate the results for correctness
    - Before responding to the user, use the think tool to jot down final thoughts and ideas.
    - Present final outputs in well-organized tables whenever possible.

    ## Using the think tool
    At every step, use the think tool as a scratchpad to:
    - Restate the object in your own words to ensure full comprehension.
    - List the  specific rules that apply to the current request
    - Check if all required information is collected and is valid
    - Verify that the planned action completes the task\
    """),
    show_tool_calls=True,
    markdown=True,
)
thinking_llama.print_response("Write a report comparing NVDA to TSLA", stream=True)



================================================
FILE: cookbook/reasoning/tools/ibm_watsonx_reasoning_tools.py
================================================
from textwrap import dedent

from agno.agent import Agent, RunResponse  # noqa
from agno.models.ibm import WatsonX
from agno.tools.reasoning import ReasoningTools

"""🧠 Problem-Solving Reasoning Agent

This example shows how to create an agent that uses the ReasoningTools to solve
complex problems through step-by-step reasoning. The agent breaks down questions,
analyzes intermediate results, and builds structured reasoning paths to arrive at
well-justified conclusions.

Example prompts to try:
- "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river."
- "Is it better to rent or buy a home given current interest rates?"
- "Evaluate the pros and cons of remote work versus office work."
- "How would increasing interest rates affect the housing market?"
- "What's the best strategy for saving for retirement in your 30s?"
"""


reasoning_agent = Agent(
    model=WatsonX(id="meta-llama/llama-3-3-70b-instruct"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        
        Your approach to problems:
        1. First, break down complex questions into component parts
        2. Clearly state your assumptions
        3. Develop a structured reasoning path
        4. Consider multiple perspectives
        5. Evaluate evidence and counter-arguments
        6. Draw well-justified conclusions
        
        When solving problems:
        - Use explicit step-by-step reasoning
        - Identify key variables and constraints
        - Explore alternative scenarios
        - Highlight areas of uncertainty
        - Explain your thought process clearly
        - Consider both short and long-term implications
        - Evaluate trade-offs explicitly
        
        For quantitative problems:
        - Show your calculations
        - Explain the significance of numbers
        - Consider confidence intervals when appropriate
        - Identify source data reliability
        
        For qualitative reasoning:
        - Assess how different factors interact
        - Consider psychological and social dynamics
        - Evaluate practical constraints
        - Address value considerations
        \
    """),
    add_datetime_to_instructions=True,
    stream_intermediate_steps=True,
    show_tool_calls=True,
    markdown=True,
)

# Example usage with a complex reasoning problem
reasoning_agent.print_response(
    "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river. "
    "The boat is only big enough for the man and one item. If left unattended together, the fox will "
    "eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?",
    stream=True,
)

# # Economic analysis example
# reasoning_agent.print_response(
#     "Is it better to rent or buy a home given current interest rates, inflation, and market trends? "
#     "Consider both financial and lifestyle factors in your analysis.",
#     stream=True
# )

# # Strategic decision-making example
# reasoning_agent.print_response(
#     "A startup has $500,000 in funding and needs to decide between spending it on marketing or "
#     "product development. They want to maximize growth and user acquisition within 12 months. "
#     "What factors should they consider and how should they analyze this decision?",
#     stream=True
# )



================================================
FILE: cookbook/reasoning/tools/knowledge_tools.py
================================================
"""
1. Run: `pip install openai agno lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY
3. Run: `python cookbook/reasoning/tools/knowledge_tools.py` to run the agent
"""

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.tools.knowledge import KnowledgeTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base containing information from a URL
agno_docs = UrlKnowledge(
    urls=["https://docs.agno.com/llms-full.txt"],
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

knowledge_tools = KnowledgeTools(
    knowledge=agno_docs,
    think=True,
    search=True,
    analyze=True,
    add_few_shot=True,
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[knowledge_tools],
    show_tool_calls=True,
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, comment after first run
    agno_docs.load(recreate=True)
    agent.print_response("How do I build multi-agent teams with Agno?", stream=True)



================================================
FILE: cookbook/reasoning/tools/llama_reasoning_tools.py
================================================
from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[
        ReasoningTools(
            think=True,
            analyze=True,
            add_instructions=True,
        ),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables where possible",
    markdown=True,
    show_tool_calls=True,
)
reasoning_agent.print_response(
    "What is the NVDA stock price? Write me a report",
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/reasoning/tools/ollama_reasoning_tools.py
================================================
from agno.agent import Agent
from agno.models.ollama.chat import Ollama
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    tools=[
        ReasoningTools(
            think=True,
            analyze=True,
            add_instructions=True,
            add_few_shot=True,
        ),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables where possible",
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/reasoning/tools/openai_reasoning_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ReasoningTools(
            think=True,
            analyze=True,
            add_instructions=True,
            add_few_shot=True,
        ),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables where possible",
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/reasoning/tools/reasoning_tools.py
================================================
"""🧠 Problem-Solving Reasoning Agent

This example shows how to create an agent that uses the ReasoningTools to solve
complex problems through step-by-step reasoning. The agent breaks down questions,
analyzes intermediate results, and builds structured reasoning paths to arrive at
well-justified conclusions.

Example prompts to try:
- "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river."
- "Is it better to rent or buy a home given current interest rates?"
- "Evaluate the pros and cons of remote work versus office work."
- "How would increasing interest rates affect the housing market?"
- "What's the best strategy for saving for retirement in your 30s?"
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠
        
        Your approach to problems:
        1. First, break down complex questions into component parts
        2. Clearly state your assumptions
        3. Develop a structured reasoning path
        4. Consider multiple perspectives
        5. Evaluate evidence and counter-arguments
        6. Draw well-justified conclusions
        
        When solving problems:
        - Use explicit step-by-step reasoning
        - Identify key variables and constraints
        - Explore alternative scenarios
        - Highlight areas of uncertainty
        - Explain your thought process clearly
        - Consider both short and long-term implications
        - Evaluate trade-offs explicitly
        
        For quantitative problems:
        - Show your calculations
        - Explain the significance of numbers
        - Consider confidence intervals when appropriate
        - Identify source data reliability
        
        For qualitative reasoning:
        - Assess how different factors interact
        - Consider psychological and social dynamics
        - Evaluate practical constraints
        - Address value considerations
        \
    """),
    add_datetime_to_instructions=True,
    stream_intermediate_steps=True,
    show_tool_calls=True,
    markdown=True,
)

# Example usage with a complex reasoning problem
reasoning_agent.print_response(
    "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river. "
    "The boat is only big enough for the man and one item. If left unattended together, the fox will "
    "eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?",
    stream=True,
)

# # Economic analysis example
# reasoning_agent.print_response(
#     "Is it better to rent or buy a home given current interest rates, inflation, and market trends? "
#     "Consider both financial and lifestyle factors in your analysis.",
#     stream=True
# )

# # Strategic decision-making example
# reasoning_agent.print_response(
#     "A startup has $500,000 in funding and needs to decide between spending it on marketing or "
#     "product development. They want to maximize growth and user acquisition within 12 months. "
#     "What factors should they consider and how should they analyze this decision?",
#     stream=True
# )



================================================
FILE: cookbook/reasoning/tools/thinking_playground.py
================================================
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.playground import Playground, serve_playground_app
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.thinking import ThinkingTools
from agno.tools.yfinance import YFinanceTools

agent_storage: str = "tmp/agents.db"

thinking_web_agent = Agent(
    name="Thinking Web Agent",
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[ThinkingTools(add_instructions=True), DuckDuckGoTools()],
    instructions=["Always include sources"],
    # Store the agent sessions in a sqlite database
    storage=SqliteAgentStorage(table_name="web_agent", db_file=agent_storage),
    # Adds the current date and time to the instructions
    add_datetime_to_instructions=True,
    # Adds the history of the conversation to the messages
    add_history_to_messages=True,
    # Number of history responses to add to the messages
    num_history_responses=5,
    # Adds markdown formatting to the messages
    markdown=True,
)

thinking_finance_agent = Agent(
    name="Thinking Finance Agent",
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ThinkingTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables to display data",
    storage=SqliteAgentStorage(table_name="finance_agent", db_file=agent_storage),
    add_datetime_to_instructions=True,
    add_history_to_messages=True,
    num_history_responses=5,
    markdown=True,
)

playground = Playground(
    agents=[thinking_web_agent, thinking_finance_agent],
    name="Thinking Playground",
    description="A playground for thinking",
    app_id="thinking-playground",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="thinking_playground:app", reload=True)



================================================
FILE: cookbook/reasoning/tools/vercel_reasoning_tools.py
================================================
from agno.agent import Agent
from agno.models.vercel import v0
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=v0(id="v0-1.0-md"),
    tools=[
        ReasoningTools(add_instructions=True, add_few_shot=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions=[
        "Use tables to display data",
        "Only output the report, no other text",
    ],
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report on TSLA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/scripts/_utils.sh
================================================
#!/bin/bash

############################################################################
# Collection of helper functions to import in other scripts
############################################################################

space_to_continue() {
  read -n1 -r -p "Press Enter/Space to continue...  " key
  if [ "$key" = '' ]; then
    # Space pressed, pass
    :
  else
    exit 1
  fi
  echo ""
}

print_horizontal_line() {
  echo "------------------------------------------------------------"
}

print_heading() {
  print_horizontal_line
  echo "-*- $1"
  print_horizontal_line
}

print_info() {
  echo "-*- $1"
}



================================================
FILE: cookbook/scripts/cookbook_runner.py
================================================
import os
import subprocess
import sys

import click
import inquirer

"""
CLI Tool: Cookbook runner

This tool allows users to interactively navigate through directories, select a target directory,
and execute all `.py` files in the selected directory. It also tracks cookbooks that fail to execute
and prompts the user to rerun all failed cookbooks until all succeed or the user decides to exit.

Usage:
    1. Run the tool from the command line:
        python cookbook/scripts/cookbook_runner.py [base_directory]

    2. Navigate through the directory structure using the interactive prompts:
        - Select a directory to drill down or choose the current directory.
        - The default starting directory is the current working directory (".").

    3. The tool runs all `.py` files in the selected directory and logs any that fail.

    4. If any cookbook fails, the tool prompts the user to rerun all failed cookbooks:
        - Select "yes" to rerun all failed cookbooks.
        - Select "no" to exit, and the tool will log remaining failures.

Dependencies:
    - click
    - inquirer

Example:
    $ python cookbook/scripts/cookbook_runner.py cookbook
    Current directory: /cookbook
    > [Select this directory]
    > folder1
    > folder2
    > [Go back]

    Running script1.py...
    Running script2.py...

    --- Error Log ---
    Script: failing_cookbook.py failed to execute.

    Some cookbooks failed. Do you want to rerun all failed cookbooks? [y/N]: y
"""


def select_directory(base_directory):
    while True:
        # Get all subdirectories and files in the current directory
        items = [
            item
            for item in os.listdir(base_directory)
            if os.path.isdir(os.path.join(base_directory, item))
        ]
        items.sort()
        # Add options to select the current directory or go back
        items.insert(0, "[Select this directory]")
        if base_directory != "/":
            items.insert(1, "[Go back]")

        # Prompt the user to select an option
        questions = [
            inquirer.List(
                "selected_item",
                message=f"Current directory: {base_directory}",
                choices=items,
            )
        ]
        answers = inquirer.prompt(questions)

        if not answers or "selected_item" not in answers:
            print("No selection made. Exiting.")
            return None

        selected_item = answers["selected_item"]

        # Handle the user's choice
        if selected_item == "[Select this directory]":
            return base_directory
        elif selected_item == "[Go back]":
            base_directory = os.path.dirname(base_directory)
        else:
            # Drill down into the selected directory
            base_directory = os.path.join(base_directory, selected_item)


def run_python_script(script_path):
    """
    Run a Python script and display its output in real time.
    Pauses execution on failure to allow user intervention.
    """
    print(f"Running {script_path}...\n")
    try:
        with subprocess.Popen(
            ["python", script_path],
            stdout=sys.stdout,
            stderr=sys.stderr,
            text=True,
        ) as process:
            process.wait()

        if process.returncode != 0:
            raise subprocess.CalledProcessError(process.returncode, script_path)

        return True  # Script ran successfully

    except subprocess.CalledProcessError as e:
        print(f"\nError: {script_path} failed with return code {e.returncode}.")
        return False  # Script failed

    except Exception as e:
        print(f"\nUnexpected error while running {script_path}: {e}")
        return False  # Script failed


@click.command()
@click.argument(
    "base_directory",
    type=click.Path(exists=True, file_okay=False, dir_okay=True),
    default=".",
)
def drill_and_run_scripts(base_directory):
    """
    A CLI tool that lets the user drill down into directories and runs all .py files in the selected directory.
    Tracks cookbooks that encounter errors and keeps prompting to rerun until user decides to exit.
    """
    selected_directory = select_directory(base_directory)

    if not selected_directory:
        print("No directory selected. Exiting.")
        return

    print(f"\nRunning .py files in directory: {selected_directory}\n")

    python_files = [
        filename
        for filename in os.listdir(selected_directory)
        if filename.endswith(".py")
        and os.path.isfile(os.path.join(selected_directory, filename))
        and filename not in ["__pycache__", "__init__.py"]
    ]

    if not python_files:
        print("No .py files found in the selected directory.")
        return

    error_log = []

    for py_file in python_files:
        file_path = os.path.join(selected_directory, py_file)
        if not run_python_script(file_path):
            error_log.append(py_file)

    while error_log:
        print("\n--- Error Log ---")
        for py_file in error_log:
            print(f"Cookbook: {py_file} failed to execute.\n")

        # Prompt the user for action
        questions = [
            inquirer.List(
                "action",
                message="Some cookbooks failed. What would you like to do?",
                choices=[
                    "Retry failed scripts",
                    "Pause for manual intervention and retry",
                    "Exit with error log",
                ],
            )
        ]
        answers = inquirer.prompt(questions)

        if answers and answers.get("action") == "Retry failed scripts":
            print("\nRe-running failed cookbooks...\n")
            new_error_log = []
            for py_file in error_log:
                file_path = os.path.join(selected_directory, py_file)
                if not run_python_script(file_path):
                    new_error_log.append(py_file)

            error_log = new_error_log

        elif (
            answers
            and answers.get("action") == "Pause for manual intervention and retry"
        ):
            print(
                "\nPaused for manual intervention. A shell is now open for you to execute commands (e.g., installing packages)."
            )
            print("Type 'exit' or 'Ctrl+D' to return and retry failed cookbooks.\n")

            # Open an interactive shell for the user
            try:
                subprocess.run(["bash"], check=True)  # For Unix-like systems
            except FileNotFoundError:
                try:
                    subprocess.run(["cmd"], check=True, shell=True)  # For Windows
                except Exception as e:
                    print(f"Error opening shell: {e}")
                    print(
                        "Please manually install required packages in a separate terminal."
                    )

            print("\nRe-running failed cookbooks after manual intervention...\n")
            new_error_log = []
            for py_file in error_log:
                file_path = os.path.join(selected_directory, py_file)
                if not run_python_script(file_path):
                    new_error_log.append(py_file)

            error_log = new_error_log

        elif answers and answers.get("action") == "Exit with error log":
            print("\nExiting. Remaining cookbooks that failed:")
            for py_file in error_log:
                print(f" - {py_file}")
            return

    print("\nAll cookbooks executed successfully!")


if __name__ == "__main__":
    drill_and_run_scripts()



================================================
FILE: cookbook/scripts/format.bat
================================================
@echo off
REM ###########################################################################
REM # Format the cookbook using ruff
REM # Usage: cookbook\scripts\format.bat
REM ###########################################################################

SETLOCAL ENABLEDELAYEDEXPANSION

REM Get current directory
SET "CURR_DIR=%~dp0"
SET "COOKBOOK_DIR=%CURR_DIR%\.."

ECHO.
ECHO ##################################################
ECHO # Formatting cookbook
ECHO ##################################################
ECHO.

REM Check if ruff is installed
python -c "import ruff" 2>nul
IF %ERRORLEVEL% NEQ 0 (
    ECHO [ERROR] ruff is not installed. Please install it with: pip install ruff
    EXIT /B 1
)

ECHO.
ECHO ##################################################
ECHO # Running: ruff format %COOKBOOK_DIR%
ECHO ##################################################
ECHO.

python -m ruff format "%COOKBOOK_DIR%"

ECHO.
ECHO ##################################################
ECHO # Running: ruff check --select I --fix %COOKBOOK_DIR%
ECHO ##################################################
ECHO.

python -m ruff check --select I --fix "%COOKBOOK_DIR%"

ECHO [INFO] cookbook formatting complete.
EXIT /B 


================================================
FILE: cookbook/scripts/format.sh
================================================
#!/bin/bash

############################################################################
# Format the cookbook using ruff
# Usage: ./libs/agno/scripts/format.sh
############################################################################

CURR_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
COOKBOOK_DIR="$(dirname ${CURR_DIR})"
source ${CURR_DIR}/_utils.sh

print_heading "Formatting cookbook"

print_heading "Running: ruff format ${COOKBOOK_DIR}"
ruff format ${COOKBOOK_DIR}

print_heading "Running: ruff check --select I --fix ${COOKBOOK_DIR}"
ruff check --select I --fix ${COOKBOOK_DIR}



================================================
FILE: cookbook/scripts/run_cassandra.bat
================================================
docker run -d ^
 -p 9042:9042 cassandra:latest ^
 --name cassandra-db 


================================================
FILE: cookbook/scripts/run_cassandra.sh
================================================
docker run -d \
 -p 9042:9042 cassandra:latest \
 --name cassandra-db


================================================
FILE: cookbook/scripts/run_clickhouse.bat
================================================
docker run -d ^
  -e CLICKHOUSE_DB=ai ^
  -e CLICKHOUSE_USER=ai ^
  -e CLICKHOUSE_PASSWORD=ai ^
  -e CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1 ^
  -v clickhouse_data:/var/lib/clickhouse/ ^
  -v clickhouse_log:/var/log/clickhouse-server/ ^
  -p 8123:8123 ^
  -p 9000:9000 ^
  --ulimit nofile=262144:262144 ^
  --name clickhouse-server ^
  clickhouse/clickhouse-server 


================================================
FILE: cookbook/scripts/run_clickhouse.sh
================================================
docker run -d \
  -e CLICKHOUSE_DB=ai \
  -e CLICKHOUSE_USER=ai \
  -e CLICKHOUSE_PASSWORD=ai \
  -e CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1 \
  -v clickhouse_data:/var/lib/clickhouse/ \
  -v clickhouse_log:/var/log/clickhouse-server/ \
  -p 8123:8123 \
  -p 9000:9000 \
  --ulimit nofile=262144:262144 \
  --name clickhouse-server \
  clickhouse/clickhouse-server



================================================
FILE: cookbook/scripts/run_couchbase.bat
================================================
docker run -d --name couchbase-server ^
   -p 8091-8096:8091-8096 ^
   -p 11210:11210 ^
   -e COUCHBASE_ADMINISTRATOR_USERNAME=Administrator ^
   -e COUCHBASE_ADMINISTRATOR_PASSWORD=password ^
   couchbase:latest 


================================================
FILE: cookbook/scripts/run_couchbase.sh
================================================
docker run -d --name couchbase-server \
   -p 8091-8096:8091-8096 \
   -p 11210:11210 \
   -e COUCHBASE_ADMINISTRATOR_USERNAME=Administrator \
   -e COUCHBASE_ADMINISTRATOR_PASSWORD=password \
   couchbase:latest



================================================
FILE: cookbook/scripts/run_mongodb.bat
================================================
docker run -d ^
  --name local-mongo ^
  -p 27017:27017 ^
  -e MONGO_INITDB_ROOT_USERNAME=mongoadmin ^
  -e MONGO_INITDB_ROOT_PASSWORD=secret ^
  mongo 


================================================
FILE: cookbook/scripts/run_mongodb.sh
================================================
docker run -d \
  --name local-mongo \
  -p 27017:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=mongoadmin \
  -e MONGO_INITDB_ROOT_PASSWORD=secret \
  mongo


================================================
FILE: cookbook/scripts/run_mysql.bat
================================================
docker run -d ^
  --name mysql ^
  -e MYSQL_ROOT_PASSWORD=ai ^
  -e MYSQL_DATABASE=ai ^
  -e MYSQL_USER=ai ^
  -e MYSQL_PASSWORD=ai ^
  -p 3306:3306 ^
  -d mysql:8 


================================================
FILE: cookbook/scripts/run_mysql.sh
================================================
docker run -d \
  --name mysql \
  -e MYSQL_ROOT_PASSWORD=ai \
  -e MYSQL_DATABASE=ai \
  -e MYSQL_USER=ai \
  -e MYSQL_PASSWORD=ai \
  -p 3306:3306 \
  -d mysql:8



================================================
FILE: cookbook/scripts/run_pgvector.bat
================================================
docker run -d ^
  -e POSTGRES_DB=ai ^
  -e POSTGRES_USER=ai ^
  -e POSTGRES_PASSWORD=ai ^
  -e PGDATA=/var/lib/postgresql/data/pgdata ^
  -v pgvolume:/var/lib/postgresql/data ^
  -p 5532:5432 ^
  --name pgvector ^
  agnohq/pgvector:16 


================================================
FILE: cookbook/scripts/run_pgvector.sh
================================================
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16



================================================
FILE: cookbook/scripts/run_qdrant.bat
================================================
docker run -d ^
  --name qdrant ^
  -p 6333:6333 ^
  -p 6334:6334 ^
  -v %cd%/tmp/qdrant_storage:/qdrant/storage:z ^
  qdrant/qdrant 


================================================
FILE: cookbook/scripts/run_qdrant.sh
================================================
docker run -d \
  --name qdrant \
  -p 6333:6333 \
  -p 6334:6334 \
  -v $(pwd)/tmp/qdrant_storage:/qdrant/storage:z \
  qdrant/qdrant


================================================
FILE: cookbook/scripts/run_redis.bat
================================================
docker run -d ^
  --name my-redis ^
  -p 6379:6379 ^
  redis 


================================================
FILE: cookbook/scripts/run_redis.sh
================================================
docker run -d \
  --name my-redis \
  -p 6379:6379 \
  redis



================================================
FILE: cookbook/scripts/run_singlestore.bat
================================================
docker run -d --name singlestoredb ^
  -p 3306:3306 ^
  -p 8080:8080 ^
  -v /tmp:/var/lib/memsql ^
  -e ROOT_PASSWORD=admin ^
  -e SINGLESTORE_DB=AGNO ^
  -e SINGLESTORE_USER=root ^
  -e SINGLESTORE_PASSWORD=password ^
  memsql/cluster-in-a-box

docker start singlestoredb

set SINGLESTORE_HOST=localhost
set SINGLESTORE_PORT=3306
set SINGLESTORE_USERNAME=root
set SINGLESTORE_PASSWORD=admin
set SINGLESTORE_DATABASE=AGNO 


================================================
FILE: cookbook/scripts/run_singlestore.sh
================================================
docker run -d --name singlestoredb \
  -p 3306:3306 \
  -p 8080:8080 \
  -v /tmp:/var/lib/memsql \
  -e ROOT_PASSWORD=admin \
  -e SINGLESTORE_DB=AGNO \
  -e SINGLESTORE_USER=root \
  -e SINGLESTORE_PASSWORD=password \
  memsql/cluster-in-a-box

docker start singlestoredb

export SINGLESTORE_HOST="localhost"
export SINGLESTORE_PORT="3306"
export SINGLESTORE_USERNAME="root"
export SINGLESTORE_PASSWORD="admin"
export SINGLESTORE_DATABASE="AGNO"



================================================
FILE: cookbook/scripts/run_surrealdb.bat
================================================
docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root 


================================================
FILE: cookbook/scripts/run_surrealdb.sh
================================================
docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root


================================================
FILE: cookbook/scripts/run_weviate.bat
================================================
docker run -d ^
        -p 8080:8080 ^
        -p 50051:50051 ^
        --name weaviate ^
        cr.weaviate.io/semitechnologies/weaviate:1.28.4 


================================================
FILE: cookbook/scripts/run_weviate.sh
================================================
docker run -d \
        -p 8080:8080 \
        -p 50051:50051 \
        --name weaviate \
        cr.weaviate.io/semitechnologies/weaviate:1.28.4 



================================================
FILE: cookbook/scripts/lightrag-init/docker-compose.yml
================================================
services:
  lightrag:
    container_name: lightrag
    image: ghcr.io/hkuds/lightrag:latest
    ports:
      - "${PORT:-9621}:9621"
    volumes:
      - ./data/rag_storage:/app/data/rag_storage
      - ./data/inputs:/app/data/inputs
      - ./config.ini:/app/config.ini
      - ./.env:/app/.env
    env_file:
      - .env
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"


================================================
FILE: cookbook/scripts/lightrag-init/example.env
================================================
### This is sample file of .env


### Server Configuration
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='My Graph KB'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"
OLLAMA_EMULATING_MODEL_TAG=latest
# WORKERS=2
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### Login Configuration
# AUTH_ACCOUNTS='admin:admin123,user1:pass456'
# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server
# TOKEN_EXPIRE_HOURS=48
# GUEST_TOKEN_EXPIRE_HOURS=24
# JWT_ALGORITHM=HS256

### API-Key to access LightRAG Server API
# LIGHTRAG_API_KEY=your-secure-api-key-here
# WHITELIST_PATHS=/health,/api/*

### Optional SSL Configuration
# SSL=true
# SSL_CERTFILE=/path/to/cert.pem
# SSL_KEYFILE=/path/to/key.pem

### Directory Configuration (defaults to current working directory)
### Should not be set if deploy by docker (Set by Dockerfile instead of .env)
### Default value is ./inputs and ./rag_storage
# INPUT_DIR=<absolute_path_for_doc_input_dir>
# WORKING_DIR=<absolute_path_for_working_dir>

### Max nodes return from grap retrieval
# MAX_GRAPH_NODES=1000

### Logging level
# LOG_LEVEL=INFO
# VERBOSE=False
# LOG_MAX_BYTES=10485760
# LOG_BACKUP_COUNT=5
### Logfile location (defaults to current working directory)
# LOG_DIR=/path/to/log/directory

### Settings for RAG query
# HISTORY_TURNS=3
# COSINE_THRESHOLD=0.2
# TOP_K=60
# MAX_TOKEN_TEXT_CHUNK=4000
# MAX_TOKEN_RELATION_DESC=4000
# MAX_TOKEN_ENTITY_DESC=4000

### Entity and ralation summarization configuration
### Language: English, Chinese, French, German ...
SUMMARY_LANGUAGE=English
### Number of duplicated entities/edges to trigger LLM re-summary on merge ( at least 3 is recommented)
# FORCE_LLM_SUMMARY_ON_MERGE=6
### Max tokens for entity/relations description after merge
# MAX_TOKEN_SUMMARY=500

### Number of parallel processing documents(Less than MAX_ASYNC/2 is recommended)
# MAX_PARALLEL_INSERT=2
### Chunk size for document splitting, 500~1500 is recommended
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100

### LLM Configuration
ENABLE_LLM_CACHE=true
ENABLE_LLM_CACHE_FOR_EXTRACT=true
### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=240
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0
### Max concurrency requests of LLM
MAX_ASYNC=4
### MAX_TOKENS: max tokens send to LLM for entity relation summaries (less than context size of the model)
### MAX_TOKENS: set as num_ctx option for Ollama by API Server
MAX_TOKENS=32768
### LLM Binding type: openai, ollama, lollms, azure_openai
LLM_BINDING=openai
LLM_MODEL=gpt-4o
LLM_BINDING_HOST=https://api.openai.com/v1
LLM_BINDING_API_KEY=sk-proj-***
### Optional for Azure
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
# AZURE_OPENAI_DEPLOYMENT=gpt-4o

### Embedding Configuration
### Embedding Binding type: openai, ollama, lollms, azure_openai
EMBEDDING_BINDING=openai
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIM=3072
EMBEDDING_BINDING_API_KEY=sk-proj-***
EMBEDDING_BINDING_HOST=https://api.openai.com/v1
### Num of chunks send to Embedding in single request
# EMBEDDING_BATCH_NUM=32
### Max concurrency requests for Embedding
# EMBEDDING_FUNC_MAX_ASYNC=16
### Maximum tokens sent to Embedding for each chunk (no longer in use?)
# MAX_EMBED_TOKENS=8192
### Optional for Azure
# AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large
# AZURE_EMBEDDING_API_VERSION=2023-05-15

### Data storage selection
# LIGHTRAG_KV_STORAGE=PGKVStorage
# LIGHTRAG_VECTOR_STORAGE=PGVectorStorage
# LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=Neo4JStorage

### TiDB Configuration (Deprecated)
# TIDB_HOST=localhost
# TIDB_PORT=4000
# TIDB_USER=your_username
# TIDB_PASSWORD='your_password'
# TIDB_DATABASE=your_database
### separating all data from difference Lightrag instances(deprecating)
# TIDB_WORKSPACE=default

### PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=ai
POSTGRES_PASSWORD='ai'
POSTGRES_DATABASE=ai
POSTGRES_MAX_CONNECTIONS=12
### separating all data from difference Lightrag instances(deprecating)
# POSTGRES_WORKSPACE=default

### Neo4j Configuration
#NEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io
NEO4J_URI=bolt://neo4j:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD='testpassword'

### Independent AGM Configuration(not for AMG embedded in PostreSQL)
# AGE_POSTGRES_DB=
# AGE_POSTGRES_USER=
# AGE_POSTGRES_PASSWORD=
# AGE_POSTGRES_HOST=
# AGE_POSTGRES_PORT=8529

# AGE Graph Name(apply to PostgreSQL and independent AGM)
### AGE_GRAPH_NAME is precated
# AGE_GRAPH_NAME=lightrag

### MongoDB Configuration
MONGO_URI=mongodb://root:root@localhost:27017/
MONGO_DATABASE=LightRAG
### separating all data from difference Lightrag instances(deprecating)
# MONGODB_GRAPH=false

### Milvus Configuration
MILVUS_URI=http://localhost:19530
MILVUS_DB_NAME=lightrag
# MILVUS_USER=root
# MILVUS_PASSWORD=your_password
# MILVUS_TOKEN=your_token

### Qdrant
QDRANT_URL=http://localhost:16333
# QDRANT_API_KEY=your-api-key

### Redis
REDIS_URI=redis://localhost:6379



================================================
FILE: cookbook/scripts/lightrag-init/run_lightrag.sh
================================================
docker-compose -f cookbook/scripts/lightrag-init/docker-compose.yml up -d


================================================
FILE: cookbook/scripts/lightrag-init/stop_lightrag.sh
================================================
docker-compose -f cookbook/scripts/lightrag-init/docker-compose.yml down



================================================
FILE: cookbook/scripts/mysql-init/init.sql
================================================
-- Create 'users' table
CREATE TABLE IF NOT EXISTS users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create 'products' table
CREATE TABLE IF NOT EXISTS products (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    price DECIMAL(10,2) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);



================================================
FILE: cookbook/storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/dynamodb_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/dynamodb_storage/dynamodb_storage_for_agent.py
================================================
"""Run `pip install ddgs boto3 openai` to install dependencies."""

from agno.agent import Agent
from agno.storage.dynamodb import DynamoDbStorage
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    storage=DynamoDbStorage(table_name="agent_sessions", region_name="us-east-1"),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/storage/dynamodb_storage/dynamodb_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/dynamodb_storage/dynamodb_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.dynamodb import DynamoDbStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=DynamoDbStorage(table_name="team_sessions", region_name="us-east-1"),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/dynamodb_storage/dynamodb_storage_for_workflow.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.dynamodb import DynamoDbStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=DynamoDbStorage(
            table_name="workflow_sessions", region_name="us-east-1"
        ),
        debug_mode=False,
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/examples/multi_user_multi_session.py
================================================
from agno.agent import Agent
from agno.memory.v2 import Memory
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Multi-user, multi-session only work with Memory.V2
    memory=Memory(),
    add_history_to_messages=True,
    num_history_runs=3,
)

user_1_id = "user_101"
user_2_id = "user_102"

user_1_session_id = "session_101"
user_2_session_id = "session_102"

# Start the session with user 1
agent.print_response(
    "Tell me a 5 second short story about a robot.",
    user_id=user_1_id,
    session_id=user_1_session_id,
)
# Continue the session with user 1
agent.print_response(
    "Now tell me a joke.", user_id=user_1_id, session_id=user_1_session_id
)

# Start the session with user 2
agent.print_response(
    "Tell me about quantum physics.", user_id=user_2_id, session_id=user_2_session_id
)
# Continue the session with user 2
agent.print_response(
    "What is the speed of light?", user_id=user_2_id, session_id=user_2_session_id
)

# Ask the agent to give a summary of the conversation, this will use the history from the previous messages
agent.print_response(
    "Give me a summary of our conversation.",
    user_id=user_1_id,
    session_id=user_1_session_id,
)



================================================
FILE: cookbook/storage/examples/sqlite_storage.py
================================================
"""Run `pip install agno openai sqlalchemy` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Fix the session id to continue the same session across execution cycles
    session_id="fixed_id_for_demo",
    storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/data.db"),
    add_history_to_messages=True,
    num_history_runs=3,
)
agent.print_response("What was my last question?")
agent.print_response("What is the capital of France?")
agent.print_response("What was my last question?")
pprint(agent.get_messages_for_session())



================================================
FILE: cookbook/storage/gcs_storage/README.md
================================================
# Google Cloud Storage for Agno

This repository provides an example of how to use the `GCSJsonStorage` class as a storage backend for an Agno agent. The storage backend stores session data as JSON blobs in a Google Cloud Storage (GCS) bucket.

> **Note:** The bucket name must be provided explicitly when initializing the storage class. Location and credentials are optional; if not provided, the default credentials (from `GOOGLE_APPLICATION_CREDENTIALS` or the current gcloud CLI project) will be used.

## Prerequisites

- **Google Cloud SDK:**
  Install the [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) and run `gcloud init` to configure your account and project.

- **GCS Permissions:**
  Ensure your account has sufficient permissions (e.g., Storage Admin) to create and manage GCS buckets. You can grant these permissions using:

```bash
  gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
      --member="user:YOUR_EMAIL@example.com" \
      --role="roles/storage.admin"
```


- **Authentication:**
To use default credentials from your gcloud CLI session, run:

```bash
gcloud auth application-default login
```

  - Alternatively, if using a service account, set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of your service account JSON file.

- **Python Dependencies:**

Install the required Python packages:


```bash
pip install google-auth google-cloud-storage openai ddgs
```


## Example Script

### Debugging and Bucket Dump

In the example script, a global variable `DEBUG_MODE` controls whether the bucket contents are printed at the end of execution.
Set `DEBUG_MODE = True` in the script to see content of the bucket.

```bash
gcloud init
gcloud auth application-default login
python gcs_json_storage_for_agent.py
```

## Local Testing with Fake GCS

If you want to test the storage functionality locally without using real GCS, you can use [fake-gcs-server](https://github.com/fsouza/fake-gcs-server) :

### Setup Fake GCS with Docker


2. **Install Docker:**

Make sure Docker is installed on your system.

4. **
Create a `docker-compose.yml` File**  in your project root with the following content:


```yaml
version: '3.8'
services:
  fake-gcs-server:
    image: fsouza/fake-gcs-server:latest
    ports:
      - "4443:4443"
    command: ["-scheme", "http", "-port", "4443", "-public-host", "localhost"]
    volumes:
      - ./fake-gcs-data:/data
```

6. **Start the Fake GCS Server:**


```bash
docker-compose up -d
```

This will start the fake GCS server on `http://localhost:4443`.


### Configuring the Script to Use Fake GCS


Set the environment variable so the GCS client directs API calls to the emulator:



```bash
export STORAGE_EMULATOR_HOST="http://localhost:4443"
python gcs_json_storage_for_agent.py
```


When using Fake GCS, authentication isn’t enforced. The client will automatically detect the emulator endpoint.



================================================
FILE: cookbook/storage/gcs_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/gcs_storage/gcs_json_storage_for_agent.py
================================================
import uuid

import google.auth
from agno.agent import Agent
from agno.storage.gcs_json import GCSJsonStorage
from agno.tools.duckduckgo import DuckDuckGoTools

DEBUG_MODE = False
# Obtain the default credentials and project id from your gcloud CLI session.
credentials, project_id = google.auth.default()

# Generate a unique bucket name using a base name and a UUID4 suffix.
base_bucket_name = "example-gcs-bucket"
unique_bucket_name = f"{base_bucket_name}-{uuid.uuid4().hex[:12]}"
print(f"Using bucket: {unique_bucket_name}")

# Initialize GCSJsonStorage with explicit credentials, unique bucket name, and project.
storage = GCSJsonStorage(
    bucket_name=unique_bucket_name,
    prefix="agent/",
    project=project_id,
    credentials=credentials,
)

# Create the bucket (if it doesn't exist, it will be created).
storage.create()


# Initialize the Agno agent1 with the new storage backend and a DuckDuckGo search tool.
agent1 = Agent(
    storage=storage,
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
    debug_mode=DEBUG_MODE,
)

# Execute sample queries.
agent1.print_response("How many people live in Canada?")
agent1.print_response("What is their national anthem called?")

# create a new agent and make sure it pursues the conversation
agent2 = Agent(
    storage=storage,
    session_id=agent1.session_id,
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
    debug_mode=DEBUG_MODE,
)

agent2.print_response("What's the name of the country we discussed?")
agent2.print_response("What is that country's national sport?")

# After running the agent1, print the content of the bucket: blob names and JSON content.
if DEBUG_MODE:
    print(f"\nBucket {storage.bucket_name} contents:")
    sessions = storage.get_all_sessions()
    for session in sessions:
        print(f"Session {session.session_id}:\n\t{session.memory}")
        print("-" * 40)



================================================
FILE: cookbook/storage/in_memory_storage/README.md
================================================
# In-Memory Storage

This directory contains examples demonstrating how to use `InMemoryStorage` with Agno agents, workflows, and teams.

## Overview

`InMemoryStorage` provides a flexible, lightweight storage solution that keeps all session data in memory, with the option to hook it up to any custom persistent storage solution.

### Highlights

- **No setup or additional dependencies**: No installations or database setup required.
- **Flexible storage**: Use the built-in dictionary or provide your own for custom persistence.

### Important Notes

- **Data Persistence**: Session data is **not persistent** across program restarts unless you provide an external dictionary with your own persistence mechanism.
- **Memory Usage**: All session data is stored in RAM. For applications with many long sessions, monitor memory usage.

## Usage

### Basic Setup

```python
from agno.storage.in_memory import InMemoryStorage

# For agents (default)
agent_storage = InMemoryStorage()

# For workflows
workflow_storage = InMemoryStorage(mode="workflow")

# For teams
team_storage = InMemoryStorage(mode="team")
```

### Bring Your Own Dictionary (Flexible Storage Integration)

The real power of InMemoryStorage comes from providing your own dictionary for custom storage mechanisms, in case the current first-class supported storage offerings are too opinionated:

```python
from agno.storage.in_memory import InMemoryStorage
from agno.agent import Agent
from agno.models.openai import OpenAIChat
import json
import boto3

# Example: Save and load sessions to/from S3
def save_sessions_to_s3(sessions_dict, bucket_name, key_name):
    """Save sessions dictionary to S3"""
    s3 = boto3.client('s3')
    s3.put_object(
        Bucket=bucket_name,
        Key=key_name,
        Body=json.dumps(sessions_dict, default=str)
    )

def load_sessions_from_s3(bucket_name, key_name):
    """Load sessions dictionary from S3"""
    s3 = boto3.client('s3')
    try:
        response = s3.get_object(Bucket=bucket_name, Key=key_name)
        return json.loads(response['Body'].read())
    except:
        return {}  # Return empty dict if file doesn't exist

# Step 1: Create agent with external dictionary
my_sessions = {}
storage = InMemoryStorage(storage_dict=my_sessions)

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=storage,
    add_history_to_messages=True,
)

# Run some conversations
agent.print_response("What is the capital of France?")
agent.print_response("What is its population?")

print(f"Sessions in memory: {len(my_sessions)}")

# Step 2: Save sessions to S3
save_sessions_to_s3(my_sessions, "my-bucket", "agent-sessions.json")
print("Sessions saved to S3!")

# Step 3: Later, load sessions from S3 and use with new agent
loaded_sessions = load_sessions_from_s3("my-bucket", "agent-sessions.json")
new_storage = InMemoryStorage(storage_dict=loaded_sessions)

new_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=new_storage,
    session_id=agent.session_id,  # Use same session ID
    add_history_to_messages=True,
)

# This agent now has access to the previous conversation
new_agent.print_response("What was my first question?")
```

### Common Operations

```python
# Create storage
storage = InMemoryStorage()

# Get all sessions
all_sessions = storage.get_all_sessions()

# Filter sessions by user
user_sessions = storage.get_all_sessions(user_id="user123")

# Get recent sessions
recent = storage.get_recent_sessions(limit=5)

# Delete a session
storage.delete_session("session_id")

# Clear all sessions
storage.drop()
```



================================================
FILE: cookbook/storage/in_memory_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/in_memory_storage/in_memory_storage_for_agent.py
================================================
"""Run `pip install ddgs openai` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.in_memory import InMemoryStorage
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=InMemoryStorage(),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/storage/in_memory_storage/in_memory_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/in_memory_storage/in_memory_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.in_memory import InMemoryStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)

hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=InMemoryStorage(mode="team"),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/in_memory_storage/in_memory_storage_for_workflow.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.in_memory import InMemoryStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=InMemoryStorage(mode="workflow"), debug_mode=False
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/in_memory_storage/in_memory_storage_for_workflows_2.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.in_memory import InMemoryStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=InMemoryStorage(mode="workflow_v2"),
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )



================================================
FILE: cookbook/storage/json_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/json_storage/json_storage_for_agent.py
================================================
"""Run `pip install ddgs openai` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.json import JsonStorage
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    storage=JsonStorage(dir_path="tmp/agent_sessions_json"),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/storage/json_storage/json_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/json_storage/json_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.json import JsonStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=JsonStorage(dir_path="tmp/team_sessions_json"),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/json_storage/json_storage_for_workflow.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.json import JsonStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=JsonStorage(dir_path="tmp/workflow_sessions_json"), debug_mode=False
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/json_storage/json_storage_for_workflows_2.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.json import JsonStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=JsonStorage(
            dir_path="tmp/workflow_v2",
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )



================================================
FILE: cookbook/storage/mongodb_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/mongodb_storage/mongodb_storage_for_agent.py
================================================
"""
This recipe shows how to store agent sessions in a MongoDB database.
Steps:
1. Run: `pip install openai pymongo agno` to install dependencies
2. Make sure you are running a local instance of mongodb
3. Run: `python cookbook/storage/mongodb_storage.py` to run the agent
"""

from agno.agent import Agent
from agno.storage.mongodb import MongoDbStorage
from agno.tools.duckduckgo import DuckDuckGoTools

# MongoDB connection settings
db_url = "mongodb://localhost:27017"

agent = Agent(
    storage=MongoDbStorage(
        collection_name="agent_sessions", db_url=db_url, db_name="agno"
    ),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/storage/mongodb_storage/mongodb_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/mongodb_storage/mongodb_storage_for_team.py` to run the agent
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.mongodb import MongoDbStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

# MongoDB connection settings
db_url = "mongodb://localhost:27017"


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=MongoDbStorage(
        collection_name="team_sessions", db_url=db_url, db_name="agno"
    ),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
    add_member_tools_to_system_message=False,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/mongodb_storage/mongodb_storage_for_workflow.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.mongodb import MongoDbStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow

db_url = "mongodb://localhost:27017"


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    storage = MongoDbStorage(
        collection_name="agent_sessions", db_url=db_url, db_name="agno"
    )
    storage.drop()
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=storage, debug_mode=False
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/mysql_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/mysql_storage/mysql_storage_for_agent.py
================================================
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.storage.mysql import MySQLStorage

db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"

agent = Agent(
    storage=MySQLStorage(
        table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")

print(
    f"Session IDs created in DB: {agent.storage.get_all_session_ids(entity_id=agent.agent_id)}"
)



================================================
FILE: cookbook/storage/mysql_storage/mysql_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/mongodb_storage/mongodb_storage_for_team.py` to run the agent
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.mysql import MySQLStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

# MySQL connection settings
db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=MySQLStorage(
        table_name="team_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
    add_member_tools_to_system_message=False,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")


print(
    f"Session IDs created in DB: {hn_team.storage.get_all_session_ids(entity_id=hn_team.team_id)}"
)



================================================
FILE: cookbook/storage/mysql_storage/mysql_storage_for_workflow.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.mysql import MySQLStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow

db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    storage = MySQLStorage(
        table_name="workflow_sessions", db_url=db_url, auto_upgrade_schema=True
    )
    storage.drop()
    report: Iterator[RunResponse] = HackerNewsReporter(storage=storage).run(
        num_stories=5
    )
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/postgres_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/postgres_storage/postgres_storage_for_agent.py
================================================
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.storage.postgres import PostgresStorage
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    storage=PostgresStorage(
        table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/storage/postgres_storage/postgres_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/postgres_storage/postgres_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.postgres import PostgresStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=PostgresStorage(
        table_name="team_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/postgres_storage/postgres_storage_for_workflow.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.postgres import PostgresStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    storage = PostgresStorage(table_name="agent_sessions", db_url=db_url)
    storage.drop()
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=storage, debug_mode=False
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/postgres_storage/postgres_storage_for_workflow_2.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.postgres import PostgresStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=PostgresStorage(
            table_name="workflow_v2",
            db_url=db_url,
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )



================================================
FILE: cookbook/storage/redis_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/redis_storage/redis_storage_for_agent.py
================================================
"""
Test script for RedisStorage implementation.
Run `pip install redis ddgs openai` to install dependencies.

We can start Redis locally using docker:
1. Start Redis container
docker run --name my-redis -p 6379:6379 -d redis

2. Verify container is running
docker ps
"""

from agno.agent import Agent
from agno.storage.redis import RedisStorage
from agno.tools.duckduckgo import DuckDuckGoTools

# Initialize Redis storage with default local connection
storage = RedisStorage(prefix="agno_test", host="localhost", port=6379)

# Create agent with Redis storage
agent = Agent(
    storage=storage,
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)

agent.print_response("How many people live in Canada?")

agent.print_response("What is their national anthem called?")

# Verify storage contents
print("\nVerifying storage contents...")
all_sessions = storage.get_all_sessions()
print(f"Total sessions in Redis: {len(all_sessions)}")

if all_sessions:
    print("\nSession details:")
    session = all_sessions[0]
    print(f"Session ID: {session.session_id}")
    print(f"Messages count: {len(session.memory['messages'])}")



================================================
FILE: cookbook/storage/redis_storage/redis_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/json_storage/json_storage_for_team.py` to run the team

We can start Redis locally using docker:
1. Start Redis container
docker run --name my-redis -p 6379:6379 -d redis

2. Verify container is running
docker ps
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.redis import RedisStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

storage = RedisStorage(prefix="agno_test", host="localhost", port=6379)


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=storage,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/redis_storage/redis_storage_for_workflow.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/json_storage/json_storage_for_team.py` to run the team

We can start Redis locally using docker:
1. Start Redis container
docker run --name my-redis -p 6379:6379 -d redis

2. Verify container is running
docker ps
"""

import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.redis import RedisStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow

storage = RedisStorage(prefix="agno_test", host="localhost", port=6379)


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=storage, debug_mode=False
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/redis_storage/redis_storage_for_workflow_2.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.redis import RedisStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=RedisStorage(
            prefix="agno_test",
            host="localhost",
            port=6379,
        ),
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )



================================================
FILE: cookbook/storage/singlestore_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/singlestore_storage/singlestore_storage_for_agent.py
================================================
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

import os
from os import getenv

from agno.agent import Agent
from agno.storage.singlestore import SingleStoreStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.certs import download_cert
from sqlalchemy.engine import create_engine

# Configure SingleStore DB connection
USERNAME = getenv("SINGLESTORE_USERNAME")
PASSWORD = getenv("SINGLESTORE_PASSWORD")
HOST = getenv("SINGLESTORE_HOST")
PORT = getenv("SINGLESTORE_PORT")
DATABASE = getenv("SINGLESTORE_DATABASE")
SSL_CERT = getenv("SINGLESTORE_SSL_CERT", None)


# Download the certificate if SSL_CERT is not provided
if not SSL_CERT:
    SSL_CERT = download_cert(
        cert_url="https://portal.singlestore.com/static/ca/singlestore_bundle.pem",
        filename="singlestore_bundle.pem",
    )
    if SSL_CERT:
        os.environ["SINGLESTORE_SSL_CERT"] = SSL_CERT


# SingleStore DB URL
db_url = (
    f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
)
if SSL_CERT:
    db_url += f"&ssl_ca={SSL_CERT}&ssl_verify_cert=true"

# Create a DB engine
db_engine = create_engine(db_url)

# Create an agent with SingleStore storage
agent = Agent(
    storage=SingleStoreStorage(
        table_name="agent_sessions",
        db_engine=db_engine,
        schema=DATABASE,
        auto_upgrade_schema=True,
    ),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/storage/singlestore_storage/singlestore_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/singlestore_storage/singlestore_storage_for_team.py` to run the team
"""

import os
from os import getenv
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.singlestore import SingleStoreStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.certs import download_cert
from pydantic import BaseModel
from sqlalchemy.engine import create_engine

# Configure SingleStore DB connection
USERNAME = getenv("SINGLESTORE_USERNAME")
PASSWORD = getenv("SINGLESTORE_PASSWORD")
HOST = getenv("SINGLESTORE_HOST")
PORT = getenv("SINGLESTORE_PORT")
DATABASE = getenv("SINGLESTORE_DATABASE")
SSL_CERT = getenv("SINGLESTORE_SSL_CERT", None)


# Download the certificate if SSL_CERT is not provided
if not SSL_CERT:
    SSL_CERT = download_cert(
        cert_url="https://portal.singlestore.com/static/ca/singlestore_bundle.pem",
        filename="singlestore_bundle.pem",
    )
    if SSL_CERT:
        os.environ["SINGLESTORE_SSL_CERT"] = SSL_CERT


# SingleStore DB URL
db_url = (
    f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
)
if SSL_CERT:
    db_url += f"&ssl_ca={SSL_CERT}&ssl_verify_cert=true"

# Create a DB engine
db_engine = create_engine(db_url)


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=SingleStoreStorage(
        table_name="team_sessions",
        db_engine=db_engine,
        schema=DATABASE,
        auto_upgrade_schema=True,
    ),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/singlestore_storage/singlestore_storage_for_workflow.py
================================================
import json
import os
from os import getenv
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.singlestore import SingleStoreStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.certs import download_cert
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow
from sqlalchemy.engine import create_engine


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    USERNAME = getenv("SINGLESTORE_USERNAME")
    PASSWORD = getenv("SINGLESTORE_PASSWORD")
    HOST = getenv("SINGLESTORE_HOST")
    PORT = getenv("SINGLESTORE_PORT")
    DATABASE = getenv("SINGLESTORE_DATABASE")
    SSL_CERT = getenv("SINGLESTORE_SSL_CERT", None)

    # Download the certificate if SSL_CERT is not provided
    if not SSL_CERT:
        SSL_CERT = download_cert(
            cert_url="https://portal.singlestore.com/static/ca/singlestore_bundle.pem",
            filename="singlestore_bundle.pem",
        )
        if SSL_CERT:
            os.environ["SINGLESTORE_SSL_CERT"] = SSL_CERT

    # SingleStore DB URL
    db_url = f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
    if SSL_CERT:
        db_url += f"&ssl_ca={SSL_CERT}&ssl_verify_cert=true"

    # Create a DB engine
    db_engine = create_engine(db_url)
    # Run workflow
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=SingleStoreStorage(
            table_name="workflow_sessions",
            mode="workflow",
            db_engine=db_engine,
            schema=DATABASE,
        ),
        debug_mode=False,
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/sqllite_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/sqllite_storage/sqlite_storage_for_agent.py
================================================
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    storage=SqliteStorage(
        table_name="agent_sessions", db_file="tmp/data.db", auto_upgrade_schema=True
    ),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem?")
agent.print_response("List my messages one by one")



================================================
FILE: cookbook/storage/sqllite_storage/sqlite_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/sqlite_storage/sqlite_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=SqliteStorage(
        table_name="team_sessions", db_file="tmp/data.db", auto_upgrade_schema=True
    ),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/sqllite_storage/sqlite_storage_for_workflow.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.sqlite import SqliteStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    storage = SqliteStorage(table_name="workflow_sessions", db_file="tmp/data.db")
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=storage, debug_mode=False
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/sqllite_storage/sqlite_storage_for_workflow_2.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )



================================================
FILE: cookbook/storage/yaml_storage/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/storage/yaml_storage/yaml_storage_for_agent.py
================================================
"""Run `pip install ddgs openai` to install dependencies."""

from agno.agent import Agent
from agno.storage.yaml import YamlStorage
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    storage=YamlStorage(dir_path="tmp/agent_sessions_yaml"),
    tools=[DuckDuckGoTools()],
    add_history_to_messages=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")



================================================
FILE: cookbook/storage/yaml_storage/yaml_storage_for_team.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/yaml_storage/yaml_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.yaml import YamlStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    storage=YamlStorage(dir_path="tmp/team_sessions_yaml"),
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")



================================================
FILE: cookbook/storage/yaml_storage/yaml_storage_for_workflow.py
================================================
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.run.response import RunResponse
from agno.storage.yaml import YamlStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    report: Iterator[RunResponse] = HackerNewsReporter(
        storage=YamlStorage(dir_path="tmp/workflow_sessions_yaml"), debug_mode=False
    ).run(num_stories=5)
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/storage/yaml_storage/yaml_storage_for_workflow_2.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.yaml import YamlStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=YamlStorage(dir_path="tmp/workflow_sessions_yaml", mode="workflow_v2"),
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )



================================================
FILE: cookbook/teams/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/teams/medical_history.txt
================================================
# Medical History Report – Thandiwe Mokoena

**Age:** 28  
**Sex:** Female  
**Ethnicity:** Black South African  
**Occupation:** Primary School Teacher  
**Location:** Cape Town, South Africa  

---

## 🩺 Chief Complaint

Persistent abdominal pain and diarrhea over the past three months.

---

## 📖 History of Present Illness

Thandiwe reports a three-month history of intermittent crampy abdominal pain, predominantly in the right lower quadrant. She experiences 4–6 loose, non-bloody stools daily, often accompanied by urgency. Over the past month, she has noticed increased fatigue and an unintended weight loss of approximately 5 kg. She denies any recent travel, antibiotic use, or dietary changes.

---

## 🧾 Past Medical History

- No prior chronic illnesses  
- Appendectomy at age 12  
- No known drug allergies  

---

## 👨‍👩‍👧 Family History

- **Mother:** Hypertension  
- **Father:** Type 2 Diabetes Mellitus  
- No family history of gastrointestinal diseases  

---

## 🌍 Social History

- Non-smoker  
- Occasional alcohol consumption  
- Engages in regular physical activity  

---

## ✅ Review of Systems

- **General:** Reports fatigue and weight loss  
- **Gastrointestinal:** Abdominal pain, diarrhea, occasional nausea  
- **Other systems:** No fevers, skin rashes, or joint pains reported  

---

## 🧑‍⚕️ Physical Examination

- **Vital Signs:**
  - BP: 118/76 mmHg
  - HR: 82 bpm
  - Temp: 37.2°C
  - RR: 16 breaths/min
- **General:** Alert and oriented; appears mildly fatigued  
- **Abdomen:** Soft with tenderness in the right lower quadrant; no rebound tenderness or guarding; no palpable masses  
- **Perianal Examination:** Normal; no fissures or fistulas observed  

---

## 🧪 Laboratory Investigations

- **CBC:**
  - Hemoglobin: 10.5 g/dL (mild anemia)
  - WBC: 9.8 x10⁹/L  
- **CRP:** Elevated at 25 mg/L  
- **ESR:** Elevated at 40 mm/hr  
- **Stool Studies:** Negative for pathogens; fecal calprotectin elevated  

---

## 🖥 Imaging and Endoscopy

- **Colonoscopy:** Patchy areas of inflammation with ulcerations in the terminal ileum; cobblestone appearance noted  



================================================
FILE: cookbook/teams/pydantic_model_as_input.py
================================================
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

team = Team(
    name="Hackernews Team",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[hackernews_agent],
    mode="collaborate",
)

team.print_response(
    message=ResearchTopic(
        topic="AI",
        focus_areas=["AI", "Machine Learning"],
        target_audience="Developers",
        sources_required=5,
    )
)



================================================
FILE: cookbook/teams/reasoning_multi_purpose_team.py
================================================
"""
This example demonstrates a team of agents that can answer a variety of questions.

The team uses reasoning tools to reason about the questions and delegate to the appropriate agent.

The team consists of:
- A web agent that can search the web for information
- A finance agent that can get financial data
- A writer agent that can write content
- A calculator agent that can calculate
- A FastAPI assistant that can explain how to write FastAPI code
- A code execution agent that can execute code in a secure E2B sandbox
"""

import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.anthropic import Claude
from agno.models.openai.chat import OpenAIChat
from agno.team.team import Team
from agno.tools.calculator import CalculatorTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.e2b import E2BTools
from agno.tools.file import FileTools
from agno.tools.github import GithubTools
from agno.tools.knowledge import KnowledgeTools
from agno.tools.pubmed import PubmedTools
from agno.tools.python import PythonTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.lancedb.lance_db import LanceDb
from agno.vectordb.search import SearchType

cwd = Path(__file__).parent.resolve()

# Agent that can search the web for information
web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[DuckDuckGoTools(cache_results=True)],
    instructions=["Always include sources"],
)

reddit_researcher = Agent(
    name="Reddit Researcher",
    role="Research a topic on Reddit",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[DuckDuckGoTools(cache_results=True)],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a Reddit researcher.
    You will be given a topic to research on Reddit.
    You will need to find the most relevant information on Reddit.
    """),
)

# Agent that can get financial data
finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Use tables to display data"],
)

# Writer agent that can write content
writer_agent = Agent(
    name="Write Agent",
    role="Write content",
    model=Claude(id="claude-3-5-sonnet-latest"),
    description="You are an AI agent that can write content.",
    instructions=[
        "You are a versatile writer who can create content on any topic.",
        "When given a topic, write engaging and informative content in the requested format and style.",
        "If you receive mathematical expressions or calculations from the calculator agent, convert them into clear written text.",
        "Ensure your writing is clear, accurate and tailored to the specific request.",
        "Maintain a natural, engaging tone while being factually precise.",
        "Write something that would be good enough to be published in a newspaper like the New York Times.",
    ],
)

# Writer agent that can write content
medical_agent = Agent(
    name="Medical Agent",
    role="Write content",
    model=Claude(id="claude-3-5-sonnet-latest"),
    description="You are an AI agent that can write content.",
    tools=[PubmedTools()],
    instructions=[
        "You are a medical agent that can answer questions about medical topics.",
    ],
)

# Calculator agent that can calculate
calculator_agent = Agent(
    name="Calculator Agent",
    model=Claude(id="claude-3-5-sonnet-latest"),
    role="Calculate",
    tools=[
        CalculatorTools(
            add=True,
            subtract=True,
            multiply=True,
            divide=True,
            exponentiate=True,
            factorial=True,
            is_prime=True,
            square_root=True,
        )
    ],
)

agno_assist_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/llms-full.txt"],
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
agno_assist = Agent(
    name="Agno Assist",
    role="You help answer questions about the Agno framework.",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Search your knowledge before answering the question. Help me to write working code for Agno Agents.",
    tools=[
        KnowledgeTools(
            knowledge=agno_assist_knowledge, add_instructions=True, add_few_shot=True
        ),
    ],
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
)

github_agent = Agent(
    name="Github Agent",
    role="Do analysis on Github repositories",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Use your tools to answer questions about the repo: agno-agi/agno",
        "Do not create any issues or pull requests unless explicitly asked to do so",
    ],
    tools=[
        GithubTools(
            list_pull_requests=True,
            list_issues=True,
            list_issue_comments=True,
            get_pull_request=True,
            get_issue=True,
            get_pull_request_comments=True,
        )
    ],
)

local_python_agent = Agent(
    name="Local Python Agent",
    role="Run Python code locally",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Use your tools to run Python code locally",
    ],
    tools=[
        FileTools(base_dir=cwd),
        PythonTools(
            base_dir=Path(cwd), list_files=True, run_files=True, uv_pip_install=True
        ),
    ],
)


agent_team = Team(
    name="Multi-Purpose Team",
    mode="coordinate",
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ReasoningTools(add_instructions=True, add_few_shot=True),
    ],
    members=[
        web_agent,
        finance_agent,
        writer_agent,
        calculator_agent,
        agno_assist,
        github_agent,
        local_python_agent,
    ],
    instructions=[
        "You are a team of agents that can answer a variety of questions.",
        "You can use your member agents to answer the questions.",
        "You can also answer directly, you don't HAVE to forward the question to a member agent.",
        "Reason about more complex questions before delegating to a member agent.",
        "If the user is only being conversational, don't use any tools, just answer directly.",
    ],
    markdown=True,
    show_tool_calls=True,
    show_members_responses=True,
    enable_agentic_context=True,
    share_member_interactions=True,
)

if __name__ == "__main__":
    # Load the knowledge base (comment out after first run)
    # asyncio.run(agno_assist_knowledge.aload())

    # asyncio.run(agent_team.aprint_response("Hi! What are you capable of doing?"))

    # Python code execution
    # asyncio.run(agent_team.aprint_response(dedent("""What is the right way to implement an Agno Agent that searches Hacker News for good articles?
    #                                        Create a minimal example for me and test it locally to ensure it won't immediately crash.
    #                                        Make save the created code in a file called `./python/hacker_news_agent.py`.
    #                                        Don't mock anything. Use the real information from the Agno documentation."""), stream=True))

    # # Reddit research
    # asyncio.run(agent_team.aprint_response(dedent("""What should I be investing in right now?
    #                                        Find some popular subreddits and do some reseach of your own.
    #                                        Write a detailed report about your findings that could be given to a financial advisor."""), stream=True))

    # Github analysis
    # asyncio.run(agent_team.aprint_response(dedent("""List open pull requests in the agno-agi/agno repository.
    #                                        Find an issue that you think you can resolve and give me the issue number,
    #                                        your suggested solution and some code snippets."""), stream=True))

    # Medical research
    txt_path = Path(__file__).parent.resolve() / "medical_history.txt"
    loaded_txt = open(txt_path, "r").read()
    asyncio.run(
        agent_team.aprint_response(
            dedent(f"""I have a patient with the following medical information:\n {loaded_txt}
                                                    What is the most likely diagnosis?
                                                """),
            stream=True,
        )
    )



================================================
FILE: cookbook/teams/response_as_variable.py
================================================
from typing import Iterator  # noqa
from pydantic import BaseModel
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response


class StockAnalysis(BaseModel):
    symbol: str
    company_name: str
    analysis: str


stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    response_model=StockAnalysis,
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
        )
    ],
)


class CompanyAnalysis(BaseModel):
    company_name: str
    analysis: str


company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    response_model=CompanyAnalysis,
    tools=[
        YFinanceTools(
            stock_price=False,
            company_info=True,
            company_news=True,
        )
    ],
)


team = Team(
    name="Stock Research Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    show_members_responses=True,
)

response = team.run("What is the current stock price of NVDA?")
assert isinstance(response.content, StockAnalysis)
pprint_run_response(response)

response = team.run("What is in the news about NVDA?")
assert isinstance(response.content, CompanyAnalysis)
pprint_run_response(response)



================================================
FILE: cookbook/teams/streaming.py
================================================
from typing import Iterator  # noqa
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools


stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
        )
    ],
)

company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            stock_price=False,
            company_info=True,
            company_news=True,
        )
    ],
)


team = Team(
    name="Stock Research Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    show_members_responses=True,
)

team.print_response("What is the current stock price of NVDA?", stream=True)



================================================
FILE: cookbook/teams/streaming_async.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
        )
    ],
)

company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            stock_price=False,
            company_info=True,
            company_news=True,
        )
    ],
)


team = Team(
    name="Stock Research Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        team.aprint_response("What is the current stock price of NVDA?", stream=True)
    )



================================================
FILE: cookbook/teams/structured_output_streaming.py
================================================
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from pydantic import BaseModel


class StockAnalysis(BaseModel):
    symbol: str
    company_name: str
    analysis: str


stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    response_model=StockAnalysis,
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
        )
    ],
)


class CompanyAnalysis(BaseModel):
    company_name: str
    analysis: str


company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    response_model=CompanyAnalysis,
    tools=[
        YFinanceTools(
            stock_price=False,
            company_info=True,
            company_news=True,
        )
    ],
)


class StockReport(BaseModel):
    symbol: str
    company_name: str
    analysis: str


team = Team(
    name="Stock Research Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    members=[stock_searcher, company_info_agent],
    response_model=StockReport,
    markdown=True,
    show_members_responses=True,
)

team.print_response(
    "Give me a stock report for NVDA", stream=True, stream_intermediate_steps=True
)

# Or async
# asyncio.run(team.aprint_response("Give me a stock report for NVDA", stream=True, stream_intermediate_steps=True))
assert isinstance(team.run_response.content, StockReport)



================================================
FILE: cookbook/teams/team_events.py
================================================
import asyncio
from uuid import uuid4

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.models.mistral.mistral import MistralChat
from agno.models.openai.chat import OpenAIChat
from agno.team import Team, TeamRunEvent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

wikipedia_agent = Agent(
    agent_id="hacker-news-agent",
    name="Hacker News Agent",
    role="Search Hacker News for information",
    model=MistralChat(id="mistral-large-latest"),
    tools=[HackerNewsTools()],
    instructions=[
        "Find articles about the company in the Hacker News",
    ],
)

website_agent = Agent(
    agent_id="website-agent",
    name="Website Agent",
    role="Search the website for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Search the website for information",
    ],
)

user_id = str(uuid4())
team_id = str(uuid4())

company_info_team = Team(
    name="Company Info Team",
    mode="coordinate",
    team_id=team_id,
    user_id=user_id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        wikipedia_agent,
        website_agent,
    ],
    show_tool_calls=True,
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)


async def run_team_with_events(prompt: str):
    content_started = False
    async for run_response_event in await company_info_team.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        if run_response_event.event in [
            TeamRunEvent.run_started,
            TeamRunEvent.run_completed,
        ]:
            print(f"\nTEAM EVENT: {run_response_event.event}")

        if run_response_event.event in [TeamRunEvent.tool_call_started]:
            print(f"\nTEAM EVENT: {run_response_event.event}")
            print(f"TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_response_event.tool.tool_args}")

        if run_response_event.event in [TeamRunEvent.tool_call_completed]:
            print(f"\nTEAM EVENT: {run_response_event.event}")
            print(f"TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TOOL CALL RESULT: {run_response_event.tool.result}")

        # Member events
        if run_response_event.event in [RunEvent.tool_call_started]:
            print(f"\nMEMBER EVENT: {run_response_event.event}")
            print(f"AGENT ID: {run_response_event.agent_id}")
            print(f"TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_response_event.tool.tool_args}")

        if run_response_event.event in [RunEvent.tool_call_completed]:
            print(f"\nMEMBER EVENT: {run_response_event.event}")
            print(f"AGENT ID: {run_response_event.agent_id}")
            print(f"TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TOOL CALL RESULT: {run_response_event.tool.result}")

        if run_response_event.event in [TeamRunEvent.run_response_content]:
            if not content_started:
                print("CONTENT")
                content_started = True
            else:
                print(run_response_event.content, end="")


if __name__ == "__main__":
    asyncio.run(
        run_team_with_events(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        )
    )



================================================
FILE: cookbook/teams/team_events_route.py
================================================
import asyncio

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team import Team, TeamRunEvent
from agno.tools.yfinance import YFinanceTools

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
        )
    ],
)

company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            stock_price=False,
            company_info=True,
            company_news=True,
        )
    ],
)

team = Team(
    name="Stock Research Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    # If you want to disable the member events, set this to False (default is True)
    # stream_member_events=False
)


async def run_team_with_events(prompt: str):
    content_started = False
    member_content_started = False
    async for run_response_event in await team.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        if run_response_event.event in [
            TeamRunEvent.run_started,
            TeamRunEvent.run_completed,
        ]:
            print(f"\nTEAM EVENT: {run_response_event.event}")
        if run_response_event.event in [
            RunEvent.run_started,
            RunEvent.run_completed,
        ]:
            print(f"\nMEMBER RUN EVENT: {run_response_event.event}")

        if run_response_event.event in [TeamRunEvent.tool_call_started]:
            print(f"\nTEAM EVENT: {run_response_event.event}")
            print(f"TEAM TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TEAM TOOL CALL ARGS: {run_response_event.tool.tool_args}")

        if run_response_event.event in [TeamRunEvent.tool_call_completed]:
            print(f"\nTEAM EVENT: {run_response_event.event}")
            print(f"TEAM TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TEAM TOOL CALL RESULT: {run_response_event.tool.result}")

        # Member events
        if run_response_event.event in [RunEvent.tool_call_started]:
            print(f"\nMEMBER EVENT: {run_response_event.event}")
            print(f"TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_response_event.tool.tool_args}")

        if run_response_event.event in [RunEvent.tool_call_completed]:
            print(f"\nMEMBER EVENT: {run_response_event.event}")
            print(f"MEMBER TOOL CALL: {run_response_event.tool.tool_name}")
            print(f"MEMBER TOOL CALL RESULT: {run_response_event.tool.result}")

        if run_response_event.event in [TeamRunEvent.run_response_content]:
            if not content_started:
                print("TEAM CONTENT:")
                content_started = True
            print(run_response_event.content, end="")

        if run_response_event.event in [RunEvent.run_response_content]:
            if not member_content_started:
                print("MEMBER CONTENT:")
                member_content_started = True
            print(run_response_event.content, end="")


if __name__ == "__main__":
    asyncio.run(
        run_team_with_events(
            "What is the current stock price of NVDA?",
        )
    )



================================================
FILE: cookbook/teams/team_metrics.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[YFinanceTools()],
)


team = Team(
    name="Stock Research Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[stock_searcher],
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
)


run_stream: Iterator[RunResponse] = team.run(
    "What is the stock price of NVDA", stream=True
)
pprint_run_response(run_stream, markdown=True)


print("---" * 5, "Team Leader Message Metrics", "---" * 5)
# Print metrics per message for lead agent
if team.run_response.messages:
    for message in team.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)


# Print the metrics
print("---" * 5, "Aggregated Metrics of Team Agent", "---" * 5)
pprint(team.run_response.metrics)

# Print the session metrics
print("---" * 5, "Session Metrics", "---" * 5)
pprint(team.session_metrics)


print("---" * 5, "Team Member Message Metrics", "---" * 5)
# Print metrics per member per message
if team.run_response.member_responses:
    for member_response in team.run_response.member_responses:
        if member_response.messages:
            for message in member_response.messages:
                if message.role == "assistant":
                    if message.content:
                        print(f"Message: {message.content}")
                    elif message.tool_calls:
                        print(f"Tool calls: {message.tool_calls}")
                    print("---" * 5, "Metrics", "---" * 5)
                    pprint(message.metrics)
                    print("---" * 20)


# Print the session metrics
print("---" * 5, "Full Team Session Metrics", "---" * 5)
pprint(team.full_team_session_metrics)



================================================
FILE: cookbook/teams/team_with_agentic_knowledge_filters.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

web_agent = Agent(
    name="Knowledge Search Agent",
    role="Handle knowledge search",
    knowledge=knowledge_base,
    model=OpenAIChat(id="gpt-4o"),
    instructions=["Always take into account filters"],
)

team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[
        web_agent
    ],  # If you omit the member, the leader will search the knowledge base itself.
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    show_members_responses=True,
    markdown=True,
    enable_agentic_knowledge_filters=True,
)

knowledge_base.load(recreate=True)

team_with_knowledge.print_response(
    "Tell me about Jordan Mitchell's work and experience with user_id as jordan_mitchell"
)



================================================
FILE: cookbook/teams/team_with_custom_tools.py
================================================
from agno.agent import Agent
from agno.team.team import Team
from agno.tools import tool
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel
from rich.pretty import pprint


@tool()
def answer_from_known_questions(agent: Team, question: str) -> str:
    """Answer a question from a list of known questions

    Args:
        question: The question to answer

    Returns:
        The answer to the question
    """

    class Answer(BaseModel):
        answer: str
        original_question: str

    faq = {
        "What is the capital of France?": "Paris",
        "What is the capital of Germany?": "Berlin",
        "What is the capital of Italy?": "Rome",
        "What is the capital of Spain?": "Madrid",
        "What is the capital of Portugal?": "Lisbon",
        "What is the capital of Greece?": "Athens",
        "What is the capital of Turkey?": "Ankara",
    }
    if agent.session_state is None:
        agent.session_state = {}

    if "last_answer" in agent.session_state:
        del agent.session_state["last_answer"]

    if question in faq:
        answer = Answer(answer=faq[question], original_question=question)
        agent.session_state["last_answer"] = answer
        return answer.answer
    else:
        return "I don't know the answer to that question."


web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    tools=[DuckDuckGoTools()],
    markdown=True,
)


team = Team(name="Q & A team", members=[web_agent], tools=[answer_from_known_questions])

team.print_response("What is the capital of France?", stream=True)

if "last_answer" in team.session_state:
    pprint(team.session_state["last_answer"])



================================================
FILE: cookbook/teams/team_with_knowledge.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.url import UrlKnowledge
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths
cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

# Initialize knowledge base
agno_docs_knowledge = UrlKnowledge(
    urls=["https://docs.agno.com/llms-full.txt"],
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[web_agent],
    model=OpenAIChat(id="gpt-4o"),
    knowledge=agno_docs_knowledge,
    show_members_responses=True,
    markdown=True,
)

if __name__ == "__main__":
    # Set to False after the knowledge base is loaded
    load_knowledge = True
    if load_knowledge:
        agno_docs_knowledge.load()

    team_with_knowledge.print_response("Tell me about the Agno framework", stream=True)



================================================
FILE: cookbook/teams/team_with_knowledge_filters.py
================================================
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

knowledge_base = PDFKnowledgeBase(
    path=[
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    vector_db=vector_db,
)

web_agent = Agent(
    name="Knowledge Search Agent",
    role="Handle knowledge search",
    knowledge=knowledge_base,
    model=OpenAIChat(id="gpt-4o"),
)

team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[
        web_agent
    ],  # If you omit the member, the leader will search the knowledge base itself.
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    show_members_responses=True,
    markdown=True,
    knowledge_filters={"user_id": "jordan_mitchell"},
)

knowledge_base.load(recreate=True)

team_with_knowledge.print_response(
    "Tell me about Jordan Mitchell's work and experience"
)



================================================
FILE: cookbook/teams/team_with_local_agentic_rag.py
================================================
"""
pip install agno
pip install fastembed, qdrant-client
pip install ollama [ollama pull qwen2.5:7b]
pip install pypdf
"""

from agno.agent import Agent
from agno.embedder.fastembed import FastEmbedEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.ollama import Ollama
from agno.team import Team
from agno.vectordb.qdrant import Qdrant

collection_name = "science"
physics_tb = "https://ncert.nic.in/textbook/pdf/keph101.pdf"
chemistry_tb = "https://ncert.nic.in/textbook/pdf/lech101.pdf"

vector_db = Qdrant(
    path="/tmp/qdrant",
    collection=collection_name,
    embedder=FastEmbedEmbedder(),
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=[physics_tb, chemistry_tb],
    vector_db=vector_db,
    num_documents=4,
)
knowledge_base.load()  # once the data is stored, comment this line

physics_agent = Agent(
    name="Physics Agent",
    role="Expert in Physics",
    instructions="Answer questions based on the knowledge base",
    model=Ollama(id="qwen2.5:7b"),
    read_chat_history=True,
    show_tool_calls=True,
    markdown=True,
)

chemistry_agent = Agent(
    name="Chemistry Agent",
    role="Expert in Chemistry",
    instructions="Answer questions based on the knowledge base",
    model=Ollama(id="qwen2.5:7b"),
    read_chat_history=True,
    show_tool_calls=True,
    markdown=True,
)

science_master = Team(
    name="Team with Knowledge",
    members=[physics_agent, chemistry_agent],
    model=Ollama(id="qwen2.5:7b"),
    knowledge=knowledge_base,
    search_knowledge=True,
    show_members_responses=True,
    markdown=True,
)

# science_master.print_response("give dimensional equation for volume, speed and force",stream=True)
science_master.print_response("state Henry's law", stream=True)



================================================
FILE: cookbook/teams/team_with_nested_shared_state.py
================================================
"""
This example demonstrates the nested Team functionality in a hierarchical team structure.
Each team and agent has a clearly defined role that guides their behavior and specialization:

Team Hierarchy & Roles:
├── Shopping List Team (Orchestrator)
│   Role: "Orchestrate comprehensive shopping list management and meal planning"
│   ├── Shopping Management Team (Operations Specialist)
│   │   Role: "Execute precise shopping list operations through delegation"
│   │   └── Shopping List Agent
│   │       Role: "Maintain and modify the shopping list with precision and accuracy"
│   └── Meal Planning Team (Culinary Expert)
│       Role: "Transform shopping list ingredients into creative meal suggestions"
│       └── Recipe Suggester Agent
│           Role: "Create innovative and practical recipe suggestions"

"""

from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team import Team


# Define tools to manage our shopping list
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list and return confirmation.

    Args:
        item (str): The item to add to the shopping list.
    """
    # Add the item if it's not already in the list
    if item.lower() not in [
        i.lower() for i in agent.team_session_state["shopping_list"]
    ]:
        agent.team_session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"


def remove_item(agent: Agent, item: str) -> str:
    """Remove an item from the shopping list by name.

    Args:
        item (str): The item to remove from the shopping list.
    """
    # Case-insensitive search
    for i, list_item in enumerate(agent.team_session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            agent.team_session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

    return f"'{item}' was not found in the shopping list. Current shopping list: {agent.team_session_state['shopping_list']}"


def remove_all_items(agent: Agent) -> str:
    """Remove all items from the shopping list."""
    agent.team_session_state["shopping_list"] = []
    return "All items removed from the shopping list"


shopping_list_agent = Agent(
    name="Shopping List Agent",
    role="Manage the shopping list",
    agent_id="shopping_list_manager",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[add_item, remove_item, remove_all_items],
    instructions=[
        "Manage the shopping list by adding and removing items",
        "Always confirm when items are added or removed",
        "If the task is done, update the session state to log the changes & chores you've performed",
    ],
)


# Shopping management team - new layer for handling all shopping list modifications
shopping_mgmt_team = Team(
    name="Shopping Management Team",
    role="Execute shopping list operations",
    team_id="shopping_management",
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o-mini"),
    show_tool_calls=True,
    members=[shopping_list_agent],
    instructions=[
        "Manage adding and removing items from the shopping list using the Shopping List Agent",
        "Forward requests to add or remove items to the Shopping List Agent",
    ],
)


def get_ingredients(agent: Agent) -> str:
    """Retrieve ingredients from the shopping list to use for recipe suggestions.

    Args:
        meal_type (str): Type of meal to suggest (breakfast, lunch, dinner, snack, or any)
    """
    shopping_list = agent.team_session_state["shopping_list"]

    if not shopping_list:
        return "The shopping list is empty. Add some ingredients first to get recipe suggestions."

    # Just return the ingredients - the agent will create recipes
    return f"Available ingredients from shopping list: {', '.join(shopping_list)}"


recipe_agent = Agent(
    name="Recipe Suggester",
    agent_id="recipe_suggester",
    role="Suggest recipes based on available ingredients",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_ingredients],
    instructions=[
        "First, use the get_ingredients tool to get the current ingredients from the shopping list",
        "After getting the ingredients, create detailed recipe suggestions based on those ingredients",
        "Create at least 3 different recipe ideas using the available ingredients",
        "For each recipe, include: name, ingredients needed (highlighting which ones are from the shopping list), and brief preparation steps",
        "Be creative but practical with recipe suggestions",
        "Consider common pantry items that people usually have available in addition to shopping list items",
        "Consider dietary preferences if mentioned by the user",
        "If no meal type is specified, suggest a variety of options (breakfast, lunch, dinner, snacks)",
    ],
)


def list_items(team: Team) -> str:
    """List all items in the shopping list."""
    shopping_list = team.team_session_state["shopping_list"]

    if not shopping_list:
        return "The shopping list is empty."

    items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"


# Create meal planning subteam
meal_planning_team = Team(
    name="Meal Planning Team",
    role="Plan meals based on shopping list items",
    team_id="meal_planning",
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[recipe_agent],
    instructions=[
        "You are a meal planning team that suggests recipes based on shopping list items.",
        "IMPORTANT: When users ask 'What can I make with these ingredients?' or any recipe-related questions, IMMEDIATELY forward the EXACT SAME request to the recipe_agent WITHOUT asking for further information.",
        "DO NOT ask the user for ingredients - the recipe_agent will work with what's already in the shopping list.",
        "Your primary job is to forward recipe requests directly to the recipe_agent without modification.",
    ],
)


def add_chore(team: Team, chore: str, priority: str = "medium") -> str:
    """Add a chore to the list with priority level.

    Args:
        chore (str): The chore to add to the list
        priority (str): Priority level of the chore (low, medium, high)

    Returns:
        str: Confirmation message
    """
    # Initialize chores list if it doesn't exist
    if "chores" not in team.session_state:
        team.session_state["chores"] = []

    # Validate priority
    valid_priorities = ["low", "medium", "high"]
    if priority.lower() not in valid_priorities:
        priority = "medium"  # Default to medium if invalid

    # Add the chore with timestamp and priority
    from datetime import datetime

    chore_entry = {
        "description": chore,
        "priority": priority.lower(),
        "added_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
    }

    team.session_state["chores"].append(chore_entry)

    return f"Added chore: '{chore}' with {priority} priority"


# Orchestrates the entire shopping and meal planning ecosystem
shopping_team = Team(
    name="Shopping List Team",
    role="Orchestrate shopping list management and meal planning",
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o-mini"),
    team_session_state={"shopping_list": []},
    tools=[list_items, add_chore],
    session_state={"chores": []},
    team_id="shopping_list_team",
    members=[
        shopping_mgmt_team,
        meal_planning_team,
    ],
    show_tool_calls=True,
    markdown=True,
    instructions=[
        "You are the orchestration layer for a comprehensive shopping and meal planning ecosystem",
        "If you need to add or remove items from the shopping list, forward the full request to the Shopping Management Team",
        "IMPORTANT: If the user asks about recipes or what they can make with ingredients, IMMEDIATELY forward the EXACT request to the meal_planning_team with NO additional questions",
        "Example: When user asks 'What can I make with these ingredients?', you should simply forward this exact request to meal_planning_team without asking for more information",
        "If you need to list the items in the shopping list, use the list_items tool",
        "If the user got something from the shopping list, it means it can be removed from the shopping list",
        "After each completed task, use the add_chore tool to log exactly what was done with high priority",
        "Provide a seamless experience by leveraging your specialized teams for their expertise",
    ],
    show_members_responses=True,
)

# =============================================================================
# DEMONSTRATION
# =============================================================================

# Example 1: Adding items (demonstrates role-based delegation)
print("Example 1: Adding Items to Shopping List")
print("-" * 50)
shopping_team.print_response(
    "Add milk, eggs, and bread to the shopping list", stream=True
)
print(f"Session state: {shopping_team.team_session_state}")
print()

# Example 2: Item consumption and removal
print("Example 2: Item Consumption & Removal")
print("-" * 50)
shopping_team.print_response("I got bread from the store", stream=True)
print(f"Session state: {shopping_team.team_session_state}")
print()

# Example 3: Adding more ingredients
print("Example 3: Adding Fresh Ingredients")
print("-" * 50)
shopping_team.print_response(
    "I need apples and oranges for my fruit salad", stream=True
)
print(f"Session state: {shopping_team.team_session_state}")
print()

# Example 4: Listing current items
print("Example 4: Viewing Current Shopping List")
print("-" * 50)
shopping_team.print_response("What's on my shopping list right now?", stream=True)
print(f"Session state: {shopping_team.team_session_state}")
print()

# Example 5: Recipe suggestions (demonstrates culinary expertise role)
print("Example 5: Recipe Suggestions from Culinary Team")
print("-" * 50)
shopping_team.print_response("What can I make with these ingredients?", stream=True)
print(f"Session state: {shopping_team.team_session_state}")
print()

# Example 6: Complete list management
print("Example 6: Complete List Reset & Restart")
print("-" * 50)
shopping_team.print_response(
    "Clear everything from my list and start over with just bananas and yogurt",
    stream=True,
)
print(f"Shared Session state: {shopping_team.team_session_state}")
print()

# Example 7: Quick recipe check with new ingredients
print("Example 7: Quick Recipe Check with New Ingredients")
print("-" * 50)
shopping_team.print_response("What healthy breakfast can I make now?", stream=True)
print()

print(f"Team Session State: {shopping_team.team_session_state}")



================================================
FILE: cookbook/teams/team_with_output_model.py
================================================
"""
This example shows how to use the output_model parameter to specify the model that should be used to generate the final response.
"""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

itinerary_planner = Agent(
    name="Itinerary Planner",
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing vacations. Use the tools at your disposal to find latest information about the destination.",
    tools=[DuckDuckGoTools()],
)

travel_expert = Team(
    model=OpenAIChat(id="gpt-4.1"),
    members=[itinerary_planner],
    output_model=OpenAIChat(id="o3-mini"),
)

travel_expert.print_response("Plan a summer vacation in Paris", stream=True)



================================================
FILE: cookbook/teams/team_with_parser_model.py
================================================
import random
from typing import Iterator, List  # noqa

from agno.agent import Agent, RunResponse, RunResponseEvent  # noqa
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team import Team
from pydantic import BaseModel, Field
from rich.pretty import pprint


class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )


itinerary_planner = Agent(
    name="Itinerary Planner",
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
)

weather_expert = Agent(
    name="Weather Expert",
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You are a weather expert and can provide detailed weather information for a given location.",
)

national_park_expert = Team(
    model=OpenAIChat(id="gpt-4.1"),
    members=[itinerary_planner, weather_expert],
    response_model=NationalParkAdventure,
    parser_model=OpenAIChat(id="gpt-4o"),
)

# Get the response in a variable
national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]
# Get the response in a variable
run: RunResponse = national_park_expert.run(
    f"What is the best season to visit {national_parks[random.randint(0, len(national_parks) - 1)]}? Please provide a detailed one week itinerary for a visit to the park."
)
pprint(run.content)

# Stream the response
# run_events: Iterator[RunResponseEvent] = national_park_expert.run(f"What is the best season to visit {national_parks[random.randint(0, len(national_parks) - 1)]}? Please provide a detailed one week itinerary for a visit to the park.", stream=True)
# for event in run_events:
#     pprint(event)



================================================
FILE: cookbook/teams/team_with_shared_state.py
================================================
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team import Team


# Define tools to manage our shopping list
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list and return confirmation.

    Args:
        item (str): The item to add to the shopping list.
    """
    # Add the item if it's not already in the list
    if item.lower() not in [
        i.lower() for i in agent.team_session_state["shopping_list"]
    ]:
        agent.team_session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"


def remove_item(agent: Agent, item: str) -> str:
    """Remove an item from the shopping list by name.

    Args:
        item (str): The item to remove from the shopping list.
    """
    # Case-insensitive search
    for i, list_item in enumerate(agent.team_session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            agent.team_session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

    return f"'{item}' was not found in the shopping list. Current shopping list: {agent.team_session_state['shopping_list']}"


def remove_all_items(agent: Agent) -> str:
    """Remove all items from the shopping list."""
    agent.team_session_state["shopping_list"] = []
    return "All items removed from the shopping list"


shopping_list_agent = Agent(
    name="Shopping List Agent",
    role="Manage the shopping list",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[add_item, remove_item, remove_all_items],
    instructions=[
        "Find information about the company in the wikipedia",
    ],
)


def list_items(team: Team) -> str:
    """List all items in the shopping list."""
    shopping_list = team.team_session_state["shopping_list"]

    if not shopping_list:
        return "The shopping list is empty."

    items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"


shopping_team = Team(
    name="Shopping List Team",
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o-mini"),
    team_session_state={"shopping_list": []},
    tools=[list_items],
    members=[
        shopping_list_agent,
    ],
    show_tool_calls=True,
    markdown=True,
    instructions=[
        "You are a team that manages a shopping list.",
        "If you need to add or remove items from the shopping list, forward the full request to the shopping list agent (don't break it up into multiple requests).",
        "If you need to list the items in the shopping list, use the list_items tool.",
        "If the user got something from the shopping list, it means it can be removed from the shopping list.",
    ],
    show_members_responses=True,
)

# Example usage
shopping_team.print_response(
    "Add milk, eggs, and bread to the shopping list", stream=True
)
print(f"Session state: {shopping_team.team_session_state}")

shopping_team.print_response("I got bread", stream=True)
print(f"Session state: {shopping_team.team_session_state}")

shopping_team.print_response("I need apples and oranges", stream=True)
print(f"Session state: {shopping_team.team_session_state}")

shopping_team.print_response("whats on my list?", stream=True)
print(f"Session state: {shopping_team.team_session_state}")

shopping_team.print_response(
    "Clear everything from my list and start over with just bananas and yogurt",
    stream=True,
)
print(f"Session state: {shopping_team.team_session_state}")



================================================
FILE: cookbook/teams/team_with_storage.py
================================================
from uuid import uuid4

from agno.agent.agent import Agent
from agno.memory.v2 import Memory
from agno.memory.v2.db.postgres import PostgresMemoryDb
from agno.models.mistral.mistral import MistralChat
from agno.models.openai.chat import OpenAIChat
from agno.storage.postgres import PostgresStorage
from agno.team import Team

french_agent = Agent(
    name="French Agent",
    role="You can only answer in French",
    model=MistralChat(id="mistral-large-latest"),
    instructions=[
        "You must only respond in French",
    ],
)

english_agent = Agent(
    name="English Agent",
    role="You can only answer in English",
    model=OpenAIChat("gpt-4o"),
    instructions=[
        "You must only respond in English",
    ],
)
user_id = str(uuid4())

multi_language_team = Team(
    name="Multi Language Team",
    mode="route",
    team_id=str(uuid4()),
    user_id=user_id,
    model=OpenAIChat("gpt-4o"),
    members=[
        french_agent,
        english_agent,
    ],
    storage=PostgresStorage(
        table_name="agent_team_sessions",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    ),
    memory=Memory(
        db=PostgresMemoryDb(
            table_name="memories", db_url="postgres://ai:ai@localhost:5532/ai"
        )
    ),
    show_tool_calls=True,
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English, Spanish, Japanese, French and German. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    show_members_responses=True,
    enable_team_history=True,
    num_history_runs=3,
)

multi_language_team.print_response(
    "Comment allez-vous?",
    stream=True,
)
multi_language_team.print_response(
    "Qu'est-ce que je viens de dire?",
    stream=True,
)



================================================
FILE: cookbook/teams/team_with_tool_hooks.py
================================================
import time
from typing import Any, Callable, Dict
from uuid import uuid4

from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.models.mistral.mistral import MistralChat
from agno.models.openai.chat import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reddit import RedditTools
from agno.utils.log import logger


def logger_hook(function_name: str, function_call: Callable, arguments: Dict[str, Any]):
    if function_name == "transfer_task_to_member":
        member_id = arguments.get("member_id")
        logger.info(f"Transferring task to member {member_id}")

    # Start timer
    start_time = time.time()
    result = function_call(**arguments)
    # End timer
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"Function {function_name} took {duration:.2f} seconds to execute")
    return result


reddit_agent = Agent(
    name="Reddit Agent",
    agent_id="reddit-agent",
    role="Search reddit for information",
    model=MistralChat(id="mistral-large-latest"),
    tools=[RedditTools(cache_results=True)],
    instructions=[
        "Find information about the company on Reddit",
    ],
    tool_hooks=[logger_hook],
)

website_agent = Agent(
    name="Website Agent",
    agent_id="website-agent",
    role="Search the website for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(cache_results=True)],
    instructions=[
        "Search the website for information",
    ],
    tool_hooks=[logger_hook],
)

user_id = str(uuid4())

company_info_team = Team(
    name="Company Info Team",
    mode="coordinate",
    user_id=user_id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        reddit_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
    tool_hooks=[logger_hook],
)

if __name__ == "__main__":
    company_info_team.print_response(
        "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        stream=True,
    )



================================================
FILE: cookbook/teams/team_with_tools.py
================================================
import asyncio
from uuid import uuid4

from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.models.mistral.mistral import MistralChat
from agno.models.openai.chat import OpenAIChat
from agno.team import Team
from agno.tools.agentql import AgentQLTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.wikipedia import WikipediaTools

wikipedia_agent = Agent(
    name="Wikipedia Agent",
    role="Search wikipedia for information",
    model=MistralChat(id="mistral-large-latest"),
    tools=[WikipediaTools()],
    instructions=[
        "Find information about the company in the wikipedia",
    ],
)

website_agent = Agent(
    name="Website Agent",
    role="Search the website for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Search the website for information",
    ],
)

# Define custom AgentQL query for specific data extraction (see https://docs.agentql.com/concepts/query-language)
custom_query = """
{
    title
    text_content[]
}
"""
user_id = str(uuid4())
team_id = str(uuid4())

company_info_team = Team(
    name="Company Info Team",
    mode="coordinate",
    team_id=team_id,
    user_id=user_id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[AgentQLTools(agentql_query=custom_query)],
    members=[
        wikipedia_agent,
        website_agent,
    ],
    show_tool_calls=True,
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        company_info_team.aprint_response(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/teams/memory/01_chat_history.py
================================================
from agno.agent import Agent
from agno.memory.v2.memory import Memory
from agno.models.anthropic.claude import Claude
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from pydantic import BaseModel
from utils import print_chat_history

# This memory is shared by all the agents in the team
memory = Memory()


class StockAnalysis(BaseModel):
    symbol: str
    company_name: str
    analysis: str


stock_searcher = Agent(
    name="Stock Searcher",
    model=Claude(id="claude-3-5-sonnet-20241022"),
    role="Searches the web for information on a stock.",
    tools=[YFinanceTools()],
    memory=memory,
)

web_searcher = Agent(
    name="Web Searcher",
    model=Claude(id="claude-3-5-sonnet-20241022"),
    tools=[DuckDuckGoTools()],
    role="Searches the web for information on a company.",
    memory=memory,
)

session_id = "stock_team_session_1"
user_id = "john_doe@example.com"

team = Team(
    name="Stock Team",
    mode="coordinate",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[stock_searcher, web_searcher],
    instructions=[
        "First, search the stock market for information about a particular company's stock.",
        "Then, ask the web searcher to search for wider company information.",
    ],
    response_model=StockAnalysis,
    memory=memory,
    # Set enable_team_history=true to add the previous chat history to the messages sent to the Model.
    enable_team_history=True,
    markdown=True,
    show_members_responses=True,
)

# -*- Create a run
team.print_response(
    "Write a report on the Apple stock.", session_id=session_id, user_id=user_id
)

# -*- Print the messages in the memory
session_run = memory.runs[session_id][-1]
print_chat_history(session_run)

# -*- Ask a follow-up question that continues the conversation
team.print_response(
    "Pull up the previous report again.", session_id=session_id, user_id=user_id
)
# -*- Print the messages in the memory
session_run = memory.runs[session_id][-1]
print_chat_history(session_run)



================================================
FILE: cookbook/teams/memory/02_persistent_chat_history.py
================================================
"""
This recipe shows how to store agent sessions in a sqlite database and use chat history.

Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/teams/memory/02_persistent_chat_history.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.memory import Memory
from agno.models.anthropic.claude import Claude
from agno.models.perplexity.perplexity import Perplexity
from agno.storage.sqlite import SqliteStorage
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from utils import print_chat_history

memory = Memory()

memory.clear()

stock_searcher = Agent(
    name="Stock Searcher",
    model=Claude(id="claude-3-5-sonnet-20241022"),
    role="Searches the web for information on a stock.",
    tools=[YFinanceTools(cache_results=True)],
    storage=SqliteStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

web_searcher = Agent(
    name="Web Searcher",
    model=Perplexity(id="sonar-pro"),
    role="Searches the web for information on a company.",
    storage=SqliteStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

team = Team(
    name="Stock Team",
    mode="coordinate",
    model=Claude(id="claude-3-5-sonnet-20241022"),
    # Store team sessions in a database
    storage=SqliteStorage(
        table_name="team_sessions", db_file="tmp/persistent_memory.db"
    ),
    members=[stock_searcher, web_searcher],
    instructions=[
        "You can search the stock market for information about a particular company's stock.",
        "You can also search the web for wider company information.",
    ],
    # Set enable_team_history=true to add the previous chat history to the messages sent to the Model.
    enable_team_history=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
    memory=memory,
)

session_id = "stock_team_session_1"

# -*- Create a run
team.print_response(
    "What is the current price of Apple stock?", stream=True, session_id=session_id
)

# -*- Print the chat history
session_run = memory.runs[session_id][-1]
print_chat_history(session_run)

# -*- Ask a follow up question that continues the conversation
team.print_response("What was that price again?", stream=True, session_id=session_id)

# -*- Print the chat history
session_run = memory.runs[session_id][-1]
print_chat_history(session_run)



================================================
FILE: cookbook/teams/memory/03_user_memories.py
================================================
"""
This recipe shows how to store personalized memories and summaries in a sqlite database.

Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/teams/memory/03_user_memories.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.models.perplexity.perplexity import Perplexity
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from utils import print_chat_history, print_team_memory

# This memory is shared by all the agents in the team
memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(model=Gemini(id="gemini-2.0-flash-exp"), db=memory_db)

# Reset the memory for this example
memory.clear()


stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[YFinanceTools(cache_results=True)],
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

web_searcher = Agent(
    name="Web Searcher",
    model=Perplexity(id="sonar-pro"),
    role="Searches the web for information on a company.",
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

team = Team(
    name="Stock Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    # Store team sessions in a database
    storage=SqliteAgentStorage(
        table_name="team_sessions", db_file="tmp/persistent_memory.db"
    ),
    # The memories are personalized for this user
    user_id="john_billings",
    # Store the memories and summary in a table: agent_memory
    memory=memory,
    members=[stock_searcher, web_searcher],
    instructions=[
        "You can search the stock market for information about a particular company's stock.",
        "You can also search the web for wider company information.",
    ],
    # Set enable_team_history=true to add the previous chat history to the messages sent to the Model.
    enable_team_history=True,
    num_of_interactions_from_history=5,
    # Create and store personalized memories for this user
    enable_user_memories=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

session_id = "stock_team_session_1"
user_id = "john_billings"

# -*- Share personal information
team.print_response(
    "My name is john billings and I live in nyc.",
    stream=True,
    session_id=session_id,
    user_id=user_id,
)

session_run = memory.runs[session_id][-1]
# -*- Print chat history
print_chat_history(session_run)
# -*- Print team memory
print_team_memory(user_id, memory.get_user_memories(user_id))

# -*- Share personal information
team.print_response(
    "What is the price of apple stock?",
    stream=True,
    session_id=session_id,
    user_id=user_id,
)

session_run = memory.runs[session_id][-1]
# -*- Print chat history
print_chat_history(session_run)
# -*- Print team memory
print_team_memory(user_id, memory.get_user_memories(user_id))

# Ask about the conversation
team.print_response(
    "What have we been talking about, do you know my name?", stream=True
)

session_run = memory.runs[session_id][-1]
# -*- Print chat history
print_chat_history(session_run)
# -*- Print team memory (you can also get the user memories from the team)
print_team_memory(user_id, team.get_user_memories(user_id))



================================================
FILE: cookbook/teams/memory/04_agentic_context.py
================================================
"""
This recipe shows how to use agentic context to improve the performance of the team.

Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/teams/memory/04_agentic_context.py` to run the agent
"""

from agno.agent import Agent
from agno.memory.v2.memory import Memory
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

memory = Memory(model=Gemini(id="gemini-2.0-flash-exp"))

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[YFinanceTools(cache_results=True)],
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat(id="gpt-4o"),
    role="Searches the web for information on a company.",
    tools=[DuckDuckGoTools(cache_results=True)],
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

team = Team(
    name="Stock Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    storage=SqliteAgentStorage(
        table_name="team_sessions", db_file="tmp/persistent_memory.db"
    ),
    members=[stock_searcher, web_searcher],
    instructions=[
        "You can search the stock market for information about a particular company's stock.",
        "You can also search the web for wider company information.",
        "Always add ALL stock or company information you get from team members to the shared team context.",
    ],
    memory=memory,
    enable_agentic_context=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
    debug_mode=True,
)

session_id = "stock_team_session_1"


team.print_response(
    "First find the stock price of apple. Then find any information about the company.",
    stream=True,
    stream_intermediate_steps=True,
    session_id=session_id,
)

team.print_response(
    "What is the price of google stock?",
    stream=True,
    stream_intermediate_steps=True,
    session_id=session_id,
)
print("Team Context: ", team.memory.team_context[session_id].text)
for interaction in team.memory.team_context[session_id].member_interactions:
    print(
        "Member Interactions: ",
        f"{interaction.member_name}: {interaction.task} - {interaction.response.content}",
    )



================================================
FILE: cookbook/teams/memory/05_team_manages_memory.py
================================================
"""
This recipe shows how to have the team manage the memory of the agents.

Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/teams/memory/05_team_manages_memory.py` to run the agent
"""

import asyncio

from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from utils import print_team_memory

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(model=Gemini(id="gemini-2.0-flash-exp"), db=memory_db)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat(id="gpt-4o"),
    role="Searches the web for information.",
    tools=[DuckDuckGoTools(cache_results=True)],
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

team = Team(
    name="Friendly Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    storage=SqliteAgentStorage(
        table_name="team_sessions", db_file="tmp/persistent_memory.db"
    ),
    members=[web_searcher],
    instructions=["You can search the web for information."],
    memory=memory,
    # Enable the team to manage the memory
    enable_agentic_memory=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    session_id = "friendly_team_session_1"
    user_id = "john_billings"

    asyncio.run(
        team.aprint_response(
            "Hi! My name is John Billings and I live in New York City.",
            stream=True,
            stream_intermediate_steps=True,
            session_id=session_id,
            user_id=user_id,
        )
    )

    asyncio.run(
        team.aprint_response(
            "What is the weather in New York City?",
            stream=True,
            stream_intermediate_steps=True,
            session_id=session_id,
            user_id=user_id,
        )
    )

    # -*- Print team memory
    print_team_memory(user_id, memory.get_user_memories(user_id))



================================================
FILE: cookbook/teams/memory/06_team_session_summaries.py
================================================
"""
This recipe shows how to have the team create summaries of the session.

Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/teams/memory/06_team_session_summaries.py` to run the agent
"""

import asyncio

from agno.agent import Agent
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

memory = Memory(model=OpenAIChat("gpt-4o"))

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat(id="gpt-4o"),
    role="Searches the web for information.",
    tools=[DuckDuckGoTools(cache_results=True)],
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

team = Team(
    name="Friendly Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    storage=SqliteAgentStorage(
        table_name="team_sessions", db_file="tmp/persistent_memory.db"
    ),
    members=[web_searcher],
    instructions=["You can search the web for information."],
    memory=memory,
    # Enable the team to make session summaries
    enable_session_summaries=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    user_id = "john_billings"
    session_id_1 = "session_1"

    asyncio.run(
        team.aprint_response(
            "Hi! My name is John Billings and I live in New York City.",
            stream=True,
            stream_intermediate_steps=True,
            session_id=session_id_1,
            user_id=user_id,
        )
    )

    asyncio.run(
        team.aprint_response(
            "How is the weather in New York City?",
            stream=True,
            stream_intermediate_steps=True,
            session_id=session_id_1,
            user_id=user_id,
        )
    )

    session_summary = team.get_session_summary(user_id=user_id, session_id=session_id_1)
    print("Session Summary: ", session_summary.summary)

    session_id_2 = "session_2"

    asyncio.run(
        team.aprint_response(
            "Ok, new topic. What is currently happening in the financial markets?",
            stream=True,
            stream_intermediate_steps=True,
            session_id=session_id_2,
            user_id=user_id,
        )
    )

    # You can also get the session summary from the team
    session_summary = team.get_session_summary(user_id=user_id, session_id=session_id_2)
    print("Session Summary: ", session_summary.summary)



================================================
FILE: cookbook/teams/memory/07_multiple_teams.py
================================================
"""
This recipe shows how to have multiple teams with one shared memory.

Steps:
1. Run: `pip install openai sqlalchemy agno` to install dependencies
2. Run: `python cookbook/teams/memory/07_multiple_teams.py` to run the agent
"""

import asyncio

from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.anthropic.claude import Claude
from agno.models.google.gemini import Gemini
from agno.models.mistral.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.storage.agent.sqlite import SqliteAgentStorage
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from utils import print_team_memory

memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")

memory = Memory(model=Gemini(id="gemini-2.0-flash-exp"), db=memory_db)

# Reset the memory for this example
memory.clear()

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat(id="gpt-4o"),
    role="Searches the web for information.",
    tools=[DuckDuckGoTools(cache_results=True)],
    storage=SqliteAgentStorage(
        table_name="agent_sessions", db_file="tmp/persistent_memory.db"
    ),
    memory=memory,
)

chat_team = Team(
    name="Chat Team",
    mode="coordinate",
    model=OpenAIChat("gpt-4o"),
    storage=SqliteAgentStorage(
        table_name="team_sessions", db_file="tmp/persistent_memory.db"
    ),
    members=[web_searcher],
    instructions=["You can search the web for information."],
    memory=memory,
    enable_user_memories=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)


french_agent = Agent(
    name="French Agent",
    role="You can only answer in French",
    model=MistralChat(id="mistral-large-latest"),
)
german_agent = Agent(
    name="German Agent",
    role="You can only answer in German",
    model=Claude("claude-3-5-sonnet-20241022"),
)

multi_language_team = Team(
    name="Multi Language Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[
        french_agent,
        german_agent,
    ],
    show_tool_calls=True,
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent. You can also answer questions in English.",
        "If the user asks in a language that is not English and is not spoken by any team member, respond in English with:",
        "'I can only answer in the following languages: English, French and German. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    memory=memory,
    enable_user_memories=True,
    show_members_responses=True,
)


if __name__ == "__main__":
    chat_session_id = "friendly_team_session_1"
    multi_language_session_id = "multi_language_team_session_1"
    user_id = "john_billings"

    asyncio.run(
        chat_team.aprint_response(
            "Hi! My name is John Billings and I love anime.",
            stream=True,
            stream_intermediate_steps=True,
            session_id=chat_session_id,
            user_id=user_id,
        )
    )

    asyncio.run(
        multi_language_team.aprint_response(
            "Ich komme aus Deutschland. Wie geht es Ihnen?",
            stream=True,
            stream_intermediate_steps=True,
            session_id=multi_language_session_id,
            user_id=user_id,
        )
    )

    # -*- Print team memory
    print_team_memory(user_id, memory.get_user_memories(user_id))



================================================
FILE: cookbook/teams/memory/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/teams/memory/utils.py
================================================
import json
from typing import List

from agno.memory.v2.schema import UserMemory
from agno.run.team import TeamRunResponse
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel

console = Console()


def print_chat_history(session_run: TeamRunResponse):
    # -*- Print history
    messages = []
    for m in session_run.messages:
        message_dict = m.model_dump(
            include={"role", "content", "tool_calls", "from_history"}
        )
        if message_dict["content"] is not None:
            del message_dict["tool_calls"]
        else:
            del message_dict["content"]
        messages.append(message_dict)

    console.print(
        Panel(
            JSON(
                json.dumps(
                    messages,
                ),
                indent=4,
            ),
            title=f"Chat History for session_id: {session_run.session_id}",
            expand=True,
        )
    )


def render_panel(title: str, content: str) -> Panel:
    return Panel(JSON(content, indent=4), title=title, expand=True)


def print_team_memory(user_id: str, memories: List[UserMemory]):
    # -*- Print memories
    console.print(
        render_panel(
            f"Memories for user_id: {user_id}",
            json.dumps(
                [m.to_dict() for m in memories],
                indent=4,
            ),
        )
    )



================================================
FILE: cookbook/teams/modes/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/teams/modes/collaborate.py
================================================
"""
This example demonstrates a collaborative team of AI agents working together to research topics across different platforms.

The team consists of two specialized agents:
1. Reddit Researcher - Uses DuckDuckGo to find and analyze relevant Reddit posts
2. HackerNews Researcher - Uses HackerNews API to find and analyze relevant HackerNews posts

The agents work in "collaborate" mode, meaning they:
- Both are given the same task at the same time
- Work towards reaching consensus through discussion
- Are coordinated by a team leader that guides the discussion

The team leader moderates the discussion and determines when consensus is reached.

This setup is useful for:
- Getting diverse perspectives from different online communities
- Cross-referencing information across platforms
- Having agents collaborate to form more comprehensive analysis
- Reaching balanced conclusions through structured discussion

"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

reddit_researcher = Agent(
    name="Reddit Researcher",
    role="Research a topic on Reddit",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a Reddit researcher.
    You will be given a topic to research on Reddit.
    You will need to find the most relevant posts on Reddit.
    """),
)

hackernews_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Research a topic on HackerNews.",
    tools=[HackerNewsTools()],
    add_name_to_instructions=True,
    instructions=dedent("""
    You are a HackerNews researcher.
    You will be given a topic to research on HackerNews.
    You will need to find the most relevant posts on HackerNews.
    """),
)


agent_team = Team(
    name="Discussion Team",
    mode="collaborate",
    model=OpenAIChat("gpt-4o"),
    members=[
        reddit_researcher,
        hackernews_researcher,
    ],
    instructions=[
        "You are a discussion master.",
        "You have to stop the discussion when you think the team has reached a consensus.",
    ],
    success_criteria="The team has reached a consensus.",
    enable_agentic_context=True,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        agent_team.aprint_response(
            message="Start the discussion on the topic: 'What is the best way to learn to code?'",
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/teams/modes/coordinate.py
================================================
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/teams/coordinate/hackernews_team.py` to run the agent

This example demonstrates a coordinated team of AI agents working together to research topics across different platforms.

The team consists of three specialized agents:
1. HackerNews Researcher - Uses HackerNews API to find and analyze relevant HackerNews posts
2. Web Searcher - Uses DuckDuckGo to find and analyze relevant web pages
3. Article Reader - Reads articles from URLs

The team leader coordinates the agents by:
- Giving each agent a specific task
- Providing clear instructions for each agent
- Collecting and summarizing the results from each agent

"""

import asyncio
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.team import TeamRunResponse  # type: ignore
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.tools.newspaper4k import Newspaper4kTools
from pydantic import BaseModel, Field


class Article(BaseModel):
    title: str = Field(..., description="The title of the article")
    summary: str = Field(..., description="A summary of the article")
    reference_links: List[str] = Field(
        ..., description="A list of reference links to the article"
    )


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)

article_reader = Agent(
    name="Article Reader",
    role="Reads articles from URLs.",
    tools=[Newspaper4kTools()],
)


hn_team = Team(
    name="HackerNews Team",
    mode="coordinate",
    model=OpenAIChat("o3"),
    members=[hn_researcher, web_searcher, article_reader],
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the article reader to read the links for the stories to get more information.",
        "Important: you must provide the article reader with the links to read.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    response_model=Article,
    add_member_tools_to_system_message=False,
    show_tool_calls=True,
    markdown=True,
    show_members_responses=True,
    enable_agentic_context=True,
)

if __name__ == "__main__":
    asyncio.run(
        hn_team.aprint_response(
            "Write an article about the top 2 stories on hackernews", stream=True
        )
    )



================================================
FILE: cookbook/teams/modes/route.py
================================================
"""
This example demonstrates a route team of AI agents working together to answer questions in different languages.

The team consists of six specialized agents:
1. English Agent - Can only answer in English
2. Japanese Agent - Can only answer in Japanese
3. Chinese Agent - Can only answer in Chinese
4. Spanish Agent - Can only answer in Spanish
5. French Agent - Can only answer in French
6. German Agent - Can only answer in German

The team leader routes the user's question to the appropriate language agent. It can only forward the question and cannot answer itself.
"""

import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.deepseek import DeepSeek
from agno.models.mistral.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.team.team import Team

english_agent = Agent(
    name="English Agent",
    role="You only answer in English",
    model=OpenAIChat(id="gpt-4o"),
)
japanese_agent = Agent(
    name="Japanese Agent",
    role="You only answer in Japanese",
    model=DeepSeek(id="deepseek-chat"),
)
chinese_agent = Agent(
    name="Chinese Agent",
    role="You only answer in Chinese",
    model=DeepSeek(id="deepseek-chat"),
)
spanish_agent = Agent(
    name="Spanish Agent",
    role="You can only answer in Spanish",
    model=OpenAIChat(id="gpt-4o"),
)
french_agent = Agent(
    name="French Agent",
    role="You can only answer in French",
    model=MistralChat(id="mistral-large-latest"),
)
german_agent = Agent(
    name="German Agent",
    role="You can only answer in German",
    model=Claude("claude-3-5-sonnet-20241022"),
)

multi_language_team = Team(
    name="Multi Language Team",
    mode="route",
    model=OpenAIChat("gpt-4o"),
    members=[
        english_agent,
        spanish_agent,
        japanese_agent,
        french_agent,
        german_agent,
        chinese_agent,
    ],
    show_tool_calls=True,
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English, Spanish, Japanese, French and German. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    show_members_responses=True,
)


if __name__ == "__main__":
    # Ask "How are you?" in all supported languages
    # asyncio.run(multi_language_team.aprint_response(
    #     "How are you?", stream=True  # English
    # ))

    # asyncio.run(multi_language_team.aprint_response(
    #     "你好吗？", stream=True  # Chinese
    # ))

    # asyncio.run(multi_language_team.aprint_response(
    #     "お元気ですか?", stream=True  # Japanese
    # ))

    asyncio.run(
        multi_language_team.aprint_response(
            "Comment allez-vous?",
            stream=True,  # French
        )
    )

    # asyncio.run(multi_language_team.aprint_response(
    #     "Wie geht es Ihnen?", stream=True  # German
    # ))

    # asyncio.run(multi_language_team.aprint_response(
    #     "Come stai?", stream=True  # Italian
    # ))



================================================
FILE: cookbook/tools/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/tools/agentql_tools.py
================================================
"""
AgentQL Tools for scraping websites.

Prerequisites:
- Set the environment variable `AGENTQL_API_KEY` with your AgentQL API key.
  You can obtain the API key from the AgentQL website:
  https://agentql.com/
- Run `playwright install` to install a browser extension for playwright.

AgentQL will open up a browser instance (don't close it) and do scraping on the site.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.agentql import AgentQLTools

# Create agent with default AgentQL tool
agent = Agent(
    model=OpenAIChat(id="gpt-4o"), tools=[AgentQLTools()], show_tool_calls=True
)
agent.print_response("https://docs.agno.com/introduction", markdown=True)

# Define custom AgentQL query for specific data extraction (see https://docs.agentql.com/concepts/query-language)
custom_query = """
{
    title
    text_content[]
}
"""

# Create AgentQL tool with custom query
custom_scraper = AgentQLTools(agentql_query=custom_query)

# Create agent with custom AgentQL tool
custom_agent = Agent(
    model=OpenAIChat(id="gpt-4o"), tools=[custom_scraper], show_tool_calls=True
)
custom_agent.print_response("https://docs.agno.com/introduction", markdown=True)



================================================
FILE: cookbook/tools/airflow_tools.py
================================================
from agno.agent import Agent
from agno.tools.airflow import AirflowTools

agent = Agent(
    tools=[AirflowTools(dags_dir="tmp/dags", save_dag=True, read_dag=True)],
    show_tool_calls=True,
    markdown=True,
)


dag_content = """
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Using 'schedule' instead of deprecated 'schedule_interval'
with DAG(
    'example_dag',
    default_args=default_args,
    description='A simple example DAG',
    schedule='@daily',  # Changed from schedule_interval
    catchup=False
) as dag:

    def print_hello():
        print("Hello from Airflow!")
        return "Hello task completed"

    task = PythonOperator(
        task_id='hello_task',
        python_callable=print_hello,
        dag=dag,
    )
"""

agent.run(f"Save this DAG file as 'example_dag.py': {dag_content}")


agent.print_response("Read the contents of 'example_dag.py'")



================================================
FILE: cookbook/tools/apify_tools.py
================================================
from agno.agent import Agent
from agno.tools.apify import ApifyTools

# Apify Tools Demonstration Script
"""
This script showcases the power of web scraping and data extraction using Apify's Actors (serverless tools). 
The Apify ecosystem has 4000+ pre-built Actors for almost any web data extraction need!

---
Configuration Instructions:
1. Install required dependencies:
   pip install agno langchain-apify apify-client

2. Set the APIFY_API_TOKEN environment variable:
   Add a .env file with APIFY_API_TOKEN=your_apify_api_key
---

Tip: Check out the Apify Store (https://apify.com/store) to find tools for almost any web scraping or data extraction task.
"""

# Create an Apify Tools agent with versatile capabilities
agent = Agent(
    name="Web Insights Explorer",
    instructions=[
        "You are a sophisticated web research assistant capable of extracting insights from various online sources. "
        "Use the available tools for your tasks to gather accurate, well-structured information."
    ],
    tools=[
        ApifyTools(
            actors=[
                "apify/rag-web-browser",
                "compass/crawler-google-places",
                "clockworks/free-tiktok-scraper",
            ]
        )
    ],
    show_tool_calls=True,
    markdown=True,
)


def demonstrate_tools():
    print("Apify Tools Exploration 🔍")

    # RAG Web Search Demonstrations
    print("\n1.1 🕵️ RAG Web Search Scenarios:")
    prompt = "Research the latest AI ethics guidelines from top tech companies. Compile a summary from at least 3 different sources comparing their approaches using RAG Web Browser."
    agent.print_response(prompt, show_full_reasoning=True)

    print("\n1.2 🕵️ RAG Web Search Scenarios:")
    prompt = "Carefully extract the key introduction details from https://docs.agno.com/introduction"  #  Extract content from specific website
    agent.print_response(prompt)

    # Google Places Demonstration
    print("\n2. Google Places Crawler:")
    prompt = "Find the top 5 highest-rated coffee shops in San Francisco with detailed information about each location"
    agent.print_response(prompt)

    # Tiktok Scraper Demonstration
    print("\n3. Tiktok Profile Analysis:")
    prompt = "Analyze two profiles on Tiktok that lately added #AI (hashtag AI), extracting their statistics and recent content trends"
    agent.print_response(prompt)


if __name__ == "__main__":
    demonstrate_tools()

"""
Want to add a new tool? It's easy!
- Browse Apify Store
- Find an Actor that matches your needs
- Add a new method to ApifyTools following the existing pattern
- Register the method in the __init__

Examples of potential tools:
- YouTube video info scraper
- Twitter/X profile analyzer
- Product price trackers
- Job board crawlers
- News article extractors
- And SO MUCH MORE!
"""



================================================
FILE: cookbook/tools/arxiv_tools.py
================================================
from agno.agent import Agent
from agno.tools.arxiv import ArxivTools

agent = Agent(tools=[ArxivTools()], show_tool_calls=True)
agent.print_response("Search arxiv for 'language models'", markdown=True)



================================================
FILE: cookbook/tools/aws_lambda_tools.py
================================================
"""Run `pip install openai boto3` to install dependencies."""

from agno.agent import Agent
from agno.tools.aws_lambda import AWSLambdaTools

# Create an Agent with the AWSLambdaTool
agent = Agent(
    tools=[AWSLambdaTools(region_name="us-east-1")],
    name="AWS Lambda Agent",
    show_tool_calls=True,
)

# Example 1: List all Lambda functions
agent.print_response("List all Lambda functions in our AWS account", markdown=True)

# Example 2: Invoke a specific Lambda function
agent.print_response(
    "Invoke the 'hello-world' Lambda function with an empty payload", markdown=True
)

# Note: Make sure you have the necessary AWS credentials set up in your environment
# or use AWS CLI's configure command to set them up before running this script.



================================================
FILE: cookbook/tools/aws_ses_tools.py
================================================
"""
AWS SES (Simple Email Service) Setup Instructions:

1. Go to AWS SES Console and verify your domain or email address:
   - For production:
     a. Go to AWS SES Console > Verified Identities > Create Identity
     b. Choose "Domain" and follow DNS verification steps
     c. Add DKIM and SPF records to your domain's DNS
   - For testing:
     a. Choose "Email Address" verification
     b. Click verification link sent to your email

2. Configure AWS Credentials:
   a. Create an IAM user:
      - Go to IAM Console > Users > Add User
      - Enable "Programmatic access"
      - Attach 'AmazonSESFullAccess' policy

   b. Set up credentials (choose one method):
      Method 1 - Using AWS CLI:
      ```
      aws configure
      # Enter your AWS Access Key ID
      # Enter your AWS Secret Access Key
      # Enter your default region
      ```

      Method 2 - Set environment variables:
      ```
      export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY
      export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
      ```

3. Install required Python packages:
   ```
   pip install boto3 agno
   ```

4. Update the variables below with your configuration:
   - sender_email: Your verified sender email address
   - sender_name: Display name that appears in email clients
   - region_name: AWS region where SES is set up (e.g., 'us-east-1', 'ap-south-1')

"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.aws_ses import AWSSESTool
from agno.tools.duckduckgo import DuckDuckGoTools

# Configure email settings
sender_email = "coolmusta@gmail.com"  # Your verified SES email
sender_name = "AI Research Updates"
region_name = "us-west-2"  # Your AWS region

# Create an agent that can research and send personalized email updates
agent = Agent(
    name="Research Newsletter Agent",
    model=OpenAIChat(id="gpt-4o"),
    description="""You are an AI research specialist who creates and sends personalized email 
    newsletters about the latest developments in artificial intelligence and technology.""",
    instructions=[
        "When given a prompt:",
        "1. Extract the recipient's email address carefully. Look for the complete email in format 'user@domain.com'.",
        "2. Research the latest AI developments using DuckDuckGo",
        "3. Compose a concise, engaging email with:",
        "   - A compelling subject line",
        "   - 3-4 key developments or news items",
        "   - Brief explanations of why they matter",
        "   - Links to sources",
        "4. Format the content in a clean, readable way",
        "5. Send the email using AWS SES. IMPORTANT: The receiver_email parameter must be the COMPLETE email address including the @ symbol and domain (e.g., if the user says 'send to mustafa@agno.com', you must use receiver_email='mustafa@agno.com', NOT 'mustafacom' or any other variation).",
    ],
    tools=[
        AWSSESTool(
            sender_email=sender_email, sender_name=sender_name, region_name=region_name
        ),
        DuckDuckGoTools(),
    ],
    markdown=True,
    show_tool_calls=True,
)

# Example 1: Send an email
agent.print_response(
    "Research AI developments in healthcare from the past week with a focus on practical applications in clinical settings. Send the summary via email to mustafa@agno.com"
)

"""
Troubleshooting:
- If emails aren't sending, check:
  * Both sender and recipient are verified (in sandbox mode)
  * AWS credentials are correctly configured
  * You're within sending limits
  * Your IAM user has correct SES permissions
- Use SES Console's 'Send Test Email' feature to verify setup
"""



================================================
FILE: cookbook/tools/baidusearch_tools.py
================================================
from agno.agent import Agent
from agno.tools.baidusearch import BaiduSearchTools

agent = Agent(
    tools=[BaiduSearchTools()],
    description="You are a search agent that helps users find the most relevant information using Baidu.",
    instructions=[
        "Given a topic by the user, respond with the 3 most relevant search results about that topic.",
        "Search for 5 results and select the top 3 unique items.",
        "Search in both English and Chinese.",
    ],
    show_tool_calls=True,
)
agent.print_response("What are the latest advancements in AI?", markdown=True)



================================================
FILE: cookbook/tools/bitbucket_tools.py
================================================
"""
Setup:
1. Generate an App Password:
   - Go to "Personal Bitbucket settings" -> "App passwords"
   - Create a new App password with the appropriate permissions

2. Set environment variables:
   - BITBUCKET_USERNAME: Your Bitbucket username
   - BITBUCKET_PASSWORD: Your generated App password
"""

from agno.agent import Agent
from agno.tools.bitbucket import BitbucketTools

repo_slug = "ai"
workspace = "MaximMFP"

agent = Agent(
    tools=[BitbucketTools(workspace=workspace, repo_slug=repo_slug)],
    show_tool_calls=True,
)

agent.print_response("List open pull requests", markdown=True)

# Example 1: Get specific pull request details
# agent.print_response("Get details of pull request #23", markdown=True)

# Example 2: Get the repo details
# agent.print_response("Get details of the repository", markdown=True)

# Example 3: List repositories
# agent.print_response("List 5 repositories for this workspace", markdown=True)

# Example 4: List commits
# agent.print_response("List the last 20 commits", markdown=True)



================================================
FILE: cookbook/tools/brandfetch_tools.py
================================================
"""
You can use the Brandfetch API to retrieve the company's brand information.

Register an account at: https://developers.brandfetch.com/register

For the Brand API, you can use the `brand` parameter to True. (default is True)
For the Brand Search API, you can use the `search` parameter to True. (default is False)

-- Brand API

Export your API key as an environment variable:
export BRANDFETCH_API_KEY=your_api_key

-- Brand Search API

Export your Client ID as an environment variable:
export BRANDFETCH_CLIENT_KEY=your_client_id

You can find it on https://developers.brandfetch.com/dashboard/brand-search-api in the provided URL after `c=...`

"""

from agno.agent import Agent
from agno.tools.brandfetch import BrandfetchTools

# Brand API

# agent = Agent(
#     tools=[BrandfetchTools()],
#     show_tool_calls=True,
#     description="You are a Brand research agent. Given a company name or company domain, you will use the Brandfetch API to retrieve the company's brand information.",
# )
# agent.print_response("What is the brand information of Google?", markdown=True)


# Brand Search API

agent = Agent(
    tools=[BrandfetchTools(search=True)],
    show_tool_calls=True,
    description="You are a Brand research agent. Given a company name or company domain, you will use the Brandfetch API to retrieve the company's brand information.",
)
agent.print_response("What is the brand information of Agno?", markdown=True)



================================================
FILE: cookbook/tools/bravesearch_tools.py
================================================
from agno.agent import Agent
from agno.tools.bravesearch import BraveSearchTools

agent = Agent(
    tools=[BraveSearchTools()],
    description="You are a news agent that helps users find the latest news.",
    instructions=[
        "Given a topic by the user, respond with 4 latest news items about that topic."
    ],
    show_tool_calls=True,
)
agent.print_response("AI Agents", markdown=True)



================================================
FILE: cookbook/tools/brightdata_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.brightdata import BrightDataTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        BrightDataTools(
            get_screenshot=True,
        )
    ],
    markdown=True,
    show_tool_calls=True,
)

# Example 1: Scrape a webpage as Markdown
agent.print_response(
    "Scrape this webpage as markdown: https://docs.agno.com/introduction",
)

# Example 2: Take a screenshot of a webpage
# agent.print_response(
#     "Take a screenshot of this webpage: https://docs.agno.com/introduction",
# )

# response = agent.run_response
# if response.images:
#     save_base64_data(response.images[0].content, "tmp/agno_screenshot.png")

# Add a new SERP API zone: https://brightdata.com/cp/zones/new
# Example 3: Search using Google
# agent.print_response(
#     "Search Google for 'Python web scraping best practices' and give me the top 5 results",
# )

# Example 4: Get structured data from Amazon product
# agent.print_response(
#     "Get detailed product information from this Amazon product: https://www.amazon.com/dp/B0D2Q9397Y?th=1&psc=1",
# )

# Example 5: Get LinkedIn profile data
# agent.print_response(
#     "Search for Satya Nadella on LinkedIn and give me a summary of his profile"
# )



================================================
FILE: cookbook/tools/browserbase_tools.py
================================================
from os import getenv

from agno.agent import Agent
from agno.tools.browserbase import BrowserbaseTools

# Browserbase Configuration
# -------------------------------
# These environment variables are required for the BrowserbaseTools to function properly.
# You can set them in your .env file or export them directly in your terminal.

# BROWSERBASE_API_KEY: Your API key from Browserbase dashboard
#   - Required for authentication
#   - Format: Starts with "bb_live_" or "bb_test_" followed by a unique string

# BROWSERBASE_PROJECT_ID: The project ID from your Browserbase dashboard
#   - Required to identify which project to use for browser sessions
#   - Format: UUID string (8-4-4-4-12 format)

# BROWSERBASE_BASE_URL: The Browserbase API endpoint
#   - Optional: Defaults to https://api.browserbase.com if not specified
#   - Only change this if you're using a custom API endpoint or proxy

agent = Agent(
    name="Web Automation Assistant",
    tools=[BrowserbaseTools()],
    instructions=[
        "You are a web automation assistant that can help with:",
        "1. Capturing screenshots of websites",
        "2. Extracting content from web pages",
        "3. Monitoring website changes",
        "4. Taking visual snapshots of responsive layouts",
        "5. Automated web testing and verification",
    ],
    show_tool_calls=True,
    markdown=True,
)

# Content Extraction and SS
# agent.print_response("""
#     Go to https://news.ycombinator.com and extract:
#     1. The page title
#     2. Take a screenshot of the top stories section
# """)

agent.print_response("""
    Visit https://quotes.toscrape.com and:
    1. Extract the first 5 quotes and their authors
    2. Navigate to page 2
    3. Extract the first 5 quotes from page 2
""")



================================================
FILE: cookbook/tools/calcom_tools.py
================================================
from datetime import datetime

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.calcom import CalComTools

"""
Example showing how to use the Cal.com Tools with Agno.

Requirements:
- Cal.com API key (get from cal.com/settings/developer/api-keys)
- Event Type ID from Cal.com
- pip install requests pytz

Usage:
- Set the following environment variables:
    export CALCOM_API_KEY="your_api_key"
    export CALCOM_EVENT_TYPE_ID="your_event_type_id"

- Or provide them when creating the CalComTools instance
"""

INSTRUCTONS = f"""You're scheduing assistant. Today is {datetime.now()}.
You can help users by:
    - Finding available time slots
    - Creating new bookings
    - Managing existing bookings (view, reschedule, cancel)
    - Getting booking details
    - IMPORTANT: In case of rescheduling or cancelling booking, call the get_upcoming_bookings function to get the booking uid. check available slots before making a booking for given time
    Always confirm important details before making bookings or changes.
"""


agent = Agent(
    name="Calendar Assistant",
    instructions=[INSTRUCTONS],
    model=OpenAIChat(id="gpt-4"),
    tools=[CalComTools(user_timezone="America/New_York")],
    show_tool_calls=True,
    markdown=True,
)

# Example usage
agent.print_response("What are my bookings for tomorrow?")



================================================
FILE: cookbook/tools/calculator_tools.py
================================================
from agno.agent import Agent
from agno.tools.calculator import CalculatorTools

agent = Agent(
    tools=[
        CalculatorTools(
            add=True,
            subtract=True,
            multiply=True,
            divide=True,
            exponentiate=True,
            factorial=True,
            is_prime=True,
            square_root=True,
        )
    ],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("What is 10*5 then to the power of 2, do it step by step")



================================================
FILE: cookbook/tools/cartesia_tools.py
================================================
"""
pip install cartesia
Get an API key from https://play.cartesia.ai/keys
"""

from agno.agent import Agent
from agno.tools.cartesia import CartesiaTools
from agno.utils.audio import write_audio_to_file

# Initialize Agent with Cartesia tools
agent = Agent(
    name="Cartesia TTS Agent",
    description="An agent that uses Cartesia for text-to-speech.",
    tools=[CartesiaTools()],
    show_tool_calls=True,
)

response = agent.run(
    f"""Generate a simple greeting using Text-to-Speech:

    Say "Welcome to Cartesia, the advanced speech synthesis platform. This speech is generated by an agent. """
)
# Save the generated audio
if response.audio:
    write_audio_to_file(
        audio=response.audio[0].base64_audio, filename="tmp/greeting.mp3"
    )



================================================
FILE: cookbook/tools/clickup_tools.py
================================================
"""
Steps to Get Your ClickUp API Key

Step 1: Log In to ClickUp
Step 2: Navigate to Settings (usually a circle with your initials) click on it
Step 3: Access the Apps Section: In the settings sidebar on the left, scroll down until you find Apps. Click on it to access the API settings.
Step 4: Generate Your API Key
In the Apps section, you should see an option labeled API Token. If it’s not already generated, look for a button that says Generate and click it.
Once generated, your API key will be displayed. Make sure to copy this key and store it as CLICKUP_API_KEY in .env file to use it.

Steps To find your MASTER_SPACE_ID :
clickup space url structure: https://app.clickup.com/{MASTER_SPACE_ID}/v/o/s/{SPACE_ID}
1. copy any space url from your clickup workspace all follow above url structure.
2. To use clickup tool copy the MASTER_SPACE_ID and store it .env file.

"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.clickup_tool import ClickUpTools

clickup_agent = Agent(
    name="ClickUp Agent",
    role="Manage ClickUp tasks and spaces",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[ClickUpTools(list_spaces=True, list_lists=True, list_tasks=True)],
    instructions=[
        "You are a ClickUp assistant that helps users manage their tasks and spaces.",
        "You can:",
        "1. List all available spaces",
        "2. List tasks from a specific space",
        "3. List all lists in a space",
        "4. Create new tasks with title, description, and status",
        "When creating tasks:",
        "- Always get space name, task name, and description",
        "- Status can be: todo, in progress, or done",
        "- If status is not specified, use 'todo' as default",
        "Be helpful and guide users if they need more information.",
    ],
    show_tool_calls=True,
    markdown=True,
)

clickup_agent.print_response(
    "List all spaces i have",
    markdown=True,
)
clickup_agent.print_response(
    "Create a task (status 'To Do') called 'QA task' in Project 1 in the Team Space. The description should be about running basic QA checks on our Python codebase.",
    markdown=True,
)



================================================
FILE: cookbook/tools/composio_tools.py
================================================
from agno.agent import Agent
from composio_agno import Action, ComposioToolSet

toolset = ComposioToolSet()
composio_tools = toolset.get_tools(
    actions=[Action.GITHUB_STAR_A_REPOSITORY_FOR_THE_AUTHENTICATED_USER]
)

agent = Agent(tools=composio_tools, show_tool_calls=True)
agent.print_response("Can you star agno-agi/agno repo?")



================================================
FILE: cookbook/tools/confluence_tools.py
================================================
from agno.agent import Agent
from agno.tools.confluence import ConfluenceTools

agent = Agent(
    name="Confluence agent",
    tools=[ConfluenceTools()],
    show_tool_calls=True,
    markdown=True,
)

## getting space details
agent.print_response("How many spaces are there and what are their names?")

## getting page_content
agent.print_response(
    "What is the content present in page 'Large language model in LLM space'"
)

## getting page details in a particular space
agent.print_response("Can you extract all the page names from 'LLM' space")

## creating a new page in a space
agent.print_response("Can you create a new page named 'TESTING' in 'LLM' space")



================================================
FILE: cookbook/tools/crawl4ai_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.crawl4ai import Crawl4aiTools

# # Example 1: Basic usage
agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    tools=[Crawl4aiTools(use_pruning=True)],
    instructions="You are a helpful assistant that can crawl the web and extract information. Use have access to crawl4ai tools to extract information from the web.",
)
agent.print_response(
    "Give me a detailed summary of the Agno project from https://github.com/agno-agi/agno and what are its main features?"
)

# Example 2: Extract main content only (remove navigation, ads, etc.)
# agent_clean = Agent(tools=[Crawl4aiTools(use_pruning=True)], show_tool_calls=True)
# agent_clean.print_response(
#     "Get the History from https://en.wikipedia.org/wiki/Python_(programming_language)"
# )

# Example 3: Search for specific content on a page
# agent_search = Agent(
#     instructions="You are a helpful assistant that can crawl the web and extract information. Use have access to crawl4ai tools to extract information from the web.",
#     tools=[Crawl4aiTools()],
#     show_tool_calls=True,
# )
# agent_search.print_response(
#     "What are the diferent Techniques used in AI? https://en.wikipedia.org/wiki/Artificial_intelligence"
# )

# Example 4: Multiple URLs with clean extraction
# agent_multi = Agent(
#     tools=[Crawl4aiTools(use_pruning=True, headless=False)], show_tool_calls=True
# )
# agent_multi.print_response(
#     "Compare the main content from https://en.wikipedia.org/wiki/Artificial_intelligence and https://en.wikipedia.org/wiki/Machine_learning"
# )



================================================
FILE: cookbook/tools/csv_tools.py
================================================
from pathlib import Path

import httpx
from agno.agent import Agent
from agno.tools.csv_toolkit import CsvTools

url = "https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv"
response = httpx.get(url)

imdb_csv = Path(__file__).parent.joinpath("imdb.csv")
imdb_csv.parent.mkdir(parents=True, exist_ok=True)
imdb_csv.write_bytes(response.content)

agent = Agent(
    tools=[CsvTools(csvs=[imdb_csv])],
    markdown=True,
    show_tool_calls=True,
    instructions=[
        "First always get the list of files",
        "Then check the columns in the file",
        "Then run the query to answer the question",
    ],
)
agent.cli_app(stream=False)



================================================
FILE: cookbook/tools/custom_api_tools.py
================================================
from agno.agent import Agent
from agno.tools.api import CustomApiTools

"""
Args:
    base_url (Optional[str]): Base URL for API calls
    username (Optional[str]): Username for basic authentication
    password (Optional[str]): Password for basic authentication
    api_key (Optional[str]): API key for authentication
    headers (Optional[Dict[str, str]]): Default headers to include in requests
    verify_ssl (bool): Whether to verify SSL certificates
    timeout (int): Request timeout in seconds
"""
agent = Agent(
    tools=[CustomApiTools(base_url="https://dog.ceo/api", make_request=True)],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response(
    'Make api calls to the following two different endpoints- /breeds/image/random and /breeds/list/all to get a random dog image and list of dog breeds respectively. Make sure that the method is "GET" for both the api calls.'
)



================================================
FILE: cookbook/tools/custom_async_tools.py
================================================
import asyncio
from dataclasses import dataclass

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from pydantic import BaseModel


async def dict_tool(name: str, age: int, city: str):
    """
    Return a dictionary with the name, age, and city of the person.
    """
    return {"name": name, "age": age, "city": city}


async def list_tool(items: list[str]):
    """
    Return a list of items.
    """
    return items


async def set_tool(items: list[str]):
    """
    Return a set of items.
    """
    return set(items)


async def tuple_tool(name: str, age: int, city: str):
    """
    Return a tuple with the name, age, and city of the person.
    """
    return (name, age, city)


async def generator_tool(items: list[str]):
    """
    Return a generator of items.
    """
    for item in items:
        yield item
        yield " "


async def pydantic_tool(name: str, age: int, city: str):
    """
    Return a Pydantic model with the name, age, and city of the person.
    """

    class CustomTool(BaseModel):
        name: str
        age: int
        city: str

    return CustomTool(name=name, age=age, city=city)


async def data_class_tool(name: str, age: int, city: str):
    """
    Return a data class with the name, age, and city of the person.
    """

    @dataclass
    class CustomTool:
        name: str
        age: int
        city: str

    return CustomTool(name=name, age=age, city=city)


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        dict_tool,
        list_tool,
        generator_tool,
        pydantic_tool,
        data_class_tool,
        set_tool,
        tuple_tool,
    ],
    show_tool_calls=True,
)

asyncio.run(
    agent.aprint_response("Call all the tools and make up interesting arguments")
)



================================================
FILE: cookbook/tools/custom_tools.py
================================================
from dataclasses import dataclass

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from pydantic import BaseModel


def dict_tool(name: str, age: int, city: str):
    """
    Return a dictionary with the name, age, and city of the person.
    """
    return {"name": name, "age": age, "city": city}


def list_tool(items: list[str]):
    """
    Return a list of items.
    """
    return items


def set_tool(items: list[str]):
    """
    Return a set of items.
    """
    return set(items)


def tuple_tool(name: str, age: int, city: str):
    """
    Return a tuple with the name, age, and city of the person.
    """
    return (name, age, city)


def generator_tool(items: list[str]):
    """
    Return a generator of items.
    """
    for item in items:
        yield item
        yield " "


def pydantic_tool(name: str, age: int, city: str):
    """
    Return a Pydantic model with the name, age, and city of the person.
    """

    class CustomTool(BaseModel):
        name: str
        age: int
        city: str

    return CustomTool(name=name, age=age, city=city)


def data_class_tool(name: str, age: int, city: str):
    """
    Return a data class with the name, age, and city of the person.
    """

    @dataclass
    class CustomTool:
        name: str
        age: int
        city: str

    return CustomTool(name=name, age=age, city=city)


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        dict_tool,
        list_tool,
        generator_tool,
        pydantic_tool,
        data_class_tool,
        set_tool,
        tuple_tool,
    ],
    show_tool_calls=True,
)
agent.print_response("Call all the tools and make up interesting arguments")



================================================
FILE: cookbook/tools/dalle_tools.py
================================================
"""Run `pip install openai` to install dependencies."""

from pathlib import Path

from agno.agent import Agent
from agno.tools.dalle import DalleTools
from agno.utils.media import download_image

# Create an Agent with the DALL-E tool
agent = Agent(tools=[DalleTools()], name="DALL-E Image Generator")

# Example 1: Generate a basic image with default settings
agent.print_response(
    "Generate an image of a futuristic city with flying cars and tall skyscrapers",
    markdown=True,
)

# Example 2: Generate an image with custom settings
custom_dalle = DalleTools(
    model="dall-e-3", size="1792x1024", quality="hd", style="natural"
)

agent_custom = Agent(
    tools=[custom_dalle],
    name="Custom DALL-E Generator",
    show_tool_calls=True,
)

response = agent_custom.run(
    "Create a panoramic nature scene showing a peaceful mountain lake at sunset",
    markdown=True,
)
if response.images:
    download_image(
        url=response.images[0].url,
        output_path=Path(__file__).parent.joinpath("tmp/nature.jpg"),
    )



================================================
FILE: cookbook/tools/daytona_tools.py
================================================
"""
👩‍💻 Agent with Daytona tools

This example shows how to use Agno's Daytona integration to run Agent-generated code in a remote, secure sandbox.

1. Get your Daytona API key and API URL: https://app.daytona.io/dashboard/keys
2. Set the API key and API URL as environment variables:
    export DAYTONA_API_KEY=<your_api_key>
    export DAYTONA_API_URL=<your_api_url> (optional)
3. Install the dependencies:
    pip install agno anthropic daytona
"""

from agno.agent import Agent
from agno.tools.daytona import DaytonaTools

agent = Agent(
    name="Coding Agent with Daytona tools",
    tools=[DaytonaTools()],
    markdown=True,
    instructions=[
        "You are an expert at writing and executing code. You have access to a remote, secure Daytona sandbox.",
        "Your primary purpose is to:",
        "1. Write clear, efficient code based on user requests",
        "2. ALWAYS execute the code in the Daytona sandbox using run_code",
        "3. Show the actual execution results to the user",
        "4. Provide explanations of how the code works and what the output means",
        "Guidelines:",
        "- NEVER just provide code without executing it",
        "- Execute all code using the run_code tool to show real results",
        "- Support Python, JavaScript, and TypeScript execution",
        "- Use file operations (create_file, read_file) when working with scripts",
        "- Install missing packages when needed using run_shell_command",
        "- Always show both the code AND the execution output",
        "- Handle errors gracefully and explain any issues encountered",
    ],
    show_tool_calls=True,
)

agent.print_response(
    "Write JavaScript code to generate 10 random numbers between 1 and 100, sort them in ascending order, and print each number"
)



================================================
FILE: cookbook/tools/desi_vocal_tools.py
================================================
"""
pip install requests
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.desi_vocal import DesiVocalTools

audio_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DesiVocalTools()],
    description="You are an AI agent that can generate audio using the DesiVocal API.",
    instructions=[
        "When the user asks you to generate audio, use the `text_to_speech` tool to generate the audio.",
        "You'll generate the appropriate prompt to send to the tool to generate audio.",
        "You don't need to find the appropriate voice first, I already specified the voice to user.",
        "Return the audio file name in your response. Don't convert it to markdown.",
        "Generate the text prompt we send in hindi language",
    ],
    markdown=True,
    show_tool_calls=True,
)

audio_agent.print_response(
    "Generate a very small audio of history of french revolution"
)



================================================
FILE: cookbook/tools/discord_tools.py
================================================
import os

from agno.agent import Agent
from agno.tools.discord import DiscordTools

# Get Discord token from environment
discord_token = os.getenv("DISCORD_BOT_TOKEN")
if not discord_token:
    raise ValueError("DISCORD_BOT_TOKEN not set")

# Initialize Discord tools
discord_tools = DiscordTools(
    bot_token=discord_token,
    enable_messaging=True,
    enable_history=True,
    enable_channel_management=True,
    enable_message_management=True,
)

# Create an agent with Discord tools
discord_agent = Agent(
    name="Discord Agent",
    instructions=[
        "You are a Discord bot that can perform various operations.",
        "You can send messages, read message history, manage channels, and delete messages.",
    ],
    tools=[discord_tools],
    show_tool_calls=True,
    markdown=True,
)

# Replace with your Discord IDs
channel_id = "YOUR_CHANNEL_ID"
server_id = "YOUR_SERVER_ID"

# Example 1: Send a message
discord_agent.print_response(
    f"Send a message 'Hello from Agno!' to channel {channel_id}", stream=True
)

# Example 2: Get channel info
discord_agent.print_response(f"Get information about channel {channel_id}", stream=True)

# Example 3: List channels
discord_agent.print_response(f"List all channels in server {server_id}", stream=True)

# Example 4: Get message history
discord_agent.print_response(
    f"Get the last 5 messages from channel {channel_id}", stream=True
)

# Example 5: Delete a message (replace message_id with an actual message ID)
# message_id = 123456789
# discord_agent.print_response(
#     f"Delete message {message_id} from channel {channel_id}",
#     stream=True
# )



================================================
FILE: cookbook/tools/docker_tools.py
================================================
import sys

from agno.agent import Agent

try:
    from agno.tools.docker import DockerTools

    docker_tools = DockerTools(
        enable_container_management=True,
        enable_image_management=True,
        enable_volume_management=True,
        enable_network_management=True,
    )

    # Create an agent with Docker tools
    docker_agent = Agent(
        name="Docker Agent",
        instructions=[
            "You are a Docker management assistant that can perform various Docker operations.",
            "You can manage containers, images, volumes, and networks.",
        ],
        tools=[docker_tools],
        show_tool_calls=True,
        markdown=True,
    )

    # Example 1: List running containers
    docker_agent.print_response("List all running Docker containers", stream=True)

    # Example 2: List all images
    docker_agent.print_response("List all Docker images on this system", stream=True)

    # Example 3: Pull an image
    docker_agent.print_response("Pull the latest nginx image", stream=True)

    # Example 4: Run a container
    docker_agent.print_response(
        "Run an nginx container named 'web-server' on port 8080", stream=True
    )

    # Example 5: Get container logs
    docker_agent.print_response("Get logs from the 'web-server' container", stream=True)

    # # Example 6: List volumes
    docker_agent.print_response("List all Docker volumes", stream=True)

    # Example 7: Create a network
    docker_agent.print_response(
        "Create a new Docker network called 'test-network'", stream=True
    )

    # Example 8: Stop and remove container
    docker_agent.print_response(
        "Stop and remove the 'web-server' container", stream=True
    )

    # Example 9: Inspect an image
    docker_agent.print_response("Inspect the nginx image", stream=True)

    # # Example 10: Build an image (uncomment and modify path as needed)
    # docker_agent.print_response(
    #     "Build a Docker image from the Dockerfile in ./app with tag 'myapp:latest'",
    #     stream=True
    # )

except ValueError as e:
    print(f"\n❌ Docker Tool Error: {e}")
    print("\n🔍 Troubleshooting steps:")

    if sys.platform == "darwin":  # macOS
        print("1. Ensure Docker Desktop is running")
        print("2. Check Docker Desktop settings")
        print("3. Try running 'docker ps' in terminal to verify access")

    elif sys.platform == "linux":
        print("1. Check if Docker service is running:")
        print("   systemctl status docker")
        print("2. Make sure your user has permissions to access Docker:")
        print("   sudo usermod -aG docker $USER")

    elif sys.platform == "win32":
        print("1. Ensure Docker Desktop is running")
        print("2. Check Docker Desktop settings")



================================================
FILE: cookbook/tools/duckdb_tools.py
================================================
from agno.agent import Agent
from agno.tools.duckdb import DuckDbTools

agent = Agent(
    tools=[DuckDbTools()],
    show_tool_calls=True,
    instructions="Use this file for Movies data: https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
)
agent.print_response(
    "What is the average rating of movies?", markdown=True, stream=False
)



================================================
FILE: cookbook/tools/duckduckgo_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
)

agent.print_response("Whats the latest about gpt 5?", markdown=True)



================================================
FILE: cookbook/tools/e2b_tools.py
================================================
"""
E2B Tools Example - Demonstrates how to use the E2B toolkit for sandboxed code execution.

This example shows how to:
1. Set up authentication with E2B API
2. Initialize the E2BTools with proper configuration
3. Create an agent that can run Python code in a secure sandbox
4. Use the sandbox for data analysis, visualization, and more

Prerequisites:
1. Create an account and get your API key from E2B:
   - Visit https://e2b.dev/
   - Sign up for an account
   - Navigate to the Dashboard to get your API key

2. Install required packages:
   pip install e2b_code_interpreter pandas matplotlib

3. Set environment variable:
   export E2B_API_KEY=your_api_key

Features:
- Run Python code in a secure sandbox environment
- Upload and download files to/from the sandbox
- Create and download data visualizations
- Run servers within the sandbox with public URLs
- Manage sandbox lifecycle (timeout, shutdown)
- Access the internet from within the sandbox

Usage:
Run this script with the E2B_API_KEY environment variable set to interact
with the E2B sandbox through natural language commands.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.e2b import E2BTools

e2b_tools = E2BTools(
    timeout=600,  # 10 minutes timeout (in seconds)
)

agent = Agent(
    name="Code Execution Sandbox",
    agent_id="e2b-sandbox",
    model=OpenAIChat(id="gpt-4o"),
    tools=[e2b_tools],
    markdown=True,
    show_tool_calls=True,
    instructions=[
        "You are an expert at writing and validating Python code using a secure E2B sandbox environment.",
        "Your primary purpose is to:",
        "1. Write clear, efficient Python code based on user requests",
        "2. Execute and verify the code in the E2B sandbox",
        "3. Share the complete code with the user, as this is the main use case",
        "4. Provide thorough explanations of how the code works",
        "",
        "You can use these tools:",
        "1. Run Python code (run_python_code)",
        "2. Upload files to the sandbox (upload_file)",
        "3. Download files from the sandbox (download_file_from_sandbox)",
        "4. Generate and add visualizations as image artifacts (download_png_result)",
        "5. List files in the sandbox (list_files)",
        "6. Read and write file content (read_file_content, write_file_content)",
        "7. Start web servers and get public URLs (run_server, get_public_url)",
        "8. Manage the sandbox lifecycle (set_sandbox_timeout, get_sandbox_status, shutdown_sandbox)",
        "",
        "Guidelines:",
        "- ALWAYS share the complete code with the user, properly formatted in code blocks",
        "- Verify code functionality by executing it in the sandbox before sharing",
        "- Iterate and debug code as needed to ensure it works correctly",
        "- Use pandas, matplotlib, and other Python libraries for data analysis when appropriate",
        "- Create proper visualizations when requested and add them as image artifacts to show inline",
        "- Handle file uploads and downloads properly",
        "- Explain your approach and the code's functionality in detail",
        "- Format responses with both code and explanations for maximum clarity",
        "- Handle errors gracefully and explain any issues encountered",
    ],
)


agent.print_response(
    "Write Python code to generate the first 10 Fibonacci numbers and calculate their sum and average"
)

# agent.print_response(
#     " upload file cookbook/tools/sample_data.csv and use it to create a matplotlib visualization of total sales by region and provide chart image or its downloaded path or any link  "
# )
# agent.print_response(" use dataset sample_data.csv and create a matplotlib visualization of total sales by region and provide chart image")
# agent.print_response(" run a server and Write a simple fast api web server that displays 'Hello from E2B Sandbox!' and run it , use run_command to get the data from the server and provide the  url of api swagger docs and host link")
# agent.print_response(
#     " run server and Create and run a Python script that fetch top 5 latest news from hackernews using hackernews api"
# )
# agent.print_response("Extend the sandbox timeout to 20 minutes")
# agent.print_response("list all sandboxes ")



================================================
FILE: cookbook/tools/elevenlabs_tools.py
================================================
"""
pip install elevenlabs
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.eleven_labs import ElevenLabsTools

audio_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ElevenLabsTools(
            voice_id="21m00Tcm4TlvDq8ikWAM",
            model_id="eleven_multilingual_v2",
            target_directory="audio_generations",
        )
    ],
    description="You are an AI agent that can generate audio using the ElevenLabs API.",
    instructions=[
        "When the user asks you to generate audio, use the `generate_audio` tool to generate the audio.",
        "You'll generate the appropriate prompt to send to the tool to generate audio.",
        "You don't need to find the appropriate voice first, I already specified the voice to user."
        "Return the audio file name in your response. Don't convert it to markdown.",
        "The audio should be long and detailed.",
    ],
    markdown=True,
    show_tool_calls=True,
)

audio_agent.print_response("Generate a very long audio of history of french revolution")

audio_agent.print_response("Generate a kick sound effect")



================================================
FILE: cookbook/tools/email_tools.py
================================================
from agno.agent import Agent
from agno.tools.email import EmailTools

receiver_email = "<receiver_email>"
sender_email = "<sender_email>"
sender_name = "<sender_name>"
sender_passkey = "<sender_passkey>"

agent = Agent(
    tools=[
        EmailTools(
            receiver_email=receiver_email,
            sender_email=sender_email,
            sender_name=sender_name,
            sender_passkey=sender_passkey,
        )
    ]
)
agent.print_response("Send an email to <receiver_email>.")



================================================
FILE: cookbook/tools/evm_tools.py
================================================
"""
EVM Tools Example

This example demonstrates how to use Agno's EVM integration to send ETH transactions
on any EVM-compatible blockchain.

1. Set your environment variables:
    export EVM_PRIVATE_KEY=0x<your-private-key>
    export EVM_RPC_URL=https://your-rpc-endpoint

2. Or pass them directly to the EvmTools constructor
3. Install dependencies:
    pip install agno web3
"""

from agno.agent import Agent
from agno.tools.evm import EvmTools

# Option 1: Use environment variables (recommended)
agent = Agent(
    tools=[EvmTools()],  # Will use EVM_PRIVATE_KEY and EVM_RPC_URL from env
    show_tool_calls=True,
)

# Option 2: Pass credentials directly (for testing only)
# private_key = "0x<private-key>"
# rpc_url = "https://0xrpc.io/sep"  # Sepolia testnet
# agent = Agent(
#     tools=[
#         EvmTools(
#             private_key=private_key,
#             rpc_url=rpc_url,
#         )
#     ],
#     show_tool_calls=True,
# )

# Convert 0.001 ETH to wei (1 ETH = 10^18 wei)
# 0.001 ETH = 1,000,000,000,000,000 wei
agent.print_response(
    "Send 0.001 eth (which is 1000000000000000 wei) to address 0x3Dfc53E3C77bb4e30Ce333Be1a66Ce62558bE395"
)



================================================
FILE: cookbook/tools/exa_tools.py
================================================
from agno.agent import Agent
from agno.tools.exa import ExaTools

agent = Agent(
    tools=[
        ExaTools(
            include_domains=["cnbc.com", "reuters.com", "bloomberg.com"],
            show_results=True,
            text=False,
            highlights=False,
        )
    ],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("Search for AAPL news", markdown=True)


agent = Agent(
    tools=[
        ExaTools(
            show_results=True,
        )
    ],
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("Search for AAPL news", markdown=True)

agent.print_response(
    "What is the paper at https://arxiv.org/pdf/2307.06435 about?", markdown=True
)

agent.print_response(
    "Find me similar papers to https://arxiv.org/pdf/2307.06435 and provide a summary of what they contain",
    markdown=True,
)

agent.print_response(
    "What is the latest valuation of SpaceX?",
    markdown=True,
)



================================================
FILE: cookbook/tools/fal_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.fal import FalTools

fal_agent = Agent(
    name="Fal Video Generator Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FalTools("fal-ai/hunyuan-video")],
    description="You are an AI agent that can generate videos using the Fal API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "Return the URL as raw to the user.",
        "Don't convert video URL to markdown or anything else.",
    ],
    markdown=True,
    show_tool_calls=True,
)

fal_agent.print_response("Generate video of balloon in the ocean")



================================================
FILE: cookbook/tools/file_tools.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.tools.file import FileTools

agent = Agent(tools=[FileTools(Path("tmp/file"))], show_tool_calls=True)
agent.print_response(
    "What is the most advanced LLM currently? Save the answer to a file.", markdown=True
)

# Example 2: Search for files with a specific extension
# agent.print_response(
#     "Search for all files which have an extension '.txt' and save the answer to a a new file name 'all_txt_files.txt'",
#     markdown=True,
# )



================================================
FILE: cookbook/tools/financial_datasets_tools.py
================================================
"""
Financial Datasets API Toolkit Example
This example demonstrates various Financial Datasets API functionalities including
financial statements, stock prices, news, insider trades, and more.

Prerequisites:
- Set the environment variable `FINANCIAL_DATASETS_API_KEY` with your Financial Datasets API key.
  You can obtain the API key by creating an account at https://financialdatasets.ai
"""

from agno.agent import Agent
from agno.tools.financial_datasets import FinancialDatasetsTools

agent = Agent(
    name="Financial Data Agent",
    tools=[
        FinancialDatasetsTools(),  # For accessing financial data
    ],
    description="You are a financial data specialist that helps analyze financial information for stocks and cryptocurrencies.",
    instructions=[
        "When given a financial query:",
        "1. Use appropriate Financial Datasets methods based on the query type",
        "2. Format financial data clearly and highlight key metrics",
        "3. For financial statements, compare important metrics with previous periods when relevant",
        "4. Calculate growth rates and trends when appropriate",
        "5. Handle errors gracefully and provide meaningful feedback",
    ],
    markdown=True,
    show_tool_calls=True,
)

# Example 1: Financial Statements
print("\n=== Income Statement Example ===")
agent.print_response(
    "Get the most recent income statement for AAPL and highlight key metrics",
    stream=True,
)

# Example 2: Balance Sheet Analysis
print("\n=== Balance Sheet Analysis Example ===")
agent.print_response(
    "Analyze the balance sheets for MSFT over the last 3 years. Focus on debt-to-equity ratio and cash position.",
    stream=True,
)

# # Example 3: Cash Flow Analysis
# print("\n=== Cash Flow Analysis Example ===")
# agent.print_response(
#     "Get the quarterly cash flow statements for TSLA for the past year and analyze their free cash flow trends",
#     stream=True,
# )

# # Example 4: Company Information
# print("\n=== Company Information Example ===")
# agent.print_response(
#     "Provide key information about NVDA including its business description, sector, and industry",
#     stream=True,
# )

# # Example 5: Stock Price Analysis
# print("\n=== Stock Price Analysis Example ===")
# agent.print_response(
#     "Analyze the daily stock prices for AMZN over the past 30 days. Calculate the average, high, low, and volatility.",
#     stream=True,
# )

# # Example 6: Earnings Comparison
# print("\n=== Earnings Comparison Example ===")
# agent.print_response(
#     "Compare the last 4 earnings reports for GOOG. Show the trend in EPS and revenue.",
#     stream=True,
# )

# # Example 7: Insider Trades Analysis
# print("\n=== Insider Trades Analysis Example ===")
# agent.print_response(
#     "Analyze recent insider trading activity for META. Are insiders buying or selling?",
#     stream=True,
# )

# # Example 8: Institutional Ownership
# print("\n=== Institutional Ownership Example ===")
# agent.print_response(
#     "Who are the largest institutional owners of INTC? Have they increased or decreased their positions recently?",
#     stream=True,
# )

# # Example 9: Financial News
# print("\n=== Financial News Example ===")
# agent.print_response(
#     "What are the latest news items about NFLX? Summarize the key stories.",
#     stream=True,
# )

# # Example 10: Multi-stock Comparison
# print("\n=== Multi-stock Comparison Example ===")
# agent.print_response(
#     """Compare the following tech companies: AAPL, MSFT, GOOG, AMZN, META
#     1. Revenue growth rate
#     2. Profit margins
#     3. P/E ratios
#     4. Debt levels
#     Present as a comparison table.""",
#     stream=True,
# )

# # Example 11: Cryptocurrency Analysis
# print("\n=== Cryptocurrency Analysis Example ===")
# agent.print_response(
#     "Analyze Bitcoin (BTC) price movements over the past week. Show daily price changes and calculate volatility.",
#     stream=True,
# )

# # Example 12: SEC Filings Analysis
# print("\n=== SEC Filings Analysis Example ===")
# agent.print_response(
#     "Get the most recent 10-K and 10-Q filings for AAPL and extract key risk factors mentioned.",
#     stream=True,
# )

# # Example 13: Financial Metrics and Ratios
# print("\n=== Financial Metrics Example ===")
# agent.print_response(
#     "Calculate and explain the following financial metrics for TSLA: P/E ratio, P/S ratio, EV/EBITDA, and ROE.",
#     stream=True,
# )

# # Example 14: Segmented Financials
# print("\n=== Segmented Financials Example ===")
# agent.print_response(
#     "Analyze AAPL's segmented financials. How much revenue comes from each product category and geographic region?",
#     stream=True,
# )

# # Example 15: Stock Ticker Search
# print("\n=== Stock Ticker Search Example ===")
# agent.print_response(
#     "Find all stock tickers related to 'artificial intelligence' and give me a brief overview of each company.",
#     stream=True,
# )

# # Example 16: Financial Statement Comparison
# print("\n=== Financial Statement Comparison Example ===")
# agent.print_response(
#     """Compare the financial statements of AAPL and MSFT for the most recent fiscal year:
#     1. Revenue and revenue growth
#     2. Net income and profit margins
#     3. Cash position and debt levels
#     4. R&D spending
#     Present the comparison in a well-formatted table.""",
#     stream=True,
# )

# # Example 17: Portfolio Analysis
# print("\n=== Portfolio Analysis Example ===")
# agent.print_response(
#     """Analyze a portfolio with the following stocks and weights:
#     - AAPL (25%)
#     - MSFT (25%)
#     - GOOG (20%)
#     - AMZN (15%)
#     - TSLA (15%)
#     Calculate the portfolio's overall financial metrics and recent performance.""",
#     stream=True,
# )

# # Example 18: Dividend Analysis
# print("\n=== Dividend Analysis Example ===")
# agent.print_response(
#     "Analyze the dividend history and dividend yield for JNJ over the past 5 years.",
#     stream=True,
# )

# # Example 19: Technical Indicator Analysis
# print("\n=== Technical Indicator Analysis Example ===")
# agent.print_response(
#     "Using daily stock prices for the past 30 days, calculate and interpret the 7-day and 21-day moving averages for AAPL.",
#     stream=True,
# )

# # Example 20: Financial Report Summary
# print("\n=== Financial Report Summary Example ===")
# agent.print_response(
#     """Create a comprehensive financial summary for NVDA including:
#     1. Company overview
#     2. Latest income statement highlights
#     3. Balance sheet strength
#     4. Cash flow analysis
#     5. Key financial ratios
#     6. Recent news affecting the stock""",
#     stream=True,
# )



================================================
FILE: cookbook/tools/firecrawl_tools.py
================================================
"""
This is an example of how to use the FirecrawlTools.

Prerequisites:
- Create a Firecrawl account and get an API key
- Set the API key as an environment variable:
    export FIRECRAWL_API_KEY=<your-api-key>
"""

from agno.agent import Agent
from agno.tools.firecrawl import FirecrawlTools

agent = Agent(
    tools=[FirecrawlTools(scrape=False, crawl=True, search=True, poll_interval=2)],
    show_tool_calls=True,
    markdown=True,
)

# Should use search
agent.print_response(
    "Search for the web for the latest on 'web scraping technologies'",
    formats=["markdown", "links"],
)

# Should use crawl
agent.print_response("Summarize this https://docs.agno.com/introduction/")



================================================
FILE: cookbook/tools/giphy_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.giphy import GiphyTools

"""Create an agent specialized in creating gifs using Giphy """

gif_agent = Agent(
    name="Gif Generator Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GiphyTools(limit=5)],
    description="You are an AI agent that can generate gifs using Giphy.",
    instructions=[
        "When the user asks you to create a gif, come up with the appropriate Giphy query and use the `search_gifs` tool to find the appropriate gif.",
    ],
    show_tool_calls=True,
)

gif_agent.print_response("I want a gif to send to a friend for their birthday.")



================================================
FILE: cookbook/tools/github_tools.py
================================================
"""
GitHub Authentication Setup Guide

1. Getting Personal Access Token (PAT):
   a. Navigate to GitHub Settings:
      - Log into GitHub
      - Click profile picture (top-right)
      - Select "Settings"
      - Go to "Developer settings" → "Personal access tokens" → "Tokens (classic)"

   b. Generate New Token:
      - Click "Generate new token (classic)"
      - Add descriptive note
      - Set expiration date
      - Select scopes (minimum 'repo' access)
      - Click "Generate token"
      - IMPORTANT: Save token immediately - only shown once!

2. Setting Environment Variables:

   # For Public GitHub
   export GITHUB_ACCESS_TOKEN="your_token_here"
   export GITHUB_BASE_URL="https://api.github.com"

   # For Enterprise GitHub
   export GITHUB_BASE_URL="https://YOUR-ENTERPRISE-HOSTNAME/api/v3"
"""

from agno.agent import Agent
from agno.tools.github import GithubTools

agent = Agent(
    instructions=[
        "Use your tools to answer questions about the repo: agno-agi/agno",
        "Do not create any issues or pull requests unless explicitly asked to do so",
    ],
    tools=[GithubTools()],
    show_tool_calls=True,
)

# Basic repository listing
agent.print_response("List open pull requests", markdown=True)

# Example: Get comprehensive repository stats
# agent.print_response(
#     "Get comprehensive stats for the agno-agi/agno repository", markdown=True
# )

# Example: Get detailed pull request information
# agent.print_response(
#     "Get comprehensive details for pull request #100 in the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Working with issues
# agent.print_response(
#     "List all open issues in the agno-agi/agno repository", markdown=True
# )

# Example: Get specific issue details
# agent.print_response(
#     "Get details for issue #50 in the agno-agi/agno repository", markdown=True
# )

# Example: File operations - checking file content
# agent.print_response(
#     "Show me the content of the README.md file in the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Directory listing
# agent.print_response(
#     "List all files in the docs directory of the agno-agi/agno repository",
#     markdown=True,
# )

# Example: List branch content
# agent.print_response(
#     "Show me the files in the main branch of the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Branch operations
# agent.print_response("List all branches in the agno-agi/agno repository", markdown=True)

# Example: Search code in repository
# agent.print_response(
#     "Search for 'Agent' class definitions in the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Search issues and pull requests
# agent.print_response(
#     "Find all issues and PRs mentioning 'bug' in the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Creating a pull request (commented out by default)
# agent.print_response("Create a pull request from 'feature-branch' to 'main' in agno-agi/agno titled 'New Feature' with description 'Implements the new feature'", markdown=True)

# Example: Creating a branch (commented out by default)
# agent.print_response("Create a new branch called 'feature-branch' from the main branch in the agno-agi/agno repository", markdown=True)

# Example: Setting default branch (commented out by default)
# agent.print_response("Set the default branch to 'develop' in the agno-agi/agno repository", markdown=True)

# Example: File creation (commented out by default)
# agent.print_response("Create a file called 'test.md' with content 'This is a test' in the agno-agi/agno repository", markdown=True)

# Example: Update file (commented out by default)
# agent.print_response("Update the README.md file in the agno-agi/agno repository to add a new section about installation", markdown=True)

# Example: Delete file (commented out by default)
# agent.print_response("Delete the file test.md from the agno-agi/agno repository", markdown=True)

# Example: Requesting a review for a pull request (commented out by default)
# agent.print_response("Request a review from user 'username' for pull request #100 in the agno-agi/agno repository", markdown=True)

# # Advanced examples (commented out by default)

# # Example usage: Search for python projects on github that have more than 1000 stars
# agent.print_response("Search for python projects on github that have more than 1000 stars", markdown=True, stream=True)

# # Example usage: Search for python projects on github that have more than 1000 stars, but return the 2nd page of results
# agent.print_response("Search for python projects on github that have more than 1000 stars, but return the 2nd page of results", markdown=True, stream=True)

# # Example usage: Get pull request details
# agent.print_response("Get details of #1239", markdown=True)

# # Example usage: Get pull request changes
# agent.print_response("Show changes for #1239", markdown=True)

# # Example usage: Get pull request count
# agent.print_response("How many pull requests are there in the agno-agi/agno repository?", markdown=True)

# # Example usage: Get pull request count by author
# agent.print_response("How many pull requests has user 'username' created in the agno-agi/agno repository?", markdown=True)

# # Example usage: List open issues
# agent.print_response("What is the latest opened issue?", markdown=True)

# # Example usage: Create an issue
# agent.print_response("Explain the comments for the most recent issue", markdown=True)

# # Example usage: Create a Repo
# agent.print_response("Create a repo called agno-test and add description hello", markdown=True)

# # Example usage: Get repository stars
# agent.print_response("How many stars does the agno-agi/agno repository have?", markdown=True)

# # Example usage: Get pull requests by query parameters
# agent.print_response("Get open pull requests from the agno-agi/agno repository on the main branch sorted by creation date", markdown=True)

# # Example usage: Get pull request comments
# agent.print_response("Show me all review comments on pull request #100 in the agno-agi/agno repository", markdown=True)

# # Example usage: Create a pull request comment
# agent.print_response("Add a comment 'Nice work!' to line 10 of file.py in the latest commit of PR #100 in the agno-agi/agno repository", markdown=True)

# # Example usage: Edit a pull request comment
# agent.print_response("Update comment #1057297855 in the agno-agi/agno repository to say 'Updated: This looks good now'", markdown=True)

# # Example usage: Get repository stars
# agent.print_response("How many stars does the agno-agi/agno repository have?", markdown=True)

# # Example usage: Get pull requests by query parameters
# agent.print_response("Get open pull requests from the agno-agi/agno repository on the main branch sorted by creation date", markdown=True)

# # Example usage: Get pull request comments
# agent.print_response("Show me all review comments on pull request #100 in the agno-agi/agno repository", markdown=True)

# # Example usage: Create a pull request comment
# agent.print_response("Add a comment 'Nice work!' to line 10 of file.py in the latest commit of PR #100 in the agno-agi/agno repository", markdown=True)

# # Example usage: Edit a pull request comment
# agent.print_response("Update comment #1057297855 in the agno-agi/agno repository to say 'Updated: This looks good now'", markdown=True)



================================================
FILE: cookbook/tools/gmail_tools.py
================================================
"""
Gmail Agent that can read, draft and send emails using the Gmail.
"""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.models.openai import OpenAIChat
from agno.tools.gmail import GmailTools
from pydantic import BaseModel, Field


class FindEmailOutput(BaseModel):
    message_id: str = Field(..., description="The message id of the email")
    thread_id: str = Field(..., description="The thread id of the email")
    references: str = Field(..., description="The references of the email")
    in_reply_to: str = Field(..., description="The in-reply-to of the email")
    subject: str = Field(..., description="The subject of the email")
    body: str = Field(..., description="The body of the email")


agent = Agent(
    name="Gmail Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GmailTools()],
    description="You are an expert Gmail Agent that can read, draft and send emails using the Gmail.",
    instructions=[
        "Based on user query, you can read, draft and send emails using Gmail.",
        "While showing email contents, you can summarize the email contents, extract key details and dates.",
        "Show the email contents in a structured markdown format.",
        "Attachments can be added to the email",
    ],
    markdown=True,
    show_tool_calls=False,
    response_model=FindEmailOutput,
)

# Example 1: Find the last email from a specific sender
email = "<replace_with_email_address>"
response: FindEmailOutput = agent.run(
    f"Find the last email from {email} along with the message id, references and in-reply-to",
    markdown=True,
    stream=True,
).content


agent.print_response(
    f"""Send an email in order to reply to the last email from {email}.
    Use the thread_id {response.thread_id} and message_id {response.in_reply_to}. The subject should be 'Re: {response.subject}' and the body should be 'Hello'""",
    markdown=True,
    stream=True,
)

# Example 2: Send a new email with attachments
# agent.print_response(
#     """Send an email to user@example.com with subject 'Subject'
#     and body 'Body' and Attach the file 'tmp/attachment.pdf'""",
#     markdown=True,
#     stream=True,
# )



================================================
FILE: cookbook/tools/google_bigquery_tools.py
================================================
"""
You can set the following environment variables for your Google Cloud project:

export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_CLOUD_LOCATION="your-location"

Or you can set the following parameters in the BQTools class:

BQTools(
    project="<your-project-id>",
    location="<your-location>",
    dataset="<your-dataset>",
)

NOTE: Instruct the agent to prepend the table name with the project name and dataset name
Describe the table schemas in instructions and use thinking tools for better responses.
"""

import os

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.google_bigquery import GoogleBigQueryTools

agent = Agent(
    instructions=[
        "You are an expert Big query Writer",
        "Always prepend the table name with your_project_id.your_dataset_name when run_sql tool is invoked",
    ],
    tools=[GoogleBigQueryTools(dataset="test_dataset")],
    show_tool_calls=True,
    model=Gemini(id="gemini-2.0-flash", vertexai=True),
)

agent.print_response(
    "List the tables in the dataset. Tell me about contents of one of the tables",
    markdown=True,
)



================================================
FILE: cookbook/tools/google_maps_tools.py
================================================
"""
Business Contact Search Agent for finding and extracting business contact information.
This example demonstrates various Google Maps API functionalities including business search,
directions, geocoding, address validation, and more.

Prerequisites:
- Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
  You can obtain the API key from the Google Cloud Console:
  https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

- You also need to activate the Address Validation API for your .
  https://console.developers.google.com/apis/api/addressvalidation.googleapis.com

"""

from agno.agent import Agent
from agno.tools.crawl4ai import Crawl4aiTools
from agno.tools.google_maps import GoogleMapTools

agent = Agent(
    name="Maps API Demo Agent",
    tools=[
        GoogleMapTools(),  # For  on Google Maps
        Crawl4aiTools(max_length=5000),  # For scraping business websites
    ],
    description="You are a location and business information specialist that can help with various mapping and location-based queries.",
    instructions=[
        "When given a search query:",
        "1. Use appropriate Google Maps methods based on the query type",
        "2. For place searches, combine Maps data with website data when available",
        "3. Format responses clearly and provide relevant details based on the query",
        "4. Handle errors gracefully and provide meaningful feedback",
    ],
    markdown=True,
    show_tool_calls=True,
)

# Example 1: Business Search
print("\n=== Business Search Example ===")
agent.print_response(
    "Find me highly rated Chinese restaurants in Phoenix, AZ with their contact details",
    stream=True,
)

# Example 2: Directions
print("\n=== Directions Example ===")
agent.print_response(
    """Get driving directions from 'Phoenix Sky Harbor Airport' to 'Desert Botanical Garden',
    avoiding highways if possible""",
    stream=True,
)

# Example 3: Address Validation and Geocoding
print("\n=== Address Validation and Geocoding Example ===")
agent.print_response(
    """Please validate and geocode this address:
    '1600 Amphitheatre Parkway, Mountain View, CA'""",
    stream=True,
)

# Example 4: Distance Matrix
print("\n=== Distance Matrix Example ===")
agent.print_response(
    """Calculate the travel time and distance between these locations in Phoenix:
    Origins: ['Phoenix Sky Harbor Airport', 'Downtown Phoenix']
    Destinations: ['Desert Botanical Garden', 'Phoenix Zoo']""",
    stream=True,
)

# Example 5: Nearby Places and Details
print("\n=== Nearby Places Example ===")
agent.print_response(
    """Find coffee shops near Arizona State University Tempe campus.
    Include ratings and opening hours if available.""",
    stream=True,
)

# Example 6: Reverse Geocoding and Timezone
print("\n=== Reverse Geocoding and Timezone Example ===")
agent.print_response(
    """Get the address and timezone information for these coordinates:
    Latitude: 33.4484, Longitude: -112.0740 (Phoenix)""",
    stream=True,
)

# Example 7: Multi-step Route Planning
print("\n=== Multi-step Route Planning Example ===")
agent.print_response(
    """Plan a route with multiple stops in Phoenix:
    Start: Phoenix Sky Harbor Airport
    Stops:
    1. Arizona Science Center
    2. Heard Museum
    3. Desert Botanical Garden
    End: Return to Airport
    Please include estimated travel times between each stop.""",
    stream=True,
)

# Example 8: Location Analysis
print("\n=== Location Analysis Example ===")
agent.print_response(
    """Analyze this location in Phoenix:
    Address: '2301 N Central Ave, Phoenix, AZ 85004'
    Please provide:
    1. Exact coordinates
    2. Nearby landmarks
    3. Elevation data
    4. Local timezone""",
    stream=True,
)

# Example 9: Business Hours and Accessibility
print("\n=== Business Hours and Accessibility Example ===")
agent.print_response(
    """Find museums in Phoenix that are:
    1. Open on Mondays
    2. Have wheelchair accessibility
    3. Within 5 miles of downtown
    Include their opening hours and contact information.""",
    stream=True,
)

# Example 10: Transit Options
print("\n=== Transit Options Example ===")
agent.print_response(
    """Compare different travel modes from 'Phoenix Convention Center' to 'Phoenix Art Museum':
    1. Driving
    2. Walking
    3. Transit (if available)
    Include estimated time and distance for each option.""",
    stream=True,
)



================================================
FILE: cookbook/tools/googlecalendar_tools.py
================================================
"""
Steps to get the Google OAuth Credentials (Reference : https://developers.google.com/calendar/api/quickstart/python)

1. Enable Google Calender API
    - Go To https://console.cloud.google.com/apis/enableflow?apiid=calendar-json.googleapis.com
    - Select Project and Enable The API

2. Go To API & Service -> OAuth Consent Screen

3.Select User Type .
    - If you are Google Workspace User select Internal
    - Else Select External

4.Fill in the app details (App name, logo, support email, etc.).

5. Select Scope
    - Click on Add or Remove Scope
    - Search for Google Calender API (Make Sure you've enabled Google calender API otherwise scopes wont be visible)
    - Select Scopes Accordingly
        - From the dropdown check on /auth/calendar scope
    - Save and Continue


6. Adding Test User
    - Click Add Users and enter the email addresses of the users you want to allow during testing.
    - NOTE : Only these users can access the app's OAuth functionality when the app is in "Testing" mode.
    If anyone else tries to authenticate, they'll see an error like: "Error 403: access_denied."
    - To make the app available to all users, you'll need to move the app's status to "In Production.".
    Before doing so, ensure the app is fully verified by Google if it uses sensitive or restricted scopes.
    - Click on Go back to Dashboard


7. Generate OAuth 2.0 Client ID
    - Go To Credentials
    - Click on Create Credentials -> OAuth Client ID
    - Select Application Type as Desktop app
    - Download JSON

8. Using Google Calender Tool
    - Pass the Path of downloaded credentials as credentials_path to Google Calender tool
"""

from agno.agent import Agent
from agno.tools.googlecalendar import GoogleCalendarTools

agent = Agent(
    tools=[
        GoogleCalendarTools(
            # credentials_path="credentials.json",  # Path to your downloaded OAuth credentials
            # token_path="token.json",  # Path to your downloaded OAuth credentials
            oauth_port=8080,  # port used for oauth authentication
            allow_update=True,
        )
    ],
    show_tool_calls=True,
    instructions=[
        """
    You are a scheduling assistant.
    You should help users to perform these actions in their Google calendar:
        - get their scheduled events from a certain date and time
        - create events based on provided details
        - update existing events
        - delete events
        - find available time slots for scheduling
    """
    ],
    add_datetime_to_instructions=True,
)

# Example 1: List calendar events
agent.print_response("Give me the list of tomorrow's events", markdown=True)

# Example 2: Create an event
# agent.print_response(
#     "create an event tomorrow from 9am to 10am, make the title as 'Team Meeting' and description as 'Weekly team sync'",
#     markdown=True,
# )

# Example 3: Find available time slots
# agent.print_response(
#     "Find available 1-hour time slots for tomorrow between 9 AM and 5 PM",
#     markdown=True,
# )

# Example 4: List available calendars
# agent.print_response(
#     "List all my calendars",
#     markdown=True,
# )

# Example 5: Update an event
# agent.print_response(
#     "update the 'Team Meeting' event to run from 5pm to 7pm and change description to 'Extended team sync'",
#     markdown=True,
# )

# Example 6: Delete an event
# agent.print_response("delete the 'Team Meeting' event", markdown=True)

# # Example 7: Find available time slots for a specific calendar
# agent.print_response(
#     "Find available 1-hour time slots for this week between 9 AM and 5 PM in the Appointments calendar",
#     markdown=True,
# )

# Example 9: Find available slots using locale-based working hours
# agent.print_response(
#     "Find available 60-minute slots for the next 3 days", markdown=True
# )



================================================
FILE: cookbook/tools/googlesearch_tools.py
================================================
from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    tools=[GoogleSearchTools()],
    description="You are a news agent that helps users find the latest news.",
    instructions=[
        "Given a topic by the user, respond with 4 latest news items about that topic.",
        "Search for 10 news items and select the top 4 unique items.",
        "Search in English and in French.",
    ],
    show_tool_calls=True,
)
agent.print_response("Mistral AI", markdown=True)



================================================
FILE: cookbook/tools/googlesheets_tools.py
================================================
"""

Google Sheets Toolkit can be used to read, create, update and duplicate Google Sheets.

Example spreadsheet: https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/
The ID is the URL of the spreadsheet and the range is the sheet name and the range of cells to read.

Note: Add the complete auth URL as an Authorised redirect URIs for the Client ID in the Google Cloud Console.

e.g for Localhost and port 8080: http://localhost:8080/flowName=GeneralOAuthFlow and pass the oauth_port to the toolkit

"""

from agno.agent import Agent
from agno.tools.googlesheets import GoogleSheetsTools

SAMPLE_SPREADSHEET_ID = "1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"
SAMPLE_RANGE_NAME = "Class Data!A2:E"

google_sheets_tools = GoogleSheetsTools(
    spreadsheet_id=SAMPLE_SPREADSHEET_ID,
    spreadsheet_range=SAMPLE_RANGE_NAME,
    oauth_port=8080,  # or any other port
)

agent = Agent(
    tools=[google_sheets_tools],
    instructions=[
        "You help users interact with Google Sheets using tools that use the Google Sheets API",
        "Before asking for spreadsheet details, first attempt the operation as the user may have already configured the ID and range in the constructor",
    ],
)
agent.print_response("Please tell me about the contents of the spreadsheet")



================================================
FILE: cookbook/tools/hackernews_tools.py
================================================
from agno.agent import Agent
from agno.tools.hackernews import HackerNewsTools

agent = Agent(
    name="Hackernews Team",
    tools=[HackerNewsTools()],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response(
    "Write an engaging summary of the users with the top 2 stories on hackernews. Please mention the stories as well.",
)



================================================
FILE: cookbook/tools/jinareader_tools.py
================================================
from agno.agent import Agent
from agno.tools.jina import JinaReaderTools

agent = Agent(tools=[JinaReaderTools()], show_tool_calls=True)
agent.print_response("Summarize: https://github.com/agno-agi/agno")



================================================
FILE: cookbook/tools/jira_tools.py
================================================
from agno.agent import Agent
from agno.tools.jira import JiraTools

agent = Agent(tools=[JiraTools()])
agent.print_response("Find all issues in project PROJ", markdown=True)



================================================
FILE: cookbook/tools/linear_tools.py
================================================
from agno.agent import Agent
from agno.tools.linear import LinearTools

agent = Agent(
    name="Linear Tool Agent",
    tools=[LinearTools()],
    show_tool_calls=True,
    markdown=True,
)


user_id = "69069"
issue_id = "6969"
team_id = "73"
new_title = "updated title for issue"
new_issue_title = "title for new issue"
desc = "issue description"

agent.print_response("Get all the details of current user")
agent.print_response(f"Show the issue with the issue id: {issue_id}")
agent.print_response(
    f"Create a new issue with the title: {new_issue_title} with description: {desc} and team id: {team_id}"
)
agent.print_response(
    f"Update the issue with the issue id: {issue_id} with new title: {new_title}"
)
agent.print_response(f"Show all the issues assigned to user id: {user_id}")
agent.print_response("Show all the high priority issues")



================================================
FILE: cookbook/tools/linkup_tools.py
================================================
from agno.agent import Agent
from agno.tools.linkup import LinkupTools

agent = Agent(tools=[LinkupTools()], show_tool_calls=True)
agent.print_response("What's the latest news in French politics?", markdown=True)



================================================
FILE: cookbook/tools/lumalabs_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.lumalab import LumaLabTools

"""Create an agent specialized for Luma AI video generation"""

luma_agent = Agent(
    name="Luma Video Agent",
    agent_id="luma-video-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[LumaLabTools()],  # Using the LumaLab tool we created
    markdown=True,
    show_tool_calls=True,
    instructions=[
        "You are an agent designed to generate videos using the Luma AI API.",
        "You can generate videos in two ways:",
        "1. Text-to-Video Generation:",
        "   - Use the generate_video function for creating videos from text prompts",
        "   - Default parameters: loop=False, aspect_ratio='16:9', keyframes=None",
        "2. Image-to-Video Generation:",
        "   - Use the image_to_video function when starting from one or two images",
        "   - Required parameters: prompt, start_image_url",
        "   - Optional parameters: end_image_url, loop=False, aspect_ratio='16:9'",
        "   - The image URLs must be publicly accessible",
        "Choose the appropriate function based on whether the user provides image URLs or just a text prompt.",
        "The video will be displayed in the UI automatically below your response, so you don't need to show the video URL in your response.",
        "Politely and courteously let the user know that the video has been generated and will be displayed below as soon as its ready.",
        "After generating any video, if generation is async (wait_for_completion=False), inform about the generation ID",
    ],
    system_message=(
        "Use generate_video for text-to-video requests and image_to_video for image-based "
        "generation. Don't modify default parameters unless specifically requested. "
        "Always provide clear feedback about the video generation status."
    ),
)

luma_agent.run("Generate a video of a car in a sky")
# luma_agent.run("Transform this image into a video of a tiger walking: https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Walking_tiger_female.jpg/1920px-Walking_tiger_female.jpg")
# luma_agent.run("""
# Create a transition video between these two images:
# Start: https://img.freepik.com/premium-photo/car-driving-dark-forest-generative-ai_634053-6661.jpg?w=1380
# End: https://img.freepik.com/free-photo/front-view-black-luxury-sedan-road_114579-5030.jpg?t=st=1733821884~exp=1733825484~hmac=735ca584a9b985c53875fc1ad343c3fd394e1de4db49e5ab1a9ab37ac5f91a36&w=1380
# Make it a smooth, natural movement
# """)



================================================
FILE: cookbook/tools/mcp_tools.py
================================================
import asyncio
import sys
from pathlib import Path

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


async def main(prompt: str) -> None:
    # Initialize the MCP server
    server_params = StdioServerParameters(
        command="npx",
        args=[
            "-y",
            "@modelcontextprotocol/server-filesystem",
            str(Path(__file__).parent.parent),
        ],
    )
    # Create a client session to connect to the MCP server
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Initialize the MCP toolkit
            mcp_tools = MCPTools(session=session)
            await mcp_tools.initialize()

            # Create an agent with the MCP toolkit
            agent = Agent(tools=[mcp_tools])

            # Run the agent
            await agent.aprint_response(prompt, stream=True)


if __name__ == "__main__":
    prompt = (
        sys.argv[1] if len(sys.argv) > 1 else "Read and summarize the file ./LICENSE"
    )
    asyncio.run(main(prompt))



================================================
FILE: cookbook/tools/mem0_tools.py
================================================
"""
This example demonstrates how to use the Mem0 toolkit with Agno agents.

To get started, please export your Mem0 API key as an environment variable. You can get your Mem0 API key from https://app.mem0.ai/dashboard/api-keys

export MEM0_API_KEY=<your-mem0-api-key>
export MEM0_ORG_ID=<your-mem0-org-id> (Optional)
export MEM0_PROJECT_ID=<your-mem0-project-id> (Optional)
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mem0 import Mem0Tools

USER_ID = "jane_doe"
SESSION_ID = "agno_session"

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[Mem0Tools()],
    user_id=USER_ID,
    session_id=SESSION_ID,
    add_state_in_messages=True,
    markdown=True,
    instructions=dedent(
        """
        You have an evolving memory of this user. Proactively capture new personal details,
        preferences, plans, and relevant context the user shares, and naturally bring them up
        in later conversation. Before answering questions about past details, recall from your memory
        to provide precise and personalized responses. Keep your memory concise: store only
        meaningful information that enhances long-term dialogue. If the user asks to start fresh,
        clear all remembered information and proceed anew.
        """
    ),
    show_tool_calls=True,
)

agent.print_response("I live in NYC")
agent.print_response("I lived in San Francisco for 5 years previously")
agent.print_response("I'm going to a Taylor Swift concert tomorrow")

agent.print_response("Summarize all the details of the conversation")

# More examples:
# agent.print_response("NYC has a famous Brooklyn Bridge")
# agent.print_response("Delete all my memories")
# agent.print_response("I moved to LA")
# agent.print_response("What is the name of the concert I am going to?")



================================================
FILE: cookbook/tools/memori_tools.py
================================================
"""
This example demonstrates how to use the Memori ToolKit with Agno Agents,
for persistent memory across conversations.

Run: `pip install memorisdk` to install dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.memori import MemoriTools

# Setup the Memori ToolKit
memori_tools = MemoriTools(
    database_connect="sqlite:///memori_cookbook_memory.db",
    namespace="cookbook_agent",
)

# Setup your Agent
agent = Agent(
    # Add the Memori ToolKit to the Agent
    tools=[memori_tools],
    model=OpenAIChat(id="gpt-4o"),
    show_tool_calls=True,
    markdown=True,
    instructions=dedent(
        """\
        Instructions:
        1. First, search your memory for relevant past conversations using the memori tool
        2. Use any relevant memories to provide a personalized response
        3. Provide a helpful and contextual answer
        4. Be conversational and friendly

        If this is the first conversation, introduce yourself and explain that you'll remember our conversations.
    """
    ),
)

# Run your Agent
agent.print_response("I'm a Python developer and I love building web applications")

# Thanks to the Memori ToolKit, your Agent can now remember the conversation:
agent.print_response("What do you remember about my programming background?")

# Using the Memori ToolKit, your Agent also gains access to memory statistics:
agent.print_response("Show me your memory statistics")


# More examples:
#
# agent.print_response("I prefer working in the morning hours, around 8-11 AM")
# agent.print_response("What were my productivity preferences again?")
# agent.print_response("I just learned React and really enjoyed it!")
# agent.print_response("Search your memory for all my technology preferences")



================================================
FILE: cookbook/tools/mlx_transcribe_tools.py
================================================
"""
MLX Transcribe: A tool for transcribing audio files using MLX Whisper

Requirements:
1. ffmpeg - Install using:
   - macOS: `brew install ffmpeg`
   - Ubuntu: `sudo apt-get install ffmpeg`
   - Windows: Download from https://ffmpeg.org/download.html

2. mlx-whisper library:
   pip install mlx-whisper

Example Usage:
- Place your audio files in the 'storage/audio' directory
    Eg: download https://www.ted.com/talks/reid_hoffman_and_kevin_scott_the_evolution_of_ai_and_how_it_will_impact_human_creativity
- Run this script to transcribe audio files
- Supports various audio formats (mp3, mp4, wav, etc.)
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mlx_transcribe import MLXTranscribeTools

# Get audio files from storage/audio directory
agno_root_dir = Path(__file__).parent.parent.parent.resolve()
audio_storage_dir = agno_root_dir.joinpath("storage/audio")
if not audio_storage_dir.exists():
    audio_storage_dir.mkdir(exist_ok=True, parents=True)

agent = Agent(
    name="Transcription Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[MLXTranscribeTools(base_dir=audio_storage_dir)],
    instructions=[
        "To transcribe an audio file, use the `transcribe` tool with the name of the audio file as the argument.",
        "You can find all available audio files using the `read_files` tool.",
    ],
    markdown=True,
)

agent.print_response(
    "Summarize the reid hoffman ted talk, split into sections", stream=True
)



================================================
FILE: cookbook/tools/models_lab_tools.py
================================================
"""Run `pip install requests` to install dependencies."""

from agno.agent import Agent
from agno.tools.models_labs import ModelsLabTools

# Create an Agent with the ModelsLabs tool
agent = Agent(tools=[ModelsLabTools()], name="ModelsLabs Agent")

agent.print_response(
    "Generate a video of a beautiful sunset over the ocean", markdown=True
)



================================================
FILE: cookbook/tools/moviepy_video_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.moviepy_video import MoviePyVideoTools
from agno.tools.openai import OpenAITools

video_tools = MoviePyVideoTools(
    process_video=True, generate_captions=True, embed_captions=True
)

openai_tools = OpenAITools()

video_caption_agent = Agent(
    name="Video Caption Generator Agent",
    model=OpenAIChat(
        id="gpt-4o",
    ),
    tools=[video_tools, openai_tools],
    description="You are an AI agent that can generate and embed captions for videos.",
    instructions=[
        "When a user provides a video, process it to generate captions.",
        "Use the video processing tools in this sequence:",
        "1. Extract audio from the video using extract_audio",
        "2. Transcribe the audio using transcribe_audio",
        "3. Generate SRT captions using create_srt",
        "4. Embed captions into the video using embed_captions",
    ],
    markdown=True,
)


video_caption_agent.print_response(
    "Generate captions for {video with location} and embed them in the video"
)



================================================
FILE: cookbook/tools/multiple_tools.py
================================================
"""Run `pip install openai ddgs yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(), YFinanceTools(enable_all=True)],
    instructions=["Use tables to display data"],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response(
    "Write a thorough report on NVDA, get all financial information and latest news",
    stream=True,
)



================================================
FILE: cookbook/tools/neo4j_tools.py
================================================
"""
Example script demonstrating the use of Neo4jTools with an Agno agent.
This script sets up an agent that can interact with a Neo4j database using natural language queries,
such as listing node labels or executing Cypher queries.

## Setting up Neo4j Locally

### Option 1: Using Docker (Recommended)

1. **Install Docker** if you haven't already from https://www.docker.com/

2. **Run Neo4j in Docker:**
   ```bash
   docker run \
       --name neo4j \
       -p 7474:7474 -p 7687:7687 \
       -d \
       -v $HOME/neo4j/data:/data \
       -v $HOME/neo4j/logs:/logs \
       -v $HOME/neo4j/import:/var/lib/neo4j/import \
       -v $HOME/neo4j/plugins:/plugins \
       --env NEO4J_AUTH=neo4j/password \
       neo4j:latest
   ```

3. **Access Neo4j Browser:** Open http://localhost:7474 in your browser
   - Username: `neo4j`
   - Password: `password`

### Option 2: Native Installation

1. **Download Neo4j Desktop** from https://neo4j.com/download/
2. **Install and create a new database**
3. **Start the database** and note the connection details

### Option 3: Using Neo4j Community Edition

1. **Download** from https://neo4j.com/download-center/#community
2. **Extract and run:**
   ```bash
   tar -xf neo4j-community-*-unix.tar.gz
   cd neo4j-community-*
   ./bin/neo4j start
   ```

## Python Setup

1. **Install required packages:**
   ```bash
   pip install neo4j python-dotenv
   ```

2. **Set environment variables** (create a `.env` file in your project root):
   ```env
   NEO4J_URI=bolt://localhost:7687
   NEO4J_USERNAME=neo4j
   NEO4J_PASSWORD=password
   ```

## Usage

1. **Ensure Neo4j is running** (check http://localhost:7474)
2. **Run this script** to create an agent that can interact with your Neo4j database
3. **Test with queries** like "What are the node labels in my graph?" or "Show me the database schema"

## Troubleshooting

- **Connection refused:** Make sure Neo4j is running on the correct port (7687)
- **Authentication failed:** Verify your username/password in the Neo4j browser first
- **Import errors:** Install the neo4j driver with `pip install neo4j`
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.neo4j import Neo4jTools
from dotenv import load_dotenv

load_dotenv()

# Optionally load from environment or hardcode here
uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
user = os.getenv("NEO4J_USERNAME", "neo4j")
password = os.getenv("NEO4J_PASSWORD", "password")

# Instantiate the toolkit
neo4j_toolkit = Neo4jTools(
    uri=uri,
    user=user,
    password=password,
)

description = """You are a Neo4j expert assistant who can help with all operations in a Neo4j database by understanding natural language context and translating it into Cypher queries."""

instructions = [
    "Analyze the user's context and convert it into Cypher queries that respect the database's current schema.",
    "Before performing any operation, query the current schema (e.g., check for existing nodes or relationships).",
    "If the necessary schema elements are missing, dynamically create or extend the schema using best practices, ensuring data integrity and consistency.",
    "If properties are required or provided for nodes or relationships, ensure that they are added correctly do not overwrite existing ones and do not create duplicates and do not create extra nodes.",
    "Optionally, use or implement a dedicated function to retrieve the current schema (e.g., via a 'get_schema' function).",
    "Ensure that all operations maintain data integrity and follow best practices.",
    "Intelligently create relationships if bi-directional relationships are required, and understand the users intent and create relationships accordingly.",
    "Intelligently handle queries that involve multiple nodes and relationships, understand has to be nodes, properties, and relationships and maintain best practices.",
    "Handle errors gracefully and provide clear feedback to the user.",
]

# Example: Use with AGNO Agent
agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    tools=[neo4j_toolkit],
    markdown=True,
    description=description,
    instructions=instructions,
)

# Agent handles tool usage automatically via LLM reasoning
agent.print_response(
    "Add some nodes in my graph to represent a person with the name John Doe and a person with the name Jane Doe, and they belong to company 'X' and they are friends."
)

agent.print_response("What is the schema of my graph?")



================================================
FILE: cookbook/tools/newspaper4k_tools.py
================================================
from agno.agent import Agent
from agno.tools.newspaper4k import Newspaper4kTools

agent = Agent(tools=[Newspaper4kTools()], show_tool_calls=True)
agent.print_response(
    "Please summarize https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime"
)



================================================
FILE: cookbook/tools/newspaper_tools.py
================================================
from agno.agent import Agent
from agno.tools.newspaper import NewspaperTools

agent = Agent(tools=[NewspaperTools()])
agent.print_response("Please summarize https://en.wikipedia.org/wiki/Language_model")



================================================
FILE: cookbook/tools/openbb_tools.py
================================================
from agno.agent import Agent
from agno.tools.openbb import OpenBBTools

agent = Agent(tools=[OpenBBTools()], show_tool_calls=True)

# Example usage showing stock analysis
agent.print_response(
    "Get me the current stock price and key information for Apple (AAPL)"
)

# Example showing market analysis
agent.print_response("What are the top gainers in the market today?")

# Example showing economic indicators
agent.print_response(
    "Show me the latest GDP growth rate and inflation numbers for the US"
)



================================================
FILE: cookbook/tools/opencv_tools.py
================================================
"""
Steps to use OpenCV Tools:

1. Install OpenCV
   - Run: pip install opencv-python

2. Camera Permissions (macOS)
   - Go to System Settings > Privacy & Security > Camera
   - Enable camera access for Terminal or your IDE

3. Camera Permissions (Linux)
   - Ensure your user is in the video group: sudo usermod -a -G video $USER
   - Restart your session after adding to the group

4. Camera Permissions (Windows)
   - Go to Settings > Privacy > Camera
   - Enable "Allow apps to access your camera"

Note: Make sure your webcam is connected and not being used by other applications.
"""

from agno.agent import Agent
from agno.tools.opencv import OpenCVTools
from agno.utils.media import save_base64_data

# Example 1: Agent with live preview enabled (interactive mode)
print("Example 1: Interactive mode with live preview")
agent = Agent(
    instructions=[
        "You can capture images and videos from the webcam using OpenCV tools",
        "With live preview enabled, users can see what they're capturing in real-time",
        "For images: show preview window, press 'c' to capture, 'q' to quit",
        "For videos: show live recording with countdown timer",
    ],
    tools=[OpenCVTools(show_preview=True)],  # Enable live preview
    show_tool_calls=True,
)

agent.print_response("Take a quick test photo to verify the camera is working")

response = agent.run_response
if response.images:
    save_base64_data(response.images[0].content, "tmp/captured_test_image.png")

# Example 2: Capture a video
agent.print_response("Capture a 5 second webcam video")

response = agent.run_response
if response.videos:
    save_base64_data(response.videos[0].content, "tmp/captured_test_video.mp4")



================================================
FILE: cookbook/tools/openweather_tools.py
================================================
"""
OpenWeatherMap API Integration Example

This example demonstrates how to use the OpenWeatherTools to get weather data
from the OpenWeatherMap API.

Prerequisites:
1. Get an API key from https://openweathermap.org/api
2. Set the OPENWEATHER_API_KEY environment variable or pass it directly to the tool

Usage:
- Get current weather for a location
- Get weather forecast for a location
- Get air pollution data for a location
- Geocode a location name to coordinates
"""

from agno.agent import Agent
from agno.tools.openweather import OpenWeatherTools

# Create an agent with OpenWeatherTools
agent = Agent(
    tools=[
        OpenWeatherTools(
            units="imperial",  # Options: 'standard', 'metric', 'imperial'
        )
    ],
    # show_tool_calls=True,
    markdown=True,
)

# Example 1: Get current weather for a location
agent.print_response(
    "What's the current weather in Tokyo?",
    markdown=True,
)

# # Example 2: Get weather forecast for a location
# agent.print_response(
#     "Give me a 3-day weather forecast for New York City",
#     markdown=True,
# )

# # Example 3: Get air pollution data for a location
# agent.print_response(
#     "What's the air quality in Beijing right now?",
#     markdown=True,
# )

# # Example 4: Compare weather between multiple cities
# agent.print_response(
#     "Compare the current weather between London, Paris, and Rome",
#     markdown=True,
# )



================================================
FILE: cookbook/tools/oxylabs_tools.py
================================================
from agno.agent import Agent
from agno.tools.oxylabs import OxylabsTools

agent = Agent(
    tools=[OxylabsTools()],
    markdown=True,
    show_tool_calls=True,
)

# Example 1: Google Search
agent.print_response(
    "Let's search for 'latest iPhone reviews' and provide a summary of the top 3 results. ",
)

# Example 2: Amazon Product Search
# agent.print_response(
#     "Let's search for an Amazon product with ASIN 'B07FZ8S74R' (Echo Dot). ",
# )

# Example 3: Multi-Domain Amazon Search
# agent.print_response(
#     "Use search_amazon_products to search for 'gaming keyboards' on both:\n"
#     "1. Amazon US (domain='com')\n"
#     "2. Amazon UK (domain='co.uk')\n"
#     "Compare the top 3 results from each region including pricing and availability."
# )



================================================
FILE: cookbook/tools/pandas_tools.py
================================================
from agno.agent import Agent
from agno.tools.pandas import PandasTools

# Create an agent with PandasTools
agent = Agent(tools=[PandasTools()])

# Example: Create a dataframe with sample data and get the first 5 rows
agent.print_response("""
Please perform these tasks:
1. Create a pandas dataframe named 'sales_data' using DataFrame() with this sample data:
   {'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],
    'product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],
    'quantity': [10, 15, 8, 12, 20],
    'price': [9.99, 15.99, 9.99, 12.99, 15.99]}
2. Show me the first 5 rows of the sales_data dataframe
""")



================================================
FILE: cookbook/tools/postgres_tools.py
================================================
from agno.agent import Agent
from agno.tools.postgres import PostgresTools

# Initialize PostgresTools with connection details
postgres_tools = PostgresTools(
    host="localhost",
    port=5532,
    db_name="ai",
    user="ai",
    password="ai",
    table_schema="ai",
)

# Create an agent with the PostgresTools
agent = Agent(tools=[postgres_tools])

agent.print_response(
    "List the tables in the database and summarize one of the tables", markdown=True
)

agent.print_response("""
Please run a SQL query to get all sessions in `agent_sessions` or `team_sessions` created in the last 24 hours and summarize the table.
""")



================================================
FILE: cookbook/tools/pubmed_tools.py
================================================
from agno.agent import Agent
from agno.tools.pubmed import PubmedTools

agent = Agent(tools=[PubmedTools()], show_tool_calls=True)
agent.print_response("Tell me about ulcerative colitis.")



================================================
FILE: cookbook/tools/python_function.py
================================================
import json

import httpx
from agno.agent import Agent


def get_top_hackernews_stories(num_stories: int = 10) -> str:
    """Use this function to get top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to return. Defaults to 10.

    Returns:
        str: JSON string of top stories.
    """

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)


agent = Agent(tools=[get_top_hackernews_stories], show_tool_calls=True, markdown=True)
agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)



================================================
FILE: cookbook/tools/python_function_as_tool.py
================================================
import json

import httpx
from agno.agent import Agent


def get_top_hackernews_stories(num_stories: int = 10) -> str:
    """Use this function to get top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to return. Defaults to 10.

    Returns:
        str: JSON string of top stories.
    """

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)


agent = Agent(tools=[get_top_hackernews_stories], show_tool_calls=True, markdown=True)
agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)



================================================
FILE: cookbook/tools/python_tools.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.tools.python import PythonTools

agent = Agent(tools=[PythonTools(base_dir=Path("tmp/python"))], show_tool_calls=True)
agent.print_response(
    "Write a python script for fibonacci series and display the result till the 10th number"
)



================================================
FILE: cookbook/tools/reddit_tools.py
================================================
"""
Steps to get Reddit credentials:

1. Create/Login to Reddit account
   - Go to https://www.reddit.com

2. Create a Reddit App
   - Go to https://www.reddit.com/prefs/apps
   - Click "Create App" or "Create Another App" button
   - Fill in required details:
     * Name: Your app name
     * App type: Select "script"
     * Description: Brief description
     * About url: Your website (can be http://localhost)
     * Redirect uri: http://localhost:8080
   - Click "Create app" button

3. Get credentials
   - client_id: Found under your app name (looks like a random string)
   - client_secret: Listed as "secret"
   - user_agent: Format as: "platform:app_id:version (by /u/username)"
   - username: Your Reddit username
   - password: Your Reddit account password

"""

from agno.agent import Agent
from agno.tools.reddit import RedditTools

agent = Agent(
    instructions=[
        "Use your tools to answer questions about Reddit content and statistics",
        "Respect Reddit's content policies and NSFW restrictions",
        "When analyzing subreddits, provide relevant statistics and trends",
    ],
    tools=[RedditTools()],
    show_tool_calls=True,
)

agent.print_response("What are the top 5 posts on r/SAAS this week ?", stream=True)



================================================
FILE: cookbook/tools/replicate_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.replicate import ReplicateTools

"""Create an agent specialized for Replicate AI content generation"""

image_agent = Agent(
    name="Image Generator Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReplicateTools(model="luma/photon-flash")],
    description="You are an AI agent that can generate images using the Replicate API.",
    instructions=[
        "When the user asks you to create an image, use the `generate_media` tool to create the image.",
        "Return the URL as raw to the user.",
        "Don't convert image URL to markdown or anything else.",
    ],
    markdown=True,
    show_tool_calls=True,
)

image_agent.print_response("Generate an image of a horse in the dessert.")



================================================
FILE: cookbook/tools/resend_tools.py
================================================
from agno.agent import Agent
from agno.tools.resend import ResendTools

from_email = "<enter_from_email>"
to_email = "<enter_to_email>"

agent = Agent(tools=[ResendTools(from_email=from_email)], show_tool_calls=True)
agent.print_response(f"Send an email to {to_email} greeting them with hello world")



================================================
FILE: cookbook/tools/scrapegraph_tools.py
================================================
from agno.agent import Agent
from agno.tools.scrapegraph import ScrapeGraphTools

# Example 1: Default behavior - only smartscraper enabled
scrapegraph = ScrapeGraphTools(smartscraper=True)

agent = Agent(tools=[scrapegraph], show_tool_calls=True, markdown=True, stream=True)

# Use smartscraper
agent.print_response("""
Use smartscraper to extract the following from https://www.wired.com/category/science/:
- News articles
- Headlines
- Images
- Links
- Author
""")

# Example 2: Only markdownify enabled (by setting smartscraper=False)
scrapegraph_md = ScrapeGraphTools(smartscraper=False)

agent_md = Agent(tools=[scrapegraph_md], show_tool_calls=True, markdown=True)

# Use markdownify
agent_md.print_response(
    "Fetch and convert https://www.wired.com/category/science/ to markdown format"
)

# Example 3: Enable searchscraper
scrapegraph_search = ScrapeGraphTools(searchscraper=True)

agent_search = Agent(tools=[scrapegraph_search], show_tool_calls=True, markdown=True)

# Use searchscraper
agent_search.print_response(
    "Use searchscraper to find the CEO of company X and their contact details from https://example.com"
)

# Example 4: Enable crawl
scrapegraph_crawl = ScrapeGraphTools(crawl=True)

agent_crawl = Agent(tools=[scrapegraph_crawl], show_tool_calls=True, markdown=True)

# Use crawl (schema must be provided as a dict in the tool call)
agent_crawl.print_response(
    "Use crawl to extract what the company does and get text content from privacy and terms from https://scrapegraphai.com/ with a suitable schema."
)



================================================
FILE: cookbook/tools/searxng_tools.py
================================================
from agno.agent import Agent
from agno.tools.searxng import SearxngTools

# Initialize Searxng with your Searxng instance URL
searxng = SearxngTools(
    host="http://localhost:53153",
    engines=[],
    fixed_max_results=5,
    news=True,
    science=True,
)

# Create an agent with Searxng
agent = Agent(tools=[searxng])

# Example: Ask the agent to search using Searxng
agent.print_response("""
Please search for information about artificial intelligence 
and summarize the key points from the top results
""")



================================================
FILE: cookbook/tools/serpapi_tools.py
================================================
from agno.agent import Agent
from agno.tools.serpapi import SerpApiTools

agent = Agent(tools=[SerpApiTools()], show_tool_calls=True)
agent.print_response("Whats happening in the USA?", markdown=True)



================================================
FILE: cookbook/tools/serper_tools.py
================================================
"""
This is a example of an agent using the Serper Toolkit.

You can obtain an API key from https://serper.dev/

 - Set your API key as an environment variable: export SERPER_API_KEY="your_api_key_here"
 - or pass api_key to the SerperTools class
"""

from agno.agent import Agent
from agno.tools.serper import SerperTools

agent = Agent(
    tools=[SerperTools()],
    show_tool_calls=True,
)

agent.print_response(
    "Search for the latest news about artificial intelligence developments",
    markdown=True,
)

# Example 2: Google Scholar Search
# agent.print_response(
#     "Find 2 recent academic papers about large language model safety and alignment",
#     markdown=True,
# )

# Example 3: Web Scraping
# agent.print_response(
#     "Scrape and summarize the main content from this OpenAI blog post: https://openai.com/index/gpt-4/",
#     markdown=True
# )



================================================
FILE: cookbook/tools/shell_tools.py
================================================
from agno.agent import Agent
from agno.tools.shell import ShellTools

agent = Agent(tools=[ShellTools()], show_tool_calls=True)
agent.print_response("Show me the contents of the current directory", markdown=True)



================================================
FILE: cookbook/tools/slack_tools.py
================================================
"""Run `pip install openai slack-sdk` to install dependencies."""

from agno.agent import Agent
from agno.tools.slack import SlackTools

slack_tools = SlackTools()

agent = Agent(tools=[slack_tools], show_tool_calls=True)

# Example 1: Send a message to a Slack channel
agent.print_response(
    "Send a message 'Hello from Agno!' to the channel #bot-test", markdown=True
)

# Example 2: List all channels in the Slack workspace
agent.print_response("List all channels in our Slack workspace", markdown=True)

# Example 3: Get the message history of a specific channel
agent.print_response(
    "Get the last 10 messages from the channel #random-junk", markdown=True
)



================================================
FILE: cookbook/tools/sleep_tools.py
================================================
from agno.agent import Agent
from agno.tools.sleep import SleepTools

# Create an Agent with the Sleep tool
agent = Agent(tools=[SleepTools()], name="Sleep Agent")

# Example 1: Sleep for 2 seconds
agent.print_response("Sleep for 2 seconds")

# Example 2: Sleep for a longer duration
agent.print_response("Sleep for 5 seconds")



================================================
FILE: cookbook/tools/spider_tools.py
================================================
from agno.agent import Agent
from agno.tools.spider import SpiderTools

agent = Agent(tools=[SpiderTools(optional_params={"proxy_enabled": True})])
agent.print_response(
    'Can you scrape the first search result from a search on "news in USA"?'
)



================================================
FILE: cookbook/tools/sql_tools.py
================================================
from agno.agent import Agent
from agno.tools.sql import SQLTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(tools=[SQLTools(db_url=db_url)])
agent.print_response(
    "List the tables in the database. Tell me about contents of one of the tables",
    markdown=True,
)



================================================
FILE: cookbook/tools/tavily_tools.py
================================================
from agno.agent import Agent
from agno.tools.tavily import TavilyTools

agent = Agent(tools=[TavilyTools()], show_tool_calls=True)
agent.print_response("Search tavily for 'language models'", markdown=True)



================================================
FILE: cookbook/tools/telegram_tools.py
================================================
from agno.agent import Agent
from agno.tools.telegram import TelegramTools

# How to get the token and chat_id:
# 1. Create a new bot with BotFather on Telegram. https://core.telegram.org/bots/features#creating-a-new-bot
# 2. Get the token from BotFather.
# 3. Send a message to the bot.
# 4. Get the chat_id by going to the URL:
#    https://api.telegram.org/bot<your-bot-token>/getUpdates

telegram_token = "<enter-your-bot-token>"
chat_id = "<enter-your-chat-id>"

agent = Agent(
    name="telegram",
    tools=[TelegramTools(token=telegram_token, chat_id=chat_id)],
)

agent.print_response("Send message to telegram chat a paragraph about the moon")



================================================
FILE: cookbook/tools/todoist_tools.py
================================================
"""
Example showing how to use the Todoist Tools with Agno

Requirements:
- Sign up/login to Todoist and get a Todoist API Token (get from https://app.todoist.com/app/settings/integrations/developer)
- pip install todoist-api-python

Usage:
- Set the following environment variables:
    export TODOIST_API_TOKEN="your_api_token"

- Or provide them when creating the TodoistTools instance
"""

from agno.agent import Agent
from agno.models.google.gemini import Gemini
from agno.tools.todoist import TodoistTools

todoist_agent = Agent(
    name="Todoist Agent",
    role="Manage your todoist tasks",
    instructions=[
        "When given a task, create a todoist task for it.",
        "When given a list of tasks, create a todoist task for each one.",
        "When given a task to update, update the todoist task.",
        "When given a task to delete, delete the todoist task.",
        "When given a task to get, get the todoist task.",
    ],
    agent_id="todoist-agent",
    model=Gemini("gemini-2.0-flash-exp"),
    tools=[TodoistTools()],
    markdown=True,
    show_tool_calls=True,
)

# Example 1: Create a task
print("\n=== Create a task ===")
todoist_agent.print_response("Create a todoist task to buy groceries tomorrow at 10am")


# Example 2: Delete a task
print("\n=== Delete a task ===")
todoist_agent.print_response(
    "Delete the todoist task to buy groceries tomorrow at 10am"
)


# Example 3: Get all tasks
print("\n=== Get all tasks ===")
todoist_agent.print_response("Get all the todoist tasks")



================================================
FILE: cookbook/tools/tool_calls_accesing_agent.py
================================================
import json

import httpx
from agno.agent import Agent


def get_top_hackernews_stories(agent: Agent) -> str:
    num_stories = agent.context.get("num_stories", 5) if agent.context else 5

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)


agent = Agent(
    context={
        "num_stories": 3,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
    show_tool_calls=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)



================================================
FILE: cookbook/tools/trafilatura_tools.py
================================================
"""
TrafilaturaTools Cookbook

This cookbook demonstrates various ways to use TrafilaturaTools for web scraping and text extraction.
TrafilaturaTools provides powerful capabilities for extracting clean, readable text from web pages
and converting raw HTML into structured, meaningful data.

Prerequisites:
- Install trafilatura: pip install trafilatura
- No API keys required
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.trafilatura import TrafilaturaTools

# =============================================================================
# Example 1: Basic Text Extraction
# =============================================================================


def basic_text_extraction():
    """
    Basic text extraction from a single URL.
    Perfect for simple content extraction tasks.
    """
    print("=== Example 1: Basic Text Extraction ===")

    agent = Agent(
        tools=[TrafilaturaTools()],  # Default configuration
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Please extract and summarize the main content from https://github.com/agno-agi/agno"
    )


# =============================================================================
# Example 2: JSON Output with Metadata
# =============================================================================


def json_with_metadata():
    """
    Extract content in JSON format with metadata.
    Useful when you need structured data including titles, authors, dates, etc.
    """
    print("\n=== Example 2: JSON Output with Metadata ===")

    # Configure tool for JSON output with metadata
    agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="json",
                with_metadata=True,
                include_comments=True,
                include_tables=True,
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Extract the article content from https://en.wikipedia.org/wiki/Web_scraping in JSON format with metadata"
    )


# =============================================================================
# Example 3: Markdown Output with Formatting
# =============================================================================


def markdown_with_formatting():
    """
    Extract content in Markdown format preserving structure.
    Great for maintaining document structure and readability.
    """
    print("\n=== Example 3: Markdown with Formatting ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="markdown",
                include_formatting=True,
                include_links=True,
                with_metadata=True,
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Convert https://docs.python.org/3/tutorial/introduction.html to markdown format while preserving the structure and links"
    )


# =============================================================================
# Example 4: Metadata-Only Extraction
# =============================================================================


def metadata_only_extraction():
    """
    Extract only metadata without main content.
    Perfect for getting quick information about pages.
    """
    print("\n=== Example 4: Metadata-Only Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                include_tools=["extract_metadata_only"],
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Get the metadata (title, author, date, etc.) from https://techcrunch.com/2024/01/15/ai-news-update/"
    )


# =============================================================================
# Example 5: High Precision Extraction
# =============================================================================


def high_precision_extraction():
    """
    Extract with high precision settings.
    Use when you need clean, accurate content and don't mind missing some text.
    """
    print("\n=== Example 5: High Precision Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                favor_precision=True,
                include_comments=False,  # Skip comments for cleaner output
                include_tables=True,
                output_format="txt",
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Extract the main article content from https://www.bbc.com/news with high precision, excluding comments and ads"
    )


# =============================================================================
# Example 6: High Recall Extraction
# =============================================================================


def high_recall_extraction():
    """
    Extract with high recall settings.
    Use when you want to capture as much content as possible.
    """
    print("\n=== Example 6: High Recall Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                favor_recall=True,
                include_comments=True,
                include_tables=True,
                include_formatting=True,
                output_format="markdown",
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Extract comprehensive content from https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags including all comments and discussions"
    )


# =============================================================================
# Example 7: Language-Specific Extraction
# =============================================================================


def language_specific_extraction():
    """
    Extract content with language filtering.
    Useful for multilingual websites or language-specific content.
    """
    print("\n=== Example 7: Language-Specific Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                target_language="en",  # Filter for English content
                output_format="json",
                with_metadata=True,
                deduplicate=True,
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Extract English content from https://www.reddit.com/r/MachineLearning/ and provide a summary"
    )


# =============================================================================
# Example 8: Website Crawling (if spider available)
# =============================================================================


def website_crawling():
    """
    Crawl a website to discover and extract content from multiple pages.
    Note: Requires trafilatura spider module to be available.
    """
    print("\n=== Example 8: Website Crawling ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                include_tools=["crawl_website"],
                max_crawl_urls=5,  # Limit for demo
                output_format="json",
                with_metadata=True,
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Crawl https://example.com and extract content from up to 5 internal pages"
    )


# =============================================================================
# Example 9: HTML to Text Conversion
# =============================================================================


def html_to_text_conversion():
    """
    Convert raw HTML content to clean text.
    Useful when you already have HTML content that needs cleaning.
    """
    print("\n=== Example 9: HTML to Text Conversion ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                include_tools=["html_to_text"],
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    # Example with HTML content
    html_content = """
    <html>
    <body>
        <h1>Sample Article</h1>
        <p>This is a paragraph with <strong>bold</strong> and <em>italic</em> text.</p>
        <ul>
            <li>List item 1</li>
            <li>List item 2</li>
        </ul>
        <div class="advertisement">This is an ad</div>
    </body>
    </html>
    """

    agent.print_response(f"Convert this HTML to clean text: {html_content}")


# =============================================================================
# Example 10: Workflow Integration Example
# =============================================================================


def research_assistant_agent():
    """
    Create a specialized research assistant using TrafilaturaTools.
    This agent is optimized for extracting and analyzing research content.
    """
    research_agent = Agent(
        name="Research Assistant",
        model=OpenAIChat(id="gpt-4"),
        tools=[
            TrafilaturaTools(
                output_format="json",
                with_metadata=True,
                include_tables=True,
                include_links=True,
                favor_recall=True,
                target_language="en",
            )
        ],
        instructions="""
        You are a research assistant specialized in gathering and analyzing information from web sources.
        
        When extracting content:
        1. Always include source metadata (title, author, date, URL)
        2. Preserve important structural elements like tables and lists
        3. Maintain links for citation purposes
        4. Focus on comprehensive content extraction
        5. Provide structured analysis of the extracted content
        
        Format your responses with:
        - Executive Summary
        - Key Findings
        - Important Data/Statistics
        - Source Information
        - Recommendations for further research
        """,
        show_tool_calls=True,
        markdown=True,
    )

    research_agent.print_response("""
        Research the current state of AI in healthcare by analyzing:
        https://www.nature.com/articles/s41591-021-01614-0
        
        Provide a comprehensive analysis including key findings, 
        methodologies mentioned, and implications for future research.
    """)


# =============================================================================
# Example 11: Multiple URLs with Different Configurations
# =============================================================================


def multiple_urls_different_configs():
    """
    Process multiple URLs with different extraction strategies.
    Demonstrates flexibility in handling various content types.
    """
    print("\n=== Example 10: Multiple URLs with Different Configurations ===")

    # Different agents for different content types
    news_agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="json",
                with_metadata=True,
                include_comments=False,
                favor_precision=True,
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    documentation_agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="markdown",
                include_formatting=True,
                include_links=True,
                include_tables=True,
                favor_recall=True,
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    print("Processing news article...")
    news_agent.print_response(
        "Extract and summarize this news article: https://techcrunch.com"
    )

    print("\nProcessing documentation...")
    documentation_agent.print_response(
        "Extract the documentation content from https://docs.python.org/3/tutorial/ preserving structure"
    )


# =============================================================================
# Example 12: Advanced Customization
# =============================================================================


def advanced_customization():
    """
    Advanced configuration with all customization options.
    Shows how to fine-tune extraction for specific needs.
    """
    print("\n=== Example 11: Advanced Customization ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="xml",
                include_comments=False,
                include_tables=True,
                include_images=True,
                include_formatting=True,
                include_links=True,
                with_metadata=True,
                favor_precision=True,
                target_language="en",
                deduplicate=True,
                max_tree_size=10000,
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Extract comprehensive structured content from https://en.wikipedia.org/wiki/Artificial_intelligence in XML format with all metadata and structural elements"
    )


# =============================================================================
# Example 13: Comparative Analysis
# =============================================================================


def comparative_analysis():
    """
    Compare content from multiple sources using different extraction strategies.
    Useful for research and content analysis tasks.
    """
    print("\n=== Example 12: Comparative Analysis ===")

    agent = Agent(
        model=OpenAIChat(id="gpt-4"),
        tools=[
            TrafilaturaTools(
                output_format="json",
                with_metadata=True,
                include_tables=True,
                favor_precision=True,
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response("""
        Compare and analyze the content about artificial intelligence from these sources:
        1. https://en.wikipedia.org/wiki/Artificial_intelligence
        2. https://www.ibm.com/cloud/learn/what-is-artificial-intelligence
        
        Provide a comparative analysis highlighting the key differences in how they present AI concepts.
    """)


# =============================================================================
# Example 14: Content Research Pipeline
# =============================================================================


def content_research_pipeline():
    """
    Create a content research pipeline using TrafilaturaTools.
    Demonstrates how to use the tool for systematic content research.
    """
    print("\n=== Example 13: Content Research Pipeline ===")

    agent = Agent(
        model=OpenAIChat(id="gpt-4"),
        tools=[
            TrafilaturaTools(
                output_format="markdown",
                with_metadata=True,
                include_links=True,
                include_tables=True,
                favor_recall=True,
            )
        ],
        instructions="""
        You are a research assistant that helps gather and analyze information from web sources.
        Use TrafilaturaTools to extract content and provide comprehensive analysis.
        Always include source metadata in your analysis.
        """,
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response("""
        Research the topic of "web scraping best practices" by:
        1. Extracting content from https://blog.apify.com/web-scraping-best-practices/
        2. Analyzing the main points and recommendations
        3. Providing a summary with key takeaways
        
        Include metadata about the source and structure your response with clear sections.
    """)


# =============================================================================
# Example 15: Performance Optimized Extraction
# =============================================================================


def performance_optimized():
    """
    Optimized configuration for fast, efficient extraction.
    Best for high-volume processing or when speed is critical.
    """
    print("\n=== Example 14: Performance Optimized Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="txt",
                include_comments=False,
                include_tables=False,
                include_images=False,
                include_formatting=False,
                include_links=False,
                with_metadata=False,
                favor_precision=True,  # Faster processing
                deduplicate=False,  # Skip deduplication for speed
            )
        ],
        show_tool_calls=True,
        markdown=True,
    )

    agent.print_response(
        "Quickly extract just the main text content from https://news.ycombinator.com optimized for speed"
    )


# =============================================================================
# Main Execution
# =============================================================================

if __name__ == "__main__":
    """
    Run specific examples or all examples.
    Uncomment the examples you want to test.
    """
    print("TrafilaturaTools Cookbook - Web Scraping and Text Extraction Examples")
    print("=" * 80)

    # Basic examples
    basic_text_extraction()

    # Format-specific examples
    # json_with_metadata()
    # markdown_with_formatting()

    # Extraction strategy examples
    # high_precision_extraction()
    # high_recall_extraction()

    # Advanced examples
    # language_specific_extraction()
    # website_crawling()
    # html_to_text_conversion()
    # research_assistant_agent()

    # Complex workflows
    # multiple_urls_different_configs()
    # advanced_customization()
    # comparative_analysis()
    # content_research_pipeline()
    # performance_optimized()

    print("\n" + "=" * 80)
    print("Cookbook execution completed!")
    print("\n")



================================================
FILE: cookbook/tools/trello_tools.py
================================================
"""
Setting Up Authentication for Trello Tools
Step 1: Get Your API Key
1. Visit the Trello Power-Ups Administration Page https://trello.com/power-ups/admin
2. (Optional) Create a Workspace
3. Create a Power Up (this is required. Its like a "App" connector)
   - If you don't already have a power-ups, create one by clicking the "New" button.
   - If you have an existing Power-Up, select it from the list.
Step 2: Generate API Key and Secret
1. On the left sidebar, click on the "API Key" option.
2. Generate a new API Key:
   - Click the button to generate your API Key.
   - Copy the generated API Key and Secret. Store as TRELLO_API_KEY and TRELLO_API_SECRET.
Step 3: Generate a Token
1. On the same page where your API Key is shown, locate the option to manually generate a Token.
2. Authorize your Trello account:
   - Follow the on-screen instructions to authorize the application.
3. Copy the generated Token. Store as TRELLO_TOKEN.
"""

from agno.agent import Agent
from agno.tools.trello import TrelloTools

agent = Agent(
    instructions=[
        "You are a Trello management assistant that helps organize and manage Trello boards, lists, and cards",
        "Help users with tasks like:",
        "- Creating and organizing boards, lists, and cards",
        "- Moving cards between lists",
        "- Retrieving board and list information",
        "- Managing card details and descriptions",
        "Always confirm successful operations and provide relevant board/list/card IDs and URLs",
        "When errors occur, provide clear explanations and suggest solutions",
    ],
    tools=[TrelloTools()],
    show_tool_calls=True,
)

agent.print_response(
    "Create a board called ai-agent and inside it create list called 'todo' and 'doing' and inside each of them create card called 'create agent'",
    stream=True,
)



================================================
FILE: cookbook/tools/twilio_tools.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.twilio import TwilioTools

"""
Example showing how to use the Twilio Tools with Agno.

Requirements:
- Twilio Account SID and Auth Token (get from console.twilio.com)
- A Twilio phone number
- pip install twilio

Usage:
- Set the following environment variables:
    export TWILIO_ACCOUNT_SID="your_account_sid"
    export TWILIO_AUTH_TOKEN="your_auth_token"

- Or provide them when creating the TwilioTools instance
"""


agent = Agent(
    name="Twilio Agent",
    instructions=[
        """You can help users by:
        - Sending SMS messages
        - Checking message history
        - getting call details
        """
    ],
    model=OpenAIChat(id="gpt-4o"),
    tools=[TwilioTools()],
    show_tool_calls=True,
    markdown=True,
)

sender_phone_number = "+1234567890"
receiver_phone_number = "+1234567890"

agent.print_response(
    f"Can you send an SMS saying 'Your package has arrived' to {receiver_phone_number} from {sender_phone_number}?"
)



================================================
FILE: cookbook/tools/valyu_tools.py
================================================
"""
This cookbook demonstrates how to use the Valyu Toolkit for academic and web search.

Prerequisites:
- Install: pip install valyu
- Get API key: https://platform.valyu.network
- Set environment variable: export VALYU_API_KEY with your api key or pass the api key while initializing the toolkit
"""

from agno.agent import Agent
from agno.tools.valyu import ValyuTools

agent = Agent(
    tools=[ValyuTools()],
    show_tool_calls=True,
    markdown=True,
)

# Example 1: Basic Academic Paper Search
agent.print_response(
    "What are the latest safety mechanisms and mitigation strategies for CRISPR off-target effects?",
    markdown=True,
)

# Example 2: Focused ArXiv Search with Date Filtering
agent.print_response(
    "Search for transformer architecture papers published between June 2023 and January 2024, focusing on attention mechanisms",
    markdown=True,
)

# Example 3: Search Within Specific Paper
agent.print_response(
    "Search within the paper https://arxiv.org/abs/1706.03762 for details about the multi-head attention mechanism architecture",
    markdown=True,
)

# Example 4: Search Web
agent.print_response(
    "What are the main developments in large language model reasoning capabilities published in 2024?",
    markdown=True,
)



================================================
FILE: cookbook/tools/visualization_tools.py
================================================
"""📊 Data Visualization Tools - Create Charts and Graphs with AI Agents

This example shows how to use the VisualizationTools to create various types of charts
and graphs for data visualization. Perfect for business reports, analytics dashboards,
and data presentations.

Run: `pip install matplotlib` to install the dependencies
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.visualization import VisualizationTools

# Create an agent with visualization capabilities
viz_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[VisualizationTools(output_dir="business_charts")],
    instructions=[
        "You are a data visualization expert and business analyst.",
        "When asked to create charts, use the visualization tools available.",
        "Always provide meaningful titles, axis labels, and context.",
        "Suggest insights based on the data visualized.",
        "Format data appropriately for each chart type.",
    ],
    show_tool_calls=True,
    markdown=True,
)

# Example 1: Sales Performance Analysis
print("📊 Example 1: Creating a Sales Performance Chart")
viz_agent.print_response(
    """
Create a bar chart showing our Q4 sales performance:
- December: $45,000
- November: $38,000  
- October: $42,000
- September: $35,000

Title it "Q4 Sales Performance" and provide insights about the trend.
""",
    stream=True,
)

print("\n" + "=" * 60 + "\n")

# Example 2: Market Share Analysis
print("🥧 Example 2: Market Share Pie Chart")
viz_agent.print_response(
    """
Create a pie chart showing our market share compared to competitors:
- Our Company: 35%
- Competitor A: 25%
- Competitor B: 20%
- Competitor C: 15%
- Others: 5%

Title it "Market Share Analysis 2024" and analyze our position.
""",
    stream=True,
)

print("\n" + "=" * 60 + "\n")

# Example 3: Growth Trend Analysis
print("📈 Example 3: Revenue Growth Trend")
viz_agent.print_response(
    """
Create a line chart showing our monthly revenue growth over the past 6 months:
- January: $120,000
- February: $135,000
- March: $128,000
- April: $145,000
- May: $158,000
- June: $162,000

Title it "Monthly Revenue Growth" and identify trends and growth rate.
""",
    stream=True,
)

print("\n" + "=" * 60 + "\n")

# Example 4: Advanced Data Analysis
print("🔹 Example 4: Customer Satisfaction vs Sales Correlation")
viz_agent.print_response(
    """
Create a scatter plot to analyze the relationship between customer satisfaction scores and sales:

Customer satisfaction scores (x-axis): [7.2, 8.1, 6.9, 8.5, 7.8, 9.1, 6.5, 8.3, 7.6, 8.9, 7.1, 8.7]
Sales in thousands (y-axis): [45, 62, 38, 71, 53, 85, 32, 68, 48, 79, 41, 75]

Title it "Customer Satisfaction vs Sales Performance" and analyze the correlation.
""",
    stream=True,
)

print("\n" + "=" * 60 + "\n")

# Example 5: Distribution Analysis
print("📊 Example 5: Score Distribution Histogram")
viz_agent.print_response(
    """
Create a histogram showing the distribution of customer review scores:
Data: [4.1, 4.5, 3.8, 4.7, 4.2, 4.9, 3.9, 4.6, 4.3, 4.8, 4.0, 4.4, 3.7, 4.5, 4.1, 4.6, 4.2, 4.7, 3.9, 4.3]

Use 6 bins, title it "Customer Review Score Distribution" and analyze the distribution pattern.
""",
    stream=True,
)

print(
    "\n🎯 All examples completed! Check the 'business_charts' folder for generated visualizations."
)

# More advanced example with business context
print("\n" + "=" * 60)
print("🚀 ADVANCED EXAMPLE: Business Intelligence Dashboard")
print("=" * 60 + "\n")

bi_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[VisualizationTools(output_dir="dashboard_charts", enable_all=True)],
    instructions=[
        "You are a Business Intelligence analyst.",
        "Create comprehensive visualizations for executive dashboards.",
        "Provide actionable insights and recommendations.",
        "Use appropriate chart types for different data scenarios.",
        "Always explain what the data reveals about business performance.",
    ],
    show_tool_calls=True,
    markdown=True,
)

# Multi-chart business analysis
bi_agent.print_response(
    """
I need to create a comprehensive quarterly business review. Please help me with these visualizations:

1. First, create a bar chart showing revenue by product line:
   - Software Licenses: $2.3M
   - Support Services: $1.8M
   - Consulting: $1.2M
   - Training: $0.7M

2. Then create a line chart showing our customer acquisition over the past 12 months:
   - Jan: 45, Feb: 52, Mar: 48, Apr: 61, May: 58, Jun: 67
   - Jul: 73, Aug: 69, Sep: 78, Oct: 84, Nov: 81, Dec: 89

3. Finally, create a pie chart showing our expense breakdown:
   - Personnel: 45%
   - Technology: 25%
   - Marketing: 15%
   - Operations: 10%
   - Other: 5%

For each chart, provide business insights and recommendations for next quarter.
""",
    stream=True,
)



================================================
FILE: cookbook/tools/web_tools.py
================================================
from agno.agent import Agent
from agno.tools.webtools import WebTools

agent = Agent(tools=[WebTools()], show_tool_calls=True)
agent.print_response("Tell me about https://tinyurl.com/57bmajz4")



================================================
FILE: cookbook/tools/webbrowser_tools.py
================================================
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.webbrowser import WebBrowserTools

agent = Agent(
    model=Gemini("gemini-2.0-flash"),
    tools=[WebBrowserTools(), DuckDuckGoTools()],
    instructions=[
        "Find related websites and pages using DuckDuckGo"
        "Use web browser to open the site"
    ],
    show_tool_calls=True,
    markdown=True,
)
agent.print_response("Find an article explaining MCP and open it in the web browser.")



================================================
FILE: cookbook/tools/webex_tools.py
================================================
"""
Run `pip install openai webexpythonsdk` to install dependencies.
To get the Webex Teams Access token refer to - https://developer.webex.com/docs/bots

Steps:

1. Sign up for Webex Teams and go to the Webex [Developer Portal](https://developer.webex.com/)
2. Create the Bot
    2.1 Click in the top-right on your profile → My Webex Apps → Create a Bot.
    2.2 Enter Bot Name, Username, Icon, and Description, then click Add Bot.
3. Get the Access Token
    3.1 Copy the Access Token shown on the confirmation page (displayed once).
    3.2 If lost, regenerate it via My Webex Apps → Edit Bot → Regenerate Access Token.
4. Set the WEBEX_ACCESS_TOKEN environment variable
5. Launch Webex itself and add your bot to a space like the Welcome space. Use the bot's email address (e.g. test@webex.bot)
"""

from agno.agent import Agent
from agno.tools.webex import WebexTools

agent = Agent(tools=[WebexTools()], show_tool_calls=True)

# List all space in Webex
agent.print_response("List all space on our Webex", markdown=True)

# Send a message to a Space in Webex
agent.print_response(
    "Send a funny ice-breaking message to the webex Welcome space", markdown=True
)



================================================
FILE: cookbook/tools/website_tools.py
================================================
from agno.agent import Agent
from agno.tools.website import WebsiteTools

agent = Agent(tools=[WebsiteTools()], show_tool_calls=True)

agent.print_response(
    "Search web page: 'https://docs.agno.com/introduction'", markdown=True
)



================================================
FILE: cookbook/tools/website_tools_knowledge.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.combined import CombinedKnowledgeBase
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.knowledge.website import WebsiteKnowledgeBase
from agno.tools.website import WebsiteTools
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create PDF URL knowledge base
pdf_url_kb = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(
        table_name="pdf_documents",
        db_url=db_url,
    ),
)

# Create Website knowledge base
website_kb = WebsiteKnowledgeBase(
    urls=["https://docs.agno.com/introduction"],
    max_links=10,
    vector_db=PgVector(
        table_name="website_documents",
        db_url=db_url,
    ),
)

# Combine knowledge bases
knowledge_base = CombinedKnowledgeBase(
    sources=[
        pdf_url_kb,
        website_kb,
    ],
    vector_db=PgVector(
        table_name="combined_documents",
        db_url=db_url,
    ),
)

# Initialize the Agent with the combined knowledge base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
    show_tool_calls=True,
    tools=[
        WebsiteTools(
            knowledge_base=knowledge_base
        )  # Set combined or website knowledge base
    ],
)

knowledge_base.load(recreate=False)

# Use the agent
agent.print_response(
    "How do I get started on Mistral: https://docs.mistral.ai/getting-started/models/models_overview",
    markdown=True,
    stream=True,
)



================================================
FILE: cookbook/tools/whatsapp_tools.py
================================================
"""
WhatsApp Cookbook
----------------

This cookbook demonstrates how to use WhatsApp integration with Agno. Before running this example,
you'll need to complete these setup steps:

1. Create Meta Developer Account
   - Go to [Meta Developer Portal](https://developers.facebook.com/) and create a new account
   - Create a new app at [Meta Apps Dashboard](https://developers.facebook.com/apps/)
   - Enable WhatsApp integration for your app [here](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)

2. Set Up WhatsApp Business API
   You can get your WhatsApp Business Account ID from [Business Settings](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)

3. Configure Environment
   - Set these environment variables:
     WHATSAPP_ACCESS_TOKEN=your_access_token          # Access Token
     WHATSAPP_PHONE_NUMBER_ID=your_phone_number_id    # Phone Number ID
     WHATSAPP_RECIPIENT_WAID=your_recipient_waid      # Recipient WhatsApp ID (e.g. 1234567890)
     WHATSAPP_VERSION=your_whatsapp_version           # WhatsApp API Version (e.g. v22.0)

Important Notes:
- For first-time outreach, you must use pre-approved message templates
  [here](https://developers.facebook.com/docs/whatsapp/cloud-api/guides/send-message-templates)
- Test messages can only be sent to numbers that are registered in your test environment

The example below shows how to send a template message using Agno's WhatsApp tools.
For more complex use cases, check out the WhatsApp Cloud API documentation:
[here](https://developers.facebook.com/docs/whatsapp/cloud-api/overview)
"""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.whatsapp import WhatsAppTools

agent = Agent(
    name="whatsapp",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[WhatsAppTools()],
)

# Example: Send a template message
# Note: Replace 'hello_world' with your actual template name
agent.print_response(
    "Send a template message using the 'hello_world' template in English to +1 123456789"
)



================================================
FILE: cookbook/tools/wikipedia_tools.py
================================================
from agno.agent import Agent
from agno.tools.wikipedia import WikipediaTools

agent = Agent(tools=[WikipediaTools()], show_tool_calls=True)
agent.print_response("Search wikipedia for 'ai'")



================================================
FILE: cookbook/tools/x_tools.py
================================================
from agno.agent import Agent
from agno.tools.x import XTools

"""
To set up an X developer account and obtain the necessary keys, follow these steps:

1. **Create an X Developer Account:**
   - Go to the X Developer website: https://developer.x.com/
   - Sign in with your X account or create a new one if you don't have an account.
   - Apply for a developer account by providing the required information about your intended use of the X API.

2. **Create a Project and App:**
   - Once your developer account is approved, log in to the X Developer portal.
   - Navigate to the "Projects & Apps" section and create a new project.
   - Within the project, create a new app. This app will be used to generate the necessary API keys and tokens.
   - You'll get a client id and client secret, but you can ignore them.

3. **Generate API Keys, Tokens, and Client Credentials:**
   - After creating the app, navigate to the "Keys and tokens" tab.
   - Generate the following keys, tokens, and client credentials:
     - **API Key (Consumer Key)**
     - **API Secret Key (Consumer Secret)**
     - **Bearer Token**
     - **Access Token**
     - **Access Token Secret**

4. **Set Environment Variables:**
   - Export the generated keys, tokens, and client credentials as environment variables in your system or provide them as arguments to the `XTools` constructor.
     - `X_CONSUMER_KEY`
     - `X_CONSUMER_SECRET`
     - `X_ACCESS_TOKEN`
     - `X_ACCESS_TOKEN_SECRET`
     - `X_BEARER_TOKEN`
"""


# Initialize the x toolkit
x_tools = XTools()

# Create an agent with the X toolkit
agent = Agent(
    instructions=[
        "Use your tools to interact with X (Twitter) as the authorized user @AgnoAgi",
        "When asked to create a post, generate appropriate content based on the request",
        "Do not actually post content unless explicitly instructed to do so",
        "Provide informative responses about the user's timeline and posts",
        "Respect X's usage policies and rate limits",
    ],
    tools=[x_tools],
    show_tool_calls=True,
    debug_mode=True,
)

# Example usage: Get your details
agent.print_response(
    "Can you return my x profile with my home timeline?", markdown=True
)

# # Example usage: Get information about a user
# agent.print_response(
#     "Can you retrieve information about this user https://x.com/AgnoAgi ",
#     markdown=True,
# )

# # Example usage: Reply To a Post
# agent.print_response(
#     "Can you reply to this [post ID] post as a general message as to how great this project is: https://x.com/AgnoAgi",
#     markdown=True,
# )

# # Example usage: Send a direct message
# agent.print_response(
#     "Send direct message to the user @AgnoAgi telling them I want to learn more about them and a link to their community.",
#     markdown=True,
# )

# # Example usage: Create a new post
# agent.print_response("Create & post content about how 2025 is the year of the AI agent", markdown=True)



================================================
FILE: cookbook/tools/yfinance_tools.py
================================================
from agno.agent import Agent
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    tools=[
        YFinanceTools(
            stock_price=True, analyst_recommendations=True, stock_fundamentals=True
        )
    ],
    show_tool_calls=True,
    description="You are an investment analyst that researches stock prices, analyst recommendations, and stock fundamentals.",
    instructions=[
        "Format your response using markdown and use tables to display data where possible."
    ],
)
agent.print_response(
    "Share the NVDA stock price and analyst recommendations", markdown=True
)



================================================
FILE: cookbook/tools/youtube_tools.py
================================================
from agno.agent import Agent
from agno.tools.youtube import YouTubeTools

agent = Agent(
    tools=[YouTubeTools()],
    show_tool_calls=True,
    description="You are a YouTube agent. Obtain the captions of a YouTube video and answer questions.",
)
agent.print_response(
    "Summarize this video https://www.youtube.com/watch?v=Iv9dewmcFbs&t", markdown=True
)



================================================
FILE: cookbook/tools/zendesk_tools.py
================================================
from agno.agent import Agent
from agno.tools.zendesk import ZendeskTools

agent = Agent(tools=[ZendeskTools()], show_tool_calls=True)
agent.print_response("How do I login?", markdown=True)



================================================
FILE: cookbook/tools/zep_async_tools.py
================================================
"""
This example demonstrates how to use the ZepAsyncTools class to interact with memories stored in Zep.

To get started, please export your Zep API key as an environment variable. You can get your Zep API key from https://app.getzep.com/

export ZEP_API_KEY=<your-zep-api-key>
"""

import asyncio
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zep import ZepAsyncTools


async def main():
    # Initialize the ZepAsyncTools
    zep_tools = ZepAsyncTools(
        user_id="agno", session_id="agno-async-session", add_instructions=True
    )

    # Initialize the Agent
    agent = Agent(
        model=OpenAIChat(),
        tools=[zep_tools],
        context={
            "memory": lambda: zep_tools.get_zep_memory(memory_type="context"),
        },
        add_context=True,
    )

    # Interact with the Agent
    await agent.aprint_response("My name is John Billings")
    await agent.aprint_response("I live in NYC")
    await agent.aprint_response("I'm going to a concert tomorrow")

    # Allow the memories to sync with Zep database
    time.sleep(10)

    # Refresh the context
    agent.context["memory"] = await zep_tools.get_zep_memory(memory_type="context")

    # Ask the Agent about the user
    await agent.aprint_response("What do you know about me?")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/tools/zep_tools.py
================================================
"""
This example demonstrates how to use the ZepTools class to interact with memories stored in Zep.

To get started, please export your Zep API key as an environment variable. You can get your Zep API key from https://app.getzep.com/

export ZEP_API_KEY=<your-zep-api-key>
"""

import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zep import ZepTools

# Initialize the ZepTools
zep_tools = ZepTools(user_id="agno", session_id="agno-session", add_instructions=True)

# Initialize the Agent
agent = Agent(
    model=OpenAIChat(),
    tools=[zep_tools],
    context={"memory": zep_tools.get_zep_memory(memory_type="context")},
    add_context=True,
)

# Interact with the Agent so that it can learn about the user
agent.print_response("My name is John Billings")
agent.print_response("I live in NYC")
agent.print_response("I'm going to a concert tomorrow")

# Allow the memories to sync with Zep database
time.sleep(10)

# Refresh the context
agent.context["memory"] = zep_tools.get_zep_memory(memory_type="context")

# Ask the Agent about the user
agent.print_response("What do you know about me?")



================================================
FILE: cookbook/tools/zoom_tools.py
================================================
"""
Zoom Tools Example - Demonstrates how to use the Zoom toolkit for meeting management.

This example shows how to:
1. Set up authentication with Zoom API
2. Initialize the ZoomTools with proper credentials
3. Create an agent that can manage Zoom meetings
4. Use various Zoom API functionalities through natural language

Prerequisites:
-------------
1. Create a Server-to-Server OAuth app in Zoom Marketplace:
   - Visit https://marketplace.zoom.us/
   - Create a new app. Go to Develop -> Build App -> Server-to-Server OAuth.
   - Add required scopes:
     * meeting:write:admin
     * meeting:read:admin
     * cloud_recording:read:admin
   - Copy Account ID, Client ID, and Client Secret

2. Set environment variables:
   export ZOOM_ACCOUNT_ID=your_account_id
   export ZOOM_CLIENT_ID=your_client_id
   export ZOOM_CLIENT_SECRET=your_client_secret

Features:
---------
- Schedule new meetings
- Get meeting details
- List all meetings
- Get upcoming meetings
- Delete meetings
- Get meeting recordings

Usage:
------
Run this script with proper environment variables set to interact with
the Zoom API through natural language commands.
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zoom import ZoomTools

# Get environment variables
ACCOUNT_ID = os.getenv("ZOOM_ACCOUNT_ID")
CLIENT_ID = os.getenv("ZOOM_CLIENT_ID")
CLIENT_SECRET = os.getenv("ZOOM_CLIENT_SECRET")

# Initialize Zoom tools with credentials
zoom_tools = ZoomTools(
    account_id=ACCOUNT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET
)

# Create an agent with Zoom capabilities
agent = Agent(
    name="Zoom Meeting Manager",
    agent_id="zoom-meeting-manager",
    model=OpenAIChat(id="gpt-4"),
    tools=[zoom_tools],
    markdown=True,
    show_tool_calls=True,
    instructions=[
        "You are an expert at managing Zoom meetings using the Zoom API.",
        "You can:",
        "1. Schedule new meetings (schedule_meeting)",
        "2. Get meeting details (get_meeting)",
        "3. List all meetings (list_meetings)",
        "4. Get upcoming meetings (get_upcoming_meetings)",
        "5. Delete meetings (delete_meeting)",
        "6. Get meeting recordings (get_meeting_recordings)",
        "",
        "For recordings, you can:",
        "- Retrieve recordings for any past meeting using the meeting ID",
        "- Include download tokens if needed",
        "- Get recording details like duration, size, download link and file types",
        "",
        "Guidelines:",
        "- Use ISO 8601 format for dates (e.g., '2024-12-28T10:00:00Z')",
        "- Accept and use user's timezone (e.g., 'America/New_York', 'Asia/Tokyo', 'UTC')",
        "- If no timezone is specified, default to UTC",
        "- Ensure meeting times are in the future",
        "- Provide meeting details after scheduling (ID, URL, time)",
        "- Handle errors gracefully",
        "- Confirm successful operations",
    ],
)

# Example usage - uncomment the ones you want to try
agent.print_response(
    "Schedule a meeting titled 'Team Sync' for tomorrow at 2 PM UTC for 45 minutes"
)

# More examples (uncomment to use):
# agent.print_response("What meetings do I have coming up?")
# agent.print_response("List all my scheduled meetings")
# agent.print_response("Get details for my most recent meeting")
# agent.print_response("Get the recordings for my last team meeting")
# agent.print_response("Delete the meeting titled 'Team Sync'")
# agent.print_response("Schedule daily standup meetings for next week at 10 AM UTC")



================================================
FILE: cookbook/tools/async/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/tools/async/groq-demo.py
================================================
import asyncio
import time

from agno.agent import Agent
from agno.models.groq import Groq
from agno.utils.log import logger

#####################################
# Async execution
#####################################


async def atask1(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 1 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 1 has slept for 1s")
    logger.info("Task 1 has completed")
    return f"Task 1 completed in {delay:.2f}s"


async def atask2(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 2 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 2 has slept for 1s")
    logger.info("Task 2 has completed")
    return f"Task 2 completed in {delay:.2f}s"


async def atask3(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 3 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 3 has slept for 1s")
    logger.info("Task 3 has completed")
    return f"Task 3 completed in {delay:.2f}s"


async_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[atask2, atask1, atask3],
    show_tool_calls=True,
    markdown=True,
)

# Non-streaming response
# asyncio.run(async_agent.aprint_response("Please run all tasks with a delay of 3s"))
# Streaming response
asyncio.run(
    async_agent.aprint_response("Please run all tasks with a delay of 3s", stream=True)
)


#####################################
# Sync execution
#####################################
def task1(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 1 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 1 has slept for 1s")
    logger.info("Task 1 has completed")
    return f"Task 1 completed in {delay:.2f}s"


def task2(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 2 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 2 has slept for 1s")
    logger.info("Task 2 has completed")
    return f"Task 2 completed in {delay:.2f}s"


def task3(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 3 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 3 has slept for 1s")
    logger.info("Task 3 has completed")
    return f"Task 3 completed in {delay:.2f}s"


sync_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[task2, task1, task3],
    show_tool_calls=True,
    markdown=True,
)

# Non-streaming response
# sync_agent.print_response("Please run all tasks with a delay of 3s")
# Streaming response
sync_agent.print_response("Please run all tasks with a delay of 3s", stream=True)



================================================
FILE: cookbook/tools/async/openai-demo.py
================================================
import asyncio
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.log import logger

#####################################
# Async execution
#####################################


async def atask1(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 1 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 1 has slept for 1s")
    logger.info("Task 1 has completed")
    return f"Task 1 completed in {delay:.2f}s"


async def atask2(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 2 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 2 has slept for 1s")
    logger.info("Task 2 has completed")
    return f"Task 2 completed in {delay:.2f}s"


async def atask3(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 3 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 3 has slept for 1s")
    logger.info("Task 3 has completed")
    return f"Task 3 completed in {delay:.2f}s"


async_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[atask2, atask1, atask3],
    show_tool_calls=True,
    markdown=True,
)

# Non-streaming response
# asyncio.run(async_agent.aprint_response("Please run all tasks with a delay of 3s"))
# Streaming response
asyncio.run(
    async_agent.aprint_response("Please run all tasks with a delay of 3s", stream=True)
)


#####################################
# Sync execution
#####################################
def task1(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 1 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 1 has slept for 1s")
    logger.info("Task 1 has completed")
    return f"Task 1 completed in {delay:.2f}s"


def task2(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 2 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 2 has slept for 1s")
    logger.info("Task 2 has completed")
    return f"Task 2 completed in {delay:.2f}s"


def task3(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 3 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 3 has slept for 1s")
    logger.info("Task 3 has completed")
    return f"Task 3 completed in {delay:.2f}s"


sync_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[task2, task1, task3],
    show_tool_calls=True,
    markdown=True,
)

# Non-streaming response
# sync_agent.print_response("Please run all tasks with a delay of 3s")
# Streaming response
sync_agent.print_response("Please run all tasks with a delay of 3s", stream=True)



================================================
FILE: cookbook/tools/mcp/README.md
================================================
# MCP Agents using Agno

Model Context Protocol (MCP) gives Agents the ability to interact with external systems through a standardized interface. Using Agno's MCP integration, you can build Agents that can connect to any MCP-compatible service.

## Examples in this Directory

1. Filesystem Agent (`filesystem.py`)

This example demonstrates how to create an agent that can explore, analyze, and provide insights about files and directories on your computer.

2. GitHub Agent (`github.py`)

This example shows how to create an agent that can explore GitHub repositories, analyze issues, pull requests, and more.

3. Groq with Llama using MCP (`groq_mcp.py`)

This example uses the file system MCP agent with Groq running the Llama 3.3-70b-versatile model.

4. Include/Exclude Tools (`include_exclude_tools.py`)

This example shows how to include and exclude tools from the MCP agent. This is useful for reducing the number of tools available to the agent, or for focusing on a specific set of tools.

5. Multiple MCP Servers (`multiple_servers.py`)

This example shows how to use multiple MCP servers in the same agent. 

6. Sequential Thinking (`sequential_thinking.py`)

This example shows how to use the MCP agent to perform sequential thinking.

7. Airbnb Agent (`airbnb.py`)

This example shows how to create an agent that uses MCP and Gemini 2.5 Pro to search for Airbnb listings.


## Getting Started

### Prerequisites

Install the required dependencies:

```bash
pip install agno mcp openai
```

Export your API keys:

```bash
export OPENAI_API_KEY="your_openai_api_key"
```

> For the GitHub example, create a Github PAT following [these steps](https://github.com/modelcontextprotocol/servers/tree/main/src/github#setup).

### Run the Examples

```bash
python filesystem.py
python github.py
```

## How It Works

These examples use Agno to create agents that leverage MCP servers. The MCP servers provide standardized access to different data sources (filesystem, GitHub), and the agents use these servers to answer questions and perform tasks.

The workflow is:
1. Agent receives a query from the user
2. Agent determines which MCP tools to use
3. Agent calls the appropriate MCP server to get information
4. Agent processes the information and provides a response

## Customizing

You can modify these examples to:
- Connect to different MCP servers
- Change the agent's instructions
- Add additional tools
- Customize the agent's behavior

## More Information

- Read more about [MCP](https://modelcontextprotocol.io/introduction)
- Read about [Agno's MCP integration](https://docs.agno.com/tools/mcp)



================================================
FILE: cookbook/tools/mcp/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/tools/mcp/airbnb.py
================================================
"""🏠 MCP Airbnb Agent - Search for Airbnb listings!

This example shows how to create an agent that uses MCP and Gemini 2.5 Pro to search for Airbnb listings.

Run: `pip install google-genai mcp agno` to install the dependencies
"""

import asyncio

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.tools.mcp import MCPTools


async def run_mcp_agent(message: str):
    # Initialize the MCP tools
    mcp_tools = MCPTools("npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt")

    # Connect to the MCP server
    await mcp_tools.connect()

    # Use the MCP tools with an Agent
    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[mcp_tools],
        markdown=True,
    )
    await agent.aprint_response(message)

    # Close the MCP connection
    await mcp_tools.close()


if __name__ == "__main__":
    asyncio.run(run_mcp_agent("Show me listings in Barcelona, for 2 people."))



================================================
FILE: cookbook/tools/mcp/brave.py
================================================
"""MCP Brave Agent - Search for Brave

This example shows how to create an agent that uses Anthropic to search for information using the Brave MCP server.

You can get the Brave API key from https://brave.com/search/api/

Run: `pip install anthropic mcp agno` to install the dependencies
"""

import asyncio
from os import getenv

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.mcp import MCPTools
from agno.utils.pprint import apprint_run_response


async def run_agent(message: str) -> None:
    async with MCPTools(
        "npx -y @modelcontextprotocol/server-brave-search",
        env={
            "BRAVE_API_KEY": getenv("BRAVE_API_KEY"),
        },
    ) as mcp_tools:
        agent = Agent(
            model=Claude(id="claude-sonnet-4-20250514"),
            tools=[mcp_tools],
            markdown=True,
        )

        response_stream = await agent.arun(message)
        await apprint_run_response(response_stream)


if __name__ == "__main__":
    asyncio.run(run_agent("What is the weather in Tokyo?"))



================================================
FILE: cookbook/tools/mcp/cli.py
================================================
"""Show how to run an interactive CLI to interact with an agent equipped with MCP tools.

This example uses the MCP GitHub Agent. Example prompts to try:
- "List open issues in the repository"
- "Show me recent pull requests"
- "What are the repository statistics?"
- "Find issues labeled as bugs"
- "Show me contributor activity"

Run: `pip install agno mcp openai` to install the dependencies
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    """Run an interactive CLI for the GitHub agent with the given message."""

    # Create a client session to connect to the MCP server
    async with MCPTools("npx -y @modelcontextprotocol/server-github") as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a GitHub assistant. Help users explore repositories and their activity.

                - Use headings to organize your responses
                - Be concise and focus on relevant information\
            """),
            markdown=True,
            show_tool_calls=True,
        )

        # Run an interactive command-line interface to interact with the agent.
        await agent.acli_app(message=message, stream=True)


if __name__ == "__main__":
    # Pull request example
    asyncio.run(
        run_agent(
            "Tell me about Agno. Github repo: https://github.com/agno-agi/agno. You can read the README for more information."
        )
    )



================================================
FILE: cookbook/tools/mcp/filesystem.py
================================================
"""📁 MCP Filesystem Agent - Your Personal File Explorer!

This example shows how to create a filesystem agent that uses MCP to explore,
analyze, and provide insights about files and directories. The agent leverages the Model
Context Protocol (MCP) to interact with the filesystem, allowing it to answer questions
about file contents, directory structures, and more.

Example prompts to try:
- "What files are in the current directory?"
- "Show me the content of README.md"
- "What is the license for this project?"
- "Find all Python files in the project"
- "Summarize the main functionality of the codebase"

Run: `pip install agno mcp openai` to install the dependencies
"""

import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    """Run the filesystem agent with the given message."""
    # Initialize the MCP server
    file_path = str(Path(__file__).parent.parent.parent.parent)

    # Create a client session to connect to the MCP server
    async with MCPTools(
        f"npx -y @modelcontextprotocol/server-filesystem {file_path}"
    ) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a filesystem assistant. Help users explore files and directories.

                - Navigate the filesystem to answer questions
                - Use the list_allowed_directories tool to find directories that you can access
                - Provide clear context about files you examine
                - Use headings to organize your responses
                - Be concise and focus on relevant information\
            """),
            markdown=True,
            show_tool_calls=True,
        )

        # Run the agent
        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    # Basic example - exploring project license
    asyncio.run(run_agent("What is the license for this project?"))

    # File content example
    asyncio.run(
        run_agent("Show me the content of README.md and explain what this project does")
    )


# More example prompts to explore:
"""
File exploration queries:
1. "What are the main Python packages used in this project?"
2. "Show me all configuration files and explain their purpose"
3. "Find all test files and summarize what they're testing"
4. "What's the project's entry point and how does it work?"
5. "Analyze the project's dependency structure"

Code analysis queries:
1. "Explain the architecture of this codebase"
2. "What design patterns are used in this project?"
3. "Find potential security issues in the codebase"
4. "How is error handling implemented across the project?"
5. "Analyze the API endpoints in this project"

Documentation queries:
1. "Generate a summary of the project documentation"
2. "What features are documented but not implemented?"
3. "Are there any TODOs or FIXMEs in the codebase?"
4. "Create a high-level overview of the project's functionality"
5. "What's missing from the documentation?"
"""



================================================
FILE: cookbook/tools/mcp/gibsonai.py
================================================
"""🛢 GibsonAI MCP Server - Create and manage databases with prompts

This example shows how to connect a local GibsonAI MCP to Agno agent.
You can instantly generate, modify database schemas
and chat with your relational database using natural language.
From prompt to a serverless database (MySQL, PostgresQL, etc.), auto-generated REST APIs for your data.

Example prompts to try:
- "Create a new GibsonAI project for my e-commerce app"
- "Show me the current schema for my project"
- "Add a 'products' table with name, price, and description fields"
- "Create a 'users' table with authentication fields"
- "Deploy my schema changes to production"

How to setup and run:

1. Install [UV](https://docs.astral.sh/uv/) package manager.
2. Install the GibsonAI CLI:
    ```bash
    uvx --from gibson-cli@latest gibson auth login
    ```
3. Install the required dependencies:
    ```bash
    pip install agno mcp openai
    ```
4. Export your API key:
    ```bash
    export OPENAI_API_KEY="your_openai_api_key"
    ```
5. Run the GibsonAI agent by running this file.
6. Check created database and schema on GibsonAI dashboard: https://app.gibsonai.com

This logs you into the [GibsonAI CLI](https://docs.gibsonai.com/reference/cli-quickstart)
so you can access all the features directly from your agent.

"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools


async def run_gibsonai_agent(message: str):
    """Run the GibsonAI agent with the given message."""
    mcp_tools = MCPTools(
        "uvx --from gibson-cli@latest gibson mcp run",
        timeout_seconds=300,  # Extended timeout for GibsonAI operations
    )

    # Connect to the MCP server
    await mcp_tools.connect()

    agent = Agent(
        name="GibsonAIAgent",
        model=OpenAIChat(id="gpt-4o"),
        tools=[mcp_tools],
        description="Agent for managing database projects and schemas",
        instructions=dedent("""\
            You are a GibsonAI database assistant. Help users manage their database projects and schemas.

            Your capabilities include:
            - Creating new GibsonAI projects
            - Managing database schemas (tables, columns, relationships)
            - Deploying schema changes to hosted databases
            - Querying database schemas and data
            - Providing insights about database structure and best practices
        """),
        markdown=True,
        show_tool_calls=True,
    )

    # Run the agent
    await agent.aprint_response(message, stream=True)

    # Close the MCP connection
    await mcp_tools.close()


# Example usage
if __name__ == "__main__":
    asyncio.run(
        run_gibsonai_agent(
            """
            Create a database for blog posts platform with users and posts tables.
            You can decide the schema of the tables without double checking with me.
            """
        )
    )



================================================
FILE: cookbook/tools/mcp/github.py
================================================
"""🐙 MCP GitHub Agent - Your Personal GitHub Explorer!

This example shows how to create a GitHub agent that uses MCP to explore,
analyze, and provide insights about GitHub repositories. The agent leverages the Model
Context Protocol (MCP) to interact with GitHub, allowing it to answer questions
about issues, pull requests, repository details and more.

Example prompts to try:
- "List open issues in the repository"
- "Show me recent pull requests"
- "What are the repository statistics?"
- "Find issues labeled as bugs"
- "Show me contributor activity"

Run: `pip install agno mcp openai` to install the dependencies
Environment variables needed:
- Create a GitHub personal access token following these steps:
    - https://github.com/modelcontextprotocol/servers/tree/main/src/github#setup
- export GITHUB_TOKEN: Your GitHub personal access token
"""

import asyncio
import os
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters


async def run_agent(message: str) -> None:
    """Run the GitHub agent with the given message."""

    # Initialize the MCP server
    server_params = StdioServerParameters(
        command="npx",
        args=["-y", "@modelcontextprotocol/server-github"],
    )

    # Create a client session to connect to the MCP server
    async with MCPTools(server_params=server_params) as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a GitHub assistant. Help users explore repositories and their activity.

                - Use headings to organize your responses
                - Be concise and focus on relevant information\
            """),
            markdown=True,
            show_tool_calls=True,
        )

        # Run the agent
        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    # Pull request example
    asyncio.run(
        run_agent(
            "Tell me about Agno. Github repo: https://github.com/agno-agi/agno. You can read the README for more information."
        )
    )


# More example prompts to explore:
"""
Issue queries:
1. "Find issues needing attention"
2. "Show me issues by label"
3. "What issues are being actively discussed?"
4. "Find related issues"
5. "Analyze issue resolution patterns"

Pull request queries:
1. "What PRs need review?"
2. "Show me recent merged PRs"
3. "Find PRs with conflicts"
4. "What features are being developed?"
5. "Analyze PR review patterns"

Repository queries:
1. "Show repository health metrics"
2. "What are the contribution guidelines?"
3. "Find documentation gaps"
4. "Analyze code quality trends"
5. "Show repository activity patterns"
"""



================================================
FILE: cookbook/tools/mcp/graphiti.py
================================================
"""
📔 MCP Graphiti Agent - A personal diary assistant

This example demonstrates how to use Agno's MCP integration together with Graphiti, to build a personal diary assistant.

- Run your Graphiti MCP server. Full instructions: https://github.com/getzep/graphiti/tree/main/mcp_server
- Run: `pip install agno mcp openai` to install the dependencies
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

mcp_server_url = "http://localhost:8000/sse"


async def run_agent(message: str) -> None:
    async with MCPTools(url=mcp_server_url, transport="sse") as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            model=OpenAIChat(id="o3-mini"),
            instructions=dedent(
                """
                You are an assistant with access to tools related to Graphiti's knowledge graph capabilities.
                You maintain a diary for the user.
                Your job is to help them add new entries and use the diary data to answer their questions.
                """
            ),
        )
        await agent.aprint_response(message, stream=True)


if __name__ == "__main__":
    asyncio.run(
        # Using the agent to add new entries to the diary
        run_agent(
            "Add the following entry to the diary: 'Today I spent some time building agents with Agno'"
        )
    )

    asyncio.run(
        # Using the agent to answer questions about the diary
        run_agent("What have I been building recently?")
    )



================================================
FILE: cookbook/tools/mcp/groq_mcp.py
================================================
"""📁 Groq + MCP = Lightning Fast Agents

This example demonstrates how to create a high-performance filesystem agent by combining
Groq's fast LLM inference with the Model Context Protocol (MCP). This combination delivers
exceptional speed while maintaining powerful filesystem exploration capabilities.

Example prompts to try:
- "What files are in the current directory?"
- "Show me the content of README.md"
- "What is the license for this project?"
- "Find all Python files in the project"
- "Analyze the performance benefits of using Groq with MCP"

Run: `pip install agno mcp openai` to install the dependencies
"""

import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.mcp import MCPTools
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


async def create_filesystem_agent(session):
    """Create and configure a high-performance filesystem agent with Groq and MCP."""
    # Initialize the MCP toolkit
    mcp_tools = MCPTools(session=session)
    await mcp_tools.initialize()

    # Create an agent with the MCP toolkit and Groq's fast LLM
    return Agent(
        model=Groq(id="llama-3.3-70b-versatile"),
        tools=[mcp_tools],
        instructions=dedent("""\
            You are a high-performance filesystem assistant powered by Groq and MCP.
            Your combination of Groq's fast inference and MCP's efficient context handling
            makes you exceptionally quick at exploring and analyzing files.

            - Navigate the filesystem with lightning speed to answer questions
            - Use the list_allowed_directories tool to find directories that you can access
            - Highlight the performance benefits of the Groq+MCP combination when relevant
            - Provide clear context about files you examine
            - Use headings to organize your responses
            - Be concise and focus on relevant information\
        """),
        markdown=True,
        show_tool_calls=True,
    )


async def run_agent(message: str) -> None:
    """Run the filesystem agent with the given message."""
    # Initialize the MCP server
    server_params = StdioServerParameters(
        command="npx",
        args=[
            "-y",
            "@modelcontextprotocol/server-filesystem",
            str(Path(__file__).parent.parent.parent.parent),
        ],
    )

    # Create a client session to connect to the MCP server
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            agent = await create_filesystem_agent(session)

            # Run the agent
            await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    # Basic example - exploring project license
    asyncio.run(run_agent("What is the license for this project?"))

    # Performance demonstration example
    asyncio.run(
        run_agent(
            "Show me the README.md and explain how Groq with MCP enables fast file analysis"
        )
    )


# More example prompts to explore:
"""
Performance-focused queries:
1. "Analyze a large Python file and explain how Groq+MCP makes this fast"
2. "Compare the directory structure and explain how MCP efficiently provides this information"
3. "Find all TODO comments in the codebase and demonstrate the speed advantage"
4. "Process multiple configuration files simultaneously and explain the performance benefits"
5. "Explain how the Groq+MCP combination optimizes context handling for large codebases"

File exploration queries:
1. "What are the main Python packages used in this project?"
2. "Show me all configuration files and explain their purpose"
3. "Find all test files and summarize what they're testing"
4. "What's the project's entry point and how does it work?"
5. "Analyze the project's dependency structure"

Code analysis queries:
1. "Explain the architecture of this codebase"
2. "What design patterns are used in this project?"
3. "Find potential security issues in the codebase"
4. "How is error handling implemented across the project?"
5. "Analyze the API endpoints in this project"
"""



================================================
FILE: cookbook/tools/mcp/include_exclude_tools.py
================================================
"""
This example demonstrates how to use multiple MCP servers in a single agent.

Prerequisites:
- Google Maps:
    - Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    You can obtain the API key from the Google Cloud Console:
    https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

    - You also need to activate the Address Validation API for your .
    https://console.developers.google.com/apis/api/addressvalidation.googleapis.com
"""

import asyncio

from agno.agent import Agent
from agno.tools.mcp import MultiMCPTools


async def run_agent(message: str) -> None:
    """Run the GitHub agent with the given message.

    Remember to set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    """

    # Initialize the MCP server
    async with MultiMCPTools(
        [
            "npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt",
            "npx -y @modelcontextprotocol/server-google-maps",
        ],
        include_tools=["airbnb_search"],
        exclude_tools=["maps_place_details"],
    ) as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            markdown=True,
            show_tool_calls=True,
        )

        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    asyncio.run(
        run_agent(
            "What listings are available in Cape Town for 2 people for 3 nights from 1 to 4 August 2025?"
        )
    )

    asyncio.run(run_agent("What restaurants are open right now in Cape Town?"))



================================================
FILE: cookbook/tools/mcp/include_tools.py
================================================
import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    file_path = str(Path(__file__).parents[3] / "libs/agno")

    # Initialize the MCP server
    async with (
        MCPTools(
            f"npx -y @modelcontextprotocol/server-filesystem {file_path}",
            include_tools=[
                "list_allowed_directories",
                "list_directory",
                "read_file",
            ],
        ) as fs_tools,
    ):
        agent = Agent(
            model=Groq(id="llama-3.3-70b-versatile"),
            tools=[fs_tools],
            instructions=dedent("""\
                - First, ALWAYS use the list_allowed_directories tool to find directories that you can access
                - Use the list_directory tool to list the contents of a directory
                - Use the read_file tool to read the contents of a file
                - Be concise and focus on relevant information\
            """),
            show_tool_calls=True,
            markdown=True,
        )
        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    asyncio.run(run_agent("What is the license for this project?"))



================================================
FILE: cookbook/tools/mcp/mem0.py
================================================
"""
👩‍💻 Mem0 MCP - Personalized Code Reviewer

This example demonstrates how to use Agno's MCP integration together with Mem0, to build a personalized code reviewer.

- Run your Mem0 MCP server. Full instructions: https://github.com/mem0ai/mem0-mcp
- Run: `pip install agno mcp` to install the dependencies
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

mcp_server_url = "http://localhost:8080/sse"


async def run_agent(message: str) -> None:
    async with MCPTools(url=mcp_server_url, transport="sse") as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            model=OpenAIChat(id="o4-mini"),
            instructions=dedent(
                """
                You are a professional code reviewer. You help users keep their code clean and on line with their preferences.
                You have access to some tools to keep track of coding preferences you need to enforce when reviewing code.
                You will be given a code snippet and you need to review it and provide feedback on it.
                """
            ),
        )
        await agent.aprint_response(message, stream=True)


if __name__ == "__main__":
    # The agent will use mem0 memory to keep track of the user's preferences.
    asyncio.run(
        run_agent(
            "When possible, use the walrus operator to make the code more readable."
        )
    )
    # The agent will review your code and propose improvements based on your preferences.
    asyncio.run(
        run_agent(
            dedent(
                """
Please, review this Python snippet:

```python
def process_data(data):
    length = len(data)
    if length > 10:
        print(f"Processing {length} items")
        return data[:10]
    return data

# Example usage
items = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
result = process_data(items)
```
"""
            )
        )
    )



================================================
FILE: cookbook/tools/mcp/multiple_servers.py
================================================
"""
This example demonstrates how to use multiple MCP servers in a single agent.

Prerequisites:
- Set the environment variable "ACCUWEATHER_API_KEY" for the weather MCP tools.
- You can get the API key from the AccuWeather website: https://developer.accuweather.com/
"""

import asyncio
from os import getenv

from agno.agent import Agent
from agno.tools.mcp import MultiMCPTools


async def run_agent(message: str) -> None:
    # Initialize the MCP tools
    mcp_tools = MultiMCPTools(
        [
            "npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt",
            "npx -y @modelcontextprotocol/server-brave-search",
        ],
        env={
            "BRAVE_API_KEY": getenv("BRAVE_API_KEY"),
        },
        timeout_seconds=30,
    )

    # Connect to the MCP servers
    await mcp_tools.connect()

    # Use the MCP tools with an Agent
    agent = Agent(
        tools=[mcp_tools],
        markdown=True,
        show_tool_calls=True,
    )
    await agent.aprint_response(message)

    # Close the MCP connection
    await mcp_tools.close()


# Example usage
if __name__ == "__main__":
    asyncio.run(run_agent("What listings are available in Barcelona tonight?"))
    asyncio.run(run_agent("What's the fastest way to get to Barcelona from London?"))



================================================
FILE: cookbook/tools/mcp/notion_mcp_agent.py
================================================
"""
Notion MCP Agent - Manages your documents

This example shows how to use the Agno MCP tools to interact with your Notion workspace.

1. Start by setting up a new internal integration in Notion: https://www.notion.so/profile/integrations
2. Export your new Notion key: `export NOTION_API_KEY=ntn_****`
3. Connect your relevant Notion pages to the integration. To do this, you'll need to visit that page, and click on the 3 dots, and select "Connect to integration".

Dependencies: pip install agno mcp openai

Usage:
  python cookbook/tools/mcp/notion_mcp_agent.py
"""

import asyncio
import json
import os
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters


async def run_agent():
    token = os.getenv("NOTION_API_KEY")
    if not token:
        raise ValueError(
            "Missing Notion API key: provide --NOTION_API_KEY or set NOTION_API_KEY environment variable"
        )

    command = "npx"
    args = ["-y", "@notionhq/notion-mcp-server"]
    env = {
        "OPENAPI_MCP_HEADERS": json.dumps(
            {"Authorization": f"Bearer {token}", "Notion-Version": "2022-06-28"}
        )
    }
    server_params = StdioServerParameters(command=command, args=args, env=env)

    async with MCPTools(server_params=server_params) as mcp_tools:
        agent = Agent(
            name="NotionDocsAgent",
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            description="Agent to query and modify Notion docs via MCP",
            instructions=dedent("""\
                You have access to Notion documents through MCP tools.
                - Use tools to read, search, or update pages.
                - Confirm with the user before making modifications.
            """),
            markdown=True,
            show_tool_calls=True,
        )

        await agent.acli_app(
            message="You are a helpful assistant that can access Notion workspaces and pages.",
            stream=True,
            markdown=True,
            exit_on=["exit", "quit"],
        )


if __name__ == "__main__":
    asyncio.run(run_agent())



================================================
FILE: cookbook/tools/mcp/oxylabs.py
================================================
import asyncio
import os

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.mcp import MCPTools


async def run_agent_prompt():
    async with MCPTools(
        command="uvx oxylabs-mcp",
        env={
            "OXYLABS_USERNAME": os.getenv("OXYLABS_USERNAME"),
            "OXYLABS_PASSWORD": os.getenv("OXYLABS_PASSWORD"),
        },
    ) as server:
        agent = Agent(
            model=Gemini(api_key=os.getenv("GEMINI_API_KEY")),
            tools=[server],
            instructions=["Use MCP tools to fulfill the requests"],
            markdown=True,
        )
        await agent.aprint_response(
            "Go to oxylabs.io, look for career page, "
            "go to it and return all job titles in markdown format. "
            "Don't invent URLs, start from one provided."
        )


if __name__ == "__main__":
    asyncio.run(run_agent_prompt())



================================================
FILE: cookbook/tools/mcp/pipedream_auth.py
================================================
"""
🔒 Using Pipedream MCP servers with authentication

This is an example of how to use Pipedream MCP servers with authentication.
This is useful if your app is interfacing with the MCP servers in behalf of your users.

1. Get your access token. You can check how in Pipedream's docs: https://pipedream.com/docs/connect/mcp/developers/
2. Get the URL of the MCP server. It will look like this: https://remote.mcp.pipedream.net/<External user id>/<MCP app slug>
3. Set the environment variables:
    - MCP_SERVER_URL: The URL of the MCP server you previously got
    - MCP_ACCESS_TOKEN: The access token you previously got
    - PIPEDREAM_PROJECT_ID: The project id of the Pipedream project you want to use
    - PIPEDREAM_ENVIRONMENT: The environment of the Pipedream project you want to use
3. Install dependencies: pip install agno mcp
"""

import asyncio
from os import getenv

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools, StreamableHTTPClientParams
from agno.utils.log import log_exception

mcp_server_url = getenv("MCP_SERVER_URL")
mcp_access_token = getenv("MCP_ACCESS_TOKEN")
pipedream_project_id = getenv("PIPEDREAM_PROJECT_ID")
pipedream_environment = getenv("PIPEDREAM_ENVIRONMENT")


server_params = StreamableHTTPClientParams(
    url=mcp_server_url,
    headers={
        "Authorization": f"Bearer {mcp_access_token}",
        "x-pd-project-id": pipedream_project_id,
        "x-pd-environment": pipedream_environment,
    },
)


async def run_agent(task: str) -> None:
    try:
        async with MCPTools(
            server_params=server_params, transport="streamable-http", timeout_seconds=20
        ) as mcp:
            agent = Agent(
                model=OpenAIChat(id="gpt-4o-mini"),
                tools=[mcp],
                markdown=True,
            )
            await agent.aprint_response(message=task, stream=True)
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    # The agent can read channels, users, messages, etc.
    asyncio.run(run_agent("Show me the latest message in the channel #general"))



================================================
FILE: cookbook/tools/mcp/pipedream_google_calendar.py
================================================
"""
🗓️ Pipedream Google Calendar MCP

This example shows how to use Pipedream MCP servers (in this case the Google Calendar one) with Agno Agents.

1. Connect your Pipedream and Google Calendar accounts: https://mcp.pipedream.com/app/google_calendar
2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/google_calendar
3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above
4. Install dependencies: pip install agno mcp
"""

import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.utils.log import log_exception

mcp_server_url = os.getenv("MCP_SERVER_URL")


async def run_agent(task: str) -> None:
    try:
        async with MCPTools(
            url=mcp_server_url, transport="sse", timeout_seconds=20
        ) as mcp:
            agent = Agent(
                model=OpenAIChat(id="gpt-4o-mini"),
                tools=[mcp],
                markdown=True,
            )
            await agent.aprint_response(message=task, stream=True)
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    asyncio.run(
        run_agent("Tell me about all events I have in my calendar for tomorrow")
    )



================================================
FILE: cookbook/tools/mcp/pipedream_linkedin.py
================================================
"""
💻 Pipedream LinkedIn MCP

This example shows how to use Pipedream MCP servers (in this case the LinkedIn one) with Agno Agents.

1. Connect your Pipedream and LinkedIn accounts: https://mcp.pipedream.com/app/linkedin
2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/linkedin
3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above
4. Install dependencies: pip install agno mcp
"""

import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.utils.log import log_exception

mcp_server_url = os.getenv("MCP_SERVER_URL")


async def run_agent(task: str) -> None:
    try:
        async with MCPTools(
            url=mcp_server_url, transport="sse", timeout_seconds=20
        ) as mcp:
            agent = Agent(
                model=OpenAIChat(id="gpt-4o-mini"),
                tools=[mcp],
                markdown=True,
            )
            await agent.aprint_response(message=task, stream=True)
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    asyncio.run(
        run_agent("Check the Pipedream organization on LinkedIn and tell me about it")
    )



================================================
FILE: cookbook/tools/mcp/pipedream_slack.py
================================================
"""
💬 Pipedream Slack MCP

This example shows how to use Pipedream MCP servers (in this case the Slack one) with Agno Agents.

1. Connect your Pipedream and Slack accounts: https://mcp.pipedream.com/app/slack
2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/slack
3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above
4. Install dependencies: pip install agno mcp

"""

import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.utils.log import log_exception

mcp_server_url = os.getenv("MCP_SERVER_URL")


async def run_agent(task: str) -> None:
    try:
        async with MCPTools(
            url=mcp_server_url, transport="sse", timeout_seconds=20
        ) as mcp:
            agent = Agent(
                model=OpenAIChat(id="gpt-4o-mini"),
                tools=[mcp],
                markdown=True,
            )
            await agent.aprint_response(message=task, stream=True)
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    # The agent can read channels, users, messages, etc.
    asyncio.run(run_agent("Show me the latest message in the channel #general"))

    # Use your real Slack name for this one to work!
    asyncio.run(
        run_agent("Send a message to <YOUR_NAME> saying 'Hello, I'm your Agno Agent!'")
    )



================================================
FILE: cookbook/tools/mcp/qdrant.py
================================================
import asyncio
from os import getenv

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.mcp import MCPTools
from agno.utils.pprint import apprint_run_response

QDRANT_URL = getenv("QDRANT_URL")
QDRANT_API_KEY = getenv("QDRANT_API_KEY")
COLLECTION_NAME = "qdrant_collection"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"


async def run_agent(message: str) -> None:
    async with MCPTools(
        "uvx mcp-server-qdrant",
        env={
            "QDRANT_URL": QDRANT_URL,
            "QDRANT_API_KEY": QDRANT_API_KEY,
            "COLLECTION_NAME": COLLECTION_NAME,
            "EMBEDDING_MODEL": EMBEDDING_MODEL,
        },
    ) as mcp_tools:
        agent = Agent(
            model=Gemini(id="gemini-2.5-flash-preview-05-20"),
            tools=[mcp_tools],
            instructions="""
            You are the storage agent for the Model Context Protocol (MCP) server.
            You need to save the files in the vector database and answer the user's questions.
            You can use the following tools:
            - qdrant-store: Store data/output in the Qdrant vector database.
            - qdrant-find: Retrieve data/output from the Qdrant vector database.
            """,
            markdown=True,
            show_tool_calls=True,
        )

        response = await agent.arun(message, stream=True)
        await apprint_run_response(response)


if __name__ == "__main__":
    query = """
    Tell me about the extinction event of dinosaurs in detail. Include all possible theories and evidence. Store the result in the vector database.
    """
    asyncio.run(run_agent(query))



================================================
FILE: cookbook/tools/mcp/sequential_thinking.py
================================================
"""
This example demonstrates how to use multiple MCP servers in a single agent.

Prerequisites:
- Google Maps:
    - Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    You can obtain the API key from the Google Cloud Console:
    https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

    - You also need to activate the Address Validation API for your .
    https://console.developers.google.com/apis/api/addressvalidation.googleapis.com
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from agno.tools.yfinance import YFinanceTools
from mcp import StdioServerParameters


async def run_agent(message: str) -> None:
    """Run the GitHub agent with the given message."""

    async with (
        MCPTools(
            command="npx -y @modelcontextprotocol/server-sequential-thinking"
        ) as sequential_thinking_mcp_tools,
    ):
        agent = Agent(
            tools=[
                sequential_thinking_mcp_tools,
                YFinanceTools(
                    stock_price=True,
                    analyst_recommendations=True,
                    company_info=True,
                    company_news=True,
                ),
            ],
            instructions=dedent("""\
                ## Using the think tool
                Before taking any action or responding to the user after receiving tool results, use the think tool as a scratchpad to:
                - List the specific rules that apply to the current request
                - Check if all required information is collected
                - Verify that the planned action complies with all policies
                - Iterate over tool results for correctness

                ## Rules
                - Its expected that you will use the think tool generously to jot down thoughts and ideas.
                - Use tables where possible\
                """),
            markdown=True,
            show_tool_calls=True,
        )

        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    # Pull request example
    asyncio.run(run_agent("Write a report comparing NVDA to TSLA"))



================================================
FILE: cookbook/tools/mcp/stagehand.py
================================================
"""
🤖 Stagehand MCP Agent - Hacker News Reader's Digest

This example demonstrates how to use Agno's agent to create a Hacker News content using the Stagehand MCP server.

Features:
- Scrapes current Hacker News headlines and metadata
- Extracts top comments from popular stories
- Creates a structured digest with key insights
- Respects rate limits and community guidelines

Prerequisites:
- Clone the Stagehand MCP server: git clone https://github.com/browserbase/mcp-server-browserbase
- Build the Stagehand MCP server: cd mcp-server-browserbase/stagehand && npm install && npm run build
  - This will create a dist/index.js file in the location where you cloned the repository
- Install dependencies: pip install agno mcp
- Set environment variables: BROWSERBASE_API_KEY, BROWSERBASE_PROJECT_ID, OPENAI_API_KEY
- Run this example: python cookbook/tools/mcp/stagehand.py
"""

import asyncio
from os import environ
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters


async def run_agent(message: str) -> None:
    server_params = StdioServerParameters(
        command="node",
        # Update this path to the location where you cloned the repository
        args=["mcp-server-browserbase/stagehand/dist/index.js"],
        env=environ.copy(),
    )

    async with MCPTools(server_params=server_params, timeout_seconds=60) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a web scraping assistant that creates concise reader's digests from Hacker News.

                CRITICAL INITIALIZATION RULES - FOLLOW EXACTLY:
                1. NEVER use screenshot tool until AFTER successful navigation
                2. ALWAYS start with stagehand_navigate first
                3. Wait for navigation success message before any other actions
                4. If you see initialization errors, restart with navigation only
                5. Use stagehand_observe and stagehand_extract to explore pages safely

                Available tools and safe usage order:
                - stagehand_navigate: Use FIRST to initialize browser
                - stagehand_extract: Use to extract structured data from pages
                - stagehand_observe: Use to find elements and understand page structure
                - stagehand_act: Use to click links and navigate to comments
                - screenshot: Use ONLY after navigation succeeds and page loads

                Your goal is to create a comprehensive but concise digest that includes:
                - Top headlines with brief summaries
                - Key themes and trends
                - Notable comments and insights
                - Overall tech news landscape overview

                Be methodical, extract structured data, and provide valuable insights.
            """),
            markdown=True,
            show_tool_calls=True,
        )
        await agent.aprint_response(message, stream=True)


if __name__ == "__main__":
    asyncio.run(
        run_agent(
            "Create a comprehensive Hacker News Reader's Digest from https://news.ycombinator.com"
        )
    )



================================================
FILE: cookbook/tools/mcp/stripe.py
================================================
"""💵 Stripe MCP Agent - Manage Your Stripe Operations

This example demonstrates how to create an Agno agent that interacts with the Stripe API via the Model Context Protocol (MCP). This agent can create and manage Stripe objects like customers, products, prices, and payment links using natural language commands.


Setup:
2. Install Python dependencies:
   ```bash
   pip install agno mcp
   ```
3. Set Environment Variable: export STRIPE_SECRET_KEY=***.

Stripe MCP Docs: https://github.com/stripe/agent-toolkit
"""

import asyncio
import os
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from agno.utils.log import log_error, log_exception, log_info


async def run_agent(message: str) -> None:
    """
    Sets up the Stripe MCP server and initialize the Agno agent
    """
    # Verify Stripe API Key is available
    stripe_api_key = os.getenv("STRIPE_SECRET_KEY")
    if not stripe_api_key:
        log_error("STRIPE_SECRET_KEY environment variable not set.")
        return

    enabled_tools = "paymentLinks.create,products.create,prices.create,customers.create,customers.read"

    # handle different Operating Systems
    npx_command = "npx.cmd" if os.name == "nt" else "npx"

    try:
        # Initialize MCP toolkit with Stripe server
        async with MCPTools(
            command=f"{npx_command} -y @stripe/mcp --tools={enabled_tools} --api-key={stripe_api_key}"
        ) as mcp_toolkit:
            agent = Agent(
                name="StripeAgent",
                instructions=dedent("""\
                    You are an AI assistant specialized in managing Stripe operations.
                    You interact with the Stripe API using the available tools.

                    - Understand user requests to create or list Stripe objects (customers, products, prices, payment links).
                    - Clearly state the results of your actions, including IDs of created objects or lists retrieved.
                    - Ask for clarification if a request is ambiguous.
                    - Use markdown formatting, especially for links or code snippets.
                    - Execute the necessary steps sequentially if a request involves multiple actions (e.g., create product, then price, then link).
                """),
                tools=[mcp_toolkit],
                markdown=True,
                show_tool_calls=True,
            )

            # Run the agent with the provided task
            log_info(f"Running agent with assignment: '{message}'")
            await agent.aprint_response(message, stream=True)

    except FileNotFoundError:
        error_msg = f"Error: '{npx_command}' command not found. Please ensure Node.js and npm/npx are installed and in your system's PATH."
        log_error(error_msg)
    except Exception as e:
        log_exception(f"An unexpected error occurred during agent execution: {e}")


if __name__ == "__main__":
    task = "Create a new Stripe product named 'iPhone'. Then create a price of $999.99 USD for it. Finally, create a payment link for that price."
    asyncio.run(run_agent(task))


# Example prompts:
"""
Customer Management:
- "Create a customer. Name: ACME Corp, Email: billing@acme.example.com"
- "List my customers."
- "Find customer by email 'jane.doe@example.com'" # Note: Requires 'customers.retrieve' or search capability

Product and Price Management:
- "Create a new product called 'Basic Plan'."
- "Create a recurring monthly price of $10 USD for product 'Basic Plan'."
- "Create a product 'Ebook Download' and a one-time price of $19.95 USD."
- "List all products." # Note: Requires 'products.list' capability
- "List all prices." # Note: Requires 'prices.list' capability

Payment Links:
- "Create a payment link for the $10 USD monthly 'Basic Plan' price."
- "Generate a payment link for the '$19.95 Ebook Download'."

Combined Tasks:
- "Create a product 'Pro Service', add a price $150 USD (one-time), and give me the payment link."
- "Register a new customer 'support@example.com' named 'Support Team'."
"""



================================================
FILE: cookbook/tools/mcp/supabase.py
================================================
"""🔑 Supabase MCP Agent - Showcase Supabase MCP Capabilities

This example demonstrates how to use the Supabase MCP server to create create projects, database schemas, edge functions, and more.

Setup:
1. Install Python dependencies:

```bash
pip install agno mcp
```

2. Create a Supabase Access Token: https://supabase.com/dashboard/account/tokens and set it as the SUPABASE_ACCESS_TOKEN environment variable.
"""

import asyncio
import os
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.tools.reasoning import ReasoningTools
from agno.utils.log import log_error, log_exception, log_info


async def run_agent(task: str) -> None:
    token = os.getenv("SUPABASE_ACCESS_TOKEN")
    if not token:
        log_error("SUPABASE_ACCESS_TOKEN environment variable not set.")
        return

    npx_cmd = "npx.cmd" if os.name == "nt" else "npx"

    try:
        async with MCPTools(
            f"{npx_cmd} -y @supabase/mcp-server-supabase@latest --access-token={token}"
        ) as mcp:
            instructions = dedent(f"""
                You are an expert Supabase MCP architect. Given the project description:
                {task}

                Automatically perform the following steps :
                1. Plan the entire database schema based on the project description.
                2. Call `list_organizations` and select the first organization in the response.
                3. Use `get_cost(type='project')` to estimate project creation cost and mention the cost in your response.
                4. Create a new Supabase project with `create_project`, passing the confirmed cost ID.
                5. Poll project status with `get_project` until the status is `ACTIVE_HEALTHY`.
                6. Analyze the project requirements and propose a complete, normalized SQL schema (tables,  columns, data types, indexes, constraints, triggers, and functions) as DDL statements.
                7. Apply the schema using `apply_migration`, naming the migration `initial_schema`.
                8. Validate the deployed schema via `list_tables` and `list_extensions`.
                8. Deploy a simple health-check edge function with `deploy_edge_function`.
                9. Retrieve and print the project URL (`get_project_url`) and anon key (`get_anon_key`).
            """)
            agent = Agent(
                model=OpenAIChat(id="o4-mini"),
                instructions=instructions,
                tools=[mcp, ReasoningTools(add_instructions=True)],
                markdown=True,
            )

            log_info(f"Running Supabase project agent for: {task}")
            await agent.aprint_response(
                message=task,
                stream=True,
                stream_intermediate_steps=True,
                show_full_reasoning=True,
            )
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    demo_description = (
        "Develop a cloud-based SaaS platform with AI-powered task suggestions, calendar syncing, predictive prioritization, "
        "team collaboration, and project analytics."
    )
    asyncio.run(run_agent(demo_description))


# Example prompts to try:
"""
A SaaS tool that helps businesses automate document processing using AI. Users can upload invoices, contracts, or PDFs and get structured data, smart summaries, and red flag alerts for compliance or anomalies. Ideal for legal teams, accountants, and enterprise back offices.

An AI-enhanced SaaS platform for streamlining the recruitment process. Features include automated candidate screening using NLP, AI interview scheduling, bias detection in job descriptions, and pipeline analytics. Designed for fast-growing startups and mid-sized HR teams.

An internal SaaS tool for HR departments to monitor employee wellbeing. Combines weekly mood check-ins, anonymous feedback, and AI-driven burnout detection models. Integrates with Slack and HR systems to support a healthier workplace culture.
"""



================================================
FILE: cookbook/tools/mcp/local_server/client.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    # Initialize the MCP server
    async with (
        MCPTools(
            f"fastmcp run cookbook/tools/mcp/local_server/server.py",  # Supply the command to run the MCP server
        ) as mcp_tools,
    ):
        agent = Agent(
            model=Groq(id="llama-3.3-70b-versatile"),
            tools=[mcp_tools],
            show_tool_calls=True,
            markdown=True,
        )
        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    asyncio.run(run_agent("What is the weather in San Francisco?"))



================================================
FILE: cookbook/tools/mcp/local_server/server.py
================================================
"""
`fastmcp` is required for this demo.

```bash
pip install fastmcp
```

Run this with `fastmcp run cookbook/tools/mcp/local_server/server.py`
"""

from fastmcp import FastMCP

mcp = FastMCP("weather_tools")


@mcp.tool()
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"


@mcp.tool()
def get_temperature(city: str) -> str:
    return f"The temperature in {city} is 70 degrees"


if __name__ == "__main__":
    mcp.run(transport="stdio")



================================================
FILE: cookbook/tools/mcp/sse_transport/README.md
================================================
# MCP server using SSE transport

This cookbook shows how to use the `MCPTool` util with an MCP server using SSE transport.

1. Run the server with SSE transport
```bash
python cookbook/tools/mcp/sse_transport/server.py
```

2. Run the agent using the MCP integration connecting to our server
```bash
python cookbook/tools/mcp/sse_transport/client.py
```


================================================
FILE: cookbook/tools/mcp/sse_transport/client.py
================================================
"""
Show how to connect to MCP servers that use the SSE transport using our MCPTools and MultiMCPTools classes.
Check the README.md file for instructions on how to run these examples.
"""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools, MultiMCPTools

# This is the URL of the MCP server we want to use.
server_url = "http://localhost:8000/sse"


async def run_agent(message: str) -> None:
    async with MCPTools(transport="sse", url=server_url) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(message=message, stream=True, markdown=True)


# Using MultiMCPTools, we can connect to multiple MCP servers at once, even if they use different transports.
# In this example we connect to both our example server (SSE transport), and a different server (stdio transport).
async def run_agent_with_multimcp(message: str) -> None:
    async with MultiMCPTools(
        commands=["npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt"],
        urls=[server_url],
        urls_transports=["sse"],
    ) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(message=message, stream=True, markdown=True)


if __name__ == "__main__":
    asyncio.run(run_agent("Do I have any birthdays this week?"))
    asyncio.run(
        run_agent_with_multimcp(
            "Can you check when is my mom's birthday, and if there are any AirBnb listings in SF for two people for that day?"
        )
    )



================================================
FILE: cookbook/tools/mcp/sse_transport/server.py
================================================
"""Start an example MCP server that uses the SSE transport."""

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("calendar_assistant")


@mcp.tool()
def get_events(day: str) -> str:
    return f"There are no events scheduled for {day}."


@mcp.tool()
def get_birthdays_this_week() -> str:
    return "It is your mom's birthday tomorrow"


if __name__ == "__main__":
    mcp.run(transport="sse")



================================================
FILE: cookbook/tools/mcp/streamable_http_transport/README.md
================================================
# MCP server using Streamable HTTP transport

This cookbook shows how to use the `MCPTool` util with an MCP server using Streamable HTTP transport.

1. Run the server with Streamable HTTP transport
```bash
python cookbook/tools/mcp/streamable_http_transport/server.py
```

2. Run the agent using the MCP integration connecting to our server
```bash
python cookbook/tools/mcp/streamable_http_transport/client.py
```


================================================
FILE: cookbook/tools/mcp/streamable_http_transport/client.py
================================================
"""
Show how to connect to MCP servers that use either SSE or Streamable HTTP transport using our MCPTools and MultiMCPTools classes.

Check the README.md file for instructions on how to run these examples.
"""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools, MultiMCPTools

# This is the URL of the MCP server we want to use.
server_url = "http://localhost:8000/mcp"


async def run_agent(message: str) -> None:
    async with MCPTools(transport="streamable-http", url=server_url) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(message=message, stream=True, markdown=True)


# Using MultiMCPTools, we can connect to multiple MCP servers at once, even if they use different transports.
# In this example we connect to both our example server (Streamable HTTP transport), and a different server (stdio transport).
async def run_agent_with_multimcp(message: str) -> None:
    async with MultiMCPTools(
        commands=["npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt"],
        urls=[server_url],
        urls_transports=["streamable-http"],
    ) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(message=message, stream=True, markdown=True)


if __name__ == "__main__":
    asyncio.run(run_agent("Do I have any birthdays this week?"))
    asyncio.run(
        run_agent_with_multimcp(
            "Can you check when is my mom's birthday, and if there are any AirBnb listings in SF for two people for that day?",
        )
    )



================================================
FILE: cookbook/tools/mcp/streamable_http_transport/server.py
================================================
"""Start an example MCP server that uses the Streamable HTTP transport."""

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("calendar_assistant")


@mcp.tool()
def get_events(day: str) -> str:
    return f"There are no events scheduled for {day}."


@mcp.tool()
def get_birthdays_this_week() -> str:
    return "It is your mom's birthday tomorrow"


if __name__ == "__main__":
    mcp.run(transport="streamable-http")



================================================
FILE: cookbook/tools/models/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/tools/models/azure_openai_tools.py
================================================
"""Example showing how to use Azure OpenAI Tools with Agno.

Requirements:
1. Azure OpenAI service setup with DALL-E deployment and chat model deployment
2. Environment variables:
   - AZURE_OPENAI_API_KEY - Your Azure OpenAI API key
   - AZURE_OPENAI_ENDPOINT - The Azure OpenAI endpoint URL
   - AZURE_OPENAI_DEPLOYMENT - The deployment name for the language model
   - AZURE_OPENAI_IMAGE_DEPLOYMENT - The deployment name for an image generation model
   - OPENAI_API_KEY (for standard OpenAI example)

The script will automatically run only the examples for which you have the necessary
environment variables set.
"""

import sys
from os import getenv

from agno.agent import Agent
from agno.models.azure import AzureOpenAI
from agno.models.openai import OpenAIChat
from agno.tools.models.azure_openai import AzureOpenAITools

# Check for base requirements first - needed for all examples
# Exit early if base requirements aren't met
if not bool(
    getenv("AZURE_OPENAI_API_KEY")
    and getenv("AZURE_OPENAI_ENDPOINT")
    and getenv("AZURE_OPENAI_IMAGE_DEPLOYMENT")
):
    print("Error: Missing base Azure OpenAI requirements.")
    print("Required for all examples:")
    print("- AZURE_OPENAI_API_KEY")
    print("- AZURE_OPENAI_ENDPOINT")
    print("- AZURE_OPENAI_IMAGE_DEPLOYMENT")
    sys.exit(1)


print("Running Example 1: Standard OpenAI model with Azure OpenAI Tools")
print(
    "This approach uses OpenAI for the agent's model but Azure for image generation.\n"
)

standard_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),  # Using standard OpenAI for the agent
    tools=[AzureOpenAITools()],  # Using Azure OpenAI for image generation
    name="Mixed OpenAI Generator",
    description="An AI assistant that uses standard OpenAI for chat and Azure OpenAI for image generation",
    instructions=[
        "You are an AI artist specializing in creating images based on user descriptions.",
        "Use the generate_image tool to create detailed visualizations of user requests.",
        "Provide creative suggestions to enhance the images if needed.",
    ],
    debug_mode=True,
)

# Generate an image with the standard OpenAI model and Azure tools
standard_agent.print_response(
    "Generate an image of a futuristic city with flying cars and tall skyscrapers",
    markdown=True,
)

print("\nRunning Example 2: Full Azure OpenAI setup")
print(
    "This approach uses Azure OpenAI for both the agent's model and image generation.\n"
)

# Create an AzureOpenAI model using Azure credentials
azure_endpoint = getenv("AZURE_OPENAI_ENDPOINT")
azure_api_key = getenv("AZURE_OPENAI_API_KEY")
azure_deployment = getenv("AZURE_OPENAI_DEPLOYMENT")

# Explicitly pass all parameters to make debugging easier
azure_model = AzureOpenAI(
    azure_endpoint=azure_endpoint,
    azure_deployment=azure_deployment,
    api_key=azure_api_key,
    id=azure_deployment,  # Using the deployment name as the model ID
)

# Create an agent with Azure OpenAI model and tools
azure_agent = Agent(
    model=azure_model,  # Using Azure OpenAI for the agent
    tools=[AzureOpenAITools()],  # Using Azure OpenAI for image generation
    name="Full Azure OpenAI Generator",
    description="An AI assistant that uses Azure OpenAI for both chat and image generation",
    instructions=[
        "You are an AI artist specializing in creating images based on user descriptions.",
        "Use the generate_image tool to create detailed visualizations of user requests.",
        "Provide creative suggestions to enhance the images if needed.",
    ],
)

# Generate an image with the full Azure setup
azure_agent.print_response(
    "Generate an image of a serene Japanese garden with cherry blossoms",
    markdown=True,
)



================================================
FILE: cookbook/tools/models/gemini_video_generation.py
================================================
"""🔧 Example: Using the GeminiTools Toolkit for Video Generation

An Agent using the Gemini video generation tool.

Video generation only works with Vertex AI.
Make sure you have set the GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION environment variables.

Example prompts to try:
- "Generate a 5-second video of a kitten playing a piano"
- "Create a short looping animation of a neon city skyline at dusk"

Run `pip install google-genai agno` to install the necessary dependencies.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[GeminiTools(vertexai=True)],  # Video Generation only works on VertexAI mode
    show_tool_calls=True,
    debug_mode=True,
)

agent.print_response(
    "create a video of a cat driving at top speed",
)
response = agent.run_response
if response.videos:
    for video in response.videos:
        save_base64_data(video.content, f"tmp/cat_driving_{video.id}.mp4")



================================================
FILE: cookbook/tools/models/morph.py
================================================
"""
Simple example showing Morph Fast Apply with file creation and editing.
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.morph import MorphTools


def create_sample_file():
    """Create a simple Python file in tmp directory for testing"""
    # Create tmp directory if it doesn't exist
    tmp_dir = Path("tmp")
    tmp_dir.mkdir(exist_ok=True)

    # Create a simple Python file
    sample_file = tmp_dir / "calculator.py"

    sample_code = """
def add(a, b):
    return a + b

def multiply(x, y):
    result = x * y
    return result

class Calculator:
    def __init__(self):
        self.history = []
    
    def calculate(self, operation, a, b):
        if operation == "add":
            result = add(a, b)
        elif operation == "multiply":
            result = multiply(a, b)
        else:
            result = None
        return result
"""

    with open(sample_file, "w") as f:
        f.write(sample_code)

    return str(sample_file)


def main():
    target_file = create_sample_file()

    code_editor = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[MorphTools(model="morph-v3-large")],
        debug_mode=True,
        markdown=True,
    )

    # Request to improve the code
    improvement_request = f"""
        Please improve the Python code in "{target_file}" by adding:

        1. Type hints for all functions and methods
        2. Docstrings for all functions and the Calculator class
        3. Error handling and input validation
    """  # <-- Or directly provide the code here

    code_editor.print_response(improvement_request)


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/tools/models/nebius_tools.py
================================================
"""Run `pip install openai agno` to install dependencies.

This example demonstrates how to use NebiusTools for text-to-image generation with Nebius AI Studio.
"""

import os
from pathlib import Path
from uuid import uuid4

from agno.agent import Agent
from agno.tools.models.nebius import NebiusTools
from agno.utils.media import save_base64_data

# Create an Agent with the Nebius text-to-image tool
agent = Agent(
    tools=[
        NebiusTools(
            # You can provide your API key here or set the NEBIUS_API_KEY environment variable
            api_key=os.getenv("NEBIUS_API_KEY"),
            image_model="black-forest-labs/flux-schnell",  # Fastest model
            image_size="1024x1024",
            image_quality="standard",
        )
    ],
    name="Nebius Image Generator",
    show_tool_calls=True,
    markdown=True,
)

# Example 1: Generate a basic image
response = agent.run(
    "Generate an image of a futuristic city with flying cars and tall skyscrapers",
)

if response.images:
    image_path = Path("tmp") / f"nebius_futuristic_city_{uuid4()}.png"
    Path("tmp").mkdir(exist_ok=True)
    save_base64_data(response.images[0].content, image_path)
    print(f"Image saved to {image_path}")

# Example 2: Generate an image with the higher quality model
high_quality_agent = Agent(
    tools=[
        NebiusTools(
            api_key=os.getenv("NEBIUS_API_KEY"),
            image_model="black-forest-labs/flux-dev",  # Better quality model
            image_size="1024x1024",
            image_quality="hd",  # Higher quality setting
        )
    ],
    name="Nebius High-Quality Image Generator",
    show_tool_calls=True,
    markdown=True,
)

response = high_quality_agent.run(
    "Create a detailed portrait of a cyberpunk character with neon lights",
)

# Save the generated image
if response.images:
    image_path = Path("tmp") / f"nebius_cyberpunk_character_{uuid4()}.png"
    Path("tmp").mkdir(exist_ok=True)
    save_base64_data(response.images[0].content, image_path)
    print(f"High-quality image saved to {image_path}")

# Example 3: Generate an image with the SDXL (Stability Diffusion XL model) model
sdxl_agent = Agent(
    tools=[
        NebiusTools(
            api_key=os.getenv("NEBIUS_API_KEY"),
            image_model="stability-ai/sdxl",  # Stability Diffusion XL model
            image_size="1024x1024",
        )
    ],
    name="Nebius SDXL Image Generator",
    show_tool_calls=True,
    markdown=True,
)

response = sdxl_agent.run(
    "Create a fantasy landscape with a castle on a floating island",
)

# Save the generated image
if response.images:
    image_path = Path("tmp") / f"nebius_fantasy_landscape_{uuid4()}.png"
    Path("tmp").mkdir(exist_ok=True)
    save_base64_data(response.images[0].content, image_path)
    print(f"SDXL image saved to {image_path}")



================================================
FILE: cookbook/tools/models/openai_tools.py
================================================
"""
This example demonstrates how to use the OpenAITools to transcribe an audio file.
"""

from pathlib import Path

from agno.agent import Agent
from agno.tools.openai import OpenAITools
from agno.utils.media import download_file, save_base64_data

# Example 1: Transcription
url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

local_audio_path = Path("tmp/sample_conversation.wav")
print(f"Downloading file to local path: {local_audio_path}")
download_file(url, local_audio_path)

transcription_agent = Agent(
    tools=[OpenAITools(transcription_model="gpt-4o-transcribe")],
    show_tool_calls=True,
    markdown=True,
)
transcription_agent.print_response(
    f"Transcribe the audio file for this file: {local_audio_path}"
)

# Example 2: Image Generation
agent = Agent(
    tools=[OpenAITools(image_model="gpt-image-1")],
    markdown=True,
    show_tool_calls=True,
)

response = agent.run(
    "Generate a photorealistic image of a cozy coffee shop interior",
)

if response.images:
    save_base64_data(response.images[0].content, "tmp/coffee_shop.png")



================================================
FILE: cookbook/workflows/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows/async_blog_post_generator.py
================================================
"""🎨 Blog Post Generator - Your AI Content Creation Studio!

This advanced example demonstrates how to build a sophisticated blog post generator that combines
web research capabilities with professional writing expertise. The workflow uses a multi-stage
approach:
1. Intelligent web research and source gathering
2. Content extraction and processing
3. Professional blog post writing with proper citations

Key capabilities:
- Advanced web research and source evaluation
- Content scraping and processing
- Professional writing with SEO optimization
- Automatic content caching for efficiency
- Source attribution and fact verification

Example blog topics to try:
- "The Rise of Artificial General Intelligence: Latest Breakthroughs"
- "How Quantum Computing is Revolutionizing Cybersecurity"
- "Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint"
- "The Future of Work: AI and Human Collaboration"
- "Space Tourism: From Science Fiction to Reality"
- "Mindfulness and Mental Health in the Digital Age"
- "The Evolution of Electric Vehicles: Current State and Future Trends"

Run `pip install openai newspaper4k lxml_html_clean sqlalchemy agno` to install dependencies.

Export SERPER_API_KEY=your_api_key_here to use the Serper API.
"""

import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.newspaper4k import Newspaper4kTools
from agno.tools.serper import SerperTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunResponse, Workflow
from pydantic import BaseModel, Field


class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )


class BlogPostGenerator(Workflow):
    """Advanced workflow for generating professional blog posts with proper research and citations."""

    description: str = dedent("""\
    An intelligent blog post generator that creates engaging, well-researched content.
    This workflow orchestrates multiple AI agents to research, analyze, and craft
    compelling blog posts that combine journalistic rigor with engaging storytelling.
    The system excels at creating content that is both informative and optimized for
    digital consumption.
    """)

    # Search Agent: Handles intelligent web searching and source gathering
    searcher: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[SerperTools()],
        description=dedent("""\
        You are BlogResearch-X, an elite research assistant specializing in discovering
        high-quality sources for compelling blog content. Your expertise includes:

        - Finding authoritative and trending sources
        - Evaluating content credibility and relevance
        - Identifying diverse perspectives and expert opinions
        - Discovering unique angles and insights
        - Ensuring comprehensive topic coverage\
        """),
        instructions=dedent("""\
        1. Search Strategy 🔍
           - Find 10-15 relevant sources and select the 5-7 best ones
           - Prioritize recent, authoritative content
           - Look for unique angles and expert insights
        2. Source Evaluation 📊
           - Verify source credibility and expertise
           - Check publication dates for timeliness
           - Assess content depth and uniqueness
        3. Diversity of Perspectives 🌐
           - Include different viewpoints
           - Gather both mainstream and expert opinions
           - Find supporting data and statistics\
        """),
        response_model=SearchResults,
    )

    # Content Scraper: Extracts and processes article content
    article_scraper: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[Newspaper4kTools()],
        description=dedent("""\
        You are ContentBot-X, a specialist in extracting and processing digital content
        for blog creation. Your expertise includes:

        - Efficient content extraction
        - Smart formatting and structuring
        - Key information identification
        - Quote and statistic preservation
        - Maintaining source attribution\
        """),
        instructions=dedent("""\
        1. Content Extraction 📑
           - Extract content from the article
           - Preserve important quotes and statistics
           - Maintain proper attribution
           - Handle paywalls gracefully
        2. Content Processing 🔄
           - Format text in clean markdown
           - Preserve key information
           - Structure content logically
        3. Quality Control ✅
           - Verify content relevance
           - Ensure accurate extraction
           - Maintain readability\
        """),
        response_model=ScrapedArticle,
    )

    # Content Writer Agent: Crafts engaging blog posts from research
    writer: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are BlogMaster-X, an elite content creator combining journalistic excellence
        with digital marketing expertise. Your strengths include:

        - Crafting viral-worthy headlines
        - Writing engaging introductions
        - Structuring content for digital consumption
        - Incorporating research seamlessly
        - Optimizing for SEO while maintaining quality
        - Creating shareable conclusions\
        """),
        instructions=dedent("""\
        1. Content Strategy 📝
           - Craft attention-grabbing headlines
           - Write compelling introductions
           - Structure content for engagement
           - Include relevant subheadings
        2. Writing Excellence ✍️
           - Balance expertise with accessibility
           - Use clear, engaging language
           - Include relevant examples
           - Incorporate statistics naturally
        3. Source Integration 🔍
           - Cite sources properly
           - Include expert quotes
           - Maintain factual accuracy
        4. Digital Optimization 💻
           - Structure for scanability
           - Include shareable takeaways
           - Optimize for SEO
           - Add engaging subheadings\
        """),
        expected_output=dedent("""\
        # {Viral-Worthy Headline}

        ## Introduction
        {Engaging hook and context}

        ## {Compelling Section 1}
        {Key insights and analysis}
        {Expert quotes and statistics}

        ## {Engaging Section 2}
        {Deeper exploration}
        {Real-world examples}

        ## {Practical Section 3}
        {Actionable insights}
        {Expert recommendations}

        ## Key Takeaways
        - {Shareable insight 1}
        - {Practical takeaway 2}
        - {Notable finding 3}

        ## Sources
        {Properly attributed sources with links}\
        """),
        markdown=True,
    )

    def run(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> RunResponse:
        logger.info(f"Generating a blog post on: {topic}")

        # Use the cached blog post if use_cache is True
        if use_cached_report:
            cached_blog_post = self.get_cached_blog_post(topic)
            if cached_blog_post:
                return RunResponse(
                    run_id=self.run_id,
                    content=cached_blog_post,
                )

        # Search the web for articles on the topic
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # If no search_results are found for the topic, end the workflow
        if search_results is None or len(search_results.articles) == 0:
            return RunResponse(
                run_id=self.run_id,
                content=f"Sorry, could not find any articles on the topic: {topic}",
            )

        # Scrape the search results
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            topic, search_results, use_scrape_cache
        )

        # Prepare the input for the writer
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }

        # Run the writer response
        writer_response: RunResponse = self.writer.run(
            json.dumps(writer_input, indent=4)
        )

        # Save the blog post in the cache
        self.add_blog_post_to_cache(topic, writer_response.content)

        return writer_response

    async def arun(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> RunResponse:
        logger.info(f"Generating a blog post on: {topic}")

        # Use the cached blog post if use_cache is True
        if use_cached_report:
            cached_blog_post = self.get_cached_blog_post(topic)
            if cached_blog_post:
                return RunResponse(
                    run_id=self.run_id,
                    content=cached_blog_post,
                )

        # Search the web for articles on the topic
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # If no search_results are found for the topic, end the workflow
        if search_results is None or len(search_results.articles) == 0:
            return RunResponse(
                run_id=self.run_id,
                content=f"Sorry, could not find any articles on the topic: {topic}",
            )

        # Scrape the search results
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            topic, search_results, use_scrape_cache
        )

        # Prepare the input for the writer
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }

        # Run the writer response
        writer_response: RunResponse = self.writer.run(
            json.dumps(writer_input, indent=4)
        )

        # Save the blog post in the cache
        self.add_blog_post_to_cache(topic, writer_response.content)

        return writer_response

    def get_cached_blog_post(self, topic: str) -> Optional[str]:
        logger.info("Checking if cached blog post exists")

        return self.session_state.get("blog_posts", {}).get(topic)

    def add_blog_post_to_cache(self, topic: str, blog_post: str):
        logger.info(f"Saving blog post for topic: {topic}")
        self.session_state.setdefault("blog_posts", {})
        self.session_state["blog_posts"][topic] = blog_post

    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:
        logger.info("Checking if cached search results exist")
        search_results = self.session_state.get("search_results", {}).get(topic)
        return (
            SearchResults.model_validate(search_results)
            if search_results and isinstance(search_results, dict)
            else search_results
        )

    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):
        logger.info(f"Saving search results for topic: {topic}")
        self.session_state.setdefault("search_results", {})
        self.session_state["search_results"][topic] = search_results

    def get_cached_scraped_articles(
        self, topic: str
    ) -> Optional[Dict[str, ScrapedArticle]]:
        logger.info("Checking if cached scraped articles exist")
        scraped_articles = self.session_state.get("scraped_articles", {}).get(topic)
        return (
            ScrapedArticle.model_validate(scraped_articles)
            if scraped_articles and isinstance(scraped_articles, dict)
            else scraped_articles
        )

    def add_scraped_articles_to_cache(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ):
        logger.info(f"Saving scraped articles for topic: {topic}")
        self.session_state.setdefault("scraped_articles", {})
        self.session_state["scraped_articles"][topic] = scraped_articles

    def get_search_results(
        self, topic: str, use_search_cache: bool, num_attempts: int = 3
    ) -> Optional[SearchResults]:
        # Get cached search_results from the session state if use_search_cache is True
        if use_search_cache:
            try:
                search_results_from_cache = self.get_cached_search_results(topic)
                if search_results_from_cache is not None:
                    search_results = SearchResults.model_validate(
                        search_results_from_cache
                    )
                    logger.info(
                        f"Found {len(search_results.articles)} articles in cache."
                    )
                    return search_results
            except Exception as e:
                logger.warning(f"Could not read search results from cache: {e}")

        # If there are no cached search_results, use the searcher to find the latest articles
        for attempt in range(num_attempts):
            try:
                searcher_response: RunResponse = self.searcher.run(topic)
                if (
                    searcher_response is not None
                    and searcher_response.content is not None
                    and isinstance(searcher_response.content, SearchResults)
                ):
                    article_count = len(searcher_response.content.articles)
                    logger.info(
                        f"Found {article_count} articles on attempt {attempt + 1}"
                    )
                    # Cache the search results
                    self.add_search_results_to_cache(topic, searcher_response.content)
                    return searcher_response.content
                else:
                    logger.warning(
                        f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                    )
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

        logger.error(f"Failed to get search results after {num_attempts} attempts")
        return None

    def scrape_articles(
        self, topic: str, search_results: SearchResults, use_scrape_cache: bool
    ) -> Dict[str, ScrapedArticle]:
        scraped_articles: Dict[str, ScrapedArticle] = {}

        # Get cached scraped_articles from the session state if use_scrape_cache is True
        if use_scrape_cache:
            try:
                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)
                if scraped_articles_from_cache is not None:
                    scraped_articles = scraped_articles_from_cache
                    logger.info(
                        f"Found {len(scraped_articles)} scraped articles in cache."
                    )
                    return scraped_articles
            except Exception as e:
                logger.warning(f"Could not read scraped articles from cache: {e}")

        # Scrape the articles that are not in the cache
        for article in search_results.articles:
            if article.url in scraped_articles:
                logger.info(f"Found scraped article in cache: {article.url}")
                continue

            article_scraper_response: RunResponse = self.article_scraper.run(
                article.url
            )
            if (
                article_scraper_response is not None
                and article_scraper_response.content is not None
                and isinstance(article_scraper_response.content, ScrapedArticle)
            ):
                scraped_articles[article_scraper_response.content.url] = (
                    article_scraper_response.content
                )
                logger.info(f"Scraped article: {article_scraper_response.content.url}")

        # Save the scraped articles in the session state
        self.add_scraped_articles_to_cache(topic, scraped_articles)
        return scraped_articles


# Run the workflow if the script is executed directly
if __name__ == "__main__":
    import random

    from rich.prompt import Prompt

    # Fun example prompts to showcase the generator's versatility
    example_prompts = [
        "Why Cats Secretly Run the Internet",
        "The Science Behind Why Pizza Tastes Better at 2 AM",
        "Time Travelers' Guide to Modern Social Media",
        "How Rubber Ducks Revolutionized Software Development",
        "The Secret Society of Office Plants: A Survival Guide",
        "Why Dogs Think We're Bad at Smelling Things",
        "The Underground Economy of Coffee Shop WiFi Passwords",
        "A Historical Analysis of Dad Jokes Through the Ages",
    ]

    # Get topic from user
    topic = Prompt.ask(
        "[bold]Enter a blog post topic[/bold] (or press Enter for a random example)\n✨",
        default=random.choice(example_prompts),
    )

    # Convert the topic to a URL-safe string for use in session_id
    url_safe_topic = topic.lower().replace(" ", "-")

    # Initialize the blog post generator workflow
    # - Creates a unique session ID based on the topic
    # - Sets up SQLite storage for caching results
    generate_blog_post = BlogPostGenerator(
        session_id=f"generate-blog-post-on-{url_safe_topic}",
        storage=SqliteStorage(
            table_name="generate_blog_post_workflows",
            mode="workflow",
            auto_upgrade_schema=True,
            db_file="tmp/agno_workflows.db",
        ),
    )

    # Execute the workflow with caching enabled
    # Run it asynchronously
    blog_post: RunResponse = asyncio.run(
        generate_blog_post.arun(
            topic=topic,
            use_search_cache=True,
            use_scrape_cache=True,
            use_cached_report=True,
        )
    )
    pprint_run_response(blog_post, markdown=True)

    # or run it synchronously
    # blog_post: RunResponse = generate_blog_post.run(
    #     topic=topic,
    #     use_search_cache=True,
    #     use_scrape_cache=True,
    #     use_cached_report=True,
    # )
    # pprint_run_response(blog_post, markdown=True)



================================================
FILE: cookbook/workflows/async_hackernews_reporter.py
================================================
"""Please install dependencies using:
pip install openai newspaper4k lxml_html_clean agno httpx
"""

import json
from typing import AsyncIterator

import httpx
from agno.agent import Agent, RunResponse
from agno.run.workflow import WorkflowCompletedEvent
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class AsyncHackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    async def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """
        async with httpx.AsyncClient() as client:
            # Fetch top story IDs
            response = await client.get(
                "https://hacker-news.firebaseio.com/v0/topstories.json"
            )
            story_ids = response.json()

            # Fetch story details concurrently
            tasks = [
                client.get(
                    f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                )
                for story_id in story_ids[:num_stories]
            ]
            responses = await asyncio.gather(*tasks)

            stories = []
            for response in responses:
                story = response.json()
                story["username"] = story["by"]
                stories.append(story)

            return json.dumps(stories)

    async def arun(self, num_stories: int = 5) -> AsyncIterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = await self.hn_agent.arun(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content="Sorry, could not get the top stories.",
            )
            return

        logger.info("Reading each story and writing a report.")
        # Get the async iterator from writer.arun()
        writer_response = await self.writer.arun(top_stories.content, stream=True)

        # Stream the writer's response directly
        async for response in writer_response:
            if response.content:
                response.run_id = self.run_id
                yield response


if __name__ == "__main__":
    import asyncio

    async def main():
        # Initialize the workflow
        workflow = AsyncHackerNewsReporter(debug_mode=False)

        # Run the workflow and collect the final response
        final_content = []
        try:
            async for response in workflow.arun(num_stories=5):
                if response.content:
                    final_content.append(response.content)
        except Exception as e:
            import traceback

            traceback.print_exc()
            logger.error(f"Error running workflow: {e}")
            return

        # Create final response with combined content
        if final_content:
            final_response = RunResponse(content="".join(final_content))
            # Pretty print the final response
            pprint_run_response(final_response, markdown=True, show_time=True)

    # Run the async main function
    asyncio.run(main())



================================================
FILE: cookbook/workflows/blog_post_generator.py
================================================
"""🎨 Blog Post Generator - Your AI Content Creation Studio!

This advanced example demonstrates how to build a sophisticated blog post generator that combines
web research capabilities with professional writing expertise. The workflow uses a multi-stage
approach:
1. Intelligent web research and source gathering
2. Content extraction and processing
3. Professional blog post writing with proper citations

Key capabilities:
- Advanced web research and source evaluation
- Content scraping and processing
- Professional writing with SEO optimization
- Automatic content caching for efficiency
- Source attribution and fact verification

Example blog topics to try:
- "The Rise of Artificial General Intelligence: Latest Breakthroughs"
- "How Quantum Computing is Revolutionizing Cybersecurity"
- "Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint"
- "The Future of Work: AI and Human Collaboration"
- "Space Tourism: From Science Fiction to Reality"
- "Mindfulness and Mental Health in the Digital Age"
- "The Evolution of Electric Vehicles: Current State and Future Trends"

Run `pip install openai ddgs newspaper4k lxml_html_clean sqlalchemy agno` to install dependencies.
"""

import json
from textwrap import dedent
from typing import Dict, Iterator, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowCompletedEvent
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunResponse, Workflow
from pydantic import BaseModel, Field


class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )


class BlogPostGenerator(Workflow):
    """Advanced workflow for generating professional blog posts with proper research and citations."""

    description: str = dedent("""\
    An intelligent blog post generator that creates engaging, well-researched content.
    This workflow orchestrates multiple AI agents to research, analyze, and craft
    compelling blog posts that combine journalistic rigor with engaging storytelling.
    The system excels at creating content that is both informative and optimized for
    digital consumption.
    """)

    # Search Agent: Handles intelligent web searching and source gathering
    searcher: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[DuckDuckGoTools()],
        description=dedent("""\
        You are BlogResearch-X, an elite research assistant specializing in discovering
        high-quality sources for compelling blog content. Your expertise includes:

        - Finding authoritative and trending sources
        - Evaluating content credibility and relevance
        - Identifying diverse perspectives and expert opinions
        - Discovering unique angles and insights
        - Ensuring comprehensive topic coverage\
        """),
        instructions=dedent("""\
        1. Search Strategy 🔍
           - Find 10-15 relevant sources and select the 5-7 best ones
           - Prioritize recent, authoritative content
           - Look for unique angles and expert insights
        2. Source Evaluation 📊
           - Verify source credibility and expertise
           - Check publication dates for timeliness
           - Assess content depth and uniqueness
        3. Diversity of Perspectives 🌐
           - Include different viewpoints
           - Gather both mainstream and expert opinions
           - Find supporting data and statistics\
        """),
        response_model=SearchResults,
    )

    # Content Scraper: Extracts and processes article content
    article_scraper: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[Newspaper4kTools()],
        description=dedent("""\
        You are ContentBot-X, a specialist in extracting and processing digital content
        for blog creation. Your expertise includes:

        - Efficient content extraction
        - Smart formatting and structuring
        - Key information identification
        - Quote and statistic preservation
        - Maintaining source attribution\
        """),
        instructions=dedent("""\
        1. Content Extraction 📑
           - Extract content from the article
           - Preserve important quotes and statistics
           - Maintain proper attribution
           - Handle paywalls gracefully
        2. Content Processing 🔄
           - Format text in clean markdown
           - Preserve key information
           - Structure content logically
        3. Quality Control ✅
           - Verify content relevance
           - Ensure accurate extraction
           - Maintain readability\
        """),
        response_model=ScrapedArticle,
    )

    # Content Writer Agent: Crafts engaging blog posts from research
    writer: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are BlogMaster-X, an elite content creator combining journalistic excellence
        with digital marketing expertise. Your strengths include:

        - Crafting viral-worthy headlines
        - Writing engaging introductions
        - Structuring content for digital consumption
        - Incorporating research seamlessly
        - Optimizing for SEO while maintaining quality
        - Creating shareable conclusions\
        """),
        instructions=dedent("""\
        1. Content Strategy 📝
           - Craft attention-grabbing headlines
           - Write compelling introductions
           - Structure content for engagement
           - Include relevant subheadings
        2. Writing Excellence ✍️
           - Balance expertise with accessibility
           - Use clear, engaging language
           - Include relevant examples
           - Incorporate statistics naturally
        3. Source Integration 🔍
           - Cite sources properly
           - Include expert quotes
           - Maintain factual accuracy
        4. Digital Optimization 💻
           - Structure for scanability
           - Include shareable takeaways
           - Optimize for SEO
           - Add engaging subheadings\
        """),
        expected_output=dedent("""\
        # {Viral-Worthy Headline}

        ## Introduction
        {Engaging hook and context}

        ## {Compelling Section 1}
        {Key insights and analysis}
        {Expert quotes and statistics}

        ## {Engaging Section 2}
        {Deeper exploration}
        {Real-world examples}

        ## {Practical Section 3}
        {Actionable insights}
        {Expert recommendations}

        ## Key Takeaways
        - {Shareable insight 1}
        - {Practical takeaway 2}
        - {Notable finding 3}

        ## Sources
        {Properly attributed sources with links}\
        """),
        markdown=True,
    )

    def run(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> Iterator[RunResponse]:
        logger.info(f"Generating a blog post on: {topic}")

        # Use the cached blog post if use_cache is True
        if use_cached_report:
            cached_blog_post = self.get_cached_blog_post(topic)
            if cached_blog_post:
                yield WorkflowCompletedEvent(
                    run_id=self.run_id,
                    content=cached_blog_post,
                )
                return

        # Search the web for articles on the topic
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # If no search_results are found for the topic, end the workflow
        if search_results is None or len(search_results.articles) == 0:
            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content=f"Sorry, could not find any articles on the topic: {topic}",
            )
            return

        # Scrape the search results
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            topic, search_results, use_scrape_cache
        )

        # Prepare the input for the writer
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }

        # Run the writer and yield the response
        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)

        # Save the blog post in the cache
        self.add_blog_post_to_cache(topic, self.writer.run_response.content)

    def get_cached_blog_post(self, topic: str) -> Optional[str]:
        logger.info("Checking if cached blog post exists")

        return self.session_state.get("blog_posts", {}).get(topic)

    def add_blog_post_to_cache(self, topic: str, blog_post: str):
        logger.info(f"Saving blog post for topic: {topic}")
        self.session_state.setdefault("blog_posts", {})
        self.session_state["blog_posts"][topic] = blog_post

    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:
        logger.info("Checking if cached search results exist")
        search_results = self.session_state.get("search_results", {}).get(topic)
        return (
            SearchResults.model_validate(search_results)
            if search_results and isinstance(search_results, dict)
            else search_results
        )

    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):
        logger.info(f"Saving search results for topic: {topic}")
        self.session_state.setdefault("search_results", {})
        self.session_state["search_results"][topic] = search_results

    def get_cached_scraped_articles(
        self, topic: str
    ) -> Optional[Dict[str, ScrapedArticle]]:
        logger.info("Checking if cached scraped articles exist")
        scraped_articles = self.session_state.get("scraped_articles", {}).get(topic)
        return (
            ScrapedArticle.model_validate(scraped_articles)
            if scraped_articles and isinstance(scraped_articles, dict)
            else scraped_articles
        )

    def add_scraped_articles_to_cache(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ):
        logger.info(f"Saving scraped articles for topic: {topic}")
        self.session_state.setdefault("scraped_articles", {})
        self.session_state["scraped_articles"][topic] = scraped_articles

    def get_search_results(
        self, topic: str, use_search_cache: bool, num_attempts: int = 3
    ) -> Optional[SearchResults]:
        # Get cached search_results from the session state if use_search_cache is True
        if use_search_cache:
            try:
                search_results_from_cache = self.get_cached_search_results(topic)
                if search_results_from_cache is not None:
                    search_results = SearchResults.model_validate(
                        search_results_from_cache
                    )
                    logger.info(
                        f"Found {len(search_results.articles)} articles in cache."
                    )
                    return search_results
            except Exception as e:
                logger.warning(f"Could not read search results from cache: {e}")

        # If there are no cached search_results, use the searcher to find the latest articles
        for attempt in range(num_attempts):
            try:
                searcher_response: RunResponse = self.searcher.run(topic)
                if (
                    searcher_response is not None
                    and searcher_response.content is not None
                    and isinstance(searcher_response.content, SearchResults)
                ):
                    article_count = len(searcher_response.content.articles)
                    logger.info(
                        f"Found {article_count} articles on attempt {attempt + 1}"
                    )
                    # Cache the search results
                    self.add_search_results_to_cache(topic, searcher_response.content)
                    return searcher_response.content
                else:
                    logger.warning(
                        f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                    )
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

        logger.error(f"Failed to get search results after {num_attempts} attempts")
        return None

    def scrape_articles(
        self, topic: str, search_results: SearchResults, use_scrape_cache: bool
    ) -> Dict[str, ScrapedArticle]:
        scraped_articles: Dict[str, ScrapedArticle] = {}

        # Get cached scraped_articles from the session state if use_scrape_cache is True
        if use_scrape_cache:
            try:
                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)
                if scraped_articles_from_cache is not None:
                    scraped_articles = scraped_articles_from_cache
                    logger.info(
                        f"Found {len(scraped_articles)} scraped articles in cache."
                    )
                    return scraped_articles
            except Exception as e:
                logger.warning(f"Could not read scraped articles from cache: {e}")

        # Scrape the articles that are not in the cache
        for article in search_results.articles:
            if article.url in scraped_articles:
                logger.info(f"Found scraped article in cache: {article.url}")
                continue

            article_scraper_response: RunResponse = self.article_scraper.run(
                article.url
            )
            if (
                article_scraper_response is not None
                and article_scraper_response.content is not None
                and isinstance(article_scraper_response.content, ScrapedArticle)
            ):
                scraped_articles[article_scraper_response.content.url] = (
                    article_scraper_response.content
                )
                logger.info(f"Scraped article: {article_scraper_response.content.url}")

        # Save the scraped articles in the session state
        self.add_scraped_articles_to_cache(topic, scraped_articles)
        return scraped_articles


# Run the workflow if the script is executed directly
if __name__ == "__main__":
    import random

    from rich.prompt import Prompt

    # Fun example prompts to showcase the generator's versatility
    example_prompts = [
        "Why Cats Secretly Run the Internet",
        "The Science Behind Why Pizza Tastes Better at 2 AM",
        "Time Travelers' Guide to Modern Social Media",
        "How Rubber Ducks Revolutionized Software Development",
        "The Secret Society of Office Plants: A Survival Guide",
        "Why Dogs Think We're Bad at Smelling Things",
        "The Underground Economy of Coffee Shop WiFi Passwords",
        "A Historical Analysis of Dad Jokes Through the Ages",
    ]

    # Get topic from user
    topic = Prompt.ask(
        "[bold]Enter a blog post topic[/bold] (or press Enter for a random example)\n✨",
        default=random.choice(example_prompts),
    )

    # Convert the topic to a URL-safe string for use in session_id
    url_safe_topic = topic.lower().replace(" ", "-")

    # Initialize the blog post generator workflow
    # - Creates a unique session ID based on the topic
    # - Sets up SQLite storage for caching results
    generate_blog_post = BlogPostGenerator(
        session_id=f"generate-blog-post-on-{url_safe_topic}",
        storage=SqliteStorage(
            table_name="generate_blog_post_workflows",
            mode="workflow",
            auto_upgrade_schema=True,
            db_file="tmp/agno_workflows.db",
        ),
    )

    # Execute the workflow with caching enabled
    # Returns an iterator of RunResponse objects containing the generated content
    blog_post: Iterator[RunResponse] = generate_blog_post.run(
        topic=topic,
        use_search_cache=True,
        use_scrape_cache=True,
        use_cached_report=True,
    )

    # Print the response
    pprint_run_response(blog_post, markdown=True)



================================================
FILE: cookbook/workflows/employee_recruiter.py
================================================
import io
import os
from datetime import datetime
from typing import List

import requests
from agno.run.response import RunResponse
from agno.tools.zoom import ZoomTools

try:
    from pypdf import PdfReader
except ImportError:
    raise ImportError(
        "pypdf is not installed. Please install it using `pip install pypdf`"
    )
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.tools.resend import ResendTools
from agno.utils.log import logger
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


class ScreeningResult(BaseModel):
    name: str = Field(description="The name of the candidate")
    email: str = Field(description="The email of the candidate")
    score: float = Field(description="The score of the candidate from 0 to 10")
    feedback: str = Field(description="The feedback for the candidate")


class CandidateScheduledCall(BaseModel):
    name: str = Field(description="The name of the candidate")
    email: str = Field(description="The email of the candidate")
    call_time: str = Field(description="The time of the call")
    url: str = Field(description="The url of the call")


class Email(BaseModel):
    subject: str = Field(description="The subject of the email")
    body: str = Field(description="The body of the email")


current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")


class EmployeeRecruitmentWorkflow(Workflow):
    screening_agent: Agent = Agent(
        description="You are an HR agent that screens candidates for a job interview.",
        model=OpenAIChat(id="gpt-4o"),
        instructions=[
            "You are an expert HR agent that screens candidates for a job interview.",
            "You are given a candidate's name and resume and job description.",
            "You need to screen the candidate and determine if they are a good fit for the job.",
            "You need to provide a score for the candidate from 0 to 10.",
            "You need to provide a feedback for the candidate on why they are a good fit or not.",
        ],
        response_model=ScreeningResult,
    )

    interview_scheduler_agent: Agent = Agent(
        description="You are an interview scheduler agent that schedules interviews for candidates.",
        model=OpenAIChat(id="gpt-4o"),
        instructions=[
            "You are an interview scheduler agent that schedules interviews for candidates.",
            "You need to schedule interviews for the candidates using the Zoom tool.",
            "You need to schedule the interview for the candidate at the earliest possible time between 10am and 6pm.",
            "Check if the candidate and interviewer are available at the time and if the time is free in the calendar.",
            "You are in IST timezone and the current time is {current_time}. So schedule the call in future time with reference to current time.",
        ],
        tools=[
            ZoomTools(
                account_id=os.getenv("ZOOM_ACCOUNT_ID"),
                client_id=os.getenv("ZOOM_CLIENT_ID"),
                client_secret=os.getenv("ZOOM_CLIENT_SECRET"),
            )
        ],
        response_model=CandidateScheduledCall,
    )

    email_writer_agent: Agent = Agent(
        description="You are an expert email writer agent that writes emails to selected candidates.",
        model=OpenAIChat(id="gpt-4o"),
        instructions=[
            "You are an expert email writer agent that writes emails to selected candidates.",
            "You need to write an email and send it to the candidates using the Resend tool.",
            "You represent the company and the job position.",
            "You need to write an email that is concise and to the point.",
            "You need to write an email that is friendly and professional.",
            "You need to write an email that is not too long and not too short.",
            "You need to write an email that is not too formal and not too informal.",
        ],
        response_model=Email,
    )

    email_sender_agent: Agent = Agent(
        description="You are an expert email sender agent that sends emails to selected candidates.",
        model=OpenAIChat(id="gpt-4o"),
        instructions=[
            "You are an expert email sender agent that sends emails to selected candidates.",
            "You need to send an email to the candidate using the Resend tool.",
            "You will be given the email subject and body and you need to send it to the candidate.",
        ],
        tools=[ResendTools(from_email="email@agno.com")],
    )

    def extract_text_from_pdf(self, pdf_url: str) -> str:
        """Download PDF from URL and extract text content"""
        try:
            # Download PDF content
            response = requests.get(pdf_url)
            response.raise_for_status()

            # Create PDF reader object
            pdf_file = io.BytesIO(response.content)
            pdf_reader = PdfReader(pdf_file)

            # Extract text from all pages
            text_content = ""
            for page in pdf_reader.pages:
                text_content += page.extract_text()

            return text_content

        except Exception as e:
            print(f"Error processing PDF: {str(e)}")
            return ""

    def run(
        self, candidate_resume_urls: List[str], job_description: str
    ) -> RunResponse:
        selected_candidates = []

        if not candidate_resume_urls:
            raise Exception("candidate_resume_urls cannot be empty")

        for resume_url in candidate_resume_urls:
            # Extract text from PDF resume
            if resume_url in self.session_state:
                resume_content = self.session_state[resume_url]
            else:
                resume_content = self.extract_text_from_pdf(resume_url)
                self.session_state[resume_url] = resume_content
            screening_result = None

            if resume_content:
                # Screen the candidate
                input = f"Candidate resume: {resume_content}, Job description: {job_description}"
                screening_result = self.screening_agent.run(input)
                logger.info(screening_result)
            else:
                logger.error(f"Could not process resume from URL: {resume_url}")

            if (
                screening_result
                and screening_result.content
                and screening_result.content.score > 7.0
            ):
                selected_candidates.append(screening_result.content)

        for selected_candidate in selected_candidates:
            input = f"Schedule a 1hr call with Candidate name: {selected_candidate.name}, Candidate email: {selected_candidate.email} and the interviewer would be Manthan Gupts with email manthan@agno.com"
            scheduled_call = self.interview_scheduler_agent.run(input)
            logger.info(scheduled_call.content)

            if (
                scheduled_call.content
                and scheduled_call.content.url
                and scheduled_call.content.call_time
            ):
                input = f"Write an email to Candidate name: {selected_candidate.name}, Candidate email: {selected_candidate.email} for the call scheduled at {scheduled_call.content.call_time} with the url {scheduled_call.content.url} and congratulate them for the interview from John Doe designation Senior Software Engineer and email john@agno.com"
                email = self.email_writer_agent.run(input)
                logger.info(email.content)

                if email.content:
                    input = f"Send email to {selected_candidate.email} with subject {email.content.subject} and body {email.content.body}"
                    self.email_sender_agent.run(input)

        return RunResponse(
            content=f"Selected {len(selected_candidates)} candidates for the interview.",
        )


if __name__ == "__main__":
    workflow = EmployeeRecruitmentWorkflow()
    result = workflow.run(
        candidate_resume_urls=[
            # Add resume URLs here
        ],
        job_description="""
            We are hiring for backend and systems engineers!
            Join our team building the future of agentic software

            Apply if:
            🧠 You know your way around Python, typescript, docker, and AWS.
            ⚙️ Love to build in public and contribute to open source.
            🚀 Are ok dealing with the pressure of an early-stage startup.
            🏆 Want to be a part of the biggest technological shift since the internet.
            🌟 Bonus: experience with infrastructure as code.
            🌟 Bonus: starred Agno repo.
        """,
    )
    print(result.content)



================================================
FILE: cookbook/workflows/hackernews_reporter.py
================================================
"""Please install dependencies using:
pip install openai newspaper4k lxml_html_clean agno
"""

import json
from typing import Iterator

import httpx
from agno.agent import Agent, RunResponse
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class HackerNewsReporter(Workflow):
    description: str = (
        "Get the top stories from Hacker News and write a report on them."
    )

    hn_agent: Agent = Agent(
        description="Get the top stories from hackernews. "
        "Share all possible information, including url, score, title and summary if available.",
        show_tool_calls=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools()],
        description="Write an engaging report on the top stories from hackernews.",
        instructions=[
            "You will be provided with top stories and their links.",
            "Carefully read each article and think about the contents",
            "Then generate a final New York Times worthy article",
            "Break the article into sections and provide key takeaways at the end.",
            "Make sure the title is catchy and engaging.",
            "Share score, title, url and summary of every article.",
            "Give the section relevant titles and provide details/facts/processes in each section."
            "Ignore articles that you cannot read or understand.",
            "REMEMBER: you are writing for the New York Times, so the quality of the article is important.",
        ],
    )

    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:
        """Use this function to get top stories from Hacker News.

        Args:
            num_stories (int): Number of stories to return. Defaults to 10.

        Returns:
            str: JSON string of top stories.
        """

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Fetch story details
        stories = []
        for story_id in story_ids[:num_stories]:
            story_response = httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            story["username"] = story["by"]
            stories.append(story)
        return json.dumps(stories)

    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:
        # Set the tools for hn_agent here to avoid circular reference
        self.hn_agent.tools = [self.get_top_hackernews_stories]

        logger.info(f"Getting top {num_stories} stories from HackerNews.")
        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)
        if top_stories is None or not top_stories.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(top_stories.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    report: Iterator[RunResponse] = HackerNewsReporter(debug_mode=False).run(
        num_stories=5
    )
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/workflows/investment_report_generator.py
================================================
"""💰 Investment Report Generator - Your AI Financial Analysis Studio!

This advanced example demonstrates how to build a sophisticated investment analysis system that combines
market research, financial analysis, and portfolio management. The workflow uses a three-stage
approach:
1. Comprehensive stock analysis and market research
2. Investment potential evaluation and ranking
3. Strategic portfolio allocation recommendations

Key capabilities:
- Real-time market data analysis
- Professional financial research
- Investment risk assessment
- Portfolio allocation strategy
- Detailed investment rationale

Example companies to analyze:
- "AAPL, MSFT, GOOGL" (Tech Giants)
- "NVDA, AMD, INTC" (Semiconductor Leaders)
- "TSLA, F, GM" (Automotive Innovation)
- "JPM, BAC, GS" (Banking Sector)
- "AMZN, WMT, TGT" (Retail Competition)
- "PFE, JNJ, MRNA" (Healthcare Focus)
- "XOM, CVX, BP" (Energy Sector)

Run `pip install openai yfinance agno` to install dependencies.
"""

from pathlib import Path
from shutil import rmtree
from textwrap import dedent
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.storage.sqlite import SqliteStorage
from agno.tools.yfinance import YFinanceTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow

reports_dir = Path(__file__).parent.joinpath("reports", "investment")
if reports_dir.is_dir():
    rmtree(path=reports_dir, ignore_errors=True)
reports_dir.mkdir(parents=True, exist_ok=True)
stock_analyst_report = str(reports_dir.joinpath("stock_analyst_report.md"))
research_analyst_report = str(reports_dir.joinpath("research_analyst_report.md"))
investment_report = str(reports_dir.joinpath("investment_report.md"))


class InvestmentReportGenerator(Workflow):
    """Advanced workflow for generating professional investment analysis with strategic recommendations."""

    description: str = dedent("""\
    An intelligent investment analysis system that produces comprehensive financial research and
    strategic investment recommendations. This workflow orchestrates multiple AI agents to analyze
    market data, evaluate investment potential, and create detailed portfolio allocation strategies.
    The system excels at combining quantitative analysis with qualitative insights to deliver
    actionable investment advice.
    """)

    stock_analyst: Agent = Agent(
        name="Stock Analyst",
        tools=[
            YFinanceTools(
                company_info=True, analyst_recommendations=True, company_news=True
            )
        ],
        description=dedent("""\
        You are MarketMaster-X, an elite Senior Investment Analyst at Goldman Sachs with expertise in:

        - Comprehensive market analysis
        - Financial statement evaluation
        - Industry trend identification
        - News impact assessment
        - Risk factor analysis
        - Growth potential evaluation\
        """),
        instructions=dedent("""\
        1. Market Research 📊
           - Analyze company fundamentals and metrics
           - Review recent market performance
           - Evaluate competitive positioning
           - Assess industry trends and dynamics
        2. Financial Analysis 💹
           - Examine key financial ratios
           - Review analyst recommendations
           - Analyze recent news impact
           - Identify growth catalysts
        3. Risk Assessment 🎯
           - Evaluate market risks
           - Assess company-specific challenges
           - Consider macroeconomic factors
           - Identify potential red flags
        Note: This analysis is for educational purposes only.\
        """),
        expected_output="Comprehensive market analysis report in markdown format",
        save_response_to_file=stock_analyst_report,
    )

    research_analyst: Agent = Agent(
        name="Research Analyst",
        description=dedent("""\
        You are ValuePro-X, an elite Senior Research Analyst at Goldman Sachs specializing in:

        - Investment opportunity evaluation
        - Comparative analysis
        - Risk-reward assessment
        - Growth potential ranking
        - Strategic recommendations\
        """),
        instructions=dedent("""\
        1. Investment Analysis 🔍
           - Evaluate each company's potential
           - Compare relative valuations
           - Assess competitive advantages
           - Consider market positioning
        2. Risk Evaluation 📈
           - Analyze risk factors
           - Consider market conditions
           - Evaluate growth sustainability
           - Assess management capability
        3. Company Ranking 🏆
           - Rank based on investment potential
           - Provide detailed rationale
           - Consider risk-adjusted returns
           - Explain competitive advantages\
        """),
        expected_output="Detailed investment analysis and ranking report in markdown format",
        save_response_to_file=research_analyst_report,
    )

    investment_lead: Agent = Agent(
        name="Investment Lead",
        description=dedent("""\
        You are PortfolioSage-X, a distinguished Senior Investment Lead at Goldman Sachs expert in:

        - Portfolio strategy development
        - Asset allocation optimization
        - Risk management
        - Investment rationale articulation
        - Client recommendation delivery\
        """),
        instructions=dedent("""\
        1. Portfolio Strategy 💼
           - Develop allocation strategy
           - Optimize risk-reward balance
           - Consider diversification
           - Set investment timeframes
        2. Investment Rationale 📝
           - Explain allocation decisions
           - Support with analysis
           - Address potential concerns
           - Highlight growth catalysts
        3. Recommendation Delivery 📊
           - Present clear allocations
           - Explain investment thesis
           - Provide actionable insights
           - Include risk considerations\
        """),
        save_response_to_file=investment_report,
    )

    def run(self, companies: str) -> Iterator[RunResponse]:
        logger.info(f"Getting investment reports for companies: {companies}")
        initial_report: RunResponse = self.stock_analyst.run(companies)
        if initial_report is None or not initial_report.content:
            yield RunResponse(
                run_id=self.run_id,
                content="Sorry, could not get the stock analyst report.",
            )
            return

        logger.info("Ranking companies based on investment potential.")
        ranked_companies: RunResponse = self.research_analyst.run(
            initial_report.content
        )
        if ranked_companies is None or not ranked_companies.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the ranked companies."
            )
            return

        logger.info(
            "Reviewing the research report and producing an investment proposal."
        )
        yield from self.investment_lead.run(ranked_companies.content, stream=True)


# Run the workflow if the script is executed directly
if __name__ == "__main__":
    import random

    from rich.prompt import Prompt

    # Example investment scenarios to showcase the analyzer's capabilities
    example_scenarios = [
        "AAPL, MSFT, GOOGL",  # Tech Giants
        "NVDA, AMD, INTC",  # Semiconductor Leaders
        "TSLA, F, GM",  # Automotive Innovation
        "JPM, BAC, GS",  # Banking Sector
        "AMZN, WMT, TGT",  # Retail Competition
        "PFE, JNJ, MRNA",  # Healthcare Focus
        "XOM, CVX, BP",  # Energy Sector
    ]

    # Get companies from user with example suggestion
    companies = Prompt.ask(
        "[bold]Enter company symbols (comma-separated)[/bold] "
        "(or press Enter for a suggested portfolio)\n✨",
        default=random.choice(example_scenarios),
    )

    # Convert companies to URL-safe string for session_id
    url_safe_companies = companies.lower().replace(" ", "-").replace(",", "")

    # Initialize the investment analyst workflow
    investment_report_generator = InvestmentReportGenerator(
        session_id=f"investment-report-{url_safe_companies}",
        storage=SqliteStorage(
            table_name="investment_report_workflows",
            mode="workflow",
            auto_upgrade_schema=True,
            db_file="tmp/agno_workflows.db",
        ),
    )

    # Execute the workflow
    report: Iterator[RunResponse] = investment_report_generator.run(companies=companies)

    # Print the report
    pprint_run_response(report, markdown=True)



================================================
FILE: cookbook/workflows/personalized_email_generator.py
================================================
"""
🎯 B2B Email Outreach - Your Personal Sales Writing Assistant!

This workflow helps sales professionals craft highly personalized cold emails by:
1. Researching target companies through their websites
2. Analyzing their business model, tech stack, and unique attributes
3. Generating personalized email drafts
4. Sending test emails to yourself for review before actual outreach

Why is this helpful?
--------------------------------------------------------------------------------
• You always have an extra review step—emails are sent to you first.
  This ensures you can fine-tune messaging before reaching your actual prospect.
• Ideal for iterating on tone, style, and personalization en masse.

Who should use this?
--------------------------------------------------------------------------------
• SDRs, Account Executives, Business Development Managers
• Founders, Marketing Professionals, B2B Sales Representatives
• Anyone building relationships or conducting outreach at scale

Example use cases:
--------------------------------------------------------------------------------
• SaaS sales outreach
• Consulting service proposals
• Partnership opportunities
• Investor relations
• Recruitment outreach
• Event invitations

Quick Start:
--------------------------------------------------------------------------------
1. Install dependencies:
   pip install openai agno

2. Set environment variables:
   - OPENAI_API_KEY

3. Update sender_details_dict with YOUR info.

4. Add target companies to "leads" dictionary.

5. Run:
   python personalized_email_generator.py

The script will send draft emails to your email first if DEMO_MODE=False.
If DEMO_MODE=True, it prints the email to the console for review.

Then you can confidently send the refined emails to your prospects!
"""

import json
from datetime import datetime
from textwrap import dedent
from typing import Dict, Iterator, List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.exa import ExaTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunResponse, Workflow
from pydantic import BaseModel, Field

# Demo mode
# - set to True to print email to console
# - set to False to send to yourself
DEMO_MODE = True
today = datetime.now().strftime("%Y-%m-%d")

# Example leads - Replace with your actual targets
leads: Dict[str, Dict[str, str]] = {
    "Notion": {
        "name": "Notion",
        "website": "https://www.notion.so",
        "contact_name": "Ivan Zhao",
        "position": "CEO",
    },
    # Add more companies as needed
}

# Updated sender details for an AI analytics company
sender_details_dict: Dict[str, str] = {
    "name": "Sarah Chen",
    "email": "your.email@company.com",  # Your email goes here
    "organization": "Data Consultants Inc",
    "service_offered": "We help build data products and offer data consulting services",
    "calendar_link": "https://calendly.com/data-consultants-inc",
    "linkedin": "https://linkedin.com/in/your-profile",
    "phone": "+1 (555) 123-4567",
    "website": "https://www.data-consultants.com",
}

email_template = """\
Hey [RECIPIENT_NAME]

[PERSONAL_NOTE]

[PROBLEM_THEY_HAVE]

[SOLUTION_YOU_OFFER]

[SOCIAL_PROOF]

Here's my cal link if you're open to a call: [CALENDAR_LINK] ☕️

[SIGNATURE]

P.S. You can also dm me on X\
"""


class CompanyInfo(BaseModel):
    """
    Stores in-depth data about a company gathered during the research phase.
    """

    # Basic Information
    company_name: str = Field(..., description="Company name")
    website_url: str = Field(..., description="Company website URL")

    # Business Details
    industry: Optional[str] = Field(None, description="Primary industry")
    core_business: Optional[str] = Field(None, description="Main business focus")
    business_model: Optional[str] = Field(None, description="B2B, B2C, etc.")

    # Marketing Information
    motto: Optional[str] = Field(None, description="Company tagline/slogan")
    value_proposition: Optional[str] = Field(None, description="Main value proposition")
    target_audience: Optional[List[str]] = Field(
        None, description="Target customer segments"
    )

    # Company Metrics
    company_size: Optional[str] = Field(None, description="Employee count range")
    founded_year: Optional[int] = Field(None, description="Year founded")
    locations: Optional[List[str]] = Field(None, description="Office locations")

    # Technical Details
    technologies: Optional[List[str]] = Field(None, description="Technology stack")
    integrations: Optional[List[str]] = Field(None, description="Software integrations")

    # Market Position
    competitors: Optional[List[str]] = Field(None, description="Main competitors")
    unique_selling_points: Optional[List[str]] = Field(
        None, description="Key differentiators"
    )
    market_position: Optional[str] = Field(None, description="Market positioning")

    # Social Proof
    customers: Optional[List[str]] = Field(None, description="Notable customers")
    case_studies: Optional[List[str]] = Field(None, description="Success stories")
    awards: Optional[List[str]] = Field(None, description="Awards and recognition")

    # Recent Activity
    recent_news: Optional[List[str]] = Field(None, description="Recent news/updates")
    blog_topics: Optional[List[str]] = Field(None, description="Recent blog topics")

    # Pain Points & Opportunities
    challenges: Optional[List[str]] = Field(None, description="Potential pain points")
    growth_areas: Optional[List[str]] = Field(None, description="Growth opportunities")

    # Contact Information
    email_address: Optional[str] = Field(None, description="Contact email")
    phone: Optional[str] = Field(None, description="Contact phone")
    social_media: Optional[Dict[str, str]] = Field(
        None, description="Social media links"
    )

    # Additional Fields
    pricing_model: Optional[str] = Field(None, description="Pricing strategy and tiers")
    user_base: Optional[str] = Field(None, description="Estimated user base size")
    key_features: Optional[List[str]] = Field(None, description="Main product features")
    integration_ecosystem: Optional[List[str]] = Field(
        None, description="Integration partners"
    )
    funding_status: Optional[str] = Field(
        None, description="Latest funding information"
    )
    growth_metrics: Optional[Dict[str, str]] = Field(
        None, description="Key growth indicators"
    )


class PersonalisedEmailGenerator(Workflow):
    """
    Personalized email generation system that:

    1. Scrapes the target company's website
    2. Gathers essential info (tech stack, position in market, new updates)
    3. Generates a personalized cold email used for B2B outreach

    This workflow is designed to help you craft outreach that resonates
    specifically with your prospect, addressing known challenges and
    highlighting tailored solutions.
    """

    description: str = dedent("""\
        AI-Powered B2B Outreach Workflow:
        --------------------------------------------------------
        1. Research & Analyze
        2. Generate Personalized Email
        3. Send Draft to Yourself
        --------------------------------------------------------
        This creates a frictionless review layer, letting you refine each
        email before sending it to real prospects.
        Perfect for data-driven, personalized B2B outreach at scale.
    """)

    scraper: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[ExaTools()],
        description=dedent("""\
            You are an expert SaaS business analyst specializing in:

            🔍 Product Intelligence
            - Feature analysis
            - User experience evaluation
            - Integration capabilities
            - Platform scalability
            - Enterprise readiness

            📊 Market Position Analysis
            - Competitive advantages
            - Market penetration
            - Growth trajectory
            - Enterprise adoption
            - International presence

            💡 Technical Architecture
            - Infrastructure setup
            - Security standards
            - API capabilities
            - Data management
            - Compliance status

            🎯 Business Intelligence
            - Revenue model analysis
            - Customer acquisition strategy
            - Enterprise pain points
            - Scaling challenges
            - Integration opportunities\
        """),
        instructions=dedent("""\
            1. Start with the company website and analyze:
            - Homepage messaging
            - Product/service pages
            - About us section
            - Blog content
            - Case studies
            - Team pages

            2. Look for specific details about:
            - Recent company news
            - Customer testimonials
            - Technology partnerships
            - Industry awards
            - Growth indicators

            3. Identify potential pain points:
            - Scaling challenges
            - Market pressures
            - Technical limitations
            - Operational inefficiencies

            4. Focus on actionable insights that could:
            - Drive business growth
            - Improve operations
            - Enhance customer experience
            - Increase market share

            Remember: Quality over quantity. Focus on insights that could lead to meaningful business conversations.\
        """),
        response_model=CompanyInfo,
    )

    email_creator: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
            You are writing for a friendly, empathetic 20-year-old sales rep whose
            style is cool, concise, and respectful. Tone is casual yet professional.

            - Be polite but natural, using simple language.
            - Never sound robotic or use big cliché words like "delve", "synergy" or "revolutionary."
            - Clearly address problems the prospect might be facing and how we solve them.
            - Keep paragraphs short and friendly, with a natural voice.
            - End on a warm, upbeat note, showing willingness to help.\
        """),
        instructions=dedent("""\
            Please craft a highly personalized email that has:

            1. A simple, personal subject line referencing the problem or opportunity.
            2. At least one area for improvement or highlight from research.
            3. A quick explanation of how we can help them (no heavy jargon).
            4. References a known challenge from the research.
            5. Avoid words like "delve", "explore", "synergy", "amplify", "game changer", "revolutionary", "breakthrough".
            6. Use first-person language ("I") naturally.
            7. Maintain a 20-year-old’s friendly style—brief and to the point.
            8. Avoid placing the recipient's name in the subject line.

            Use the following structural template, but ensure the final tone
            feels personal and conversation-like, not automatically generated:
            ----------------------------------------------------------------------
            """)
        + "Email Template to work with:\n"
        + email_template,
        markdown=False,
        add_datetime_to_instructions=True,
    )

    def get_cached_company_data(self, company_name: str) -> Optional[CompanyInfo]:
        """Retrieve cached company research data"""
        logger.info(f"Checking cache for company data: {company_name}")
        cached_data = self.session_state.get("company_research", {}).get(company_name)
        if cached_data:
            return CompanyInfo.model_validate(cached_data)
        return None

    def cache_company_data(self, company_name: str, company_data: CompanyInfo):
        """Cache company research data"""
        logger.info(f"Caching company data for: {company_name}")
        self.session_state.setdefault("company_research", {})
        self.session_state["company_research"][company_name] = company_data.model_dump()
        self.write_to_storage()

    def get_cached_email(self, company_name: str) -> Optional[str]:
        """Retrieve cached email content"""
        logger.info(f"Checking cache for email: {company_name}")
        return self.session_state.get("generated_emails", {}).get(company_name)

    def cache_email(self, company_name: str, email_content: str):
        """Cache generated email content"""
        logger.info(f"Caching email for: {company_name}")
        self.session_state.setdefault("generated_emails", {})
        self.session_state["generated_emails"][company_name] = email_content
        self.write_to_storage()

    def run(
        self,
        use_research_cache: bool = True,
        use_email_cache: bool = True,
    ) -> Iterator[RunResponse]:
        """
        Orchestrates the entire personalized marketing workflow:

        1. Looks up or retrieves from cache the company's data.
        2. If uncached, uses the scraper agent to research the company website.
        3. Passes that data to the email_creator agent to generate a targeted email.
        4. Yields the generated email content for review or distribution.
        """
        logger.info("Starting personalized marketing workflow...")

        for company_name, company_info in leads.items():
            try:
                logger.info(f"Processing company: {company_name}")

                # Check email cache first
                if use_email_cache:
                    cached_email = self.get_cached_email(company_name)
                    if cached_email:
                        logger.info(f"Using cached email for {company_name}")
                        yield RunResponse(content=cached_email)
                        continue

                # 1. Research Phase with caching
                company_data = None
                if use_research_cache:
                    company_data = self.get_cached_company_data(company_name)
                    if company_data:
                        logger.info(f"Using cached company data for {company_name}")

                if not company_data:
                    logger.info("Starting company research...")
                    scraper_response = self.scraper.run(
                        json.dumps(company_info, indent=4)
                    )

                    if not scraper_response or not scraper_response.content:
                        logger.warning(
                            f"No data returned for {company_name}. Skipping."
                        )
                        continue

                    company_data = scraper_response.content
                    if not isinstance(company_data, CompanyInfo):
                        logger.error(
                            f"Invalid data format for {company_name}. Skipping."
                        )
                        continue

                    # Cache the research results
                    self.cache_company_data(company_name, company_data)

                # 2. Generate email
                logger.info("Generating personalized email...")
                email_context = json.dumps(
                    {
                        "contact_name": company_info.get(
                            "contact_name", "Decision Maker"
                        ),
                        "position": company_info.get("position", "Leader"),
                        "company_info": company_data.model_dump(),
                        "recipient_email": sender_details_dict["email"],
                        "sender_details": sender_details_dict,
                    },
                    indent=4,
                )
                yield from self.email_creator.run(
                    f"Generate a personalized email using this context:\n{email_context}",
                    stream=True,
                )

                # Cache the generated email content
                self.cache_email(company_name, self.email_creator.run_response.content)

                # Obtain final email content:
                email_content = self.email_creator.run_response.content

                # 3. If not in demo mode, you'd handle sending the email here.
                #    Implementation details omitted.
                if not DEMO_MODE:
                    logger.info(
                        "Production mode: Attempting to send email to yourself..."
                    )
                    # Implementation for sending the email goes here.

            except Exception as e:
                logger.error(f"Error processing {company_name}: {e}")
                raise


def main():
    """
    Main entry point for running the personalized email generator workflow.
    """
    try:
        # Create workflow with SQLite storage
        workflow = PersonalisedEmailGenerator(
            session_id="personalized-email-generator",
            storage=SqliteStorage(
                table_name="personalized_email_workflows",
                mode="workflow",
                auto_upgrade_schema=True,
                db_file="tmp/agno_workflows.db",
            ),
        )

        # Run workflow with caching
        responses = workflow.run(
            use_research_cache=True,
            use_email_cache=False,
        )

        # Process and pretty-print responses
        pprint_run_response(responses, markdown=True)

        logger.info("Workflow completed successfully!")
    except Exception as e:
        logger.error(f"Workflow failed: {e}")
        raise


if __name__ == "__main__":
    main()



================================================
FILE: cookbook/workflows/reddit_post_generator.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reddit import RedditTools

web_searcher = Agent(
    name="Web Searcher",
    role="Searches the web for information on a topic",
    model=OpenAIChat(id="gpt-4o"),
    description="An intelligent agent that performs comprehensive web searches to gather current and accurate information",
    tools=[DuckDuckGoTools()],
    instructions=[
        "1. Perform focused web searches using relevant keywords",
        "2. Filter results for credibility and recency",
        "3. Extract key information and main points",
        "4. Organize information in a logical structure",
        "5. Verify facts from multiple sources when possible",
        "6. Focus on authoritative and reliable sources",
    ],
)

reddit_agent = Agent(
    name="Reddit Agent",
    role="Uploads post on Reddit",
    model=OpenAIChat(id="gpt-4o"),
    description="Specialized agent for crafting and publishing engaging Reddit posts",
    tools=[RedditTools()],
    instructions=[
        "1. Get information regarding the subreddit",
        "2. Create attention-grabbing yet accurate titles",
        "3. Format posts using proper Reddit markdown",
        "4. Avoid including links ",
        "5. Follow subreddit-specific rules and guidelines",
        "6. Structure content for maximum readability",
        "7. Add appropriate tags and flairs if required",
    ],
    show_tool_calls=True,
)

post_team = Team(
    members=[web_searcher, reddit_agent],
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Work together to create engaging and informative Reddit posts",
        "Start by researching the topic thoroughly using web searches",
        "Craft a well-structured post with accurate information and sources",
        "Follow Reddit guidelines and best practices for posting",
    ],
    show_tool_calls=True,
    markdown=True,
    success_criteria="A post on Reddit with a title and content that is engaging and informative in the subreddit.",
    enable_agentic_context=True,
)

post_team.print_response(
    "Create a post on web technologies and frameworks to focus in 2025 on the subreddit r/webdev ",
    stream=True,
)



================================================
FILE: cookbook/workflows/self_evaluating_content_creator.py
================================================
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.run.response import RunResponse
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


class Feedback(BaseModel):
    content: str = Field(description="The content that you need to give feedback on")
    feedback: str = Field(description="The feedback on the content")
    score: int = Field(description="The score of the content from 0 to 10")


class SelfEvaluationWorkflow(Workflow):
    description: str = "Self Evaluation Workflow"

    content_creator_agent: Agent = Agent(
        name="Content Creator",
        description="Content Creator Agent",
        instructions=[
            "You are a content creator intern that creates content for LinkedIn that have no experience in creating content. So you make a lot of mistakes.",
            "You are given a task and you need to create content for LinkedIn.",
            "You need to create content that is engaging and interesting.",
            "You need to create content that is relevant to the task.",
            "You do an ok job at creating content, but you need to improve your content based on the feedback.",
        ],
        model=OpenAIChat(id="gpt-4o"),
        debug_mode=True,
    )

    content_reviewer_agent: Agent = Agent(
        name="Content Reviewer",
        description="Content Reviewer Agent",
        instructions=[
            "You are a senior content reviewer agent that reviews content for LinkedIn and have a lot of experience in creating content.",
            "You are given a content and you need to review content for LinkedIn.",
            "You need to make sure the content is not too long and not too short.",
            "You need to make sure the content doesn't have any spelling or grammar mistakes.",
            "You need to make sure the content doesn't have a lot of emojis.",
            "You need to make sure the content is not too promotional and not too salesy.",
            "You need to make sure the content is not too technical and not too complex.",
        ],
        response_model=Feedback,
        model=OpenAIChat(id="gpt-4o"),
        debug_mode=True,
    )

    def run(self, topic: str) -> RunResponse:
        content_response = self.content_creator_agent.run(topic)
        max_tries = 3
        for _ in range(max_tries):
            feedback = self.content_reviewer_agent.run(content_response.content)
            if feedback.content and feedback.content.score > 8:
                break
            content_feedback_input = f"Here is the feedback: {feedback.content.feedback if feedback.content else ''} for your content {content_response.content if content_response.content else ''}. \nPlease improve the content based on the feedback."
            content_response = self.content_creator_agent.run(content_feedback_input)
        return content_response


if __name__ == "__main__":
    self_evaluation_workflow = SelfEvaluationWorkflow()
    response = self_evaluation_workflow.run(
        topic="create a post about the latest trends in AI"
    )
    pprint_run_response(response)



================================================
FILE: cookbook/workflows/simple_cache_workflow.py
================================================
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class CacheWorkflow(Workflow):
    description: str = "A workflow that caches previous outputs"

    agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))

    def run(self, message: str) -> Iterator[RunResponse]:
        logger.info(f"Checking cache for '{message}'")
        if self.session_state.get(message):
            logger.info(f"Cache hit for '{message}'")
            yield RunResponse(
                run_id=self.run_id, content=self.session_state.get(message)
            )
            return

        logger.info(f"Cache miss for '{message}'")
        yield from self.agent.run(message, stream=True)
        self.session_state[message] = self.agent.run_response.content


if __name__ == "__main__":
    workflow = CacheWorkflow()
    # Run workflow
    response: Iterator[RunResponse] = workflow.run(message="Tell me a joke.")
    # Print the response
    pprint_run_response(response, markdown=True, show_time=True)
    # Run workflow again
    response: Iterator[RunResponse] = workflow.run(message="Tell me a joke.")
    # Print the response
    pprint_run_response(response, markdown=True, show_time=True)



================================================
FILE: cookbook/workflows/startup_idea_validator.py
================================================
"""
🚀 Startup Idea Validator - Your Personal Business Validation Assistant!

This workflow helps entrepreneurs validate their startup ideas by:
1. Clarifying and refining the core business concept
2. Evaluating originality compared to existing solutions
3. Defining clear mission and objectives
4. Conducting comprehensive market research and analysis

Why is this helpful?
--------------------------------------------------------------------------------
• Get objective feedback on your startup idea before investing resources
• Understand your total addressable market and target segments
• Validate assumptions about market opportunity and competition
• Define clear mission and objectives to guide execution

Who should use this?
--------------------------------------------------------------------------------
• Entrepreneurs and Startup Founders
• Product Managers and Business Strategists
• Innovation Teams
• Angel Investors and VCs doing initial screening

Example use cases:
--------------------------------------------------------------------------------
• New product/service validation
• Market opportunity assessment
• Competitive analysis
• Business model validation
• Target customer segmentation
• Mission/vision refinement

Quick Start:
--------------------------------------------------------------------------------
1. Install dependencies:
   pip install openai agno

2. Set environment variables:
   - OPENAI_API_KEY

3. Run:
   python startup_idea_validator.py

The workflow will guide you through validating your startup idea with AI-powered
analysis and research. Use the insights to refine your concept and business plan!
"""

import json
from typing import Iterator, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowCompletedEvent
from agno.storage.sqlite import SqliteStorage
from agno.tools.googlesearch import GoogleSearchTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunEvent, RunResponse, Workflow
from pydantic import BaseModel, Field


class IdeaClarification(BaseModel):
    originality: str = Field(..., description="Originality of the idea.")
    mission: str = Field(..., description="Mission of the company.")
    objectives: str = Field(..., description="Objectives of the company.")


class MarketResearch(BaseModel):
    total_addressable_market: str = Field(
        ..., description="Total addressable market (TAM)."
    )
    serviceable_available_market: str = Field(
        ..., description="Serviceable available market (SAM)."
    )
    serviceable_obtainable_market: str = Field(
        ..., description="Serviceable obtainable market (SOM)."
    )
    target_customer_segments: str = Field(..., description="Target customer segments.")


class StartupIdeaValidator(Workflow):
    idea_clarifier_agent: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        instructions=[
            "Given a user's startup idea, its your goal to refine that idea. ",
            "Evaluates the originality of the idea by comparing it with existing concepts. ",
            "Define the mission and objectives of the startup.",
        ],
        add_history_to_messages=True,
        add_datetime_to_instructions=True,
        response_model=IdeaClarification,
        debug_mode=False,
    )

    market_research_agent: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[GoogleSearchTools()],
        instructions=[
            "You are provided with a startup idea and the company's mission and objectives. ",
            "Estimate the total addressable market (TAM), serviceable available market (SAM), and serviceable obtainable market (SOM). ",
            "Define target customer segments and their characteristics. ",
            "Search the web for resources if you need to.",
        ],
        add_history_to_messages=True,
        add_datetime_to_instructions=True,
        response_model=MarketResearch,
    )

    competitor_analysis_agent: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[GoogleSearchTools()],
        instructions=[
            "You are provided with a startup idea and some market research related to the idea. ",
            "Identify existing competitors in the market. ",
            "Perform Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis for each competitor. ",
            "Assess the startup’s potential positioning relative to competitors.",
        ],
        add_history_to_messages=True,
        add_datetime_to_instructions=True,
        markdown=True,
        debug_mode=False,
    )

    report_agent: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        instructions=[
            "You are provided with a startup idea and other data about the idea. ",
            "Summarise everything into a single report.",
        ],
        add_history_to_messages=True,
        add_datetime_to_instructions=True,
        markdown=True,
        debug_mode=False,
    )

    def get_idea_clarification(self, startup_idea: str) -> Optional[IdeaClarification]:
        try:
            response: RunResponse = self.idea_clarifier_agent.run(startup_idea)

            # Check if we got a valid response
            if not response or not response.content:
                logger.warning("Empty Idea Clarification response")
            # Check if the response is of the expected type
            if not isinstance(response.content, IdeaClarification):
                logger.warning("Invalid response type")

            return response.content

        except Exception as e:
            logger.warning(f"Failed: {str(e)}")

        return None

    def get_market_research(
        self, startup_idea: str, idea_clarification: IdeaClarification
    ) -> Optional[MarketResearch]:
        agent_input = {"startup_idea": startup_idea, **idea_clarification.model_dump()}

        try:
            response: RunResponse = self.market_research_agent.run(
                json.dumps(agent_input, indent=4)
            )

            # Check if we got a valid response
            if not response or not response.content:
                logger.warning("Empty Market Research response")

            # Check if the response is of the expected type
            if not isinstance(response.content, MarketResearch):
                logger.warning("Invalid response type")

            return response.content

        except Exception as e:
            logger.warning(f"Failed: {str(e)}")

        return None

    def get_competitor_analysis(
        self, startup_idea: str, market_research: MarketResearch
    ) -> Optional[str]:
        agent_input = {"startup_idea": startup_idea, **market_research.model_dump()}

        try:
            response: RunResponse = self.competitor_analysis_agent.run(
                json.dumps(agent_input, indent=4)
            )

            # Check if we got a valid response
            if not response or not response.content:
                logger.warning("Empty Competitor Analysis response")

            return response.content

        except Exception as e:
            logger.warning(f"Failed: {str(e)}")

        return None

    def run(self, startup_idea: str) -> Iterator[RunResponse]:
        logger.info(f"Generating a startup validation report for: {startup_idea}")

        # Clarify and quantify the idea
        idea_clarification: Optional[IdeaClarification] = self.get_idea_clarification(
            startup_idea
        )

        if idea_clarification is None:
            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content=f"Sorry, could not even clarify the idea: {startup_idea}",
            )
            return

        # Do some market research
        market_research: Optional[MarketResearch] = self.get_market_research(
            startup_idea, idea_clarification
        )

        if market_research is None:
            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content="Market research failed",
            )
            return

        competitor_analysis: Optional[str] = self.get_competitor_analysis(
            startup_idea, market_research
        )

        # Compile the final report
        final_response: RunResponse = self.report_agent.run(
            json.dumps(
                {
                    "startup_idea": startup_idea,
                    **idea_clarification.model_dump(),
                    **market_research.model_dump(),
                    "competitor_analysis_report": competitor_analysis,
                },
                indent=4,
            )
        )

        yield WorkflowCompletedEvent(run_id=self.run_id, content=final_response.content)


# Run the workflow if the script is executed directly
if __name__ == "__main__":
    from rich.prompt import Prompt

    # Get idea from user
    idea = Prompt.ask(
        "[bold]What is your startup idea?[/bold]\n✨",
        default="A marketplace for Christmas Ornaments made from leather",
    )

    # Convert the idea to a URL-safe string for use in session_id
    url_safe_idea = idea.lower().replace(" ", "-")

    startup_idea_validator = StartupIdeaValidator(
        description="Startup Idea Validator",
        session_id=f"validate-startup-idea-{url_safe_idea}",
        storage=SqliteStorage(
            table_name="validate_startup_ideas_workflow",
            mode="workflow",
            auto_upgrade_schema=True,
            db_file="tmp/agno_workflows.db",
        ),
    )

    final_report: Iterator[RunResponse] = startup_idea_validator.run(startup_idea=idea)

    pprint_run_response(final_report, markdown=True)



================================================
FILE: cookbook/workflows/team_workflow.py
================================================
from textwrap import dedent
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.run.team import TeamRunResponse
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class TeamWorkflow(Workflow):
    description: str = (
        "Get the top stories from Hacker News and Reddit and write a report on them."
    )

    reddit_researcher = Agent(
        name="Reddit Researcher",
        role="Research a topic on Reddit",
        model=OpenAIChat(id="gpt-4o"),
        tools=[DuckDuckGoTools(cache_results=True)],
        add_name_to_instructions=True,
        instructions=dedent("""
            You are a Reddit researcher.
            You will be given a topic to research on Reddit.
            You will need to find the most relevant posts on Reddit.
        """),
    )

    hackernews_researcher = Agent(
        name="HackerNews Researcher",
        model=OpenAIChat("gpt-4o"),
        role="Research a topic on HackerNews.",
        tools=[HackerNewsTools()],
        add_name_to_instructions=True,
        instructions=dedent("""
            You are a HackerNews researcher.
            You will be given a topic to research on HackerNews.
            You will need to find the most relevant posts on HackerNews.
        """),
    )

    agent_team = Team(
        name="Discussion Team",
        mode="collaborate",
        model=OpenAIChat("gpt-4o"),
        members=[
            reddit_researcher,
            hackernews_researcher,
        ],
        instructions=[
            "You are a discussion coordinator.",
            "Your primary role is to facilitate the research process.",
            "Once both team members have provided their research results with links to top stories from their respective platforms (Reddit and HackerNews), you should stop the discussion.",
            "Do not continue the discussion after receiving the links - your goal is to collect the research results, not to reach a consensus on content.",
            "Ensure each member provides relevant links with brief descriptions before concluding.",
        ],
        success_criteria="The team has reached a consensus.",
        enable_agentic_context=True,
        show_tool_calls=True,
        markdown=True,
        show_members_responses=True,
    )

    writer: Agent = Agent(
        tools=[Newspaper4kTools(), ExaTools()],
        description="Write an engaging report on the top stories from various sources.",
        instructions=[
            "You will receive links to top stories from Reddit and HackerNews from the agent team.",
            "Your task is to access these links and thoroughly read each article.",
            "Extract key information, insights, and notable points from each source.",
            "Write a comprehensive, well-structured report that synthesizes the information.",
            "Create a catchy and engaging title for your report.",
            "Organize the content into relevant sections with descriptive headings.",
            "For each article, include its source, title, URL, and a brief summary.",
            "Provide detailed analysis and context for the most important stories.",
            "End with key takeaways that summarize the main insights.",
            "Maintain a professional tone similar to New York Times reporting.",
            "If you cannot access or understand certain articles, note this and focus on the ones you can analyze.",
        ],
    )

    def run(self) -> Iterator[RunResponse]:
        logger.info("Getting top stories from HackerNews.")
        discussion: TeamRunResponse = self.agent_team.run(
            "Getting 2 top stories from HackerNews and reddit and write a brief report on them"
        )
        if discussion is None or not discussion.content:
            yield RunResponse(
                run_id=self.run_id, content="Sorry, could not get the top stories."
            )
            return

        logger.info("Reading each story and writing a report.")
        yield from self.writer.run(discussion.content, stream=True)


if __name__ == "__main__":
    # Run workflow
    report: Iterator[RunResponse] = TeamWorkflow(debug_mode=False).run()
    # Print the report
    pprint_run_response(report, markdown=True, show_time=True)



================================================
FILE: cookbook/workflows/workflows_playground.py
================================================
"""
1. Install dependencies using: `pip install openai ddgs sqlalchemy 'fastapi[standard]' newspaper4k lxml_html_clean yfinance agno`
2. Run the script using: `python cookbook/workflows/workflows_playground.py`
"""

from agno.playground import Playground
from agno.storage.sqlite import SqliteStorage

# Import the workflows
from blog_post_generator import BlogPostGenerator
from investment_report_generator import (
    InvestmentReportGenerator,
)
from personalized_email_generator import PersonalisedEmailGenerator
from startup_idea_validator import StartupIdeaValidator

# Initialize the workflows with SQLite storage

blog_post_generator = BlogPostGenerator(
    workflow_id="generate-blog-post",
    storage=SqliteStorage(
        table_name="generate_blog_post_workflows",
        db_file="tmp/agno_workflows.db",
        mode="workflow",
        auto_upgrade_schema=True,
    ),
)
personalised_email_generator = PersonalisedEmailGenerator(
    workflow_id="personalized-email-generator",
    storage=SqliteStorage(
        table_name="personalized_email_workflows",
        db_file="tmp/agno_workflows.db",
        mode="workflow",
        auto_upgrade_schema=True,
    ),
)

investment_report_generator = InvestmentReportGenerator(
    workflow_id="generate-investment-report",
    storage=SqliteStorage(
        table_name="investment_report_workflows",
        db_file="tmp/agno_workflows.db",
        mode="workflow",
        auto_upgrade_schema=True,
    ),
)

startup_idea_validator = StartupIdeaValidator(
    workflow_id="validate-startup-idea",
    storage=SqliteStorage(
        table_name="validate_startup_ideas_workflow",
        db_file="tmp/agno_workflows.db",
        mode="workflow",
        auto_upgrade_schema=True,
    ),
)

# Initialize the Playground with the workflows
playground = Playground(
    workflows=[
        blog_post_generator,
        personalised_email_generator,
        investment_report_generator,
        startup_idea_validator,
    ],
    app_id="workflows-playground-app",
    name="Workflows Playground",
)
app = playground.get_app(use_async=False)

if __name__ == "__main__":
    # Start the playground server
    playground.serve(
        app="workflows_playground:app",
        host="localhost",
        port=7777,
        reload=True,
    )



================================================
FILE: cookbook/workflows/content_creator/readme.md
================================================
# Content Creator Agent Workflow

The Content Creator Agent Workflow is a multi-agent workflow designed to streamline the process of generating and managing social media content. It assists content creators in planning, creating, and scheduling posts across various platforms.

## Key Features

- **Scraping Blog Posts:** Scrape a blog post and convert it to understandable draft.

- **Automated Content Generation:** Draft engaging posts tailored to your audience.

- **Scheduling and Management:** Allows for efficient scheduling of posts, ensuring a consistent online presence.

- **Platform Integration:** Supports multiple social media platforms for broad outreach (Linkedin and X).

## Getting Started

1. **Clone the Repository:**

   ```bash
   git clone https://github.com/agno-agi/agno.git
   ```

2. **Navigate to the Workflow Directory:**

   ```bash
   cd agno/workflows/content-creator-workflow
   ```

3. **Create Virtual Environment**

  ```bash
  python3 -m venv ~/.venvs/aienv
  source ~/.venvs/aienv/bin/activate
  ```

4. **Install Dependencies:**

   Ensure you have Python installed, then run:

   ```bash
   pip install -r requirements.txt
   ```

5. **Set the Environment Variables**

    ```bash
    export OPENAI_API_KEY="your_openai_api_key_here"
    export FIRECRAWL_API_KEY="your_firecrawl_api_key_here"
    export TYPEFULLY_API_KEY="your_typefully_api_key_here"
    ```

    These keys are used to authenticate requests to the respective APIs.

6. **Configure the Workflow**

    The `config.py` file is used to centralize configurations for your project. It includes:

    - **API Configuration**:
        - Defines the base URLs and headers required for API requests, with keys loaded from the `.env` file.
    - **Enums**:
        - `PostType`: Defines the type of social media posts, such as `TWITTER` or `LINKEDIN`.

    Update the `.env` file with your API keys and customize the enums in `config.py` if additional blog URLs or post types are needed.


7. **Run the Workflow:**

   Execute the main script to start the content creation process:

   ```bash
   python workflow.py
   ```

## Customization

The workflow is designed to be flexible. You can adjust the model provider parameters, content templates, and scheduling settings within the configuration files to better suit your needs.



================================================
FILE: cookbook/workflows/content_creator/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows/content_creator/config.py
================================================
import os
from enum import Enum

from dotenv import load_dotenv

load_dotenv()


TYPEFULLY_API_URL = "https://api.typefully.com/v1/drafts/"
TYPEFULLY_API_KEY = os.getenv("TYPEFULLY_API_KEY")
HEADERS = {"X-API-KEY": f"Bearer {TYPEFULLY_API_KEY}"}


# Define the enums
class PostType(Enum):
    TWITTER = "Twitter"
    LINKEDIN = "LinkedIn"



================================================
FILE: cookbook/workflows/content_creator/prompts.py
================================================
# Planner Agents Configuration
agents_config = {
    "blog_analyzer": {
        "role": "Blog Analyzer",
        "goal": "Analyze blog and identify key ideas, sections, and technical concepts",
        "backstory": (
            "You are a technical writer with years of experience writing, editing, and reviewing technical blogs. "
            "You have a talent for understanding and documenting technical concepts.\n\n"
        ),
        "verbose": False,
    },
    "twitter_thread_planner": {
        "role": "Twitter Thread Planner",
        "goal": "Create a Twitter thread plan based on the provided blog analysis",
        "backstory": (
            "You are a technical writer with years of experience in converting long technical blogs into Twitter threads. "
            "You have a talent for breaking longform content into bite-sized tweets that are engaging and informative. "
            "And identify relevant URLs to media that can be associated with a tweet.\n\n"
        ),
        "verbose": False,
    },
    "linkedin_post_planner": {
        "role": "LinkedIn Post Planner",
        "goal": "Create an engaging LinkedIn post based on the provided blog analysis",
        "backstory": (
            "You are a technical writer with extensive experience crafting technical LinkedIn content. "
            "You excel at distilling technical concepts into clear, authoritative posts that resonate with a professional audience "
            "while maintaining technical accuracy. You know how to balance technical depth with accessibility and incorporate "
            "relevant hashtags and mentions to maximize engagement.\n\n"
        ),
        "verbose": False,
    },
}

# Planner Tasks Configuration
tasks_config = {
    "analyze_blog": {
        "description": (
            "Analyze the markdown file at {blog_path} to create a developer-focused technical overview\n\n"
            "1. Map out the core idea that the blog discusses\n"
            "2. Identify key sections and what each section is about\n"
            "3. For each section, extract all URLs that appear inside image markdown syntax ![](image_url)\n"
            "4. You must associate these identified image URLs to their corresponding sections, so that we can use them with the tweets as media pieces\n\n"
            "Focus on details that are important for a comprehensive understanding of the blog.\n\n"
        ),
        "expected_output": (
            "A technical analysis containing:\n"
            "- Blog title and core concept/idea\n"
            "- Key technical sections identified with their main points\n"
            "- Important code examples or technical concepts covered\n"
            "- Key takeaways for developers\n"
            "- Relevant URLs to media that are associated with the key sections and can be associated with a tweet, this must be done.\n\n"
        ),
    },
    "create_twitter_thread_plan": {
        "description": (
            "Develop an engaging Twitter thread based on the blog analysis provided and closely follow the writing style provided in the {path_to_example_threads}\n\n"
            "The thread should break down complex technical concepts into digestible, tweet-sized chunks "
            "that maintain technical accuracy while being accessible.\n\n"
            "Plan should include:\n"
            "- A strong hook tweet that captures attention, it should be under 10 words, it must be the same as the title of the blog\n"
            "- Logical flow from basic to advanced concepts\n"
            "- Code snippets or key technical highlights that fit Twitter's format\n"
            "- Relevant URLs to media that are associated with the key sections and must be associated with their corresponding tweets\n"
            "- Clear takeaways for engineering audience\n\n"
            "Make sure to cover:\n"
            "- The core problem being solved\n"
            "- Key technical innovations or approaches\n"
            "- Interesting implementation details\n"
            "- Real-world applications or benefits\n"
            "- Call to action for the conclusion\n"
            "- Add relevant URLs to each tweet that can be associated with a tweet\n\n"
            "Focus on creating a narrative that technical audiences will find valuable "
            "while keeping each tweet concise, accessible, and impactful.\n\n"
        ),
        "expected_output": (
            "A Twitter thread with a list of tweets, where each tweet has the following:\n"
            "- content\n"
            "- URLs to media that are associated with the tweet, whenever possible\n"
            "- is_hook: true if the tweet is a hook tweet, false otherwise\n\n"
        ),
    },
    "create_linkedin_post_plan": {
        "description": (
            "Develop a comprehensive LinkedIn post based on the blog analysis provided\n\n"
            "The post should present technical content in a professional, long-form format "
            "while maintaining engagement and readability.\n\n"
            "Plan should include:\n"
            "- An attention-grabbing opening statement, it should be the same as the title of the blog\n"
            "- Well-structured body that breaks down the technical content\n"
            "- Professional tone suitable for LinkedIn's business audience\n"
            "- One main blog URL placed strategically at the end of the post\n"
            "- Strategic use of line breaks and formatting\n"
            "- Relevant hashtags (3-5 maximum)\n\n"
            "Make sure to cover:\n"
            "- The core technical problem and its business impact\n"
            "- Key solutions and technical approaches\n"
            "- Real-world applications and benefits\n"
            "- Professional insights or lessons learned\n"
            "- Clear call to action\n\n"
            "Focus on creating content that resonates with both technical professionals "
            "and business leaders while maintaining technical accuracy.\n\n"
        ),
        "expected_output": (
            "A LinkedIn post plan containing:\n- content\n- a main blog URL that is associated with the post\n\n"
        ),
    },
}



================================================
FILE: cookbook/workflows/content_creator/requirements.txt
================================================
agno
firecrawl-py
openai
packaging
requests
typing
pydantic
python-dotenv
requests



================================================
FILE: cookbook/workflows/content_creator/scheduler.py
================================================
import datetime
from typing import Any, Dict, Optional

import requests
from agno.utils.log import logger
from dotenv import load_dotenv
from pydantic import BaseModel

from cookbook.workflows.content_creator.config import (
    HEADERS,
    TYPEFULLY_API_URL,
    PostType,
)

load_dotenv()


def json_to_typefully_content(thread_json: Dict[str, Any]) -> str:
    """Convert JSON thread format to Typefully's format with 4 newlines between tweets."""
    tweets = thread_json["tweets"]
    formatted_tweets = []
    for tweet in tweets:
        tweet_text = tweet["content"]
        if "media_urls" in tweet and tweet["media_urls"]:
            tweet_text += f"\n{tweet['media_urls'][0]}"
        formatted_tweets.append(tweet_text)

    return "\n\n\n\n".join(formatted_tweets)


def json_to_linkedin_content(thread_json: Dict[str, Any]) -> str:
    """Convert JSON thread format to Typefully's format."""
    content = thread_json["content"]
    if "url" in thread_json and thread_json["url"]:
        content += f"\n{thread_json['url']}"
    return content


def schedule_thread(
    content: str,
    schedule_date: str = "next-free-slot",
    threadify: bool = False,
    share: bool = False,
    auto_retweet_enabled: bool = False,
    auto_plug_enabled: bool = False,
) -> Optional[Dict[str, Any]]:
    """Schedule a thread on Typefully."""
    payload = {
        "content": content,
        "schedule-date": schedule_date,
        "threadify": threadify,
        "share": share,
        "auto_retweet_enabled": auto_retweet_enabled,
        "auto_plug_enabled": auto_plug_enabled,
    }

    payload = {key: value for key, value in payload.items() if value is not None}

    try:
        response = requests.post(TYPEFULLY_API_URL, json=payload, headers=HEADERS)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"Error: {e}")
        return None


def schedule(
    thread_model: BaseModel,
    hours_from_now: int = 1,
    threadify: bool = False,
    share: bool = True,
    post_type: PostType = PostType.TWITTER,
) -> Optional[Dict[str, Any]]:
    """
    Schedule a thread from a Pydantic model.

    Args:
        thread_model: Pydantic model containing thread data
        hours_from_now: Hours from now to schedule the thread (default: 1)
        threadify: Whether to let Typefully split the content (default: False)
        share: Whether to get a share URL in response (default: True)

    Returns:
        API response dictionary or None if failed
    """
    try:
        thread_content = ""
        # Convert Pydantic model to dict
        thread_json = thread_model.model_dump()
        logger.info("######## Thread JSON: ", thread_json)
        # Convert to Typefully format
        if post_type == PostType.TWITTER:
            thread_content = json_to_typefully_content(thread_json)
        elif post_type == PostType.LINKEDIN:
            thread_content = json_to_linkedin_content(thread_json)

        # Calculate schedule time
        schedule_date = (
            datetime.datetime.utcnow() + datetime.timedelta(hours=hours_from_now)
        ).isoformat() + "Z"

        if thread_content:
            # Schedule the thread
            response = schedule_thread(
                content=thread_content,
                schedule_date=schedule_date,
                threadify=threadify,
                share=share,
            )

            if response:
                logger.info("Thread scheduled successfully!")
                return response
            else:
                logger.error("Failed to schedule the thread.")
                return None
        return None

    except Exception as e:
        logger.error(f"Error: {str(e)}")
        return None



================================================
FILE: cookbook/workflows/content_creator/workflow.py
================================================
import json
from typing import List, Optional

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.tools.firecrawl import FirecrawlTools
from agno.utils.log import logger
from agno.workflow import Workflow
from dotenv import load_dotenv
from pydantic import BaseModel, Field

from cookbook.workflows.content_creator.config import PostType
from cookbook.workflows.content_creator.prompts import (
    agents_config,
    tasks_config,
)
from cookbook.workflows.content_creator_workflow.scheduler import schedule

# Load environment variables
load_dotenv()


# Define Pydantic models to structure responses
class BlogAnalyzer(BaseModel):
    """
    Represents the response from the Blog Analyzer agent.
    Includes the blog title and content in Markdown format.
    """

    title: str
    blog_content_markdown: str


class Tweet(BaseModel):
    """
    Represents an individual tweet within a Twitter thread.
    """

    content: str
    is_hook: bool = Field(
        default=False, description="Marks if this tweet is the 'hook' (first tweet)"
    )
    media_urls: Optional[List[str]] = Field(
        default_factory=list, description="Associated media URLs, if any"
    )  # type: ignore


class Thread(BaseModel):
    """
    Represents a complete Twitter thread containing multiple tweets.
    """

    topic: str
    tweets: List[Tweet]


class LinkedInPost(BaseModel):
    """
    Represents a LinkedIn post.
    """

    content: str
    media_url: Optional[List[str]] = None  # Optional media attachment URL


class ContentPlanningWorkflow(Workflow):
    """
    This workflow automates the process of:
    1. Scraping a blog post using the Blog Analyzer agent.
    2. Generating a content plan for either Twitter or LinkedIn based on the scraped content.
    3. Scheduling and publishing the planned content.
    """

    # This description is used only in workflow UI
    description: str = (
        "Plan, schedule, and publish social media content based on a blog post."
    )

    # Blog Analyzer Agent: Extracts blog content (title, sections) and converts it into Markdown format for further use.
    blog_analyzer: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[
            FirecrawlTools(scrape=True, crawl=False)
        ],  # Enables blog scraping capabilities
        description=f"{agents_config['blog_analyzer']['role']} - {agents_config['blog_analyzer']['goal']}",
        instructions=[
            f"{agents_config['blog_analyzer']['backstory']}",
            tasks_config["analyze_blog"][
                "description"
            ],  # Task-specific instructions for blog analysis
        ],
        response_model=BlogAnalyzer,  # Expects response to follow the BlogAnalyzer Pydantic model
    )

    # Twitter Thread Planner: Creates a Twitter thread from the blog content, each tweet is concise, engaging,
    # and logically connected with relevant media.
    twitter_thread_planner: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=f"{agents_config['twitter_thread_planner']['role']} - {agents_config['twitter_thread_planner']['goal']}",
        instructions=[
            f"{agents_config['twitter_thread_planner']['backstory']}",
            tasks_config["create_twitter_thread_plan"]["description"],
        ],
        response_model=Thread,  # Expects response to follow the Thread Pydantic model
    )

    # LinkedIn Post Planner: Converts blog content into a structured LinkedIn post, optimized for a professional
    # audience with relevant hashtags.
    linkedin_post_planner: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=f"{agents_config['linkedin_post_planner']['role']} - {agents_config['linkedin_post_planner']['goal']}",
        instructions=[
            f"{agents_config['linkedin_post_planner']['backstory']}",
            tasks_config["create_linkedin_post_plan"]["description"],
        ],
        response_model=LinkedInPost,  # Expects response to follow the LinkedInPost Pydantic model
    )

    def scrape_blog_post(self, blog_post_url: str, use_cache: bool = True):
        if use_cache and blog_post_url in self.session_state:
            logger.info(f"Using cache for blog post: {blog_post_url}")
            return self.session_state[blog_post_url]
        else:
            response: RunResponse = self.blog_analyzer.run(blog_post_url)
            if isinstance(response.content, BlogAnalyzer):
                result = response.content
                logger.info(f"Blog title: {result.title}")
                self.session_state[blog_post_url] = result.blog_content_markdown
                return result.blog_content_markdown
            else:
                raise ValueError("Unexpected content type received from blog analyzer.")

    def generate_plan(self, blog_content: str, post_type: PostType):
        plan_response: RunResponse = RunResponse(content=None)
        if post_type == PostType.TWITTER:
            logger.info(f"Generating post plan for {post_type}")
            plan_response = self.twitter_thread_planner.run(blog_content)
        elif post_type == PostType.LINKEDIN:
            logger.info(f"Generating post plan for {post_type}")
            plan_response = self.linkedin_post_planner.run(blog_content)
        else:
            raise ValueError(f"Unsupported post type: {post_type}")

        if isinstance(plan_response.content, (Thread, LinkedInPost)):
            return plan_response.content
        elif isinstance(plan_response.content, str):
            data = json.loads(plan_response.content)
            if post_type == PostType.TWITTER:
                return Thread(**data)
            else:
                return LinkedInPost(**data)
        else:
            raise ValueError("Unexpected content type received from planner.")

    def schedule_and_publish(self, plan, post_type: PostType) -> RunResponse:
        """
        Schedules and publishes the content leveraging Typefully api.
        """
        logger.info(f"# Publishing content for post type: {post_type}")

        # Use the `scheduler` module directly to schedule the content
        response = schedule(
            thread_model=plan,
            post_type=post_type,  # Either "Twitter" or "LinkedIn"
        )

        logger.info(f"Response: {response}")

        if response:
            return RunResponse(content=response)
        else:
            return RunResponse(content="Failed to schedule content.")

    def run(self, blog_post_url, post_type) -> RunResponse:
        """
        Args:
            blog_post_url: URL of the blog post to analyze.
            post_type: Type of post to generate (e.g., Twitter or LinkedIn).
        """
        # Scrape the blog post
        blog_content = self.scrape_blog_post(blog_post_url)

        # Generate the plan based on the blog and post type
        plan = self.generate_plan(blog_content, post_type)

        # Schedule and publish the content
        response = self.schedule_and_publish(plan, post_type)

        return response


if __name__ == "__main__":
    # Initialize and run the workflow
    blogpost_url = "https://blog.dailydoseofds.com/p/5-chunking-strategies-for-rag"
    workflow = ContentPlanningWorkflow()
    post_response = workflow.run(
        blog_post_url=blogpost_url, post_type=PostType.TWITTER
    )  # PostType.LINKEDIN for LinkedIn post
    logger.info(post_response.content)



================================================
FILE: cookbook/workflows/product_manager/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows/product_manager/meeting_notes.txt
================================================
Daily Standup Meeting - Technical Team
Date: 2024-01-15
Time: 9:30 AM - 9:45 AM

Attendees:
- Sarah (Tech Lead)
- Mike (Backend Developer) 
- Emma (Frontend Developer)
- Alex (DevOps Engineer)
- James (QA Engineer)

Sarah (Tech Lead):
"Good morning everyone! Let's go through our updates and new assignments for today. Mike, would you like to start?"

Mike (Backend Developer):
"Sure. I'll be working on implementing the new authentication service we discussed last week. The main tasks include setting up JWT token management and integrating with the user service. Estimated completion time is about 3-4 days."

Emma (Frontend Developer):
"I'm picking up the user dashboard redesign today. This includes implementing the new analytics widgets and improving the mobile responsiveness. I should have a preliminary version ready for review by Thursday."

Alex (DevOps Engineer):
"I'm focusing on setting up the new monitoring system. Will be configuring Prometheus and Grafana for better observability. Also need to update our CI/CD pipeline to include the new security scanning tools."

James (QA Engineer):
"I'll be creating automated test cases for Mike's authentication service once it's ready. In the meantime, I'm updating our end-to-end test suite and documenting the new test procedures for the dashboard features."

Sarah (Tech Lead):
"Great updates, everyone. Remember we have the architecture review meeting tomorrow at 2 PM. Please prepare your components documentation. Let me know if anyone needs any help or runs into blockers. Let's have a productive day!"

Meeting ended at 9:45 AM



================================================
FILE: cookbook/workflows/product_manager/product_manager.py
================================================
import os
from datetime import datetime
from typing import Dict, List, Optional

from agno.agent.agent import Agent
from agno.run.response import RunResponse
from agno.run.workflow import WorkflowCompletedEvent
from agno.storage.postgres import PostgresStorage
from agno.tools.linear import LinearTools
from agno.tools.slack import SlackTools
from agno.utils.log import logger
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


class Task(BaseModel):
    task_title: str = Field(..., description="The title of the task")
    task_description: Optional[str] = Field(
        None, description="The description of the task"
    )
    task_assignee: Optional[str] = Field(None, description="The assignee of the task")


class LinearIssue(BaseModel):
    issue_title: str = Field(..., description="The title of the issue")
    issue_description: Optional[str] = Field(
        None, description="The description of the issue"
    )
    issue_assignee: Optional[str] = Field(None, description="The assignee of the issue")
    issue_link: Optional[str] = Field(None, description="The link to the issue")


class LinearIssueList(BaseModel):
    issues: List[LinearIssue] = Field(..., description="A list of issues")


class TaskList(BaseModel):
    tasks: List[Task] = Field(..., description="A list of tasks")


class ProductManagerWorkflow(Workflow):
    description: str = "Generate linear tasks and send slack notifications to the team from meeting notes."

    task_agent: Agent = Agent(
        name="Task Agent",
        instructions=[
            "Given a meeting note, generate a list of tasks with titles, descriptions and assignees."
        ],
        response_model=TaskList,
    )

    linear_agent: Agent = Agent(
        name="Linear Agent",
        instructions=["Given a list of tasks, create issues in Linear."],
        tools=[LinearTools()],
        response_model=LinearIssueList,
    )

    slack_agent: Agent = Agent(
        name="Slack Agent",
        instructions=[
            "Send a slack notification to the #test channel with a heading (bold text) including the current date and tasks in the following format: ",
            "*Title*: <issue_title>",
            "*Description*: <issue_description>",
            "*Assignee*: <issue_assignee>",
            "*Issue Link*: <issue_link>",
        ],
        tools=[SlackTools()],
    )

    def get_tasks_from_cache(self, current_date: str) -> Optional[TaskList]:
        if "meeting_notes" in self.session_state:
            for cached_tasks in self.session_state["meeting_notes"]:
                if cached_tasks["date"] == current_date:
                    return cached_tasks["tasks"]
        return None

    def get_tasks_from_meeting_notes(self, meeting_notes: str) -> Optional[TaskList]:
        num_tries = 0
        tasks: Optional[TaskList] = None
        while tasks is None and num_tries < 3:
            num_tries += 1
            try:
                response: RunResponse = self.task_agent.run(meeting_notes)
                if (
                    response
                    and response.content
                    and isinstance(response.content, TaskList)
                ):
                    tasks = response.content
                else:
                    logger.warning("Invalid response from task agent, trying again...")
            except Exception as e:
                logger.warning(f"Error generating tasks: {e}")

        return tasks

    def create_linear_issues(
        self, tasks: TaskList, linear_users: Dict[str, str]
    ) -> Optional[LinearIssueList]:
        project_id = os.getenv("LINEAR_PROJECT_ID")
        team_id = os.getenv("LINEAR_TEAM_ID")
        if project_id is None:
            raise Exception("LINEAR_PROJECT_ID is not set")
        if team_id is None:
            raise Exception("LINEAR_TEAM_ID is not set")

        # Create issues in Linear
        logger.info(f"Creating issues in Linear: {tasks.model_dump_json()}")
        linear_response: RunResponse = self.linear_agent.run(
            f"Create issues in Linear for project {project_id} and team {team_id}: {tasks.model_dump_json()} and here is the dictionary of users and their uuid: {linear_users}. If you fail to create an issue, try again."
        )
        linear_issues = None
        if linear_response:
            logger.info(f"Linear response: {linear_response}")
            linear_issues = linear_response.content

        return linear_issues

    def run(
        self, meeting_notes: str, linear_users: Dict[str, str], use_cache: bool = False
    ) -> RunResponse:
        logger.info(f"Generating tasks from meeting notes: {meeting_notes}")
        current_date = datetime.now().strftime("%Y-%m-%d")

        if use_cache:
            tasks: Optional[TaskList] = self.get_tasks_from_cache(current_date)
        else:
            tasks = self.get_tasks_from_meeting_notes(meeting_notes)

        if tasks is None or len(tasks.tasks) == 0:
            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content="Sorry, could not generate tasks from meeting notes.",
            )

        if "meeting_notes" not in self.session_state:
            self.session_state["meeting_notes"] = []
        self.session_state["meeting_notes"].append(
            {"date": current_date, "tasks": tasks.model_dump_json()}
        )

        linear_issues = self.create_linear_issues(tasks, linear_users)

        # Send slack notification with tasks
        if linear_issues:
            logger.info(
                f"Sending slack notification with tasks: {linear_issues.model_dump_json()}"
            )
            slack_response: RunResponse = self.slack_agent.run(
                linear_issues.model_dump_json()
            )
            logger.info(f"Slack response: {slack_response}")

        return slack_response


# Create the workflow
product_manager = ProductManagerWorkflow(
    session_id="product-manager",
    storage=PostgresStorage(
        table_name="product_manager_workflows",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    ),
)

meeting_notes = open("cookbook/workflows/product_manager/meeting_notes.txt", "r").read()
users_uuid = {
    "Sarah": "8d4e1c9a-b5f2-4e3d-9a76-f12d8e3b4c5a",
    "Mike": "2f9b7d6c-e4a3-42f1-b890-1c5d4e8f9a3b",
    "Emma": "7a1b3c5d-9e8f-4d2c-a6b7-8c9d0e1f2a3b",
    "Alex": "4c5d6e7f-8a9b-0c1d-2e3f-4a5b6c7d8e9f",
    "James": "1a2b3c4d-5e6f-7a8b-9c0d-1e2f3a4b5c6d",
}

# Run workflow
product_manager.run(meeting_notes=meeting_notes, linear_users=users_uuid)



================================================
FILE: cookbook/workflows_2/README.md
================================================
# Agno Workflows 2.0 - Developer Guide

Welcome to **Agno Workflows 2.0** - the next generation of intelligent, flexible workflow orchestration. This guide covers all workflow patterns, from simple linear sequences to complex conditional logic with parallel execution.

## Table of Contents

- [Overview](#overview)
- [Core Concepts](#core-concepts)
- [Workflow Patterns](#workflow-patterns)
  - [1. Basic Sequential Workflows](#1-basic-sequential-workflows)
  - [2. Workflows 1.0 type execution](#2-workflows-10-type-execution)
  - [3. Basic Step Based Execution](#3-basic-step-based-execution)
  - [4. Parallel Execution](#4-parallel-execution)
  - [5. Conditional Workflows](#5-conditional-workflows)
  - [6. Loop/Iteration Workflows](#6-loopiteration-workflows)
  - [7. Condition-Based Branching](#7-Condition-based-branching)
  - [8. Steps: Grouping a list of steps](#8-steps-grouping-a-list-of-steps)
  - [9. Complex Combinations](#9-complex-combinations)
- [Advanced Features](#advanced-features)
- [Best Practices](#best-practices)
- [Migration from Workflows 1.0](#migration-from-workflows-10)

## Overview

Agno Workflows 2.0 provides a powerful, declarative way to orchestrate multi-step AI processes. Unlike traditional linear workflows, you can now create sophisticated branching logic, parallel execution, and dynamic routing based on content analysis.

![Workflows 2.0 flow](/cookbook/workflows_2/assets/workflows_v2_flow.png)

### Key Features

- 🔄 **Flexible Execution**: Sequential, parallel, conditional, and loop-based execution
- 🎯 **Smart Routing**: Dynamic step selection based on content analysis
- 🔧 **Mixed Components**: Combine agents, teams, and functions seamlessly
- 💾 **State Management**: Share data across steps with session state
- 🌊 **Streaming Support**: Having support for event-based streamed information
- 📝 **Structured Inputs**: Type-safe inputs with Pydantic models

## Core Concepts

### Building Blocks

| Component | Purpose | Example Use Case |
|-----------|---------|------------------|
| **Step** | Basic execution unit | Single research task |
| **Agent** | AI assistant with specific role | Content writer, researcher |
| **Team** | Coordinated group of agents | Research team with specialists |
| **Function** | Custom Python logic | Data processing, API calls |
| **Parallel** | Concurrent execution | Multiple research streams |
| **Condition** | Conditional execution | Topic-specific processing |
| **Loop** | Iterative execution | Quality-driven research |
| **Router** | Dynamic routing | Content-based step selection |

### Atomic Units with Controlled Execution
The workflow system is built around the concept of atomic execution units in Agno- `Agents` and `Teams`, these are individual components that can work independently but gain enhanced capabilities when orchestrated together:
- **Agents**: Individual AI executors with specific capabilities and instructions
- **Teams**: Coordinated groups of agents working together on complex problems
- **Custom Python Functions**: Custom Python functions for specialized processing logic and full control

The beauty of this approach is that you maintain the full power and flexibility of each atomic unit while gaining sophisticated orchestration capabilities. Your agents and teams retain their individual characteristics, memory, and behavior patterns, but now operate within a structured workflow that provides:
- Sequential step execution with output chaining
- Session management and state persistence
- Error handling and retry mechanisms
- Streaming capabilities for real-time feedback

## Workflow Patterns

### 1. Basic Sequential Workflows

**When to use**: Linear processes where each step depends on the previous one.

**Example**: Research → Preprocess data in a function before next step → Content Creation

```python
from agno.workflow.v2 import Step, Workflow

def data_preprocessor(step_input):
    # Custom preprocessing logic

    # Or you can also run any agent/team over here itself
    # response = some_agent.run(...)
    return StepOutput(content=f"Processed: {step_input.message}") # <-- Now pass the agent/team response in content here

workflow = Workflow(
    name="Mixed Execution Pipeline",
    steps=[
        research_team,      # Team
        data_preprocessor,  # Function
        content_agent,      # Agent
    ]
)

workflow.print_response("Analyze the competitive landscape for fintech startups", markdown=True)
```

**See Examples**: 
- [`sequence_of_functions_and_agents.py`](/cookbook/workflows_2/sync/01_basic_workflows/sequence_of_functions_and_agents.py)
- [`sequence_of_functions_and_agents_stream.py`](/cookbook/workflows_2/sync/01_basic_workflows/sequence_of_functions_and_agents_stream.py)


> **Note**: `StepInput` and `StepOutput` provides standardized interfaces for data flow between steps:
![Workflows Step IO](/cookbook/workflows_2/assets/step_io_flow.png)

> So if you make a custom function as an executor for a step, do make sure that the input and output types are compatible with the `StepInput` and `StepOutput` interfaces. This will ensure that your custom function can seamlessly integrate into the workflow system.

### 2. `Workflows 1.0` type execution

**Keep it Simple with Pure Python**: If you prefer the Workflows 1.0 approach or need maximum flexibility, you can still use a single Python function to handle everything. This approach gives you complete control over the execution flow while still benefiting from workflow features like storage, streaming, and session management.

Replace all the steps in the workflow with a single executable function where you can control everything.

```python
def custom_workflow_function(workflow: Workflow, execution_input: WorkflowExecutionInput):
    # Custom orchestration logic
    research_result = research_team.run(execution_input.message)
    analysis_result = analysis_agent.run(research_result.content)
    return f"Final: {analysis_result.content}"

workflow = Workflow(
    name="Function-Based Workflow",
    steps=custom_workflow_function  # Single function replaces all steps
)

workflow.print_response("Evaluate the market potential for quantum computing applications", markdown=True)
```

**See Examples**:
- [`function_instead_of_steps.py`](/cookbook/workflows_2/sync/01_basic_workflows/function_instead_of_steps.py) - Complete function-based workflow
- [`function_instead_of_steps_stream.py`](/cookbook/workflows_2/sync/01_basic_workflows/function_instead_of_steps_stream.py) - Streaming version

For migration to 2.0 refer to this section- [Migration from Workflows 1.0](#migration-from-workflows-10)

### 3. Basic Step Based Execution

**You can name your steps** for better logging and future support on the Agno platform:

```python
from agno.workflow.v2 import Step, Workflow

# Named steps for better tracking
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Step(name="Research Phase", team=researcher),
        Step(name="Analysis Phase", executor=custom_function), 
        Step(name="Writing Phase", agent=writer),
    ]
)

workflow.print_response(
    "AI trends in 2024",
    markdown=True,
)
```

**See Examples**: 
- [`sequence_of_steps.py`](cookbook/workflows_2/sync/01_basic_workflows/sequence_of_steps.py)
- [`sequence_of_steps_stream.py`](cookbook/workflows_2/sync/01_basic_workflows/sequence_of_steps_stream.py)
- [`step_with_function.py`](/cookbook/workflows_2/sync/01_basic_workflows/step_with_function.py)
- [`step_with_function_stream.py`](/cookbook/workflows_2/sync/01_basic_workflows/step_with_function_stream.py)

### 4. Parallel Execution

**When to use**: Independent tasks that can run simultaneously to save time.

**Example**: Multiple research sources, parallel content creation

![Parallel Steps](/cookbook/workflows_2/assets/parallel_steps.png)

```python
from agno.workflow.v2 import Parallel, Step, Workflow

workflow = Workflow(
    name="Parallel Research Pipeline",
    steps=[
        Parallel(
            Step(name="HackerNews Research", agent=hn_researcher),
            Step(name="Web Research", agent=web_researcher),
            Step(name="Academic Research", agent=academic_researcher),
            name="Research Phase"
        ),
        Step(name="Synthesis", agent=synthesizer),
    ]
)

workflow.print_response("Write about the latest AI developments", markdown=True)
```

**See Examples**: 
- [`parallel_steps_workflow.py`](/cookbook/workflows_2/sync/04_workflows_parallel_execution/parallel_steps_workflow.py)
- [`parallel_steps_workflow_stream.py`](/cookbook/workflows_2/sync/04_workflows_parallel_execution/parallel_steps_workflow_stream.py)

### 5. Conditional Steps

**When to use**: Conditional step execution based on business logic.

**Example**: Topic-specific research strategies, content type routing

![Condition Steps](/cookbook/workflows_2/assets/condition_steps.png)

```python
from agno.workflow.v2 import Condition, Step, Workflow

def is_tech_topic(step_input) -> bool:
    topic = step_input.message.lower()
    return any(keyword in topic for keyword in ["ai", "tech", "software"])

workflow = Workflow(
    name="Conditional Research",
    steps=[
        Condition(
            name="Tech Topic Check",
            evaluator=is_tech_topic,
            steps=[Step(name="Tech Research", agent=tech_researcher)]
        ),
        Step(name="General Analysis", agent=general_analyst),
    ]
)

workflow.print_response("Comprehensive analysis of AI and machine learning trends", markdown=True)
```

**See Examples**: 
- [`condition_with_list_of_steps.py`](/cookbook/workflows_2/sync/02_workflows_conditional_execution/condition_with_list_of_steps.py)
- [`condition_steps_workflow_stream.py`](/cookbook/workflows_2/sync/02_workflows_conditional_execution/condition_steps_workflow_stream.py)

### 6. Loop/Iteration Workflows

**When to use**: Quality-driven processes, iterative refinement, or retry logic.

**Example**: Research until sufficient quality, iterative improvement

![Loop Steps](/cookbook/workflows_2/assets/loop_steps.png)

```python
from agno.workflow.v2 import Loop, Step, Workflow

def quality_check(outputs) -> bool:
    # Return True to break loop, False to continue
    return any(len(output.content) > 500 for output in outputs)

workflow = Workflow(
    name="Quality-Driven Research",
    steps=[
        Loop(
            name="Research Loop",
            steps=[Step(name="Deep Research", agent=researcher)],
            end_condition=quality_check,
            max_iterations=3
        ),
        Step(name="Final Analysis", agent=analyst),
    ]
)

workflow.print_response("Research the impact of renewable energy on global markets", markdown=True)
```

**See Examples**: 
- [`loop_steps_workflow.py`](/cookbook/workflows_2/sync/03_workflows_loop_execution/loop_steps_workflow.py)
- [`loop_steps_workflow_stream.py`](/cookbook/workflows_2/sync/03_workflows_loop_execution/loop_steps_workflow_stream.py)

### 7. Condition-Based Branching

**When to use**: Complex decision trees, topic-specific workflows, dynamic routing.

**Example**: Content type detection, expertise routing

![Router Steps](/cookbook/workflows_2/assets/router_steps.png)

```python
from agno.workflow.v2 import Router, Step, Workflow

def route_by_topic(step_input) -> List[Step]:
    topic = step_input.message.lower()
    
    if "tech" in topic:
        return [Step(name="Tech Research", agent=tech_expert)]
    elif "business" in topic:
        return [Step(name="Business Research", agent=biz_expert)]
    else:
        return [Step(name="General Research", agent=generalist)]

workflow = Workflow(
    name="Expert Routing",
    steps=[
        Router(
            name="Topic Router",
            selector=route_by_topic,
            choices=[tech_step, business_step, general_step]
        ),
        Step(name="Synthesis", agent=synthesizer),
    ]
)

workflow.print_response("Latest developments in artificial intelligence and machine learning", markdown=True)
```

**See Examples**: 
- [`router_steps_workflow.py`](/cookbook/workflows_2/sync/05_workflows_conditional_branching/router_steps_workflow.py)
- [`router_steps_workflow_stream.py`](/cookbook/workflows_2/sync/05_workflows_conditional_branching/router_steps_workflow_stream.py)

### 8. Steps: Grouping a list of steps

**When to use**: When you need to group multiple steps into logical sequences, create reusable workflows, or organize complex workflows with multiple branching paths.

Better Routing: Use with Router for clean branching logic

```python
from agno.workflow.v2 import Steps, Step, Workflow

# Create a reusable content creation sequence
article_creation_sequence = Steps(
    name="ArticleCreation",
    description="Complete article creation workflow from research to final edit",
    steps=[
        Step(name="research", agent=researcher),
        Step(name="writing", agent=writer), 
        Step(name="editing", agent=editor),
    ],
)

# Use the sequence in a workflow
workflow = Workflow(
    name="Article Creation Workflow",
    steps=[article_creation_sequence]  # Single sequence
)

workflow.print_response("Write an article about renewable energy", markdown=True)
```

#### Steps with Router for Clean Branching
This is where Steps really shines - creating distinct sequences for different content types or workflows:

```python
from agno.workflow.v2 import Steps, Router, Step, Workflow

# Define two completely different workflows as Steps
image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[
        Step(name="generate_image", agent=image_generator),
        Step(name="describe_image", agent=image_describer),
    ],
)

video_sequence = Steps(
    name="video_generation", 
    description="Complete video production and analysis workflow",
    steps=[
        Step(name="generate_video", agent=video_generator),
        Step(name="describe_video", agent=video_describer),
    ],
)

def media_sequence_selector(step_input) -> List[Step]:
    """Route to appropriate media generation pipeline"""
    if not step_input.message:
        return [image_sequence]
        
    message_lower = step_input.message.lower()
    
    if "video" in message_lower:
        return [video_sequence]
    elif "image" in message_lower:
        return [image_sequence]
    else:
        return [image_sequence]  # Default

# Clean workflow with clear branching
media_workflow = Workflow(
    name="AI Media Generation Workflow",
    description="Generate and analyze images or videos using AI agents",
    steps=[
        Router(
            name="Media Type Router",
            description="Routes to appropriate media generation pipeline",
            selector=media_sequence_selector,
            choices=[image_sequence, video_sequence],  # Clear choices
        )
    ],
)

# Usage examples
media_workflow.print_response("Create an image of a magical forest", markdown=True)
media_workflow.print_response("Create a cinematic video of city timelapse", markdown=True)
```

**See Examples**
- [`workflow_using_steps.py`](/cookbook/workflows_2/sync/01_basic_workflows/workflow_using_steps.py)
- [`workflow_using_steps_nested.py`](/cookbook/workflows_2/sync/01_basic_workflows/workflow_using_steps_nested.py)
- [`selector_for_image_video_generation_pipelines.py`](/cookbook/workflows_2/sync/05_workflows_conditional_branching/selector_for_image_video_generation_pipelines.py)

### 9. Complex Combinations

**When to use**: Sophisticated workflows requiring multiple patterns.

**Example**: Conditions + Parallel + Loops + Custom Post-Processing Function + Routing

```python
from agno.workflow.v2 import Condition, Loop, Parallel, Router, Step, Workflow

def research_post_processor(step_input) -> StepOutput:
    """Post-process and consolidate research data from parallel conditions"""
    research_data = step_input.previous_step_content or ""
    
    try:
        # Analyze research quality and completeness
        word_count = len(research_data.split())
        has_tech_content = any(keyword in research_data.lower() 
                              for keyword in ["technology", "ai", "software", "tech"])
        has_business_content = any(keyword in research_data.lower() 
                                  for keyword in ["market", "business", "revenue", "strategy"])
        
        # Create enhanced research summary
        enhanced_summary = f"""
            ## Research Analysis Report
            
            **Data Quality:** {"✓ High-quality" if word_count > 200 else "⚠ Limited data"}
            
            **Content Coverage:**
            - Technical Analysis: {"✓ Completed" if has_tech_content else "✗ Not available"}
            - Business Analysis: {"✓ Completed" if has_business_content else "✗ Not available"}
            
            **Research Findings:**
            {research_data}
        """.strip()
        
        return StepOutput(
            content=enhanced_summary,
            success=True,
        )
        
    except Exception as e:
        return StepOutput(
            content=f"Research post-processing failed: {str(e)}",
            success=False,
            error=str(e)
        )

# Complex workflow combining multiple patterns
workflow = Workflow(
    name="Advanced Multi-Pattern Workflow",
    steps=[
        Parallel(
            Condition(
                name="Tech Check",
                evaluator=is_tech_topic,
                steps=[Step(name="Tech Research", agent=tech_researcher)]
            ),
            Condition(
                name="Business Check", 
                evaluator=is_business_topic,
                steps=[
                    Loop(
                        name="Deep Business Research",
                        steps=[Step(name="Market Research", agent=market_researcher)],
                        end_condition=research_quality_check,
                        max_iterations=3
                    )
                ]
            ),
            name="Conditional Research Phase"
        ),
        Step(
            name="Research Post-Processing",
            executor=research_post_processor,
            description="Consolidate and analyze research findings with quality metrics"
        ),
        Router(
            name="Content Type Router",
            selector=content_type_selector,
            choices=[blog_post_step, social_media_step, report_step]
        ),
        Step(name="Final Review", agent=reviewer),
    ]
)

workflow.print_response("Create a comprehensive analysis of sustainable technology trends and their business impact for 2024", markdown=True)
```

**See Examples**: 
- [`condition_and_parallel_steps_stream.py`](/cookbook/workflows_2/sync/02_workflows_conditional_execution/condition_and_parallel_steps_stream.py)
- [`loop_with_parallel_steps_stream.py`](/cookbook/workflows_2/sync/03_workflows_loop_execution/loop_with_parallel_steps_stream.py)
- [`router_with_loop_steps.py`](/cookbook/workflows_2/sync/05_workflows_conditional_branching/router_with_loop_steps.py)
 
## Advanced Features

### Early Stopping

Workflows can be terminated early when certain conditions are met, preventing unnecessary processing and ensuring safety gates work properly. Any step can trigger early termination by returning `StepOutput(stop=True)`.

![Early Stop Workflows](/cookbook/workflows_2/assets/early_stop.png)

```python
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.types import StepInput, StepOutput

def security_gate(step_input: StepInput) -> StepOutput:
    """Security gate that stops deployment if vulnerabilities found"""
    security_result = step_input.previous_step_content or ""
    
    if "VULNERABLE" in security_result.upper():
        return StepOutput(
            content="🚨 SECURITY ALERT: Critical vulnerabilities detected. Deployment blocked.",
            stop=True  # Stop the entire workflow
        )
    else:
        return StepOutput(
            content="✅ Security check passed. Proceeding with deployment...",
            stop=False
        )

# Secure deployment pipeline
workflow = Workflow(
    name="Secure Deployment Pipeline",
    steps=[
        Step(name="Security Scan", agent=security_scanner),
        Step(name="Security Gate", executor=security_gate),  # May stop here
        Step(name="Deploy Code", agent=code_deployer),       # Only if secure
        Step(name="Setup Monitoring", agent=monitoring_agent), # Only if deployed
    ]
)

# Test with vulnerable code - workflow stops at security gate
workflow.print_response("Scan this code: exec(input('Enter command: '))")
```

**See Examples**: 
- [`early_stop_workflow_with_agents.py`](/cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_agents.py)
- [`early_stop_workflow_with_loop.py`](/cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_loop.py)
- [`early_stop_workflow_with_router.py`](/cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_router.py)

### Access Multiple Previous Steps Output

Advanced workflows often need to access data from multiple previous steps, not just the immediate previous step. The `StepInput` object provides powerful methods to access any previous step's output by name or get all previous content.

```python
def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

    # Access original workflow input
    original_topic = step_input.message or ""

    # Access specific step outputs by name
    hackernews_data = step_input.get_step_content("research_hackernews") or ""
    web_data = step_input.get_step_content("research_web") or ""

    # Or access ALL previous content
    all_research = step_input.get_all_previous_content()

    # Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

        ## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

        ## HackerNews Insights
        {hackernews_data[:500]}...

        ## Web Research Findings  
        {web_data[:500]}...
    """

    return StepOutput(
        step_name="comprehensive_report", 
        content=report.strip(), 
        success=True
    )

# Use in workflow
workflow = Workflow(
    name="Enhanced Research Workflow",
    steps=[
        Step(name="research_hackernews", agent=hackernews_agent),
        Step(name="research_web", agent=web_agent),
        Step(name="comprehensive_report", executor=create_comprehensive_report),  # Accesses both previous steps
        Step(name="final_reasoning", agent=reasoning_agent),
    ],
)
```

> **Key Methods:**
> - `step_input.get_step_content("step_name")` - Get content from specific step by name
> - `step_input.get_all_previous_content()` - Get all previous step content combined
> - `step_input.message` - Access the original workflow input message
> - `step_input.previous_step_content` - Get content from immediate previous step

### Event Storage and Filtering

Workflows can automatically store all events for later analysis, debugging, or audit purposes. You can also filter out specific event types to reduce noise and storage overhead. You can access these events on the `WorkflowRunResponse` and in the `runs` column in your `Workflow's Session DB`

**Key Features:**

- **`store_events=True`**: Automatically stores all workflow events in the database
- **`events_to_skip=[]`**: Filter out specific event types to reduce storage and noise
- **Persistent Storage**: Events are stored in your configured storage backend (SQLite, PostgreSQL, etc.)
- **Post-Execution Access**: Access all stored events via `workflow.run_response.events`

**Available Events to Skip:**
```python
from agno.run.v2.workflow import WorkflowRunEvent

# Common events you might want to skip
events_to_skip = [
    WorkflowRunEvent.workflow_started,
    WorkflowRunEvent.workflow_completed,
    WorkflowRunEvent.step_started,
    WorkflowRunEvent.step_completed,
    WorkflowRunEvent.parallel_execution_started,
    WorkflowRunEvent.parallel_execution_completed,
    WorkflowRunEvent.condition_execution_started,
    WorkflowRunEvent.condition_execution_completed,
    WorkflowRunEvent.loop_execution_started,
    WorkflowRunEvent.loop_execution_completed,
    WorkflowRunEvent.router_execution_started,
    WorkflowRunEvent.router_execution_completed,
]
```

**When to use:**
- **Debugging**: Store all events to analyze workflow execution flow
- **Audit Trails**: Keep records of all workflow activities for compliance
- **Performance Analysis**: Analyze timing and execution patterns
- **Error Investigation**: Review event sequences leading to failures
- **Noise Reduction**: Skip verbose events like `step_started` to focus on results

**Example Use Cases:**
```python
# store everything
debug_workflow = Workflow(
    name="Debug Workflow",
    store_events=True,
    steps=[...]
)

# store only important events
production_workflow = Workflow(
    name="Production Workflow", 
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.step_started,
        WorkflowRunEvent.parallel_execution_started,
        # keep step_completed and workflow_completed
    ],
    steps=[...]
)

# No event storage
fast_workflow = Workflow(
    name="Fast Workflow",
    store_events=False,  
    steps=[...]
)
```

**See Examples**:
- [`store_events_and_events_to_skip_in_a_workflow.py`](/cookbook/workflows_2/sync/06_workflows_advanced_concepts/store_events_and_events_to_skip_in_a_workflow.py)

### Additional Data

**When to use**: When you need to pass metadata, configuration, or contextual information to specific steps without it being part of the main workflow message flow.
- Separation of Concerns: Keep workflow logic separate from metadata
- Step-Specific Context: Access additional information in custom functions
- Clean Message Flow: Main message stays focused on content
- Flexible Configuration: Pass user info, priorities, settings, etc.

Access Pattern: `step_input.additional_data` provides dictionary access to all additional data

```python
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.types import StepInput, StepOutput

def custom_content_planning_function(step_input: StepInput) -> StepOutput:
    """Custom function that uses additional_data for enhanced context"""
    
    # Access the main workflow message
    message = step_input.message
    previous_content = step_input.previous_step_content
    
    # Access additional_data that was passed with the workflow
    additional_data = step_input.additional_data or {}
    user_email = additional_data.get("user_email", "No email provided")
    priority = additional_data.get("priority", "normal")
    client_type = additional_data.get("client_type", "standard")
    
    # Create enhanced planning prompt with context
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:
        
        Core Topic: {message}
        Research Results: {previous_content[:500] if previous_content else "No research results"}
        
        Additional Context:
        - Client Type: {client_type}
        - Priority Level: {priority}
        - Contact Email: {user_email}
        
        {"🚨 HIGH PRIORITY - Expedited delivery required" if priority == "high" else "📝 Standard delivery timeline"}
        
        Please create a detailed, actionable content plan.
    """
    
    response = content_planner.run(planning_prompt)
    
    enhanced_content = f"""
        ## Strategic Content Plan
        
        **Planning Topic:** {message}
        **Client Details:** {client_type} | {priority.upper()} priority | {user_email}
        
        **Content Strategy:**
        {response.content}
    """
    
    return StepOutput(content=enhanced_content, response=response)

# Define workflow with steps
workflow = Workflow(
    name="Content Creation Workflow",
    steps=[
        Step(name="Research Step", team=research_team),
        Step(name="Content Planning Step", executor=custom_content_planning_function),
    ]
)

# Run workflow with additional_data
workflow.print_response(
    message="AI trends in 2024",
    additional_data={
        "user_email": "kaustubh@agno.com",
        "priority": "high",
        "client_type": "enterprise",
        "budget": "$50000",
        "deadline": "2024-12-15"
    },
    markdown=True,
    stream=True
)
```

**See**: [`step_with_function_additional_data.py`](/cookbook/workflows_2/sync/01_basic_workflows/step_with_function_additional_data.py)

### Streaming Support

This adds support for having streaming event-based information for your workflows:

```python
from agno.workflow.v2 import Workflow
from agno.run.v2.workflow import (
    WorkflowStartedEvent,
    StepStartedEvent, 
    StepCompletedEvent,
    WorkflowCompletedEvent
)

# Enable streaming for any workflow pattern
workflow = Workflow(
    name="Streaming Pipeline",
    steps=[research_step, analysis_step, writing_step]
)

# Stream with proper event handling
for event in workflow.run(message="AI trends", stream=True, stream_intermediate_steps=True):
    if isinstance(event, WorkflowStartedEvent):
        print(f"🚀 Workflow Started: {event.workflow_name}")
        print(f"   Run ID: {event.run_id}")
        
    elif isinstance(event, StepStartedEvent):
        print(f"📍 Step Started: {event.step_name}")
        print(f"   Step Index: {event.step_index}")
        
    elif isinstance(event, StepCompletedEvent):
        print(f"✅ Step Completed: {event.step_name}")
        # Show content preview instead of full content
        if hasattr(event, 'content') and event.content:
            preview = str(event.content)[:100] + "..." if len(str(event.content)) > 100 else str(event.content)
            print(f"   Preview: {preview}")
            
    elif isinstance(event, WorkflowCompletedEvent):
        print(f"🎉 Workflow Completed: {event.workflow_name}")
        print(f"   Total Steps: {len(event.step_responses)}")
        # Show final output preview
        if hasattr(event, 'content') and event.content:
            preview = str(event.content)[:150] + "..." if len(str(event.content)) > 150 else str(event.content)
            print(f"   Final Output: {preview}")
```

**See**: Any `*_stream.py` file for streaming examples.

### Session State Management

Share data across workflow steps:

```python
from agno.workflow.v2 import Workflow
from agno.agent.agent import Agent

# Access state in agent tools
def add_to_shared_data(agent: Agent, data: str) -> str:
    agent.workflow_session_state["collected_data"] = data
    return f"Added: {data}"

shopping_assistant = Agent(
    name="Shopping Assistant",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[add_to_shared_data],
    instructions=[
        "You are a helpful shopping assistant.",
        "You can help users manage their shopping list by adding, removing, and listing items.",
        "Always use the provided tools to interact with the shopping list.",
        "Be friendly and helpful in your responses.",
    ],
)

workflow = Workflow(
    name="Stateful Workflow",
    workflow_session_state={},  # Initialize shared state
    steps=[data_collector_step, data_processor_step, data_finalizer_step]
)

workflow.print_response("Add apples and oranges to my shopping list")
```

**See**: 
- [`shared_session_state_with_agent.py`](/cookbook/workflows_2/sync/06_workflows_advanced_concepts/shared_session_state_with_agent.py)
- [`shared_session_state_with_team.py`](/cookbook/workflows_2/sync/06_workflows_advanced_concepts/shared_session_state_with_team.py)

### Structured Inputs

Use Pydantic models for type-safe inputs:

```python
from pydantic import BaseModel, Field

class ResearchRequest(BaseModel):
    topic: str = Field(description="Research topic")
    depth: int = Field(description="Research depth (1-10)")
    sources: List[str] = Field(description="Preferred sources")

workflow.print_response(
    message=ResearchRequest(
        topic="AI trends 2024",
        depth=8,
        sources=["academic", "industry"]
    )
)
```

**See**: [`pydantic_model_as_input.py`](/cookbook/workflows_2/sync/06_workflows_advanced_concepts/pydantic_model_as_input.py)

## Best Practices

### When to Use Each Pattern

| Pattern | Best For | Avoid When |
|---------|----------|------------|
| **Sequential** | Linear processes, dependencies | Independent tasks |
| **Parallel** | Independent tasks, speed optimization | Sequential dependencies |
| **Conditional** | Topic-specific logic, branching | Simple linear flows |
| **Loop** | Quality assurance, retry logic | Known finite processes |
| **Router** | Complex decision trees | Simple if/else logic |
| **Mixed** | Maximum flexibility | Simple workflows |

## Migration from Workflows 1.0

### Key Differences

| Workflows 1.0 | Workflows 2.0 | Migration Path |
|---------------|---------------|----------------|
| Linear only | Multiple patterns | Add Parallel/Condition as needed |
| Agent-focused | Mixed components | Convert functions to Steps |
| Limited branching | Smart routing | Replace if/else with Router |
| Manual loops | Built-in Loop | Use Loop component |

### Migration Steps

1. **Assess current workflow**: Identify parallel opportunities
2. **Add conditions**: Convert if/else logic to Condition components
3. **Extract functions**: Move custom logic to function-based steps
4. **Enable streaming**: For event-based information
5. **Add state management**: Use `workflow_session_state` for data sharing

For more examples and advanced patterns, explore the [`cookbook/workflows/sync/`](/cookbook/workflows_2/sync) and [`cookbook/workflows/async/`](/cookbook/workflows_2/async) directory. Each file demonstrates a specific pattern with detailed comments and real-world use cases.



================================================
FILE: cookbook/workflows_2/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/blog_post_generator.py
================================================
"""🎨 Blog Post Generator v2.0 - Your AI Content Creation Studio!

This advanced example demonstrates how to build a sophisticated blog post generator using
the new workflow v2.0 architecture. The workflow combines web research capabilities with
professional writing expertise using a multi-stage approach:

1. Intelligent web research and source gathering
2. Content extraction and processing
3. Professional blog post writing with proper citations

Key capabilities:
- Advanced web research and source evaluation
- Content scraping and processing
- Professional writing with SEO optimization
- Automatic content caching for efficiency
- Source attribution and fact verification
"""

import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


# --- Response Models ---
class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )


# --- Agents ---
research_agent = Agent(
    name="Blog Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    description=dedent("""\
    You are BlogResearch-X, an elite research assistant specializing in discovering
    high-quality sources for compelling blog content. Your expertise includes:

    - Finding authoritative and trending sources
    - Evaluating content credibility and relevance
    - Identifying diverse perspectives and expert opinions
    - Discovering unique angles and insights
    - Ensuring comprehensive topic coverage
    """),
    instructions=dedent("""\
    1. Search Strategy 🔍
       - Find 10-15 relevant sources and select the 5-7 best ones
       - Prioritize recent, authoritative content
       - Look for unique angles and expert insights
    2. Source Evaluation 📊
       - Verify source credibility and expertise
       - Check publication dates for timeliness
       - Assess content depth and uniqueness
    3. Diversity of Perspectives 🌐
       - Include different viewpoints
       - Gather both mainstream and expert opinions
       - Find supporting data and statistics
    """),
    response_model=SearchResults,
)

content_scraper_agent = Agent(
    name="Content Scraper Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[Newspaper4kTools()],
    description=dedent("""\
    You are ContentBot-X, a specialist in extracting and processing digital content
    for blog creation. Your expertise includes:

    - Efficient content extraction
    - Smart formatting and structuring
    - Key information identification
    - Quote and statistic preservation
    - Maintaining source attribution
    """),
    instructions=dedent("""\
    1. Content Extraction 📑
       - Extract content from the article
       - Preserve important quotes and statistics
       - Maintain proper attribution
       - Handle paywalls gracefully
    2. Content Processing 🔄
       - Format text in clean markdown
       - Preserve key information
       - Structure content logically
    3. Quality Control ✅
       - Verify content relevance
       - Ensure accurate extraction
       - Maintain readability
    """),
    response_model=ScrapedArticle,
)

blog_writer_agent = Agent(
    name="Blog Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are BlogMaster-X, an elite content creator combining journalistic excellence
    with digital marketing expertise. Your strengths include:

    - Crafting viral-worthy headlines
    - Writing engaging introductions
    - Structuring content for digital consumption
    - Incorporating research seamlessly
    - Optimizing for SEO while maintaining quality
    - Creating shareable conclusions
    """),
    instructions=dedent("""\
    1. Content Strategy 📝
       - Craft attention-grabbing headlines
       - Write compelling introductions
       - Structure content for engagement
       - Include relevant subheadings
    2. Writing Excellence ✍️
       - Balance expertise with accessibility
       - Use clear, engaging language
       - Include relevant examples
       - Incorporate statistics naturally
    3. Source Integration 🔍
       - Cite sources properly
       - Include expert quotes
       - Maintain factual accuracy
    4. Digital Optimization 💻
       - Structure for scanability
       - Include shareable takeaways
       - Optimize for SEO
       - Add engaging subheadings

    Format your blog post with this structure:
    # {Viral-Worthy Headline}

    ## Introduction
    {Engaging hook and context}

    ## {Compelling Section 1}
    {Key insights and analysis}
    {Expert quotes and statistics}

    ## {Engaging Section 2}
    {Deeper exploration}
    {Real-world examples}

    ## {Practical Section 3}
    {Actionable insights}
    {Expert recommendations}

    ## Key Takeaways
    - {Shareable insight 1}
    - {Practical takeaway 2}
    - {Notable finding 3}

    ## Sources
    {Properly attributed sources with links}
    """),
    markdown=True,
)


# --- Helper Functions ---
def get_cached_blog_post(workflow: Workflow, topic: str) -> Optional[str]:
    """Get cached blog post from workflow session state"""
    logger.info("Checking if cached blog post exists")
    return workflow.workflow_session_state.get("blog_posts", {}).get(topic)


def cache_blog_post(workflow: Workflow, topic: str, blog_post: str):
    """Cache blog post in workflow session state"""
    logger.info(f"Saving blog post for topic: {topic}")
    if "blog_posts" not in workflow.workflow_session_state:
        workflow.workflow_session_state["blog_posts"] = {}
    workflow.workflow_session_state["blog_posts"][topic] = blog_post


def get_cached_search_results(
    workflow: Workflow, topic: str
) -> Optional[SearchResults]:
    """Get cached search results from workflow session state"""
    logger.info("Checking if cached search results exist")
    search_results = workflow.workflow_session_state.get("search_results", {}).get(
        topic
    )
    if search_results and isinstance(search_results, dict):
        try:
            return SearchResults.model_validate(search_results)
        except Exception as e:
            logger.warning(f"Could not validate cached search results: {e}")
    return search_results if isinstance(search_results, SearchResults) else None


def cache_search_results(workflow: Workflow, topic: str, search_results: SearchResults):
    """Cache search results in workflow session state"""
    logger.info(f"Saving search results for topic: {topic}")
    if "search_results" not in workflow.workflow_session_state:
        workflow.workflow_session_state["search_results"] = {}
    workflow.workflow_session_state["search_results"][topic] = (
        search_results.model_dump()
    )


def get_cached_scraped_articles(
    workflow: Workflow, topic: str
) -> Optional[Dict[str, ScrapedArticle]]:
    """Get cached scraped articles from workflow session state"""
    logger.info("Checking if cached scraped articles exist")
    scraped_articles = workflow.workflow_session_state.get("scraped_articles", {}).get(
        topic
    )
    if scraped_articles and isinstance(scraped_articles, dict):
        try:
            return {
                url: ScrapedArticle.model_validate(article)
                for url, article in scraped_articles.items()
            }
        except Exception as e:
            logger.warning(f"Could not validate cached scraped articles: {e}")
    return scraped_articles if isinstance(scraped_articles, dict) else None


def cache_scraped_articles(
    workflow: Workflow, topic: str, scraped_articles: Dict[str, ScrapedArticle]
):
    """Cache scraped articles in workflow session state"""
    logger.info(f"Saving scraped articles for topic: {topic}")
    if "scraped_articles" not in workflow.workflow_session_state:
        workflow.workflow_session_state["scraped_articles"] = {}
    workflow.workflow_session_state["scraped_articles"][topic] = {
        url: article.model_dump() for url, article in scraped_articles.items()
    }


async def get_search_results(
    workflow: Workflow, topic: str, use_cache: bool = True, num_attempts: int = 3
) -> Optional[SearchResults]:
    """Get search results with caching support"""

    # Check cache first
    if use_cache:
        cached_results = get_cached_search_results(workflow, topic)
        if cached_results:
            logger.info(f"Found {len(cached_results.articles)} articles in cache.")
            return cached_results

    # Search for new results
    for attempt in range(num_attempts):
        try:
            print(
                f"🔍 Searching for articles about: {topic} (attempt {attempt + 1}/{num_attempts})"
            )
            response = await research_agent.arun(topic)

            if (
                response
                and response.content
                and isinstance(response.content, SearchResults)
            ):
                article_count = len(response.content.articles)
                logger.info(f"Found {article_count} articles on attempt {attempt + 1}")
                print(f"✅ Found {article_count} relevant articles")

                # Cache the results
                cache_search_results(workflow, topic, response.content)
                return response.content
            else:
                logger.warning(
                    f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                )

        except Exception as e:
            logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

    logger.error(f"Failed to get search results after {num_attempts} attempts")
    return None


async def scrape_articles(
    workflow: Workflow,
    topic: str,
    search_results: SearchResults,
    use_cache: bool = True,
) -> Dict[str, ScrapedArticle]:
    """Scrape articles with caching support"""

    # Check cache first
    if use_cache:
        cached_articles = get_cached_scraped_articles(workflow, topic)
        if cached_articles:
            logger.info(f"Found {len(cached_articles)} scraped articles in cache.")
            return cached_articles

    scraped_articles: Dict[str, ScrapedArticle] = {}

    print(f"📄 Scraping {len(search_results.articles)} articles...")

    for i, article in enumerate(search_results.articles, 1):
        try:
            print(
                f"📖 Scraping article {i}/{len(search_results.articles)}: {article.title[:50]}..."
            )
            response = await content_scraper_agent.arun(article.url)

            if (
                response
                and response.content
                and isinstance(response.content, ScrapedArticle)
            ):
                scraped_articles[response.content.url] = response.content
                logger.info(f"Scraped article: {response.content.url}")
                print(f"✅ Successfully scraped: {response.content.title[:50]}...")
            else:
                print(f"❌ Failed to scrape: {article.title[:50]}...")

        except Exception as e:
            logger.warning(f"Failed to scrape {article.url}: {str(e)}")
            print(f"❌ Error scraping: {article.title[:50]}...")

    # Cache the scraped articles
    cache_scraped_articles(workflow, topic, scraped_articles)
    return scraped_articles


# --- Main Execution Function ---
async def blog_generation_execution(
    workflow: Workflow,
    topic: str = None,
    use_search_cache: bool = True,
    use_scrape_cache: bool = True,
    use_blog_cache: bool = True,
) -> str:
    """
    Blog post generation workflow execution function.

    Args:
        workflow: The workflow instance
        execution_input: Standard workflow execution input
        topic: Blog post topic (if not provided, uses execution_input.message)
        use_search_cache: Whether to use cached search results
        use_scrape_cache: Whether to use cached scraped articles
        use_blog_cache: Whether to use cached blog posts
        **kwargs: Additional parameters
    """

    blog_topic = topic

    if not blog_topic:
        return "❌ No blog topic provided. Please specify a topic."

    print(f"🎨 Generating blog post about: {blog_topic}")
    print("=" * 60)

    # Check for cached blog post first
    if use_blog_cache:
        cached_blog = get_cached_blog_post(workflow, blog_topic)
        if cached_blog:
            print("📋 Found cached blog post!")
            return cached_blog

    # Phase 1: Research and gather sources
    print(f"\n🔍 PHASE 1: RESEARCH & SOURCE GATHERING")
    print("=" * 50)

    search_results = await get_search_results(workflow, blog_topic, use_search_cache)

    if not search_results or len(search_results.articles) == 0:
        return f"❌ Sorry, could not find any articles on the topic: {blog_topic}"

    print(f"📊 Found {len(search_results.articles)} relevant sources:")
    for i, article in enumerate(search_results.articles, 1):
        print(f"   {i}. {article.title[:60]}...")

    # Phase 2: Content extraction
    print(f"\n📄 PHASE 2: CONTENT EXTRACTION")
    print("=" * 50)

    scraped_articles = await scrape_articles(
        workflow, blog_topic, search_results, use_scrape_cache
    )

    if not scraped_articles:
        return f"❌ Could not extract content from any articles for topic: {blog_topic}"

    print(f"📖 Successfully extracted content from {len(scraped_articles)} articles")

    # Phase 3: Blog post writing
    print(f"\n✍️ PHASE 3: BLOG POST CREATION")
    print("=" * 50)

    # Prepare input for the writer
    writer_input = {
        "topic": blog_topic,
        "articles": [article.model_dump() for article in scraped_articles.values()],
    }

    print("🤖 AI is crafting your blog post...")
    writer_response = await blog_writer_agent.arun(json.dumps(writer_input, indent=2))

    if not writer_response or not writer_response.content:
        return f"❌ Failed to generate blog post for topic: {blog_topic}"

    blog_post = writer_response.content

    # Cache the blog post
    cache_blog_post(workflow, blog_topic, blog_post)

    print("✅ Blog post generated successfully!")
    print(f"📝 Length: {len(blog_post)} characters")
    print(f"📚 Sources: {len(scraped_articles)} articles")

    return blog_post


# --- Workflow Definition ---
blog_generator_workflow = Workflow(
    name="Blog Post Generator",
    description="Advanced blog post generator with research and content creation capabilities",
    storage=SqliteStorage(
        table_name="blog_generator_v2",
        db_file="tmp/blog_generator_v2.db",
        mode="workflow_v2",
    ),
    steps=blog_generation_execution,
    workflow_session_state={},  # Initialize empty session state for caching
)


if __name__ == "__main__":
    import random

    async def main():
        # Fun example topics to showcase the generator's versatility
        example_topics = [
            "The Rise of Artificial General Intelligence: Latest Breakthroughs",
            "How Quantum Computing is Revolutionizing Cybersecurity",
            "Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint",
            "The Future of Work: AI and Human Collaboration",
            "Space Tourism: From Science Fiction to Reality",
            "Mindfulness and Mental Health in the Digital Age",
            "The Evolution of Electric Vehicles: Current State and Future Trends",
            "Why Cats Secretly Run the Internet",
            "The Science Behind Why Pizza Tastes Better at 2 AM",
            "How Rubber Ducks Revolutionized Software Development",
        ]

        # Test with a random topic
        topic = random.choice(example_topics)

        print("🧪 Testing Blog Post Generator v2.0")
        print("=" * 60)
        print(f"📝 Topic: {topic}")
        print()

        # Generate the blog post
        resp = await blog_generator_workflow.arun(
            topic=topic,
            use_search_cache=True,
            use_scrape_cache=True,
            use_blog_cache=True,
        )

        pprint_run_response(resp, markdown=True, show_time=True)

    asyncio.run(main())



================================================
FILE: cookbook/workflows_2/employee_recruiter.py
================================================
"""
This workflow is a simple example of a recruitment workflow where in you can also pass custom prameters like job description,
candidate resume urls, etc. (**kwargs) in the workflow along with workflow execution input.
"""

import io
import random
from datetime import datetime, timedelta
from typing import Any, List

import requests
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.workflow.v2.types import WorkflowExecutionInput
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field
from pypdf import PdfReader


# --- Response models ---
class ScreeningResult(BaseModel):
    name: str
    email: str
    score: float
    feedback: str


class ScheduledCall(BaseModel):
    name: str
    email: str
    call_time: str
    url: str


class EmailContent(BaseModel):
    subject: str
    body: str


# --- PDF utility ---
def extract_text_from_pdf(url: str) -> str:
    try:
        resp = requests.get(url)
        resp.raise_for_status()
        reader = PdfReader(io.BytesIO(resp.content))
        return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception as e:
        print(f"Error extracting PDF from {url}: {e}")
        return ""


# --- Simulation tools ---
def simulate_zoom_scheduling(
    agent: Agent, candidate_name: str, candidate_email: str
) -> str:
    """Simulate Zoom call scheduling"""
    # Generate a future time slot (1-7 days from now, between 10am-6pm IST)
    base_time = datetime.now() + timedelta(days=random.randint(1, 7))
    hour = random.randint(10, 17)  # 10am to 5pm
    scheduled_time = base_time.replace(hour=hour, minute=0, second=0, microsecond=0)

    # Generate fake Zoom URL
    meeting_id = random.randint(100000000, 999999999)
    zoom_url = f"https://zoom.us/j/{meeting_id}"

    result = f"✅ Zoom call scheduled successfully!\n"
    result += f"📅 Time: {scheduled_time.strftime('%Y-%m-%d %H:%M')} IST\n"
    result += f"🔗 Meeting URL: {zoom_url}\n"
    result += f"👤 Participant: {candidate_name} ({candidate_email})"

    return result


def simulate_email_sending(agent: Agent, to_email: str, subject: str, body: str) -> str:
    """Simulate email sending"""
    result = f"📧 Email sent successfully!\n"
    result += f"📮 To: {to_email}\n"
    result += f"📝 Subject: {subject}\n"
    result += f"✉️ Body length: {len(body)} characters\n"
    result += f"🕐 Sent at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

    return result


# --- Agents ---
screening_agent = Agent(
    name="Screening Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Screen candidate given resume text and job description.",
        "Provide a score from 0-10 based on how well they match the job requirements.",
        "Give specific feedback on strengths and areas of concern.",
        "Extract the candidate's name and email from the resume if available.",
    ],
    response_model=ScreeningResult,
)

scheduler_agent = Agent(
    name="Scheduler Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        f"You are scheduling interview calls. Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} IST",
        "Schedule calls between 10am-6pm IST on weekdays.",
        "Use the simulate_zoom_scheduling tool to create the meeting.",
        "Provide realistic future dates and times.",
    ],
    tools=[simulate_zoom_scheduling],
    response_model=ScheduledCall,
)

email_writer_agent = Agent(
    name="Email Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Write professional, friendly interview invitation emails.",
        "Include congratulations, interview details, and next steps.",
        "Keep emails concise but warm and welcoming.",
        "Sign emails as 'John Doe, Senior Software Engineer' with email john@agno.com",
    ],
    response_model=EmailContent,
)

email_sender_agent = Agent(
    name="Email Sender Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You send emails using the simulate_email_sending tool.",
        "Always confirm successful delivery with details.",
    ],
    tools=[simulate_email_sending],
)


# --- Execution function ---
def recruitment_execution(
    workflow: Workflow,
    execution_input: WorkflowExecutionInput,
    job_description: str,
    **kwargs: Any,
) -> str:
    """Execute the complete recruitment workflow"""

    # Get inputs
    message: str = execution_input.message
    jd: str = job_description
    resumes: List[str] = kwargs.get("candidate_resume_urls", [])

    if not resumes:
        return "❌ No candidate resume URLs provided"

    if not jd:
        return "❌ No job description provided"

    print(f"🚀 Starting recruitment process for {len(resumes)} candidates")
    print(f"📋 Job Description: {jd[:100]}{'...' if len(jd) > 100 else ''}")

    selected_candidates: List[ScreeningResult] = []

    # Phase 1: Screening
    print(f"\n📊 PHASE 1: CANDIDATE SCREENING")
    print("=" * 50)

    for i, url in enumerate(resumes, 1):
        print(f"\n🔍 Processing candidate {i}/{len(resumes)}")

        # Extract resume text (with caching)
        if url not in workflow.workflow_session_state:
            print(f"📄 Extracting text from: {url}")
            workflow.workflow_session_state[url] = extract_text_from_pdf(url)
        else:
            print(f"📋 Using cached resume content")

        resume_text = workflow.workflow_session_state[url]

        if not resume_text:
            print(f"❌ Could not extract text from resume")
            continue

        # Screen the candidate
        screening_prompt = f"""
        {message}
        Please screen this candidate for the job position.

        RESUME:
        {resume_text}

        JOB DESCRIPTION:
        {jd}

        Evaluate how well this candidate matches the job requirements and provide a score from 0-10.
        """

        result = screening_agent.run(screening_prompt)
        candidate = result.content

        print(f"👤 Candidate: {candidate.name}")
        print(f"📧 Email: {candidate.email}")
        print(f"⭐ Score: {candidate.score}/10")
        print(
            f"💭 Feedback: {candidate.feedback[:150]}{'...' if len(candidate.feedback) > 150 else ''}"
        )

        if candidate.score >= 5.0:
            selected_candidates.append(candidate)
            print(f"✅ SELECTED for interview!")
        else:
            print(f"❌ Not selected (score below 5.0)")

    # Phase 2: Interview Scheduling & Email Communication
    if selected_candidates:
        print(f"\n📅 PHASE 2: INTERVIEW SCHEDULING")
        print("=" * 50)

        for i, candidate in enumerate(selected_candidates, 1):
            print(
                f"\n🗓️ Scheduling interview {i}/{len(selected_candidates)} for {candidate.name}"
            )

            # Schedule interview
            schedule_prompt = f"""
            Schedule a 1-hour interview call for:
            - Candidate: {candidate.name}
            - Email: {candidate.email}
            - Interviewer: Dirk Brand (dirk@phidata.com)

            Use the simulate_zoom_scheduling tool to create the meeting.
            """

            call_result = scheduler_agent.run(schedule_prompt)
            scheduled_call = call_result.content

            print(f"📅 Scheduled for: {scheduled_call.call_time}")
            print(f"🔗 Meeting URL: {scheduled_call.url}")

            # Write congratulatory email
            email_prompt = f"""
            Write a professional interview invitation email for:
            - Candidate: {candidate.name} ({candidate.email})
            - Interview time: {scheduled_call.call_time}
            - Meeting URL: {scheduled_call.url}
            - Congratulate them on being selected
            - Include next steps and what to expect
            """

            email_result = email_writer_agent.run(email_prompt)
            email_content = email_result.content

            print(f"✏️ Email subject: {email_content.subject}")

            # Send email
            send_prompt = f"""
            Send the interview invitation email:
            - To: {candidate.email}
            - Subject: {email_content.subject}
            - Body: {email_content.body}

            Use the simulate_email_sending tool.
            """

            send_result = email_sender_agent.run(send_prompt)
            print(f"📧 Email sent to {candidate.email}")

    # Final summary
    summary = f"""
    🎉 RECRUITMENT WORKFLOW COMPLETED!

    📊 Summary:
    • Processed: {len(resumes)} candidate resumes
    • Selected: {len(selected_candidates)} candidates for interviews
    • Interviews scheduled: {len(selected_candidates)}
    • Emails sent: {len(selected_candidates)}

    ✅ Selected candidates:
    """

    for candidate in selected_candidates:
        summary += (
            f"\n   • {candidate.name} ({candidate.email}) - Score: {candidate.score}/10"
        )

    return summary


# --- Workflow definition ---
recruitment_workflow = Workflow(
    name="Employee Recruitment Workflow (Simulated)",
    description="Automated candidate screening with simulated scheduling and email",
    storage=SqliteStorage(
        table_name="recruiter_workflow_sessions",
        db_file="tmp/workflows.db",
        mode="workflow_v2",
    ),
    steps=recruitment_execution,
    workflow_session_state={},  # Initialize empty workflow session state
)


if __name__ == "__main__":
    # Test with sample data
    print("🧪 Testing Employee Recruitment Workflow with Simulated Tools")
    print("=" * 60)

    result = recruitment_workflow.print_response(
        message="Process candidates for backend engineer position",
        candidate_resume_urls=[
            "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/cv_1.pdf",
            "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/cv_2.pdf",
        ],
        job_description="""
        We are hiring for backend and systems engineers!
        Join our team building the future of agentic software

        Requirements:
        🧠 You know your way around Python, typescript, docker, and AWS.
        ⚙️ Love to build in public and contribute to open source.
        🚀 Are ok dealing with the pressure of an early-stage startup.
        🏆 Want to be a part of the biggest technological shift since the internet.
        🌟 Bonus: experience with infrastructure as code.
        🌟 Bonus: starred Agno repo.
        """,
    )



================================================
FILE: cookbook/workflows_2/employee_recruiter_async_stream.py
================================================
import asyncio
import io
import random
from datetime import datetime, timedelta
from typing import Any, List

import requests
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.workflow.v2.types import WorkflowExecutionInput
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field
from pypdf import PdfReader


# --- Response models ---
class ScreeningResult(BaseModel):
    name: str
    email: str
    score: float
    feedback: str


class ScheduledCall(BaseModel):
    name: str
    email: str
    call_time: str
    url: str


class EmailContent(BaseModel):
    subject: str
    body: str


# --- PDF utility ---
def extract_text_from_pdf(url: str) -> str:
    try:
        resp = requests.get(url)
        resp.raise_for_status()
        reader = PdfReader(io.BytesIO(resp.content))
        return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception as e:
        print(f"Error extracting PDF from {url}: {e}")
        return ""


# --- Simulation tools ---
def simulate_zoom_scheduling(
    agent: Agent, candidate_name: str, candidate_email: str
) -> str:
    """Simulate Zoom call scheduling"""
    # Generate a future time slot (1-7 days from now, between 10am-6pm IST)
    base_time = datetime.now() + timedelta(days=random.randint(1, 7))
    hour = random.randint(10, 17)  # 10am to 5pm
    scheduled_time = base_time.replace(hour=hour, minute=0, second=0, microsecond=0)

    # Generate fake Zoom URL
    meeting_id = random.randint(100000000, 999999999)
    zoom_url = f"https://zoom.us/j/{meeting_id}"

    result = f"✅ Zoom call scheduled successfully!\n"
    result += f"📅 Time: {scheduled_time.strftime('%Y-%m-%d %H:%M')} IST\n"
    result += f"🔗 Meeting URL: {zoom_url}\n"
    result += f"👤 Participant: {candidate_name} ({candidate_email})"

    return result


def simulate_email_sending(agent: Agent, to_email: str, subject: str, body: str) -> str:
    """Simulate email sending"""
    result = f"📧 Email sent successfully!\n"
    result += f"📮 To: {to_email}\n"
    result += f"📝 Subject: {subject}\n"
    result += f"✉️ Body length: {len(body)} characters\n"
    result += f"🕐 Sent at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

    return result


# --- Agents ---
screening_agent = Agent(
    name="Screening Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Screen candidate given resume text and job description.",
        "Provide a score from 0-10 based on how well they match the job requirements.",
        "Give specific feedback on strengths and areas of concern.",
        "Extract the candidate's name and email from the resume if available.",
    ],
    response_model=ScreeningResult,
)

scheduler_agent = Agent(
    name="Scheduler Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        f"You are scheduling interview calls. Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} IST",
        "Schedule calls between 10am-6pm IST on weekdays.",
        "Use the simulate_zoom_scheduling tool to create the meeting.",
        "Provide realistic future dates and times.",
    ],
    tools=[simulate_zoom_scheduling],
    response_model=ScheduledCall,
)

email_writer_agent = Agent(
    name="Email Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Write professional, friendly interview invitation emails.",
        "Include congratulations, interview details, and next steps.",
        "Keep emails concise but warm and welcoming.",
        "Sign emails as 'John Doe, Senior Software Engineer' with email john@agno.com",
    ],
    response_model=EmailContent,
)

email_sender_agent = Agent(
    name="Email Sender Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You send emails using the simulate_email_sending tool.",
        "Always confirm successful delivery with details.",
    ],
    tools=[simulate_email_sending],
)


# --- Execution function ---
async def recruitment_execution(
    workflow: Workflow,
    execution_input: WorkflowExecutionInput,
    job_description: str,
    **kwargs: Any,
):
    """Execute the complete recruitment workflow"""

    # Get inputs
    message: str = execution_input.message
    jd: str = job_description
    resumes: List[str] = kwargs.get("candidate_resume_urls", [])

    if not resumes:
        yield "❌ No candidate resume URLs provided"

    if not jd:
        yield "❌ No job description provided"

    print(f"🚀 Starting recruitment process for {len(resumes)} candidates")
    print(f"📋 Job Description: {jd[:100]}{'...' if len(jd) > 100 else ''}")

    selected_candidates: List[ScreeningResult] = []

    # Phase 1: Screening
    print(f"\n📊 PHASE 1: CANDIDATE SCREENING")
    print("=" * 50)

    for i, url in enumerate(resumes, 1):
        print(f"\n🔍 Processing candidate {i}/{len(resumes)}")

        # Extract resume text (with caching)
        if url not in workflow.workflow_session_state:
            print(f"📄 Extracting text from: {url}")
            workflow.workflow_session_state[url] = extract_text_from_pdf(url)
        else:
            print(f"📋 Using cached resume content")

        resume_text = workflow.workflow_session_state[url]

        if not resume_text:
            print(f"❌ Could not extract text from resume")
            continue

        # Screen the candidate
        screening_prompt = f"""
        {message}
        Please screen this candidate for the job position.

        RESUME:
        {resume_text}

        JOB DESCRIPTION:
        {jd}

        Evaluate how well this candidate matches the job requirements and provide a score from 0-10.
        """

        async for response in await screening_agent.arun(
            screening_prompt, stream=True, stream_intermediate_steps=True
        ):
            if hasattr(response, "content") and response.content:
                candidate = response.content

        print(f"👤 Candidate: {candidate.name}")
        print(f"📧 Email: {candidate.email}")
        print(f"⭐ Score: {candidate.score}/10")
        print(
            f"💭 Feedback: {candidate.feedback[:150]}{'...' if len(candidate.feedback) > 150 else ''}"
        )

        if candidate.score >= 5.0:
            selected_candidates.append(candidate)
            print(f"✅ SELECTED for interview!")
        else:
            print(f"❌ Not selected (score below 5.0)")

    # Phase 2: Interview Scheduling & Email Communication
    if selected_candidates:
        print(f"\n📅 PHASE 2: INTERVIEW SCHEDULING")
        print("=" * 50)

        for i, candidate in enumerate(selected_candidates, 1):
            print(
                f"\n🗓️ Scheduling interview {i}/{len(selected_candidates)} for {candidate.name}"
            )

            # Schedule interview
            schedule_prompt = f"""
            Schedule a 1-hour interview call for:
            - Candidate: {candidate.name}
            - Email: {candidate.email}
            - Interviewer: Dirk Brand (dirk@phidata.com)

            Use the simulate_zoom_scheduling tool to create the meeting.
            """

            async for response in await scheduler_agent.arun(
                schedule_prompt, stream=True, stream_intermediate_steps=True
            ):
                if hasattr(response, "content") and response.content:
                    scheduled_call = response.content

            print(f"📅 Scheduled for: {scheduled_call.call_time}")
            print(f"🔗 Meeting URL: {scheduled_call.url}")

            # Write congratulatory email
            email_prompt = f"""
            Write a professional interview invitation email for:
            - Candidate: {candidate.name} ({candidate.email})
            - Interview time: {scheduled_call.call_time}
            - Meeting URL: {scheduled_call.url}
            - Congratulate them on being selected
            - Include next steps and what to expect
            """

            async for response in await email_writer_agent.arun(
                email_prompt, stream=True, stream_intermediate_steps=True
            ):
                if hasattr(response, "content") and response.content:
                    email_content = response.content

            print(f"✏️ Email subject: {email_content.subject}")

            # Send email
            send_prompt = f"""
            Send the interview invitation email:
            - To: {candidate.email}
            - Subject: {email_content.subject}
            - Body: {email_content.body}

            Use the simulate_email_sending tool.
            """

            async for response in await email_sender_agent.arun(
                send_prompt, stream=True, stream_intermediate_steps=True
            ):
                yield response


# --- Workflow definition ---
recruitment_workflow = Workflow(
    name="Employee Recruitment Workflow (Simulated)",
    description="Automated candidate screening with simulated scheduling and email",
    storage=SqliteStorage(
        table_name="recruiter_workflow_sessions",
        db_file="tmp/workflows.db",
        mode="workflow_v2",
    ),
    steps=recruitment_execution,
    workflow_session_state={},
)


if __name__ == "__main__":
    # Test with sample data
    print("🧪 Testing Employee Recruitment Workflow with Simulated Tools")
    print("=" * 60)

    asyncio.run(
        recruitment_workflow.aprint_response(
            message="Process candidates for backend engineer position",
            candidate_resume_urls=[
                "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/cv_1.pdf",
                "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/cv_2.pdf",
            ],
            job_description="""
        We are hiring for backend and systems engineers!
        Join our team building the future of agentic software

        Requirements:
        🧠 You know your way around Python, typescript, docker, and AWS.
        ⚙️ Love to build in public and contribute to open source.
        🚀 Are ok dealing with the pressure of an early-stage startup.
        🏆 Want to be a part of the biggest technological shift since the internet.
        🌟 Bonus: experience with infrastructure as code.
        🌟 Bonus: starred Agno repo.
        """,
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/workflows_2/fastapi_demo.py
================================================
from agno.agent import Agent
from agno.app.fastapi import FastAPIApp
from agno.models.openai.chat import OpenAIChat
from agno.utils.log import log_info
from agno.workflow.v2 import Workflow

agent_storage_file: str = "tmp/agents.db"

# Define agents
support_agent = Agent(
    name="Solution Developer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""
        You are a solution developer for customer support. Your job is to create clear,
        step-by-step solutions for customer issues.

        Based on research and knowledge base information, create:
        1. Clear problem diagnosis
        2. Step-by-step solution instructions
        3. Alternative approaches if the main solution fails
        4. Prevention tips for the future

        Make solutions customer-friendly with numbered steps and clear language.
        Include any relevant screenshots, links, or additional resources.
        """,
    markdown=True,
)
triage_agent = Agent(
    name="Ticket Classifier",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""
            You are a customer support ticket classifier. Your job is to analyze customer queries and extract key information.

            For each customer query, provide:
            1. Category (billing, technical, account_access, product_info, bug_report, feature_request)
            2. Priority (low, medium, high, urgent)
            3. Key tags/keywords (extract 3-5 relevant terms)
            4. Brief summary of the issue

            Format your response as:
            Category: [category]
            Priority: [priority]
            Tags: [tag1, tag2, tag3]
            Summary: [brief summary]
            """,
    markdown=True,
)


def cache_solution(workflow: Workflow, query: str, solution: str):
    if "solutions" not in workflow.workflow_session_state:
        workflow.workflow_session_state["solutions"] = {}
    workflow.workflow_session_state["solutions"][query] = solution


def customer_support_execution(workflow: Workflow, query: str) -> str:
    cached_solution = workflow.workflow_session_state.get("solutions", {}).get(query)
    if cached_solution:
        log_info(f"Cache hit! Returning cached solution for query: {query}")
        return cached_solution

    log_info(f"No cached solution found for query: {query}")

    classification_response = triage_agent.run(query)
    classification = classification_response.content

    solution_context = f"""
    Customer Query: {query}

    Classification: {classification}

    Please provide a clear, step-by-step solution for this customer issue.
    Make sure to format it in a customer-friendly way with clear instructions.
    """

    solution_response = support_agent.run(solution_context)
    solution = solution_response.content

    cache_solution(workflow, query, solution)

    return solution


# Create the customer support workflow
customer_support_workflow = Workflow(
    workflow_id="customer-support-resolution-pipeline",
    name="Customer Support Resolution Pipeline",
    description="AI-powered customer support with intelligent caching",
    steps=customer_support_execution,
    workflow_session_state={},  # Initialize empty session state
)

fastapi_app = FastAPIApp(
    workflows=[customer_support_workflow],
    app_id="workflows-fastapi-app",
    name="Workflows FastAPI App",
)
app = fastapi_app.get_app(use_async=False)

if __name__ == "__main__":
    # Start the fastapi server
    fastapi_app.serve(app="fastapi_demo:app", reload=True)



================================================
FILE: cookbook/workflows_2/investment_report_generator.py
================================================
"""💰 Investment Report Generator - Your AI Financial Analysis Studio!

This advanced example demonstrates how to build a sophisticated investment analysis system that combines
market research, financial analysis, and portfolio management. The workflow uses a three-stage
approach:
1. Comprehensive stock analysis and market research
2. Investment potential evaluation and ranking
3. Strategic portfolio allocation recommendations

Key capabilities:
- Real-time market data analysis
- Professional financial research
- Investment risk assessment
- Portfolio allocation strategy
- Detailed investment rationale

Example companies to analyze:
- "AAPL, MSFT, GOOGL" (Tech Giants)
- "NVDA, AMD, INTC" (Semiconductor Leaders)
- "TSLA, F, GM" (Automotive Innovation)
- "JPM, BAC, GS" (Banking Sector)
- "AMZN, WMT, TGT" (Retail Competition)
- "PFE, JNJ, MRNA" (Healthcare Focus)
- "XOM, CVX, BP" (Energy Sector)

Run `pip install openai yfinance agno` to install dependencies.
"""

import asyncio
import random
from pathlib import Path
from shutil import rmtree
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.v2.types import WorkflowExecutionInput
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel


# --- Response models ---
class StockAnalysisResult(BaseModel):
    company_symbols: str
    market_analysis: str
    financial_metrics: str
    risk_assessment: str
    recommendations: str


class InvestmentRanking(BaseModel):
    ranked_companies: str
    investment_rationale: str
    risk_evaluation: str
    growth_potential: str


class PortfolioAllocation(BaseModel):
    allocation_strategy: str
    investment_thesis: str
    risk_management: str
    final_recommendations: str


# --- File management ---
reports_dir = Path(__file__).parent.joinpath("reports", "investment")
if reports_dir.is_dir():
    rmtree(path=reports_dir, ignore_errors=True)
reports_dir.mkdir(parents=True, exist_ok=True)

stock_analyst_report = str(reports_dir.joinpath("stock_analyst_report.md"))
research_analyst_report = str(reports_dir.joinpath("research_analyst_report.md"))
investment_report = str(reports_dir.joinpath("investment_report.md"))


# --- Agents ---
stock_analyst = Agent(
    name="Stock Analyst",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            company_info=True, analyst_recommendations=True, company_news=True
        )
    ],
    description=dedent("""\
    You are MarketMaster-X, an elite Senior Investment Analyst at Goldman Sachs with expertise in:

    - Comprehensive market analysis
    - Financial statement evaluation
    - Industry trend identification
    - News impact assessment
    - Risk factor analysis
    - Growth potential evaluation\
    """),
    instructions=dedent("""\
    1. Market Research 📊
       - Analyze company fundamentals and metrics
       - Review recent market performance
       - Evaluate competitive positioning
       - Assess industry trends and dynamics
    2. Financial Analysis 💹
       - Examine key financial ratios
       - Review analyst recommendations
       - Analyze recent news impact
       - Identify growth catalysts
    3. Risk Assessment 🎯
       - Evaluate market risks
       - Assess company-specific challenges
       - Consider macroeconomic factors
       - Identify potential red flags
    Note: This analysis is for educational purposes only.\
    """),
    response_model=StockAnalysisResult,
)

research_analyst = Agent(
    name="Research Analyst",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are ValuePro-X, an elite Senior Research Analyst at Goldman Sachs specializing in:

    - Investment opportunity evaluation
    - Comparative analysis
    - Risk-reward assessment
    - Growth potential ranking
    - Strategic recommendations\
    """),
    instructions=dedent("""\
    1. Investment Analysis 🔍
       - Evaluate each company's potential
       - Compare relative valuations
       - Assess competitive advantages
       - Consider market positioning
    2. Risk Evaluation 📈
       - Analyze risk factors
       - Consider market conditions
       - Evaluate growth sustainability
       - Assess management capability
    3. Company Ranking 🏆
       - Rank based on investment potential
       - Provide detailed rationale
       - Consider risk-adjusted returns
       - Explain competitive advantages\
    """),
    response_model=InvestmentRanking,
)

investment_lead = Agent(
    name="Investment Lead",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are PortfolioSage-X, a distinguished Senior Investment Lead at Goldman Sachs expert in:

    - Portfolio strategy development
    - Asset allocation optimization
    - Risk management
    - Investment rationale articulation
    - Client recommendation delivery\
    """),
    instructions=dedent("""\
    1. Portfolio Strategy 💼
       - Develop allocation strategy
       - Optimize risk-reward balance
       - Consider diversification
       - Set investment timeframes
    2. Investment Rationale 📝
       - Explain allocation decisions
       - Support with analysis
       - Address potential concerns
       - Highlight growth catalysts
    3. Recommendation Delivery 📊
       - Present clear allocations
       - Explain investment thesis
       - Provide actionable insights
       - Include risk considerations\
    """),
    response_model=PortfolioAllocation,
)


# --- Execution function ---
async def investment_analysis_execution(
    execution_input: WorkflowExecutionInput,
    companies: str,
) -> str:
    """Execute the complete investment analysis workflow"""

    # Get inputs
    message: str = execution_input.message
    company_symbols: str = companies

    if not company_symbols:
        return "❌ No company symbols provided"

    print(f"🚀 Starting investment analysis for companies: {company_symbols}")
    print(f"💼 Analysis request: {message}")

    # Phase 1: Stock Analysis
    print(f"\n📊 PHASE 1: COMPREHENSIVE STOCK ANALYSIS")
    print("=" * 60)

    analysis_prompt = f"""
    {message}

    Please conduct a comprehensive analysis of the following companies: {company_symbols}

    For each company, provide:
    1. Current market position and financial metrics
    2. Recent performance and analyst recommendations
    3. Industry trends and competitive landscape
    4. Risk factors and growth potential
    5. News impact and market sentiment

    Companies to analyze: {company_symbols}
    """

    print(f"🔍 Analyzing market data and fundamentals...")
    stock_analysis_result = await stock_analyst.arun(analysis_prompt)
    stock_analysis = stock_analysis_result.content

    # Save to file
    with open(stock_analyst_report, "w") as f:
        f.write(f"# Stock Analysis Report\n\n")
        f.write(f"**Companies:** {stock_analysis.company_symbols}\n\n")
        f.write(f"## Market Analysis\n{stock_analysis.market_analysis}\n\n")
        f.write(f"## Financial Metrics\n{stock_analysis.financial_metrics}\n\n")
        f.write(f"## Risk Assessment\n{stock_analysis.risk_assessment}\n\n")
        f.write(f"## Recommendations\n{stock_analysis.recommendations}\n")

    print(f"✅ Stock analysis completed and saved to {stock_analyst_report}")

    # Phase 2: Investment Ranking
    print(f"\n🏆 PHASE 2: INVESTMENT POTENTIAL RANKING")
    print("=" * 60)

    ranking_prompt = f"""
    Based on the comprehensive stock analysis below, please rank these companies by investment potential.

    STOCK ANALYSIS:
    - Market Analysis: {stock_analysis.market_analysis}
    - Financial Metrics: {stock_analysis.financial_metrics}
    - Risk Assessment: {stock_analysis.risk_assessment}
    - Initial Recommendations: {stock_analysis.recommendations}

    Please provide:
    1. Detailed ranking of companies from best to worst investment potential
    2. Investment rationale for each company
    3. Risk evaluation and mitigation strategies
    4. Growth potential assessment
    """

    print(f"📈 Ranking companies by investment potential...")
    ranking_result = await research_analyst.arun(ranking_prompt)
    ranking_analysis = ranking_result.content

    # Save to file
    with open(research_analyst_report, "w") as f:
        f.write(f"# Investment Ranking Report\n\n")
        f.write(f"## Company Rankings\n{ranking_analysis.ranked_companies}\n\n")
        f.write(f"## Investment Rationale\n{ranking_analysis.investment_rationale}\n\n")
        f.write(f"## Risk Evaluation\n{ranking_analysis.risk_evaluation}\n\n")
        f.write(f"## Growth Potential\n{ranking_analysis.growth_potential}\n")

    print(f"✅ Investment ranking completed and saved to {research_analyst_report}")

    # Phase 3: Portfolio Allocation Strategy
    print(f"\n💼 PHASE 3: PORTFOLIO ALLOCATION STRATEGY")
    print("=" * 60)

    portfolio_prompt = f"""
    Based on the investment ranking and analysis below, create a strategic portfolio allocation.

    INVESTMENT RANKING:
    - Company Rankings: {ranking_analysis.ranked_companies}
    - Investment Rationale: {ranking_analysis.investment_rationale}
    - Risk Evaluation: {ranking_analysis.risk_evaluation}
    - Growth Potential: {ranking_analysis.growth_potential}

    Please provide:
    1. Specific allocation percentages for each company
    2. Investment thesis and strategic rationale
    3. Risk management approach
    4. Final actionable recommendations
    """

    print(f"💰 Developing portfolio allocation strategy...")
    portfolio_result = await investment_lead.arun(portfolio_prompt)
    portfolio_strategy = portfolio_result.content

    # Save to file
    with open(investment_report, "w") as f:
        f.write(f"# Investment Portfolio Report\n\n")
        f.write(f"## Allocation Strategy\n{portfolio_strategy.allocation_strategy}\n\n")
        f.write(f"## Investment Thesis\n{portfolio_strategy.investment_thesis}\n\n")
        f.write(f"## Risk Management\n{portfolio_strategy.risk_management}\n\n")
        f.write(
            f"## Final Recommendations\n{portfolio_strategy.final_recommendations}\n"
        )

    print(f"✅ Portfolio strategy completed and saved to {investment_report}")

    # Final summary
    summary = f"""
    🎉 INVESTMENT ANALYSIS WORKFLOW COMPLETED!

    📊 Analysis Summary:
    • Companies Analyzed: {company_symbols}
    • Market Analysis: ✅ Completed
    • Investment Ranking: ✅ Completed
    • Portfolio Strategy: ✅ Completed

    📁 Reports Generated:
    • Stock Analysis: {stock_analyst_report}
    • Investment Ranking: {research_analyst_report}
    • Portfolio Strategy: {investment_report}

    💡 Key Insights:
    {portfolio_strategy.allocation_strategy[:200]}...

    ⚠️ Disclaimer: This analysis is for educational purposes only and should not be considered as financial advice.
    """

    return summary


# --- Workflow definition ---
investment_workflow = Workflow(
    name="Investment Report Generator",
    description="Automated investment analysis with market research and portfolio allocation",
    storage=SqliteStorage(
        table_name="investment_workflow_sessions",
        db_file="tmp/workflows.db",
        mode="workflow_v2",
    ),
    steps=investment_analysis_execution,
    workflow_session_state={},  # Initialize empty workflow session state
)


if __name__ == "__main__":

    async def main():
        from rich.prompt import Prompt

        # Example investment scenarios to showcase the analyzer's capabilities
        example_scenarios = [
            "AAPL, MSFT, GOOGL",  # Tech Giants
            "NVDA, AMD, INTC",  # Semiconductor Leaders
            "TSLA, F, GM",  # Automotive Innovation
            "JPM, BAC, GS",  # Banking Sector
            "AMZN, WMT, TGT",  # Retail Competition
            "PFE, JNJ, MRNA",  # Healthcare Focus
            "XOM, CVX, BP",  # Energy Sector
        ]

        # Get companies from user with example suggestion
        companies = Prompt.ask(
            "[bold]Enter company symbols (comma-separated)[/bold] "
            "(or press Enter for a suggested portfolio)\n✨",
            default=random.choice(example_scenarios),
        )

        print("🧪 Testing Investment Report Generator with New Workflow Structure")
        print("=" * 70)

        result = await investment_workflow.arun(
            message="Generate comprehensive investment analysis and portfolio allocation recommendations",
            companies=companies,
        )

        pprint_run_response(result, markdown=True)

    asyncio.run(main())



================================================
FILE: cookbook/workflows_2/playground_demo.py
================================================
"""
1. Install dependencies using: `pip install openai ddgs sqlalchemy 'fastapi[standard]' newspaper4k lxml_html_clean yfinance agno`
2. Run the script using: `python cookbook/workflows/workflows_playground.py`
"""

from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.playground import Playground

# Import the workflows
from agno.storage.sqlite import SqliteStorage
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow
from blog_post_generator import blog_generator_workflow
from investment_report_generator import investment_workflow
from startup_idea_validator import startup_validation_workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    agent=hackernews_agent,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    storage=SqliteStorage(
        table_name="workflow_v2",
        db_file="tmp/workflow_v2.db",
        mode="workflow_v2",
    ),
    steps=[research_step, content_planning_step],
)


# Initialize the Playground with the workflows
playground = Playground(
    workflows=[
        blog_generator_workflow,
        investment_workflow,
        startup_validation_workflow,
        content_creation_workflow,
    ],
    app_id="workflows-playground-app",
    name="Workflows Playground",
)
app = playground.get_app()

if __name__ == "__main__":
    # Start the playground server
    playground.serve(
        app="playground_demo:app",
        host="localhost",
        port=7777,
        reload=True,
    )



================================================
FILE: cookbook/workflows_2/startup_idea_validator.py
================================================
"""
🚀 Startup Idea Validator - Your Personal Business Validation Assistant!

This workflow helps entrepreneurs validate their startup ideas by:
1. Clarifying and refining the core business concept
2. Evaluating originality compared to existing solutions
3. Defining clear mission and objectives
4. Conducting comprehensive market research and analysis

Why is this helpful?
--------------------------------------------------------------------------------
• Get objective feedback on your startup idea before investing resources
• Understand your total addressable market and target segments
• Validate assumptions about market opportunity and competition
• Define clear mission and objectives to guide execution

Who should use this?
--------------------------------------------------------------------------------
• Entrepreneurs and Startup Founders
• Product Managers and Business Strategists
• Innovation Teams
• Angel Investors and VCs doing initial screening

Example use cases:
--------------------------------------------------------------------------------
• New product/service validation
• Market opportunity assessment
• Competitive analysis
• Business model validation
• Target customer segmentation
• Mission/vision refinement

Quick Start:
--------------------------------------------------------------------------------
1. Install dependencies:
   pip install openai agno

2. Set environment variables:
   - OPENAI_API_KEY

3. Run:
   python startup_idea_validator.py

The workflow will guide you through validating your startup idea with AI-powered
analysis and research. Use the insights to refine your concept and business plan!
"""

import asyncio
from typing import Any

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.googlesearch import GoogleSearchTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.v2.types import WorkflowExecutionInput
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


# --- Response models ---
class IdeaClarification(BaseModel):
    originality: str = Field(..., description="Originality of the idea.")
    mission: str = Field(..., description="Mission of the company.")
    objectives: str = Field(..., description="Objectives of the company.")


class MarketResearch(BaseModel):
    total_addressable_market: str = Field(
        ..., description="Total addressable market (TAM)."
    )
    serviceable_available_market: str = Field(
        ..., description="Serviceable available market (SAM)."
    )
    serviceable_obtainable_market: str = Field(
        ..., description="Serviceable obtainable market (SOM)."
    )
    target_customer_segments: str = Field(..., description="Target customer segments.")


class CompetitorAnalysis(BaseModel):
    competitors: str = Field(..., description="List of identified competitors.")
    swot_analysis: str = Field(..., description="SWOT analysis for each competitor.")
    positioning: str = Field(
        ..., description="Startup's potential positioning relative to competitors."
    )


class ValidationReport(BaseModel):
    executive_summary: str = Field(
        ..., description="Executive summary of the validation."
    )
    idea_assessment: str = Field(..., description="Assessment of the startup idea.")
    market_opportunity: str = Field(..., description="Market opportunity analysis.")
    competitive_landscape: str = Field(
        ..., description="Competitive landscape overview."
    )
    recommendations: str = Field(..., description="Strategic recommendations.")
    next_steps: str = Field(..., description="Recommended next steps.")


# --- Agents ---
idea_clarifier_agent = Agent(
    name="Idea Clarifier",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "Given a user's startup idea, your goal is to refine that idea.",
        "Evaluate the originality of the idea by comparing it with existing concepts.",
        "Define the mission and objectives of the startup.",
        "Provide clear, actionable insights about the core business concept.",
    ],
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    response_model=IdeaClarification,
    debug_mode=False,
)

market_research_agent = Agent(
    name="Market Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    instructions=[
        "You are provided with a startup idea and the company's mission and objectives.",
        "Estimate the total addressable market (TAM), serviceable available market (SAM), and serviceable obtainable market (SOM).",
        "Define target customer segments and their characteristics.",
        "Search the web for resources and data to support your analysis.",
        "Provide specific market size estimates with supporting data sources.",
    ],
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    response_model=MarketResearch,
)

competitor_analysis_agent = Agent(
    name="Competitor Analysis Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    instructions=[
        "You are provided with a startup idea and market research data.",
        "Identify existing competitors in the market.",
        "Perform Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis for each competitor.",
        "Assess the startup's potential positioning relative to competitors.",
        "Search for recent competitor information and market positioning.",
    ],
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    response_model=CompetitorAnalysis,
    debug_mode=False,
)

report_agent = Agent(
    name="Report Generator",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are provided with comprehensive data about a startup idea including clarification, market research, and competitor analysis.",
        "Synthesize all information into a comprehensive validation report.",
        "Provide clear executive summary, assessment, and actionable recommendations.",
        "Structure the report professionally with clear sections and insights.",
        "Include specific next steps for the entrepreneur.",
    ],
    add_history_to_messages=True,
    add_datetime_to_instructions=True,
    response_model=ValidationReport,
    debug_mode=False,
)


# --- Execution function ---
async def startup_validation_execution(
    workflow: Workflow,
    execution_input: WorkflowExecutionInput,
    startup_idea: str,
    **kwargs: Any,
) -> str:
    """Execute the complete startup idea validation workflow"""

    # Get inputs
    message: str = execution_input.message
    idea: str = startup_idea

    if not idea:
        return "❌ No startup idea provided"

    print(f"🚀 Starting startup idea validation for: {idea}")
    print(f"💡 Validation request: {message}")

    # Phase 1: Idea Clarification
    print(f"\n🎯 PHASE 1: IDEA CLARIFICATION & REFINEMENT")
    print("=" * 60)

    clarification_prompt = f"""
    {message}

    Please analyze and refine the following startup idea:

    STARTUP IDEA: {idea}

    Evaluate:
    1. The originality of this idea compared to existing solutions
    2. Define a clear mission statement for this startup
    3. Outline specific, measurable objectives

    Provide insights on how to strengthen and focus the core concept.
    """

    print(f"🔍 Analyzing and refining the startup concept...")

    try:
        clarification_result = await idea_clarifier_agent.arun(clarification_prompt)
        idea_clarification = clarification_result.content

        print(f"✅ Idea clarification completed")
        print(f"📝 Mission: {idea_clarification.mission[:100]}...")

    except Exception as e:
        return f"❌ Failed to clarify idea: {str(e)}"

    # Phase 2: Market Research
    print(f"\n📊 PHASE 2: MARKET RESEARCH & ANALYSIS")
    print("=" * 60)

    market_research_prompt = f"""
    Based on the refined startup idea and clarification below, conduct comprehensive market research:

    STARTUP IDEA: {idea}
    ORIGINALITY: {idea_clarification.originality}
    MISSION: {idea_clarification.mission}
    OBJECTIVES: {idea_clarification.objectives}

    Please research and provide:
    1. Total Addressable Market (TAM) - overall market size
    2. Serviceable Available Market (SAM) - portion you could serve
    3. Serviceable Obtainable Market (SOM) - realistic market share
    4. Target customer segments with detailed characteristics

    Use web search to find current market data and trends.
    """

    print(f"📈 Researching market size and customer segments...")

    try:
        market_result = await market_research_agent.arun(market_research_prompt)
        market_research = market_result.content

        print(f"✅ Market research completed")
        print(f"🎯 TAM: {market_research.total_addressable_market[:100]}...")

    except Exception as e:
        return f"❌ Failed to complete market research: {str(e)}"

    # Phase 3: Competitor Analysis
    print(f"\n🏢 PHASE 3: COMPETITIVE LANDSCAPE ANALYSIS")
    print("=" * 60)

    competitor_prompt = f"""
    Based on the startup idea and market research below, analyze the competitive landscape:

    STARTUP IDEA: {idea}
    TAM: {market_research.total_addressable_market}
    SAM: {market_research.serviceable_available_market}
    SOM: {market_research.serviceable_obtainable_market}
    TARGET SEGMENTS: {market_research.target_customer_segments}

    Please research and provide:
    1. Identify direct and indirect competitors
    2. SWOT analysis for each major competitor
    3. Assessment of startup's potential competitive positioning
    4. Market gaps and opportunities

    Use web search to find current competitor information.
    """

    print(f"🔎 Analyzing competitive landscape...")

    try:
        competitor_result = await competitor_analysis_agent.arun(competitor_prompt)
        competitor_analysis = competitor_result.content

        print(f"✅ Competitor analysis completed")
        print(f"🏆 Positioning: {competitor_analysis.positioning[:100]}...")

    except Exception as e:
        return f"❌ Failed to complete competitor analysis: {str(e)}"

    # Phase 4: Final Validation Report
    print(f"\n📋 PHASE 4: COMPREHENSIVE VALIDATION REPORT")
    print("=" * 60)

    report_prompt = f"""
    Synthesize all the research and analysis into a comprehensive startup validation report:

    STARTUP IDEA: {idea}

    IDEA CLARIFICATION:
    - Originality: {idea_clarification.originality}
    - Mission: {idea_clarification.mission}
    - Objectives: {idea_clarification.objectives}

    MARKET RESEARCH:
    - TAM: {market_research.total_addressable_market}
    - SAM: {market_research.serviceable_available_market}
    - SOM: {market_research.serviceable_obtainable_market}
    - Target Segments: {market_research.target_customer_segments}

    COMPETITOR ANALYSIS:
    - Competitors: {competitor_analysis.competitors}
    - SWOT: {competitor_analysis.swot_analysis}
    - Positioning: {competitor_analysis.positioning}

    Create a professional validation report with:
    1. Executive summary
    2. Idea assessment (strengths/weaknesses)
    3. Market opportunity analysis
    4. Competitive landscape overview
    5. Strategic recommendations
    6. Specific next steps for the entrepreneur
    """

    print(f"📝 Generating comprehensive validation report...")

    try:
        final_result = await report_agent.arun(report_prompt)
        validation_report = final_result.content

        print(f"✅ Validation report completed")

    except Exception as e:
        return f"❌ Failed to generate final report: {str(e)}"

    # Final summary
    summary = f"""
    🎉 STARTUP IDEA VALIDATION COMPLETED!

    📊 Validation Summary:
    • Startup Idea: {idea}
    • Idea Clarification: ✅ Completed
    • Market Research: ✅ Completed
    • Competitor Analysis: ✅ Completed
    • Final Report: ✅ Generated

    📈 Key Market Insights:
    • TAM: {market_research.total_addressable_market[:150]}...
    • Target Segments: {market_research.target_customer_segments[:150]}...

    🏆 Competitive Positioning:
    {competitor_analysis.positioning[:200]}...

    📋 COMPREHENSIVE VALIDATION REPORT:

    ## Executive Summary
    {validation_report.executive_summary}

    ## Idea Assessment
    {validation_report.idea_assessment}

    ## Market Opportunity
    {validation_report.market_opportunity}

    ## Competitive Landscape
    {validation_report.competitive_landscape}

    ## Strategic Recommendations
    {validation_report.recommendations}

    ## Next Steps
    {validation_report.next_steps}

    ⚠️ Disclaimer: This validation is for informational purposes only. Conduct additional due diligence before making investment decisions.
    """

    return summary


# --- Workflow definition ---
startup_validation_workflow = Workflow(
    name="Startup Idea Validator",
    description="Comprehensive startup idea validation with market research and competitive analysis",
    storage=SqliteStorage(
        table_name="startup_ideas_workflow_sessions",
        db_file="tmp/workflows.db",
        mode="workflow_v2",
    ),
    steps=startup_validation_execution,
    workflow_session_state={},  # Initialize empty workflow session state
)


if __name__ == "__main__":

    async def main():
        from rich.prompt import Prompt

        # Get idea from user
        idea = Prompt.ask(
            "[bold]What is your startup idea?[/bold]\n✨",
            default="A marketplace for Christmas Ornaments made from leather",
        )

        print("🧪 Testing Startup Idea Validator with New Workflow Structure")
        print("=" * 70)

        result = await startup_validation_workflow.arun(
            message="Please validate this startup idea with comprehensive market research and competitive analysis",
            startup_idea=idea,
        )

        pprint_run_response(result, markdown=True)

    asyncio.run(main())



================================================
FILE: cookbook/workflows_2/async/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/function_instead_of_steps.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.types import WorkflowExecutionInput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


async def custom_execution_function(
    workflow: Workflow, execution_input: WorkflowExecutionInput
):
    print(f"Executing workflow: {workflow.name}")

    # Run the research team
    run_response = research_team.run(execution_input.message)
    research_content = run_response.content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {execution_input.message}

        Research Results: {research_content[:500]}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """
    content_plan = await content_planner.arun(planning_prompt)

    # Return the content plan
    return content_plan.content


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=custom_execution_function,
    )

    asyncio.run(
        content_creation_workflow.aprint_response(
            message="AI trends in 2024",
        )
    )



================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/function_instead_of_steps_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.types import WorkflowExecutionInput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Research key insights and content from Hackernews posts",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


async def custom_execution_function(
    workflow: Workflow, execution_input: WorkflowExecutionInput
):
    print(f"Executing workflow: {workflow.name}")

    # Run the Hackernews agent to gather research content
    research_content = ""
    async for response in await hackernews_agent.arun(
        execution_input.message, stream=True, stream_intermediate_steps=True
    ):
        if hasattr(response, "content") and response.content:
            research_content += str(response.content)

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:
        
        Core Topic: {execution_input.message}
        
        Research Results: {research_content[:500]}
        
        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies
        
        Please create a detailed, actionable content plan.
    """

    async for response in await content_planner.arun(
        planning_prompt, stream=True, stream_intermediate_steps=True
    ):
        yield response


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=custom_execution_function,
    )
    asyncio.run(
        content_creation_workflow.aprint_response(
            message="AI trends in 2024",
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/sequence_of_functions_and_agents.py
================================================
import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


async def prepare_input_for_web_search(step_input: StepInput) -> StepOutput:
    topic = step_input.message
    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post on the topic
	<topic>
	{topic}
	</topic>
	
	Search the web for atleast 10 articles\
	""")
    )


async def prepare_input_for_writer(step_input: StepInput) -> StepOutput:
    topic = step_input.message
    research_team_output = step_input.previous_step_content

    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post on the topic:
	<topic>
	{topic}
	</topic>
    
	Here is information from the web:
	<research_results>
	{research_team_output}
	<research_results>\
	""")
    )


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )
    asyncio.run(
        content_creation_workflow.aprint_response(
            message="AI trends in 2024",
            markdown=True,
        )
    )



================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/sequence_of_functions_and_agents_stream.py
================================================
import asyncio
from textwrap import dedent
from typing import AsyncIterator

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


async def prepare_input_for_web_search(
    step_input: StepInput,
) -> AsyncIterator[StepOutput]:
    """Generator function that yields StepOutput"""
    topic = step_input.message

    # Create proper StepOutput content
    content = dedent(f"""\
        I'm writing a blog post on the topic
        <topic>
        {topic}
        </topic>
        
        Search the web for atleast 10 articles\
        """)

    # Yield a StepOutput as the final result
    yield StepOutput(content=content)


async def prepare_input_for_writer(step_input: StepInput) -> AsyncIterator[StepOutput]:
    """Generator function that yields StepOutput"""
    topic = step_input.message
    research_team_output = step_input.previous_step_content

    # Create proper StepOutput content
    content = dedent(f"""\
        I'm writing a blog post on the topic:
        <topic>
        {topic}
        </topic>
        
        Here is information from the web:
        <research_results>
        {research_team_output}
        </research_results>\
        """)

    # Yield a StepOutput as the final result
    yield StepOutput(content=content)


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )

    asyncio.run(
        content_creation_workflow.aprint_response(
            message="AI trends in 2024",
            markdown=True,
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/sequence_of_steps.py
================================================
"""
This example shows a basic sequential sequence of steps that run agents and teams.

It is for a content writer that creates posts about tech trends from Hackernews and the web.
"""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    storage=SqliteStorage(
        table_name="workflow_v2",
        db_file="tmp/workflow_v2.db",
        mode="workflow_v2",
    ),
    steps=[research_step, content_planning_step],
)


# Create and use workflow
async def main():
    await content_creation_workflow.aprint_response(
        message="AI agent frameworks 2025",
        markdown=True,
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/sequence_of_steps_stream.py
================================================
"""
This example shows a basic sequential sequence of steps that run agents and teams.

This shows how to stream the response from the steps.
"""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)
content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)


# Create and use workflow
async def main():
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )
    await content_creation_workflow.aprint_response(
        message="AI agent frameworks 2025",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/step_with_function_additional_data.py
================================================
import asyncio
from typing import AsyncIterator, Union

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.v2.workflow import WorkflowRunResponseEvent
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step, StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


async def custom_content_planning_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunResponseEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness
    Now also uses additional_data for extra context
    """
    message = step_input.message
    previous_step_content = step_input.previous_step_content

    # Access additional_data that was passed with the workflow
    additional_data = step_input.additional_data or {}
    user_email = additional_data.get("user_email", "No email provided")
    priority = additional_data.get("priority", "normal")
    client_type = additional_data.get("client_type", "standard")

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Additional Context:
        - Client Type: {client_type}
        - Priority Level: {priority}
        - Contact Email: {user_email}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies
        {"6. Mark as HIGH PRIORITY delivery" if priority == "high" else "6. Standard delivery timeline"}

        Please create a detailed, actionable content plan.
    """

    try:
        response_iterator = await content_planner.arun(
            planning_prompt, stream=True, stream_intermediate_steps=True
        )
        async for event in response_iterator:
            yield event
        response = content_planner.run_response

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Client Details:**
            - Type: {client_type}
            - Priority: {priority.upper()}
            - Contact: {user_email}

            **Research Integration:** {"✓ Research-based" if previous_step_content else "✗ No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
            - Priority Level: {priority.upper()}
        """.strip()

        yield StepOutput(content=enhanced_content, response=response)

    except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


# Define and use examples
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )

    # Run workflow with additional_data
    asyncio.run(
        content_creation_workflow.aprint_response(
            message="AI trends in 2024",
            additional_data={
                "user_email": "kaustubh@agno.com",
                "priority": "high",
                "client_type": "enterprise",
            },
            markdown=True,
            stream=True,
            stream_intermediate_steps=True,
        )
    )

    print("\n" + "=" * 60 + "\n")



================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/step_with_function_stream.py
================================================
import asyncio
from typing import AsyncIterator, Union

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.v2.workflow import WorkflowRunResponseEvent
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step, StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GoogleSearchTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


async def custom_content_planning_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunResponseEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness
    """
    message = step_input.message
    previous_step_content = step_input.previous_step_content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    try:
        response_iterator = await content_planner.arun(
            planning_prompt, stream=True, stream_intermediate_steps=True
        )
        async for event in response_iterator:
            yield event

        response = content_planner.run_response

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Research Integration:** {"✓ Research-based" if previous_step_content else "✗ No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

        yield StepOutput(content=enhanced_content, response=response)

    except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


async def main():
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )
    await content_creation_workflow.aprint_response(
        message="AI agent frameworks 2025",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/workflows_2/async/01_basic_workflows/workflow_using_steps.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.steps import Steps
from agno.workflow.v2.workflow import Workflow

# Define agents for different tasks
researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

editor = Agent(
    name="Editor Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Review and edit the article for clarity, grammar, and flow. Provide a polished final version.",
)

# Define individual steps
research_step = Step(
    name="research",
    agent=researcher,
    description="Research the topic and gather information",
)

writing_step = Step(
    name="writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="editing",
    agent=editor,
    description="Edit and polish the article",
)

# Create a Steps sequence that chains these above steps together
article_creation_sequence = Steps(
    name="article_creation",
    description="Complete article creation workflow from research to final edit",
    steps=[research_step, writing_step, editing_step],
)

# Create and use workflow
if __name__ == "__main__":
    article_workflow = Workflow(
        name="Article Creation Workflow",
        description="Automated article creation from research to publication",
        steps=[article_creation_sequence],
    )

    asyncio.run(
        article_workflow.aprint_response(
            message="Write an article about the benefits of renewable energy",
            markdown=True,
        )
    )



================================================
FILE: cookbook/workflows_2/async/02_workflows_conditional_execution/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/async/02_workflows_conditional_execution/condition_and_parallel_steps.py
================================================
import asyncio

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.message or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_we_should_search_web(step_input: StepInput) -> bool:
    """Check if we should search the web"""
    topic = step_input.message or step_input.previous_step_content or ""
    general_keywords = ["news", "information", "research", "facts", "data"]
    return any(keyword in topic.lower() for keyword in general_keywords)


def check_if_we_should_search_x(step_input: StepInput) -> bool:
    """Check if we should search X/Twitter"""
    topic = step_input.message or step_input.previous_step_content or ""
    social_keywords = [
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "twitter",
        "x",
    ]
    return any(keyword in topic.lower() for keyword in social_keywords)


def check_if_we_should_search_exa(step_input: StepInput) -> bool:
    """Check if we should use Exa search"""
    topic = step_input.message or step_input.previous_step_content or ""
    advanced_keywords = ["deep", "academic", "research", "analysis", "comprehensive"]
    return any(keyword in topic.lower() for keyword in advanced_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],
                ),
                Condition(
                    name="WebSearchCondition",
                    description="Check if we should search the web for general information",
                    evaluator=check_if_we_should_search_web,
                    steps=[research_web_step],
                ),
                Condition(
                    name="ExaSearchCondition",
                    description="Check if we should use Exa for advanced search",
                    evaluator=check_if_we_should_search_exa,
                    steps=[research_exa_step],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        asyncio.run(
            workflow.aprint_response(
                message="Latest AI developments in machine learning"
            )
        )
    except Exception as e:
        print(f"❌ Error: {e}")
    print()



================================================
FILE: cookbook/workflows_2/async/02_workflows_conditional_execution/condition_and_parallel_steps_stream.py
================================================
import asyncio

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.message or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_we_should_search_web(step_input: StepInput) -> bool:
    """Check if we should search the web"""
    topic = step_input.message or step_input.previous_step_content or ""
    general_keywords = ["news", "information", "research", "facts", "data"]
    return any(keyword in topic.lower() for keyword in general_keywords)


def check_if_we_should_search_x(step_input: StepInput) -> bool:
    """Check if we should search X/Twitter"""
    topic = step_input.message or step_input.previous_step_content or ""
    social_keywords = [
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "twitter",
        "x",
    ]
    return any(keyword in topic.lower() for keyword in social_keywords)


def check_if_we_should_search_exa(step_input: StepInput) -> bool:
    """Check if we should use Exa search"""
    topic = step_input.message or step_input.previous_step_content or ""
    advanced_keywords = ["deep", "academic", "research", "analysis", "comprehensive"]
    return any(keyword in topic.lower() for keyword in advanced_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],
                ),
                Condition(
                    name="WebSearchCondition",
                    description="Check if we should search the web for general information",
                    evaluator=check_if_we_should_search_web,
                    steps=[research_web_step],
                ),
                Condition(
                    name="ExaSearchCondition",
                    description="Check if we should use Exa for advanced search",
                    evaluator=check_if_we_should_search_exa,
                    steps=[research_exa_step],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        asyncio.run(
            workflow.aprint_response(
                message="Latest AI developments in machine learning",
                stream=True,
                stream_intermediate_steps=True,
            )
        )
    except Exception as e:
        print(f"❌ Error: {e}")
    print()



================================================
FILE: cookbook/workflows_2/async/02_workflows_conditional_execution/condition_steps_workflow_stream.py
================================================
import asyncio

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === BASIC AGENTS ===
researcher = Agent(
    name="Researcher",
    instructions="Research the given topic and provide detailed findings.",
    tools=[DuckDuckGoTools()],
)

summarizer = Agent(
    name="Summarizer",
    instructions="Create a clear summary of the research findings.",
)

fact_checker = Agent(
    name="Fact Checker",
    instructions="Verify facts and check for accuracy in the research.",
    tools=[DuckDuckGoTools()],
)

writer = Agent(
    name="Writer",
    instructions="Write a comprehensive article based on all available research and verification.",
)

# === CONDITION EVALUATOR ===


def needs_fact_checking(step_input: StepInput) -> bool:
    """Determine if the research contains claims that need fact-checking"""
    summary = step_input.previous_step_content or ""

    # Look for keywords that suggest factual claims
    fact_indicators = [
        "study shows",
        "research indicates",
        "according to",
        "statistics",
        "data shows",
        "survey",
        "report",
        "million",
        "billion",
        "percent",
        "%",
        "increase",
        "decrease",
    ]

    return any(indicator in summary.lower() for indicator in fact_indicators)


# === WORKFLOW STEPS ===
research_step = Step(
    name="research",
    description="Research the topic",
    agent=researcher,
)

summarize_step = Step(
    name="summarize",
    description="Summarize research findings",
    agent=summarizer,
)

# Conditional fact-checking step
fact_check_step = Step(
    name="fact_check",
    description="Verify facts and claims",
    agent=fact_checker,
)

write_article = Step(
    name="write_article",
    description="Write final article",
    agent=writer,
)

# === BASIC LINEAR WORKFLOW ===
basic_workflow = Workflow(
    name="Basic Linear Workflow",
    description="Research -> Summarize -> Condition(Fact Check) -> Write Article",
    steps=[
        research_step,
        summarize_step,
        Condition(
            name="fact_check_condition",
            description="Check if fact-checking is needed",
            evaluator=needs_fact_checking,
            steps=[fact_check_step],
        ),
        write_article,
    ],
)

if __name__ == "__main__":
    print("🚀 Running Basic Linear Workflow Example")
    print("=" * 50)

    try:
        asyncio.run(
            basic_workflow.aprint_response(
                message="Recent breakthroughs in quantum computing",
                stream=True,
                stream_intermediate_steps=True,
            )
        )
    except Exception as e:
        print(f"❌ Error: {e}")
        import traceback

        traceback.print_exc()



================================================
FILE: cookbook/workflows_2/async/02_workflows_conditional_execution/condition_with_list_of_steps.py
================================================
import asyncio

from agno.agent.agent import Agent
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# Additional agents for multi-step condition
trend_analyzer_agent = Agent(
    name="Trend Analyzer",
    instructions="Analyze trends and patterns from research data",
)

fact_checker_agent = Agent(
    name="Fact Checker",
    instructions="Verify facts and cross-reference information",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

# === MULTI-STEP CONDITION STEPS ===
deep_exa_analysis_step = Step(
    name="DeepExaAnalysis",
    description="Conduct deep analysis using Exa search capabilities",
    agent=exa_agent,
)

trend_analysis_step = Step(
    name="TrendAnalysis",
    description="Analyze trends and patterns from the research data",
    agent=trend_analyzer_agent,
)

fact_verification_step = Step(
    name="FactVerification",
    description="Verify facts and cross-reference information",
    agent=fact_checker_agent,
)

# === FINAL STEPS ===
write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.message or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_comprehensive_research_needed(step_input: StepInput) -> bool:
    """Check if comprehensive multi-step research is needed"""
    topic = step_input.message or step_input.previous_step_content or ""
    comprehensive_keywords = [
        "comprehensive",
        "detailed",
        "thorough",
        "in-depth",
        "complete analysis",
        "full report",
        "extensive research",
    ]
    return any(keyword in topic.lower() for keyword in comprehensive_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow with Multi-Step Condition",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],  # Single step
                ),
                Condition(
                    name="ComprehensiveResearchCondition",
                    description="Check if comprehensive multi-step research is needed",
                    evaluator=check_if_comprehensive_research_needed,
                    steps=[  # Multiple steps
                        deep_exa_analysis_step,
                        trend_analysis_step,
                        fact_verification_step,
                    ],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            write_step,
        ],
    )

    try:
        asyncio.run(
            workflow.aprint_response(
                message="Comprehensive analysis of climate change research",
            )
        )
    except Exception as e:
        print(f"❌ Error: {e}")
    print()



================================================
FILE: cookbook/workflows_2/async/03_workflows_loop_execution/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/async/03_workflows_loop_execution/loop_steps_workflow.py
================================================
import asyncio
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Loop, Step, Workflow
from agno.workflow.v2.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

# End condition function


def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Simple check - if any output contains substantial content, we're good
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f"✅ Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print("❌ Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            message="Research the latest trends in AI and machine learning, then create a summary",
        )
    )



================================================
FILE: cookbook/workflows_2/async/03_workflows_loop_execution/loop_steps_workflow_stream.py
================================================
import asyncio
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Loop, Step, Workflow
from agno.workflow.v2.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

# End condition function


def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Simple check - if any output contains substantial content, we're good
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f"✅ Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print("❌ Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            message="Research the latest trends in AI and machine learning, then create a summary",
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/workflows_2/async/03_workflows_loop_execution/loop_with_parallel_steps_stream.py
================================================
import asyncio
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Loop, Parallel, Step, Workflow
from agno.workflow.v2.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

analysis_agent = Agent(
    name="Analysis Agent",
    role="Data analyst",
    instructions="You are a data analyst. Analyze and summarize research findings.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

# Create analysis steps
trend_analysis_step = Step(
    name="Trend Analysis",
    agent=analysis_agent,
    description="Analyze trending patterns in the research",
)

sentiment_analysis_step = Step(
    name="Sentiment Analysis",
    agent=analysis_agent,
    description="Analyze sentiment and opinions from the research",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)


# End condition function
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Calculate total content length from all outputs
    total_content_length = sum(len(output.content or "") for output in outputs)

    # Check if we have substantial content (more than 500 chars total)
    if total_content_length > 500:
        print(
            f"✅ Research evaluation passed - found substantial content ({total_content_length} chars total)"
        )
        return True

    print(
        f"❌ Research evaluation failed - need more substantial research (current: {total_content_length} chars)"
    )
    return False


# Create workflow with loop containing parallel steps
workflow = Workflow(
    name="Advanced Research and Content Workflow",
    description="Research topics with parallel execution in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop with Parallel Execution",
            steps=[
                Parallel(
                    research_hackernews_step,
                    research_web_step,
                    trend_analysis_step,
                    name="Parallel Research & Analysis",
                    description="Execute research and analysis in parallel for efficiency",
                ),
                sentiment_analysis_step,
            ],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            message="Research the latest trends in AI and machine learning, then create a summary",
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/workflows_2/async/04_workflows_parallel_execution/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/async/04_workflows_parallel_execution/parallel_and_condition_steps_stream.py
================================================
import asyncio

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def should_conduct_research(step_input: StepInput) -> bool:
    """Check if we should conduct comprehensive research"""
    topic = step_input.message or step_input.previous_step_content or ""

    # Keywords that indicate research is needed
    research_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
        "news",
        "information",
        "research",
        "facts",
        "data",
        "analysis",
        "comprehensive",
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "developments",
    ]

    # If the topic contains any research-worthy keywords, conduct research
    return any(keyword in topic.lower() for keyword in research_keywords)


def is_tech_related(step_input: StepInput) -> bool:
    """Check if the topic is tech-related for additional tech research"""
    topic = step_input.message or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Research Workflow",
        description="Conditionally execute parallel research based on topic relevance",
        steps=[
            # Main research condition - if topic needs research, run parallel research steps
            Condition(
                name="ResearchCondition",
                description="Check if comprehensive research is needed for this topic",
                evaluator=should_conduct_research,
                steps=[
                    Parallel(
                        research_hackernews_step,
                        research_web_step,
                        name="ComprehensiveResearch",
                        description="Run multiple research sources in parallel",
                    ),
                    research_exa_step,
                ],
            ),
            # # Additional tech-specific research if needed
            Condition(
                name="TechResearchCondition",
                description="Additional tech-focused research if topic is tech-related",
                evaluator=is_tech_related,
                steps=[
                    Step(
                        name="TechAnalysis",
                        description="Deep dive tech analysis and trend identification",
                        agent=content_agent,
                    ),
                ],
            ),
            # Content preparation and writing
            prepare_input_for_write_step,
            # write_step,
        ],
    )

    try:
        asyncio.run(
            workflow.aprint_response(
                message="Latest AI developments in machine learning",
                stream=True,
                stream_intermediate_steps=True,
            )
        )
    except Exception as e:
        print(f"❌ Error: {e}")
    print()



================================================
FILE: cookbook/workflows_2/async/04_workflows_parallel_execution/parallel_steps_workflow.py
================================================
import asyncio

from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.parallel import Parallel

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

if __name__ == "__main__":
    asyncio.run(workflow.aprint_response("Write about the latest AI developments"))



================================================
FILE: cookbook/workflows_2/async/04_workflows_parallel_execution/parallel_steps_workflow_stream.py
================================================
import asyncio

from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.parallel import Parallel

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

asyncio.run(
    workflow.aprint_response(
        "Write about the latest AI developments",
        stream=True,
        stream_intermediate_steps=True,
    )
)



================================================
FILE: cookbook/workflows_2/async/05_workflows_conditional_branching/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/async/05_workflows_conditional_branching/router_steps_workflow.py
================================================
import asyncio
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.message or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f"🔍 Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f"🌐 General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            "Latest developments in artificial intelligence and machine learning"
        )
    )



================================================
FILE: cookbook/workflows_2/async/05_workflows_conditional_branching/router_steps_workflow_stream.py
================================================
import asyncio
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.message or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f"🔍 Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f"🌐 General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            "Latest developments in artificial intelligence and machine learning",
            stream=True,
            stream_intermediate_steps=True,
        )
    )



================================================
FILE: cookbook/workflows_2/async/05_workflows_conditional_branching/router_with_loop_steps.py
================================================
import asyncio
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.loop import Loop
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)

# End condition function for the loop


def research_quality_check(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    if not outputs:
        return False

    # Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 300:
            print(
                f"✅ Research quality check passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print("❌ Research quality check failed - need more substantial research")
    return False


# Create a Loop step for deep tech research
deep_tech_research_loop = Loop(
    name="Deep Tech Research Loop",
    steps=[research_hackernews],
    end_condition=research_quality_check,
    max_iterations=3,
    description="Perform iterative deep research on tech topics",
)

# Router function that selects between simple web research or deep tech research loop


def research_strategy_router(step_input: StepInput) -> List[Step]:
    """
    Decide between simple web research or deep tech research loop based on the input topic.
    Returns either a single web research step or a tech research loop.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.message or ""
    topic = topic.lower()

    # Check if the topic requires deep tech research
    deep_tech_keywords = [
        "startup trends",
        "ai developments",
        "machine learning research",
        "programming languages",
        "developer tools",
        "silicon valley",
        "venture capital",
        "cryptocurrency analysis",
        "blockchain technology",
        "open source projects",
        "github trends",
        "tech industry",
        "software engineering",
    ]

    # Check if it's a complex tech topic that needs deep research
    if any(keyword in topic for keyword in deep_tech_keywords) or (
        "tech" in topic and len(topic.split()) > 3
    ):
        print(
            f"🔬 Deep tech topic detected: Using iterative research loop for '{topic}'"
        )
        return [deep_tech_research_loop]
    else:
        print(f"🌐 Simple topic detected: Using basic web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Adaptive Research Workflow",
    description="Intelligently selects between simple web research or deep iterative tech research based on topic complexity",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_strategy_router,
            choices=[research_web, deep_tech_research_loop],
            description="Chooses between simple web research or deep tech research loop",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    print("=== Testing with deep tech topic ===")
    asyncio.run(
        workflow.aprint_response(
            "Latest developments in artificial intelligence and machine learning and deep tech research trends"
        )
    )



================================================
FILE: cookbook/workflows_2/async/05_workflows_conditional_branching/selector_for_image_video_generation_pipeline.py
================================================
import asyncio
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.tools.openai import OpenAITools
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.steps import Steps
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel


# Define the structured message data
class MediaRequest(BaseModel):
    topic: str
    content_type: str  # "image" or "video"
    prompt: str
    style: Optional[str] = "realistic"
    duration: Optional[int] = None  # For video, duration in seconds
    resolution: Optional[str] = "1024x1024"  # For image resolution


# Define specialized agents for different media types
image_generator = Agent(
    name="Image Generator",
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    instructions="""You are an expert image generation specialist.
    When users request image creation, you should ACTUALLY GENERATE the image using your available image generation tools.

    Always use the generate_image tool to create the requested image based on the user's specifications.
    Include detailed, creative prompts that incorporate style, composition, lighting, and mood details.

    After generating the image, provide a brief description of what you created.""",
)

image_describer = Agent(
    name="Image Describer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""You are an expert image analyst and describer.
    When you receive an image (either as input or from a previous step), analyze and describe it in vivid detail, including:
    - Visual elements and composition
    - Colors, lighting, and mood
    - Artistic style and technique
    - Emotional impact and narrative

    If no image is provided, work with the image description or prompt from the previous step.
    Provide rich, engaging descriptions that capture the essence of the visual content.""",
)

video_generator = Agent(
    name="Video Generator",
    model=OpenAIChat(id="gpt-4o"),
    # Video Generation only works on VertexAI mode
    tools=[GeminiTools(vertexai=True)],
    instructions="""You are an expert video production specialist.
    Create detailed video generation prompts and storyboards based on user requests.
    Include scene descriptions, camera movements, transitions, and timing.
    Consider pacing, visual storytelling, and technical aspects like resolution and duration.
    Format your response as a comprehensive video production plan.""",
)

video_describer = Agent(
    name="Video Describer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""You are an expert video analyst and critic.
    Analyze and describe videos comprehensively, including:
    - Scene composition and cinematography
    - Narrative flow and pacing
    - Visual effects and production quality
    - Audio-visual harmony and mood
    - Technical execution and artistic merit
    Provide detailed, professional video analysis.""",
)

# Define steps for image pipeline
generate_image_step = Step(
    name="generate_image",
    agent=image_generator,
    description="Generate a detailed image creation prompt based on the user's request",
)

describe_image_step = Step(
    name="describe_image",
    agent=image_describer,
    description="Analyze and describe the generated image concept in vivid detail",
)

# Define steps for video pipeline
generate_video_step = Step(
    name="generate_video",
    agent=video_generator,
    description="Create a comprehensive video production plan and storyboard",
)

describe_video_step = Step(
    name="describe_video",
    agent=video_describer,
    description="Analyze and critique the video production plan with professional insights",
)

# Define the two distinct pipelines
image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[generate_image_step, describe_image_step],
)

video_sequence = Steps(
    name="video_generation",
    description="Complete video production and analysis workflow",
    steps=[generate_video_step, describe_video_step],
)


def media_sequence_selector(step_input: StepInput) -> List[Step]:
    """
    Simple pipeline selector based on keywords in the message.

    Args:
        step_input: StepInput containing message

    Returns:
        List of Steps to execute
    """

    # Check if message exists and is a string
    if not step_input.message or not isinstance(step_input.message, str):
        return [image_sequence]  # Default to image sequence

    # Convert message to lowercase for case-insensitive matching
    message_lower = step_input.message.lower()

    # Check for video keywords
    if "video" in message_lower:
        return [video_sequence]
    # Check for image keywords
    elif "image" in message_lower:
        return [image_sequence]
    else:
        # Default to image for any other case
        return [image_sequence]


# Usage examples
if __name__ == "__main__":
    # Create the media generation workflow
    media_workflow = Workflow(
        name="AI Media Generation Workflow",
        description="Generate and analyze images or videos using AI agents",
        steps=[
            Router(
                name="Media Type Router",
                description="Routes to appropriate media generation pipeline based on content type",
                selector=media_sequence_selector,
                choices=[image_sequence, video_sequence],
            )
        ],
    )

    print("=== Example 1: Image Generation (using message_data) ===")
    image_request = MediaRequest(
        topic="Create an image of magical forest for a movie scene",
        content_type="image",
        prompt="A mystical forest with glowing mushrooms",
        style="fantasy art",
        resolution="1920x1080",
    )

    asyncio.run(
        media_workflow.aprint_response(
            message="Create an image of magical forest for a movie scene",
            markdown=True,
        )
    )

    # print("\n=== Example 2: Video Generation (using message_data) ===")
    # video_request = MediaRequest(
    #     topic="Create a cinematic video city timelapse",
    #     content_type="video",
    #     prompt="A time-lapse of a city skyline from day to night",
    #     style="cinematic",
    #     duration=30,
    #     resolution="4K"
    # )

    # asyncio.run(media_workflow.aprint_response(
    #     message="Create a cinematic video city timelapse",
    #     markdown=True,
    # ))



================================================
FILE: cookbook/workflows_2/async/06_workflows_advanced_concepts/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/async/06_workflows_advanced_concepts/background_execution.py
================================================
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.base import RunStatus
from agno.run.v2.workflow import WorkflowRunResponse
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    storage=SqliteStorage(
        table_name="workflow_v2_bg",
        db_file="tmp/workflow_v2_bg.db",
        mode="workflow_v2",
    ),
    steps=[research_step, content_planning_step],
)


async def main():
    print(" Starting Async Background Workflow Test")

    # Start background execution (async)
    bg_response = await content_creation_workflow.arun(
        message="AI trends in 2024", background=True
    )
    print(f" Initial Response: {bg_response.status} - {bg_response.content}")
    print(f" Run ID: {bg_response.run_id}")

    # Poll every 5 seconds until completion
    poll_count = 0

    while True:
        poll_count += 1
        print(f"\n Poll #{poll_count} (every 5s)")

        result = content_creation_workflow.get_run(bg_response.run_id)

        if result is None:
            print(" Workflow not found yet, still waiting...")
            if poll_count > 50:
                print(f"⏰ Timeout after {poll_count} attempts")
                break
            await asyncio.sleep(5)
            continue

        if result.has_completed():
            break

        if poll_count > 200:
            print(f"⏰ Timeout after {poll_count} attempts")
            break

        await asyncio.sleep(5)

    final_result = content_creation_workflow.get_run(bg_response.run_id)

    print(f"\n Final Result:")
    print("=" * 50)
    pprint_run_response(final_result, markdown=True)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cookbook/workflows_2/sync/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/basic_workflow.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.postgres import PostgresStorage
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


# Define response format
class Source(BaseModel):
    name: str = Field(description="The name of the source")
    page_number: int = Field(description="The page number of the source")


class Response(BaseModel):
    response: str = Field(description="The response to the user's query")
    sources: list[Source] = Field(
        description="The sources used to generate the response"
    )


# Define agents
information_agent = Agent(
    name="Information Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Gathers sufficient context from the user regarding their query",
)

knowledge_search_agent = Agent(
    name="Knowledge Search Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Searches the knowledge base for relevant information to answer the user's query",
    # knowledge=knowledge, # Commented out for now, but will have its own knowledge base
)

response_agent = Agent(
    name="Response Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Respond to the user's query based on the provided information and sources",
    response_model=Response,
)

# Define steps
information_gather_step = Step(
    name="Research Step",
    agent=information_agent,
)

knowledge_search_step = Step(
    name="Knowledge Search Step",
    agent=knowledge_search_agent,
)

response_step = Step(
    name="Response Step",
    agent=response_agent,
)

# Create and use workflow
if __name__ == "__main__":
    rag_workflow = Workflow(
        name="RAG Workflow",
        description="A RAG workflow tasked with answering user queries based on a provided knowledge base.",
        storage=PostgresStorage(
            table_name="workflow_v2",
            db_url=db_url,
            mode="workflow_v2",
        ),
        steps=[information_gather_step, knowledge_search_step, response_step],
    )
    rag_workflow.print_response(
        message="What is the latest news in AI?",
        markdown=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/function_instead_of_steps.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.v2.types import WorkflowExecutionInput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_execution_function(
    workflow: Workflow, execution_input: WorkflowExecutionInput
):
    print(f"Executing workflow: {workflow.name}")

    # Run the research team
    run_response = research_team.run(execution_input.message)
    research_content = run_response.content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {execution_input.message}

        Research Results: {research_content[:500]}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """
    content_plan = content_planner.run(planning_prompt)

    # Return the content plan
    return content_plan.content


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=custom_execution_function,
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/function_instead_of_steps_stream.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.v2.types import WorkflowExecutionInput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Research key insights and content from Hackernews posts",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_execution_function(
    workflow: Workflow, execution_input: WorkflowExecutionInput
):
    print(f"Executing workflow: {workflow.name}")

    # Run the Hackernews agent to gather research content
    research_content = ""
    for response in hackernews_agent.run(
        execution_input.message, stream=True, stream_intermediate_steps=True
    ):
        if hasattr(response, "content") and response.content:
            research_content += str(response.content)

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {execution_input.message}

        Research Results: {research_content[:500]}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """
    yield from content_planner.run(
        planning_prompt, stream=True, stream_intermediate_steps=True
    )


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=custom_execution_function,
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/sequence_of_functions_and_agents.py
================================================
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


def prepare_input_for_web_search(step_input: StepInput) -> StepOutput:
    topic = step_input.message
    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post on the topic
	<topic>
	{topic}
	</topic>
	
	Search the web for atleast 10 articles\
	""")
    )


def prepare_input_for_writer(step_input: StepInput) -> StepOutput:
    topic = step_input.message
    research_team_output = step_input.previous_step_content

    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post on the topic:
	<topic>
	{topic}
	</topic>
    
	Here is information from the web:
	<research_results>
	{research_team_output}
	<research_results>\
	""")
    )


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/sequence_of_functions_and_agents_stream.py
================================================
from textwrap import dedent
from typing import Iterator

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


def prepare_input_for_web_search(step_input: StepInput) -> Iterator[StepOutput]:
    """Generator function that yields StepOutput"""
    topic = step_input.message

    # Create proper StepOutput content
    content = dedent(f"""\
        I'm writing a blog post on the topic
        <topic>
        {topic}
        </topic>
        
        Search the web for atleast 10 articles\
        """)

    # Yield a StepOutput as the final result
    yield StepOutput(content=content)


def prepare_input_for_writer(step_input: StepInput) -> Iterator[StepOutput]:
    """Generator function that yields StepOutput"""
    topic = step_input.message
    research_team_output = step_input.previous_step_content

    # Create proper StepOutput content
    content = dedent(f"""\
        I'm writing a blog post on the topic:
        <topic>
        {topic}
        </topic>
        
        Here is information from the web:
        <research_results>
        {research_team_output}
        </research_results>\
        """)

    # Yield a StepOutput as the final result
    yield StepOutput(content=content)


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/sequence_of_steps.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    storage=SqliteStorage(
        table_name="workflow_v2",
        db_file="tmp/workflow_v2.db",
        mode="workflow_v2",
    ),
    # Define the sequence of steps
    # First run the research team, then the content planner Agent
    # You can mix and match agents, teams, and even regular python functions as steps
    steps=[research_step, content_planning_step],
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/sequence_of_steps_stream.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        # Define the sequence of steps
        # First run the research team, then the content planner Agent
        # You can mix and match agents, teams, and even regular python functions as steps
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/step_with_function.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step, StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_content_planning_function(step_input: StepInput) -> StepOutput:
    """
    Custom function that does intelligent content planning with context awareness
    """
    message = step_input.message
    previous_step_content = step_input.previous_step_content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    try:
        response = content_planner.run(planning_prompt)

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Research Integration:** {"✓ Research-based" if previous_step_content else "✗ No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

        return StepOutput(content=enhanced_content, response=response)

    except Exception as e:
        return StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


# Define and use examples
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        # Define the sequence of steps
        # First run the research_step, then the content_planning_step
        # You can mix and match agents, teams, and even regular python functions directly as steps
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
    )

    print("\n" + "=" * 60 + "\n")



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/step_with_function_additional_data.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step, StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_content_planning_function(step_input: StepInput) -> StepOutput:
    """
    Custom function that does intelligent content planning with context awareness
    Now also uses additional_data for extra context
    """
    message = step_input.message
    previous_step_content = step_input.previous_step_content

    # Access additional_data that was passed with the workflow
    additional_data = step_input.additional_data or {}
    user_email = additional_data.get("user_email", "No email provided")
    priority = additional_data.get("priority", "normal")
    client_type = additional_data.get("client_type", "standard")

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Additional Context:
        - Client Type: {client_type}
        - Priority Level: {priority}
        - Contact Email: {user_email}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies
        {"6. Mark as HIGH PRIORITY delivery" if priority == "high" else "6. Standard delivery timeline"}

        Please create a detailed, actionable content plan.
    """

    try:
        response = content_planner.run(planning_prompt)

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Client Details:**
            - Type: {client_type}
            - Priority: {priority.upper()}
            - Contact: {user_email}

            **Research Integration:** {"✓ Research-based" if previous_step_content else "✗ No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
            - Priority Level: {priority.upper()}
        """.strip()

        return StepOutput(content=enhanced_content, response=response)

    except Exception as e:
        return StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


# Define and use examples
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )

    # Run workflow with additional_data
    content_creation_workflow.print_response(
        message="AI trends in 2024",
        additional_data={
            "user_email": "kaustubh@agno.com",
            "priority": "high",
            "client_type": "enterprise",
        },
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )

    print("\n" + "=" * 60 + "\n")



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/step_with_function_stream.py
================================================
from typing import Iterator, Union

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.v2.workflow import WorkflowRunResponseEvent
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step, StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_content_planning_function(
    step_input: StepInput,
) -> Iterator[Union[WorkflowRunResponseEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness
    """
    message = step_input.message
    previous_step_content = step_input.previous_step_content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:
        
        Core Topic: {message}
        
        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}
        
        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies
        
        Please create a detailed, actionable content plan.
    """

    try:
        response_iterator = content_planner.run(
            planning_prompt, stream=True, stream_intermediate_steps=True
        )
        for event in response_iterator:
            yield event

        response = content_planner.run_response

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Research Integration:** {"✓ Research-based" if previous_step_content else "✗ No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

        yield StepOutput(content=enhanced_content, response=response)

    except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


# Define and use examples
if __name__ == "__main__":
    streaming_content_workflow = Workflow(
        name="Streaming Content Creation Workflow",
        description="Automated content creation with streaming custom execution functions",
        storage=SqliteStorage(
            table_name="workflow_v2_streaming",
            db_file="tmp/workflow_v2_streaming.db",
            mode="workflow_v2",
        ),
        # Define the sequence of steps
        # First run the research_step, then the content_planning_step
        # You can mix and match agents, teams, and even regular python functions directly as steps
        steps=[
            research_step,
            content_planning_step,
        ],
    )

    streaming_content_workflow.print_response(
        message="AI trends in 2024",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/workflow_using_steps.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.steps import Steps
from agno.workflow.v2.workflow import Workflow

# Define agents for different tasks
researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

editor = Agent(
    name="Editor Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Review and edit the article for clarity, grammar, and flow. Provide a polished final version.",
)

# Define individual steps
research_step = Step(
    name="research",
    agent=researcher,
    description="Research the topic and gather information",
)

writing_step = Step(
    name="writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="editing",
    agent=editor,
    description="Edit and polish the article",
)

# Create a Steps sequence that chains these above steps together
article_creation_sequence = Steps(
    name="article_creation",
    description="Complete article creation workflow from research to final edit",
    steps=[research_step, writing_step, editing_step],
)

# Create and use workflow
if __name__ == "__main__":
    article_workflow = Workflow(
        name="Article Creation Workflow",
        description="Automated article creation from research to publication",
        steps=[article_creation_sequence],
    )

    article_workflow.print_response(
        message="Write an article about the benefits of renewable energy",
        markdown=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/workflow_using_steps_nested.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.steps import Steps
from agno.workflow.v2.workflow import Workflow

# Define agents for different tasks
researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

tech_researcher = Agent(
    name="Tech Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    instructions="Research tech-related topics from Hacker News and provide latest developments.",
)

news_researcher = Agent(
    name="News Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[ExaTools()],
    instructions="Research current news and trends using Exa search.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

editor = Agent(
    name="Editor Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Review and edit the article for clarity, grammar, and flow. Provide a polished final version.",
)

content_agent = Agent(
    name="Content Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Prepare and format content for writing based on research inputs.",
)

# Define individual steps
initial_research_step = Step(
    name="InitialResearch",
    agent=researcher,
    description="Initial research on the topic",
)


# Condition evaluator function
def is_tech_topic(step_input) -> bool:
    """Check if the topic is tech-related and needs specialized research"""
    message = step_input.message.lower() if step_input.message else ""
    tech_keywords = [
        "ai",
        "machine learning",
        "technology",
        "software",
        "programming",
        "tech",
        "startup",
        "blockchain",
    ]
    return any(keyword in message for keyword in tech_keywords)


# Define parallel research steps
tech_research_step = Step(
    name="TechResearch",
    agent=tech_researcher,
    description="Research tech developments from Hacker News",
)

news_research_step = Step(
    name="NewsResearch",
    agent=news_researcher,
    description="Research current news and trends",
)

# Define content preparation step
content_prep_step = Step(
    name="ContentPreparation",
    agent=content_agent,
    description="Prepare and organize all research for writing",
)

writing_step = Step(
    name="Writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="Editing",
    agent=editor,
    description="Edit and polish the article",
)

# Create a Steps sequence with a Condition containing Parallel steps
article_creation_sequence = Steps(
    name="ArticleCreation",
    description="Complete article creation workflow from research to final edit",
    steps=[
        initial_research_step,
        # Condition with Parallel steps inside
        Condition(
            name="TechResearchCondition",
            description="If topic is tech-related, do specialized parallel research",
            evaluator=is_tech_topic,
            steps=[
                Parallel(
                    tech_research_step,
                    news_research_step,
                    name="SpecializedResearch",
                    description="Parallel tech and news research",
                ),
                content_prep_step,
            ],
        ),
        writing_step,
        editing_step,
    ],
)

# Create and use workflow
if __name__ == "__main__":
    article_workflow = Workflow(
        name="Enhanced Article Creation Workflow",
        description="Automated article creation with conditional parallel research",
        steps=[article_creation_sequence],
    )

    article_workflow.print_response(
        message="Write an article about the latest AI developments in machine learning",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/01_basic_workflows/workflow_with_file_input.py
================================================
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
read_agent = Agent(
    name="Agent",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Read the contents of the attached file.",
)

summarize_agent = Agent(
    name="Summarize Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Summarize the contents of the attached file.",
    ],
)

# Define steps
read_step = Step(
    name="Read Step",
    agent=read_agent,
)

summarize_step = Step(
    name="Summarize Step",
    agent=summarize_agent,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    storage=SqliteStorage(
        table_name="workflow_v2",
        db_file="tmp/workflow_v2.db",
        mode="workflow_v2",
    ),
    steps=[read_step, summarize_step],
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow.print_response(
        message="Summarize the contents of the attached file.",
        files=[
            File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")
        ],
        markdown=True,
    )



================================================
FILE: cookbook/workflows_2/sync/02_workflows_conditional_execution/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/sync/02_workflows_conditional_execution/condition_and_parallel_steps.py
================================================
from typing import List, Union

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.message or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_we_should_search_web(step_input: StepInput) -> bool:
    """Check if we should search the web"""
    topic = step_input.message or step_input.previous_step_content or ""
    general_keywords = ["news", "information", "research", "facts", "data"]
    return any(keyword in topic.lower() for keyword in general_keywords)


def check_if_we_should_search_x(step_input: StepInput) -> bool:
    """Check if we should search X/Twitter"""
    topic = step_input.message or step_input.previous_step_content or ""
    social_keywords = [
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "twitter",
        "x",
    ]
    return any(keyword in topic.lower() for keyword in social_keywords)


def check_if_we_should_search_exa(step_input: StepInput) -> bool:
    """Check if we should use Exa search"""
    topic = step_input.message or step_input.previous_step_content or ""
    advanced_keywords = ["deep", "academic", "research", "analysis", "comprehensive"]
    return any(keyword in topic.lower() for keyword in advanced_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],
                ),
                Condition(
                    name="WebSearchCondition",
                    description="Check if we should search the web for general information",
                    evaluator=check_if_we_should_search_web,
                    steps=[research_web_step],
                ),
                Condition(
                    name="ExaSearchCondition",
                    description="Check if we should use Exa for advanced search",
                    evaluator=check_if_we_should_search_exa,
                    steps=[research_exa_step],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        workflow.print_response(message="Latest AI developments in machine learning")
    except Exception as e:
        print(f"❌ Error: {e}")
    print()



================================================
FILE: cookbook/workflows_2/sync/02_workflows_conditional_execution/condition_and_parallel_steps_stream.py
================================================
from typing import List, Union

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.message or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_we_should_search_web(step_input: StepInput) -> bool:
    """Check if we should search the web"""
    topic = step_input.message or step_input.previous_step_content or ""
    general_keywords = ["news", "information", "research", "facts", "data"]
    return any(keyword in topic.lower() for keyword in general_keywords)


def check_if_we_should_search_x(step_input: StepInput) -> bool:
    """Check if we should search X/Twitter"""
    topic = step_input.message or step_input.previous_step_content or ""
    social_keywords = [
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "twitter",
        "x",
    ]
    return any(keyword in topic.lower() for keyword in social_keywords)


def check_if_we_should_search_exa(step_input: StepInput) -> bool:
    """Check if we should use Exa search"""
    topic = step_input.message or step_input.previous_step_content or ""
    advanced_keywords = ["deep", "academic", "research", "analysis", "comprehensive"]
    return any(keyword in topic.lower() for keyword in advanced_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],
                ),
                Condition(
                    name="WebSearchCondition",
                    description="Check if we should search the web for general information",
                    evaluator=check_if_we_should_search_web,
                    steps=[research_web_step],
                ),
                Condition(
                    name="ExaSearchCondition",
                    description="Check if we should use Exa for advanced search",
                    evaluator=check_if_we_should_search_exa,
                    steps=[research_exa_step],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        workflow.print_response(
            message="Latest AI developments in machine learning",
            stream=True,
            stream_intermediate_steps=True,
        )
    except Exception as e:
        print(f"❌ Error: {e}")
    print()



================================================
FILE: cookbook/workflows_2/sync/02_workflows_conditional_execution/condition_steps_workflow_stream.py
================================================
from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === BASIC AGENTS ===
researcher = Agent(
    name="Researcher",
    instructions="Research the given topic and provide detailed findings.",
    tools=[DuckDuckGoTools()],
)

summarizer = Agent(
    name="Summarizer",
    instructions="Create a clear summary of the research findings.",
)

fact_checker = Agent(
    name="Fact Checker",
    instructions="Verify facts and check for accuracy in the research.",
    tools=[DuckDuckGoTools()],
)

writer = Agent(
    name="Writer",
    instructions="Write a comprehensive article based on all available research and verification.",
)

# === CONDITION EVALUATOR ===


def needs_fact_checking(step_input: StepInput) -> bool:
    """Determine if the research contains claims that need fact-checking"""
    summary = step_input.previous_step_content or ""

    # Look for keywords that suggest factual claims
    fact_indicators = [
        "study shows",
        "research indicates",
        "according to",
        "statistics",
        "data shows",
        "survey",
        "report",
        "million",
        "billion",
        "percent",
        "%",
        "increase",
        "decrease",
    ]

    return any(indicator in summary.lower() for indicator in fact_indicators)


# === WORKFLOW STEPS ===
research_step = Step(
    name="research",
    description="Research the topic",
    agent=researcher,
)

summarize_step = Step(
    name="summarize",
    description="Summarize research findings",
    agent=summarizer,
)

# Conditional fact-checking step
fact_check_step = Step(
    name="fact_check",
    description="Verify facts and claims",
    agent=fact_checker,
)

write_article = Step(
    name="write_article",
    description="Write final article",
    agent=writer,
)

# === BASIC LINEAR WORKFLOW ===
basic_workflow = Workflow(
    name="Basic Linear Workflow",
    description="Research -> Summarize -> Condition(Fact Check) -> Write Article",
    steps=[
        research_step,
        summarize_step,
        Condition(
            name="fact_check_condition",
            description="Check if fact-checking is needed",
            evaluator=needs_fact_checking,
            steps=[fact_check_step],
        ),
        write_article,
    ],
)

if __name__ == "__main__":
    print("🚀 Running Basic Linear Workflow Example")
    print("=" * 50)

    try:
        basic_workflow.print_response(
            message="Recent breakthroughs in quantum computing",
            stream=True,
            stream_intermediate_steps=True,
        )
    except Exception as e:
        print(f"❌ Error: {e}")
        import traceback

        traceback.print_exc()



================================================
FILE: cookbook/workflows_2/sync/02_workflows_conditional_execution/condition_with_list_of_steps.py
================================================
from agno.agent.agent import Agent
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# Additional agents for multi-step condition
trend_analyzer_agent = Agent(
    name="Trend Analyzer",
    instructions="Analyze trends and patterns from research data",
)

fact_checker_agent = Agent(
    name="Fact Checker",
    instructions="Verify facts and cross-reference information",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

# === MULTI-STEP CONDITION STEPS ===
deep_exa_analysis_step = Step(
    name="DeepExaAnalysis",
    description="Conduct deep analysis using Exa search capabilities",
    agent=exa_agent,
)

trend_analysis_step = Step(
    name="TrendAnalysis",
    description="Analyze trends and patterns from the research data",
    agent=trend_analyzer_agent,
)

fact_verification_step = Step(
    name="FactVerification",
    description="Verify facts and cross-reference information",
    agent=fact_checker_agent,
)

# === FINAL STEPS ===
write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.message or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_comprehensive_research_needed(step_input: StepInput) -> bool:
    """Check if comprehensive multi-step research is needed"""
    topic = step_input.message or step_input.previous_step_content or ""
    comprehensive_keywords = [
        "comprehensive",
        "detailed",
        "thorough",
        "in-depth",
        "complete analysis",
        "full report",
        "extensive research",
    ]
    return any(keyword in topic.lower() for keyword in comprehensive_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow with Multi-Step Condition",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],  # Single step
                ),
                Condition(
                    name="ComprehensiveResearchCondition",
                    description="Check if comprehensive multi-step research is needed",
                    evaluator=check_if_comprehensive_research_needed,
                    steps=[  # Multiple steps
                        deep_exa_analysis_step,
                        trend_analysis_step,
                        fact_verification_step,
                    ],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            write_step,
        ],
    )

    try:
        workflow.print_response(
            message="Comprehensive analysis of climate change research",
            stream=True,
            stream_intermediate_steps=True,
        )
    except Exception as e:
        print(f"❌ Error: {e}")
    print()



================================================
FILE: cookbook/workflows_2/sync/03_workflows_loop_execution/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/sync/03_workflows_loop_execution/loop_steps_workflow.py
================================================
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Loop, Step, Workflow
from agno.workflow.v2.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)


# End condition function
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if any outputs are present
    if not outputs:
        return False

    # Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f"✅ Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print("❌ Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    # Test the workflow
    workflow.print_response(
        message="Research the latest trends in AI and machine learning, then create a summary",
    )



================================================
FILE: cookbook/workflows_2/sync/03_workflows_loop_execution/loop_steps_workflow_stream.py
================================================
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Loop, Step, Workflow
from agno.workflow.v2.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

# End condition function


def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Simple check - if any output contains substantial content, we're good
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f"✅ Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print("❌ Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        message="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/03_workflows_loop_execution/loop_with_parallel_steps.py
================================================
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Loop, Parallel, Step, Workflow
from agno.workflow.v2.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

analysis_agent = Agent(
    name="Analysis Agent",
    role="Data analyst",
    instructions="You are a data analyst. Analyze and summarize research findings.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

# Create analysis steps
trend_analysis_step = Step(
    name="Trend Analysis",
    agent=analysis_agent,
    description="Analyze trending patterns in the research",
)

sentiment_analysis_step = Step(
    name="Sentiment Analysis",
    agent=analysis_agent,
    description="Analyze sentiment and opinions from the research",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)


# End condition function
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Calculate total content length from all outputs
    total_content_length = sum(len(output.content or "") for output in outputs)

    # Check if we have substantial content (more than 500 chars total)
    if total_content_length > 500:
        print(
            f"✅ Research evaluation passed - found substantial content ({total_content_length} chars total)"
        )
        return True

    print(
        f"❌ Research evaluation failed - need more substantial research (current: {total_content_length} chars)"
    )
    return False


# Create workflow with loop containing parallel steps
workflow = Workflow(
    name="Advanced Research and Content Workflow",
    description="Research topics with parallel execution in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop with Parallel Execution",
            steps=[
                Parallel(
                    research_hackernews_step,
                    research_web_step,
                    trend_analysis_step,
                    name="Parallel Research & Analysis",
                    description="Execute research and analysis in parallel for efficiency",
                ),
                sentiment_analysis_step,
            ],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        message="Research the latest trends in AI and machine learning, then create a summary",
    )



================================================
FILE: cookbook/workflows_2/sync/03_workflows_loop_execution/loop_with_parallel_steps_stream.py
================================================
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Loop, Parallel, Step, Workflow
from agno.workflow.v2.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

analysis_agent = Agent(
    name="Analysis Agent",
    role="Data analyst",
    instructions="You are a data analyst. Analyze and summarize research findings.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

# Create analysis steps
trend_analysis_step = Step(
    name="Trend Analysis",
    agent=analysis_agent,
    description="Analyze trending patterns in the research",
)

sentiment_analysis_step = Step(
    name="Sentiment Analysis",
    agent=analysis_agent,
    description="Analyze sentiment and opinions from the research",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)


# End condition function
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Calculate total content length from all outputs
    total_content_length = sum(len(output.content or "") for output in outputs)

    # Check if we have substantial content (more than 500 chars total)
    if total_content_length > 500:
        print(
            f"✅ Research evaluation passed - found substantial content ({total_content_length} chars total)"
        )
        return True

    print(
        f"❌ Research evaluation failed - need more substantial research (current: {total_content_length} chars)"
    )
    return False


# Create workflow with loop containing parallel steps
workflow = Workflow(
    name="Advanced Research and Content Workflow",
    description="Research topics with parallel execution in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop with Parallel Execution",
            steps=[
                Parallel(
                    research_hackernews_step,
                    research_web_step,
                    trend_analysis_step,
                    name="Parallel Research & Analysis",
                    description="Execute research and analysis in parallel for efficiency",
                ),
                sentiment_analysis_step,
            ],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        message="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/04_workflows_parallel_execution/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/sync/04_workflows_parallel_execution/parallel_and_condition_steps_stream.py
================================================
from typing import List, Union

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def should_conduct_research(step_input: StepInput) -> bool:
    """Check if we should conduct comprehensive research"""
    topic = step_input.message or step_input.previous_step_content or ""

    # Keywords that indicate research is needed
    research_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
        "news",
        "information",
        "research",
        "facts",
        "data",
        "analysis",
        "comprehensive",
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "developments",
    ]

    # If the topic contains any research-worthy keywords, conduct research
    return any(keyword in topic.lower() for keyword in research_keywords)


def is_tech_related(step_input: StepInput) -> bool:
    """Check if the topic is tech-related for additional tech research"""
    topic = step_input.message or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Research Workflow",
        description="Conditionally execute parallel research based on topic relevance",
        steps=[
            # Main research condition - if topic needs research, run parallel research steps
            Condition(
                name="ResearchCondition",
                description="Check if comprehensive research is needed for this topic",
                evaluator=should_conduct_research,
                steps=[
                    Parallel(
                        research_hackernews_step,
                        research_web_step,
                        name="ComprehensiveResearch",
                        description="Run multiple research sources in parallel",
                    ),
                    research_exa_step,
                ],
            ),
            # # Additional tech-specific research if needed
            Condition(
                name="TechResearchCondition",
                description="Additional tech-focused research if topic is tech-related",
                evaluator=is_tech_related,
                steps=[
                    Step(
                        name="TechAnalysis",
                        description="Deep dive tech analysis and trend identification",
                        agent=content_agent,
                    ),
                ],
            ),
            # Content preparation and writing
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        workflow.print_response(
            message="Latest AI developments in machine learning",
            stream=True,
            stream_intermediate_steps=True,
        )
    except Exception as e:
        print(f"❌ Error: {e}")
    print()



================================================
FILE: cookbook/workflows_2/sync/04_workflows_parallel_execution/parallel_steps_workflow.py
================================================
from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.parallel import Parallel

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

workflow.print_response("Write about the latest AI developments")



================================================
FILE: cookbook/workflows_2/sync/04_workflows_parallel_execution/parallel_steps_workflow_stream.py
================================================
from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.parallel import Parallel

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

workflow.print_response(
    "Write about the latest AI developments",
    stream=True,
    stream_intermediate_steps=True,
)



================================================
FILE: cookbook/workflows_2/sync/05_workflows_conditional_branching/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/sync/05_workflows_conditional_branching/router_steps_workflow.py
================================================
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.message or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f"🔍 Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f"🌐 General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning"
    )



================================================
FILE: cookbook/workflows_2/sync/05_workflows_conditional_branching/router_steps_workflow_stream.py
================================================
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.message or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f"🔍 Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f"🌐 General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/05_workflows_conditional_branching/router_with_loop_steps.py
================================================
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.loop import Loop
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)

# End condition function for the loop


def research_quality_check(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    if not outputs:
        return False

    # Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 300:
            print(
                f"✅ Research quality check passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print("❌ Research quality check failed - need more substantial research")
    return False


# Create a Loop step for deep tech research
deep_tech_research_loop = Loop(
    name="Deep Tech Research Loop",
    steps=[research_hackernews],
    end_condition=research_quality_check,
    max_iterations=3,
    description="Perform iterative deep research on tech topics",
)

# Router function that selects between simple web research or deep tech research loop


def research_strategy_router(step_input: StepInput) -> List[Step]:
    """
    Decide between simple web research or deep tech research loop based on the input topic.
    Returns either a single web research step or a tech research loop.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.message or ""
    topic = topic.lower()

    # Check if the topic requires deep tech research
    deep_tech_keywords = [
        "startup trends",
        "ai developments",
        "machine learning research",
        "programming languages",
        "developer tools",
        "silicon valley",
        "venture capital",
        "cryptocurrency analysis",
        "blockchain technology",
        "open source projects",
        "github trends",
        "tech industry",
        "software engineering",
    ]

    # Check if it's a complex tech topic that needs deep research
    if any(keyword in topic for keyword in deep_tech_keywords) or (
        "tech" in topic and len(topic.split()) > 3
    ):
        print(
            f"🔬 Deep tech topic detected: Using iterative research loop for '{topic}'"
        )
        return [deep_tech_research_loop]
    else:
        print(f"🌐 Simple topic detected: Using basic web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Adaptive Research Workflow",
    description="Intelligently selects between simple web research or deep iterative tech research based on topic complexity",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_strategy_router,
            choices=[research_web, deep_tech_research_loop],
            description="Chooses between simple web research or deep tech research loop",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    print("=== Testing with deep tech topic ===")
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning and deep tech research trends"
    )



================================================
FILE: cookbook/workflows_2/sync/05_workflows_conditional_branching/selector_for_image_video_generation_pipelines.py
================================================
from typing import Any, Dict, List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.tools.openai import OpenAITools
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.steps import Steps
from agno.workflow.v2.types import StepInput
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel


# Define the structured message data
class MediaRequest(BaseModel):
    topic: str
    content_type: str  # "image" or "video"
    prompt: str
    style: Optional[str] = "realistic"
    duration: Optional[int] = None  # For video, duration in seconds
    resolution: Optional[str] = "1024x1024"  # For image resolution


# Define specialized agents for different media types
image_generator = Agent(
    name="Image Generator",
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    instructions="""You are an expert image generation specialist.
    When users request image creation, you should ACTUALLY GENERATE the image using your available image generation tools.

    Always use the generate_image tool to create the requested image based on the user's specifications.
    Include detailed, creative prompts that incorporate style, composition, lighting, and mood details.

    After generating the image, provide a brief description of what you created.""",
)

image_describer = Agent(
    name="Image Describer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""You are an expert image analyst and describer.
    When you receive an image (either as input or from a previous step), analyze and describe it in vivid detail, including:
    - Visual elements and composition
    - Colors, lighting, and mood
    - Artistic style and technique
    - Emotional impact and narrative

    If no image is provided, work with the image description or prompt from the previous step.
    Provide rich, engaging descriptions that capture the essence of the visual content.""",
)

video_generator = Agent(
    name="Video Generator",
    model=OpenAIChat(id="gpt-4o"),
    # Video Generation only works on VertexAI mode
    tools=[GeminiTools(vertexai=True)],
    instructions="""You are an expert video production specialist.
    Create detailed video generation prompts and storyboards based on user requests.
    Include scene descriptions, camera movements, transitions, and timing.
    Consider pacing, visual storytelling, and technical aspects like resolution and duration.
    Format your response as a comprehensive video production plan.""",
)

video_describer = Agent(
    name="Video Describer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""You are an expert video analyst and critic.
    Analyze and describe videos comprehensively, including:
    - Scene composition and cinematography
    - Narrative flow and pacing
    - Visual effects and production quality
    - Audio-visual harmony and mood
    - Technical execution and artistic merit
    Provide detailed, professional video analysis.""",
)

# Define steps for image pipeline
generate_image_step = Step(
    name="generate_image",
    agent=image_generator,
    description="Generate a detailed image creation prompt based on the user's request",
)

describe_image_step = Step(
    name="describe_image",
    agent=image_describer,
    description="Analyze and describe the generated image concept in vivid detail",
)

# Define steps for video pipeline
generate_video_step = Step(
    name="generate_video",
    agent=video_generator,
    description="Create a comprehensive video production plan and storyboard",
)

describe_video_step = Step(
    name="describe_video",
    agent=video_describer,
    description="Analyze and critique the video production plan with professional insights",
)

# Define the two distinct pipelines
image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[generate_image_step, describe_image_step],
)

video_sequence = Steps(
    name="video_generation",
    description="Complete video production and analysis workflow",
    steps=[generate_video_step, describe_video_step],
)


def media_sequence_selector(step_input: StepInput) -> List[Step]:
    """
    Simple pipeline selector based on keywords in the message.

    Args:
        step_input: StepInput containing message

    Returns:
        List of Steps to execute
    """

    # Check if message exists and is a string
    if not step_input.message or not isinstance(step_input.message, str):
        return [image_sequence]  # Default to image sequence

    # Convert message to lowercase for case-insensitive matching
    message_lower = step_input.message.lower()

    # Check for video keywords
    if "video" in message_lower:
        return [video_sequence]
    # Check for image keywords
    elif "image" in message_lower:
        return [image_sequence]
    else:
        # Default to image for any other case
        return [image_sequence]


# Usage examples
if __name__ == "__main__":
    # Create the media generation workflow
    media_workflow = Workflow(
        name="AI Media Generation Workflow",
        description="Generate and analyze images or videos using AI agents",
        steps=[
            Router(
                name="Media Type Router",
                description="Routes to appropriate media generation pipeline based on content type",
                selector=media_sequence_selector,
                choices=[image_sequence, video_sequence],
            )
        ],
    )

    print("=== Example 1: Image Generation (using message_data) ===")
    image_request = MediaRequest(
        topic="Create an image of magical forest for a movie scene",
        content_type="image",
        prompt="A mystical forest with glowing mushrooms",
        style="fantasy art",
        resolution="1920x1080",
    )

    media_workflow.print_response(
        message="Create an image of magical forest for a movie scene",
        markdown=True,
    )

    # print("\n=== Example 2: Video Generation (using message_data) ===")
    # video_request = MediaRequest(
    #     topic="Create a cinematic video city timelapse",
    #     content_type="video",
    #     prompt="A time-lapse of a city skyline from day to night",
    #     style="cinematic",
    #     duration=30,
    #     resolution="4K"
    # )

    # media_workflow.print_response(
    #     message="Create a cinematic video city timelapse",
    #     markdown=True,
    # )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/__init__.py
================================================
[Empty file]


================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/access_multiple_previous_step_output_stream_2.py
================================================
"""
This example shows how to access the output of multiple previous steps in a workflow.

The workflow is defined as a list of steps, where each step is directly an agent or a function.
We dont use Step objects in this example.
"""

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

reasoning_agent = Agent(
    instructions="You are an expert analyst who creates comprehensive reports by analyzing and synthesizing information from multiple sources. Create well-structured, insightful reports.",
)


# Custom function step that has access to ALL previous step outputs
def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

    # Access original workflow input
    original_topic = step_input.message or ""

    # Access specific step outputs by name
    hackernews_data = step_input.get_step_content("step_1") or ""
    web_data = step_input.get_step_content("step_2") or ""

    # Or access ALL previous content
    all_research = step_input.get_all_previous_content()

    # Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

        ## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

        ## HackerNews Insights
        {hackernews_data[:500]}...

        ## Web Research Findings  
        {web_data[:500]}...
    """

    return StepOutput(content=report.strip(), success=True)


# Custom function to print the comprehensive report
def print_final_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that receives the comprehensive report and prints it.
    """

    # Get the output from the comprehensive_report step
    comprehensive_report = step_input.get_step_content("create_comprehensive_report")

    # Print the report
    print("=" * 80)
    print("FINAL COMPREHENSIVE REPORT")
    print("=" * 80)
    print(comprehensive_report)
    print("=" * 80)

    # Also print all previous step outputs for debugging
    print("\nDEBUG: All previous step outputs:")
    if step_input.previous_step_outputs:
        for step_name, output in step_input.previous_step_outputs.items():
            print(f"- {step_name}: {len(str(output.content))} characters")

    return StepOutput(
        step_name="print_final_report",
        content=f"Printed comprehensive report ({len(comprehensive_report)} characters)",
        success=True,
    )


# Final reasoning step using reasoning agent
reasoning_step = Step(
    name="final_reasoning",
    agent=reasoning_agent,
    description="Apply reasoning to create final insights and recommendations",
)

workflow = Workflow(
    name="Enhanced Research Workflow",
    description="Multi-source research with custom data flow and reasoning",
    steps=[
        hackernews_agent,
        web_agent,
        create_comprehensive_report,  # Has access to both previous steps
        print_final_report,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning",
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/access_multiple_previous_steps_output_stream_1.py
================================================
"""
This example shows how to access the output of multiple previous steps in a workflow.

The workflow is defined as a list of Step objects. Where each Step is an agent or a function.
"""

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

reasoning_agent = Agent(
    name="Reasoning Agent",
    instructions="You are an expert analyst who creates comprehensive reports by analyzing and synthesizing information from multiple sources. Create well-structured, insightful reports.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

# Custom function step that has access to ALL previous step outputs


def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

    # Access original workflow input
    original_topic = step_input.message or ""

    # Access specific step outputs by name
    hackernews_data = step_input.get_step_content("research_hackernews") or ""
    web_data = step_input.get_step_content("research_web") or ""

    # Or access ALL previous content
    all_research = step_input.get_all_previous_content()

    # Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

        ## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

        ## HackerNews Insights
        {hackernews_data[:500]}...

        ## Web Research Findings  
        {web_data[:500]}...
    """

    return StepOutput(
        step_name="comprehensive_report", content=report.strip(), success=True
    )


comprehensive_report_step = Step(
    name="comprehensive_report",
    executor=create_comprehensive_report,
    description="Create comprehensive report from all research sources",
)

# Final reasoning step using reasoning agent
reasoning_step = Step(
    name="final_reasoning",
    agent=reasoning_agent,
    description="Apply reasoning to create final insights and recommendations",
)

workflow = Workflow(
    name="Enhanced Research Workflow",
    description="Multi-source research with custom data flow and reasoning",
    steps=[
        research_hackernews,
        research_web,
        comprehensive_report_step,  # Has access to both previous steps
        reasoning_step,  # Gets the last step output (comprehensive report)
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_agents.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.workflow.v2 import Workflow
from agno.workflow.v2.types import StepInput, StepOutput

# Create agents with more specific validation criteria
data_validator = Agent(
    name="Data Validator",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are a data validator. Analyze the provided data and determine if it's valid.",
        "For data to be VALID, it must meet these criteria:",
        "- user_count: Must be a positive number (> 0)",
        "- revenue: Must be a positive number (> 0)",
        "- date: Must be in a reasonable date format (YYYY-MM-DD)",
        "",
        "Return exactly 'VALID' if all criteria are met.",
        "Return exactly 'INVALID' if any criteria fail.",
        "Also briefly explain your reasoning.",
    ],
)

data_processor = Agent(
    name="Data Processor",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Process and transform the validated data.",
)

report_generator = Agent(
    name="Report Generator",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Generate a final report from processed data.",
)


def early_exit_validator(step_input: StepInput) -> StepOutput:
    """
    Custom function that checks data quality and stops workflow early if invalid
    """
    # Get the validation result from previous step
    validation_result = step_input.previous_step_content or ""

    if "INVALID" in validation_result.upper():
        return StepOutput(
            content="❌ Data validation failed. Workflow stopped early to prevent processing invalid data.",
            stop=True,  # Stop the entire workflow here
        )
    else:
        return StepOutput(
            content="✅ Data validation passed. Continuing with processing...",
            stop=False,  # Continue normally
        )


# Create workflow with conditional early termination
workflow = Workflow(
    name="Data Processing with Early Exit",
    description="Process data but stop early if validation fails",
    steps=[
        data_validator,  # Step 1: Validate data
        early_exit_validator,  # Step 2: Check validation and possibly stop early
        data_processor,  # Step 3: Process data (only if validation passed)
        report_generator,  # Step 4: Generate report (only if processing completed)
    ],
)

if __name__ == "__main__":
    print("\n=== Testing with INVALID data ===")
    workflow.print_response(
        message="Process this data: {'user_count': -50, 'revenue': 'invalid_amount', 'date': 'bad_date'}"
    )

    print("=== Testing with VALID data ===")
    workflow.print_response(
        message="Process this data: {'user_count': 1000, 'revenue': 50000, 'date': '2024-01-15'}"
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_condition.py
================================================
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.condition import Condition
from agno.workflow.v2.types import StepInput, StepOutput

# Create agents
researcher = Agent(
    name="Researcher",
    instructions="Research the given topic thoroughly and provide detailed findings.",
    tools=[DuckDuckGoTools()],
)

writer = Agent(
    name="Writer",
    instructions="Create engaging content based on research findings.",
)

reviewer = Agent(
    name="Reviewer",
    instructions="Review and improve the written content.",
)


# Custom compliance checker function
def compliance_checker(step_input: StepInput) -> StepOutput:
    """Compliance checker that can stop the condition if violations are found"""
    content = step_input.previous_step_content or ""

    # Simulate detecting compliance violations
    if "violation" in content.lower() or "illegal" in content.lower():
        return StepOutput(
            step_name="Compliance Checker",
            content="🚨 COMPLIANCE VIOLATION DETECTED! Content contains material that violates company policies. Stopping content creation workflow immediately.",
            stop=True,  # ✅ Request early termination from condition
        )
    else:
        return StepOutput(
            step_name="Compliance Checker",
            content="✅ Compliance check passed. Content meets all company policy requirements.",
            stop=False,
        )


# Custom quality assurance function
def quality_assurance(step_input: StepInput) -> StepOutput:
    """Quality assurance that runs after compliance check"""
    content = step_input.previous_step_content or ""

    return StepOutput(
        step_name="Quality Assurance",
        content="✅ Quality assurance completed. Content meets quality standards and is ready for publication.",
        stop=False,
    )


# Condition evaluator function
def should_run_compliance_check(step_input: StepInput) -> bool:
    """Evaluate if compliance check should run based on content type"""
    content = step_input.message or ""

    # Run compliance check for sensitive topics
    sensitive_keywords = ["legal", "financial", "medical", "violation", "illegal"]
    return any(keyword in content.lower() for keyword in sensitive_keywords)


# Create workflow steps
research_step = Step(name="Research Content", agent=researcher)
compliance_check_step = Step(
    name="Compliance Check", executor=compliance_checker
)  # ✅ Can stop workflow
quality_assurance_step = Step(name="Quality Assurance", executor=quality_assurance)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with conditional compliance checks
workflow = Workflow(
    name="Content Creation with Conditional Compliance",
    description="Creates content with conditional compliance checks that can stop the workflow",
    steps=[
        research_step,  # Always runs first
        Condition(
            name="Compliance and QA Gate",
            evaluator=should_run_compliance_check,  # Only runs for sensitive content
            steps=[
                compliance_check_step,  # Step 1: Compliance check (may stop here)
                quality_assurance_step,  # Step 2: QA (only if compliance passes)
            ],
        ),
        write_step,  # This should NOT execute if compliance check stops
        review_step,  # This should NOT execute if compliance check stops
    ],
)

if __name__ == "__main__":
    print("=== Testing Condition Early Termination with Compliance Check ===")
    print(
        "Expected: Compliance check should detect 'violation' and stop the entire workflow"
    )
    print(
        "Note: Condition will evaluate to True (sensitive content), then compliance check will stop"
    )
    print()

    workflow.print_response(
        message="Research legal violation cases and create content about illegal financial practices",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_loop.py
================================================
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Loop, Step, Workflow
from agno.workflow.v2.types import StepInput, StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)


# Custom function that will trigger early termination
def safety_checker(step_input: StepInput) -> StepOutput:
    """Safety checker that stops the loop if certain keywords are detected"""
    content = step_input.previous_step_content or ""

    # Simulate finding problematic content that requires stopping
    if (
        "AI" in content or "machine learning" in content
    ):  # Will trigger on our test message
        return StepOutput(
            step_name="Safety Checker",
            content="🚨 SAFETY CONCERN DETECTED! Content contains sensitive AI-related information. Stopping research loop for review.",
            stop=True,  # ✅ Request early termination
        )
    else:
        return StepOutput(
            step_name="Safety Checker",
            content="✅ Safety check passed. Content is safe to continue.",
            stop=False,
        )


# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

safety_check_step = Step(
    name="Safety Check",
    executor=safety_checker,  # ✅ Custom function that can stop the loop
    description="Check if research content is safe to continue",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)


# Normal end condition (keeps the original logic) + early termination check
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient or if early termination was requested
    Returns True to break the loop, False to continue
    """
    if not outputs:
        print("❌ No research outputs - continuing loop")
        return False

    # Original logic: Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f"✅ Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print("❌ Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop that includes safety checker
workflow = Workflow(
    name="Research with Safety Check Workflow",
    description="Research topics in loop with safety checks, stop if safety issues found",
    steps=[
        Loop(
            name="Research Loop with Safety",
            steps=[
                research_hackernews_step,  # Step 1: Research
                safety_check_step,  # Step 2: Safety check (may stop here)
                research_web_step,  # Step 3: More research (only if safety passes)
            ],
            end_condition=research_evaluator,
            max_iterations=3,
        ),
        content_agent,  # This should NOT execute if safety check stops the loop
    ],
)

if __name__ == "__main__":
    print("=== Testing Loop Early Termination with Safety Check ===")
    print("Expected: Safety check should detect 'AI' and stop the entire workflow")
    print()

    workflow.print_response(
        message="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_parallel.py
================================================
from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.types import StepInput, StepOutput

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")


# Custom safety checker function that can stop the entire workflow
def content_safety_checker(step_input: StepInput) -> StepOutput:
    """Safety checker that runs in parallel and can stop the workflow"""
    content = step_input.message or ""

    # Simulate detecting unsafe content that requires immediate stopping
    if "unsafe" in content.lower() or "dangerous" in content.lower():
        return StepOutput(
            step_name="Safety Checker",
            content="🚨 UNSAFE CONTENT DETECTED! Content contains dangerous material. Stopping entire workflow immediately for safety review.",
            stop=True,  # ✅ Request early termination from parallel execution
        )
    else:
        return StepOutput(
            step_name="Safety Checker",
            content="✅ Content safety verification passed. Material is safe to proceed.",
            stop=False,
        )


# Custom quality checker function
def quality_checker(step_input: StepInput) -> StepOutput:
    """Quality checker that runs in parallel"""
    content = step_input.message or ""

    # Simulate quality check
    if len(content) < 10:
        return StepOutput(
            step_name="Quality Checker",
            content="⚠️ Quality check failed: Content too short for processing.",
            stop=False,
        )
    else:
        return StepOutput(
            step_name="Quality Checker",
            content="✅ Quality check passed. Content meets processing standards.",
            stop=False,
        )


# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
safety_check_step = Step(
    name="Safety Check", executor=content_safety_checker
)  # ✅ Can stop workflow
quality_check_step = Step(name="Quality Check", executor=quality_checker)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with parallel safety/quality checks
workflow = Workflow(
    name="Content Creation with Parallel Safety Checks",
    description="Creates content with parallel safety and quality checks that can stop the workflow",
    steps=[
        Parallel(
            research_hn_step,  # Research task 1
            research_web_step,  # Research task 2
            safety_check_step,  # Safety check (may stop here)
            quality_check_step,  # Quality check
            name="Research and Validation Phase",
        ),
        write_step,  # This should NOT execute if safety check stops
        review_step,  # This should NOT execute if safety check stops
    ],
)

if __name__ == "__main__":
    print("=== Testing Parallel Early Termination with Safety Check ===")
    print("Expected: Safety check should detect 'unsafe' and stop the entire workflow")
    print(
        "Note: All parallel steps run concurrently, but safety check will stop the workflow"
    )
    print()

    workflow.print_response(
        message="Write about unsafe and dangerous AI developments that could harm society",
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_router.py
================================================
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.router import Router
from agno.workflow.v2.step import Step
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)


# Custom safety checker function
def content_safety_checker(step_input: StepInput) -> StepOutput:
    """Safety checker that can stop the router if inappropriate content is detected"""
    content = step_input.previous_step_content or ""

    # Simulate detecting inappropriate content that requires stopping
    if "controversial" in content.lower() or "sensitive" in content.lower():
        return StepOutput(
            step_name="Content Safety Checker",
            content="🚨 CONTENT SAFETY VIOLATION! Research contains controversial or sensitive material. Stopping workflow for manual review.",
            stop=True,  # ✅ Request early termination
        )
    else:
        return StepOutput(
            step_name="Content Safety Checker",
            content="✅ Content safety check passed. Material is appropriate for publication.",
            stop=False,
        )


# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

safety_check = Step(
    name="safety_check",
    executor=content_safety_checker,  # ✅ Custom function that can stop the router
    description="Check if research content is safe for publication",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Router function that returns multiple steps including safety check
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute including safety check.
    """
    topic = step_input.previous_step_content or step_input.message or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f"🔍 Tech topic detected: Using HackerNews research for '{topic}'")
        return [
            research_hackernews,  # Step 1: Research
            safety_check,  # Step 2: Safety check (may stop here)
            research_web,  # Step 3: Additional research (only if safety passes)
        ]
    else:
        print(f"🌐 General topic detected: Using web research for '{topic}'")
        return [
            research_web,  # Step 1: Research
            safety_check,  # Step 2: Safety check (may stop here)
        ]


workflow = Workflow(
    name="Research with Safety Router Workflow",
    description="Intelligently routes research methods with safety checks that can stop the workflow",
    steps=[
        Router(
            name="research_safety_router",
            selector=research_router,
            choices=[
                research_hackernews,
                safety_check,
                research_web,
            ],  # Available choices
            description="Intelligently selects research method with safety checks",
        ),
        publish_content,  # This should NOT execute if safety check stops the router
    ],
)

if __name__ == "__main__":
    print("=== Testing Router Early Termination with Safety Check ===")
    print(
        "Expected: Safety check should detect 'controversial' and stop the entire workflow"
    )
    print()

    workflow.print_response(
        message="Research the latest controversial trends in AI and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_step.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.workflow.v2 import Step, Workflow
from agno.workflow.v2.types import StepInput, StepOutput

# Create agents
security_scanner = Agent(
    name="Security Scanner",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are a security scanner. Analyze the provided code or system for security vulnerabilities.",
        "Return 'SECURE' if no critical vulnerabilities found.",
        "Return 'VULNERABLE' if critical security issues are detected.",
        "Explain your findings briefly.",
    ],
)

code_deployer = Agent(
    name="Code Deployer",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Deploy the security-approved code to production environment.",
)

monitoring_agent = Agent(
    name="Monitoring Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Set up monitoring and alerts for the deployed application.",
)


def security_gate(step_input: StepInput) -> StepOutput:
    """
    Security gate that stops deployment if vulnerabilities found
    """
    security_result = step_input.previous_step_content or ""
    print(f"🔍 Security scan result: {security_result}")

    if "VULNERABLE" in security_result.upper():
        return StepOutput(
            content="🚨 SECURITY ALERT: Critical vulnerabilities detected. Deployment blocked for security reasons.",
            stop=True,  # Stop the entire workflow to prevent insecure deployment
        )
    else:
        return StepOutput(
            content="✅ Security check passed. Proceeding with deployment...",
            stop=False,
        )


# Create deployment workflow with security gate
workflow = Workflow(
    name="Secure Deployment Pipeline",
    description="Deploy code only if security checks pass",
    steps=[
        Step(name="Security Scan", agent=security_scanner),
        Step(name="Security Gate", executor=security_gate),  # May stop here
        Step(name="Deploy Code", agent=code_deployer),  # Only if secure
        Step(name="Setup Monitoring", agent=monitoring_agent),  # Only if deployed
    ],
)

if __name__ == "__main__":
    print("\n=== Testing VULNERABLE code deployment ===")
    workflow.print_response(message="Scan this code: exec(input('Enter command: '))")

    print("=== Testing SECURE code deployment ===")
    workflow.print_response(message="Scan this code: def hello(): return 'Hello World'")



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/early_stop_workflow_with_steps.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.steps import Steps
from agno.workflow.v2.types import StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow

# Create agents
content_creator = Agent(
    name="Content Creator",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Create engaging content on the given topic. Research and write comprehensive articles.",
)

fact_checker = Agent(
    name="Fact Checker",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Verify facts and check accuracy of content. Flag any misinformation.",
)

editor = Agent(
    name="Editor",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Edit and polish content for publication. Ensure clarity and flow.",
)

publisher = Agent(
    name="Publisher",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Prepare content for publication and handle final formatting.",
)


# Custom content quality check function
def content_quality_gate(step_input: StepInput) -> StepOutput:
    """Quality gate that checks content and may stop the workflow"""
    content = step_input.previous_step_content or ""

    # Simulate quality check - stop if content is too short or mentions certain topics
    if len(content) < 100:
        return StepOutput(
            step_name="content_quality_gate",
            content="❌ QUALITY CHECK FAILED: Content too short. Stopping workflow.",
            stop=True,  # ✅ Early termination
        )

    # Check for problematic content
    problematic_keywords = ["fake", "misinformation", "unverified", "conspiracy"]
    if any(keyword in content.lower() for keyword in problematic_keywords):
        return StepOutput(
            step_name="content_quality_gate",
            content="❌ QUALITY CHECK FAILED: Problematic content detected. Stopping workflow.",
            stop=True,  # ✅ Early termination
        )

    return StepOutput(
        step_name="content_quality_gate",
        content="✅ QUALITY CHECK PASSED: Content meets quality standards.",
        stop=False,  # Continue workflow
    )


# Create Steps sequence with early termination
content_pipeline = Steps(
    name="content_pipeline",
    description="Content creation pipeline with quality gates",
    steps=[
        Step(name="create_content", agent=content_creator),
        Step(
            name="quality_gate", executor=content_quality_gate
        ),  # ✅ Can stop workflow
        Step(name="fact_check", agent=fact_checker),  # ✅ Won't execute if stopped
        Step(name="edit_content", agent=editor),  # ✅ Won't execute if stopped
        Step(name="publish", agent=publisher),  # ✅ Won't execute if stopped
    ],
)

# Create workflow
if __name__ == "__main__":
    workflow = Workflow(
        name="Content Creation with Quality Gate",
        description="Content creation workflow with early termination on quality issues",
        steps=[
            content_pipeline,
            Step(
                name="final_review", agent=editor
            ),  # ✅ Won't execute if pipeline stopped
        ],
    )

    print("\n=== Test 2: Short content (should stop early) ===")
    workflow.print_response(
        message="Write a short note about conspiracy theories",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/pydantic_model_as_input.py
================================================
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )

    print("=== Example: Research with Structured Topic ===")
    research_topic = ResearchTopic(
        topic="AI trends in 2024",
        focus_areas=[
            "Machine Learning",
            "Natural Language Processing",
            "Computer Vision",
            "AI Ethics",
        ],
        target_audience="Tech professionals and business leaders",
        sources_required=8,
    )
    content_creation_workflow.print_response(
        message=research_topic,
        markdown=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/run_stream_with_debug_mode.py
================================================
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.v2 import Loop, Step, Workflow
from agno.workflow.v2.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

# End condition function


def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        print("❌ Research evaluation: No outputs found")
        return False

    # Simple check - if any output contains substantial content, we're good
    for i, output in enumerate(outputs):
        if output.content and len(output.content) > 200:
            print(
                f"✅ Research evaluation passed - Step {i + 1} found substantial content ({len(output.content)} chars)"
            )
            return True

    print("❌ Research evaluation failed - need more substantial research")
    print(
        f"   Found {len(outputs)} outputs with lengths: {[len(o.content) if o.content else 0 for o in outputs]}"
    )
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    debug_mode=True,  # Enable debug mode for workflow
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    print("🧪 Testing Research and Content Workflow with Debug Mode")
    print("=" * 60)
    print("🔍 Topic: Latest trends in AI and machine learning")
    print("🌊 Streaming: Enabled with intermediate steps")
    print()

    print("🚀 Starting workflow execution...")
    print("-" * 40)

    # Collect all chunks to build the final response
    all_chunks = []

    for chunk in workflow.run(
        message="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
        stream_intermediate_steps=True,
    ):
        all_chunks.append(chunk)

    # Print the final results
    print("\n" + "=" * 60)
    print("📊 FINAL WORKFLOW EXECUTION RESULTS")
    print("=" * 60)

    if all_chunks:
        # Use the workflow's run_response which should be the complete response
        if hasattr(workflow, "run_response") and workflow.run_response:
            pprint_run_response(workflow.run_response, markdown=True, show_time=True)
        else:
            # Fallback: just print the last chunk content if it exists
            final_chunk = all_chunks[-1]
            if hasattr(final_chunk, "content") and final_chunk.content:
                print("📝 Final Content:")
                print(final_chunk.content)
            else:
                print("❌ No final content found")
                print(f"Last chunk type: {type(final_chunk).__name__}")
    else:
        print("❌ No chunks received")



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/shared_session_state_with_agent.py
================================================
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow


# Define tools to manage a shopping list in workflow session state
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list in workflow session state.

    Args:
        item (str): The item to add to the shopping list
    """
    if agent.workflow_session_state is None:
        agent.workflow_session_state = {}

    if "shopping_list" not in agent.workflow_session_state:
        agent.workflow_session_state["shopping_list"] = []

    # Check if item already exists (case-insensitive)
    existing_items = [
        existing_item.lower()
        for existing_item in agent.workflow_session_state["shopping_list"]
    ]
    if item.lower() not in existing_items:
        agent.workflow_session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list."
    else:
        return f"'{item}' is already in the shopping list."


def remove_item(agent: Agent, item: str) -> str:
    """Remove an item from the shopping list in workflow session state.

    Args:
        item (str): The item to remove from the shopping list
    """
    if agent.workflow_session_state is None:
        agent.workflow_session_state = {}

    if "shopping_list" not in agent.workflow_session_state:
        agent.workflow_session_state["shopping_list"] = []
        return f"Shopping list is empty. Cannot remove '{item}'."

    # Find and remove item (case-insensitive)
    shopping_list = agent.workflow_session_state["shopping_list"]
    for i, existing_item in enumerate(shopping_list):
        if existing_item.lower() == item.lower():
            removed_item = shopping_list.pop(i)
            return f"Removed '{removed_item}' from the shopping list."

    return f"'{item}' not found in the shopping list."


def remove_all_items(agent: Agent) -> str:
    """Remove all items from the shopping list in workflow session state."""
    if agent.workflow_session_state is None:
        agent.workflow_session_state = {}

    agent.workflow_session_state["shopping_list"] = []
    return "Removed all items from the shopping list."


def list_items(agent: Agent) -> str:
    """List all items in the shopping list from workflow session state."""
    if (
        agent.workflow_session_state is None
        or "shopping_list" not in agent.workflow_session_state
        or not agent.workflow_session_state["shopping_list"]
    ):
        return "Shopping list is empty."

    items = agent.workflow_session_state["shopping_list"]
    items_str = "\n".join([f"- {item}" for item in items])
    return f"Shopping list:\n{items_str}"


# Create agents with tools that use workflow session state
shopping_assistant = Agent(
    name="Shopping Assistant",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[add_item, remove_item, list_items],
    instructions=[
        "You are a helpful shopping assistant.",
        "You can help users manage their shopping list by adding, removing, and listing items.",
        "Always use the provided tools to interact with the shopping list.",
        "Be friendly and helpful in your responses.",
    ],
)

list_manager = Agent(
    name="List Manager",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[list_items, remove_all_items],
    instructions=[
        "You are a list management specialist.",
        "You can view the current shopping list and clear it when needed.",
        "Always show the current list when asked.",
        "Confirm actions clearly to the user.",
    ],
)

# Create steps
manage_items_step = Step(
    name="manage_items",
    description="Help manage shopping list items (add/remove)",
    agent=shopping_assistant,
)

view_list_step = Step(
    name="view_list",
    description="View and manage the complete shopping list",
    agent=list_manager,
)

# Create workflow with workflow_session_state
shopping_workflow = Workflow(
    name="Shopping List Workflow",
    steps=[manage_items_step, view_list_step],
    workflow_session_state={},  # Initialize empty workflow session state
)

if __name__ == "__main__":
    # Example 1: Add items to the shopping list
    print("=== Example 1: Adding Items ===")
    shopping_workflow.print_response(
        message="Please add milk, bread, and eggs to my shopping list."
    )
    print("Workflow session state:", shopping_workflow.workflow_session_state)

    # Example 2: Add more items and view list
    print("\n=== Example 2: Adding More Items ===")
    shopping_workflow.print_response(
        message="Add apples and bananas to the list, then show me the complete list."
    )
    print("Workflow session state:", shopping_workflow.workflow_session_state)

    # Example 3: Remove items
    print("\n=== Example 3: Removing Items ===")
    shopping_workflow.print_response(
        message="Remove bread from the list and show me what's left."
    )
    print("Workflow session state:", shopping_workflow.workflow_session_state)

    # Example 4: Clear the entire list
    print("\n=== Example 4: Clearing List ===")
    shopping_workflow.print_response(
        message="Clear the entire shopping list and confirm it's empty."
    )
    print("Final workflow session state:", shopping_workflow.workflow_session_state)



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/shared_session_state_with_team.py
================================================
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team.team import Team
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow


# === TEAM TOOLS FOR STEP MANAGEMENT ===
def add_step(
    team: Team, step_name: str, assignee: str, priority: str = "medium"
) -> str:
    """Add a step to the team's workflow session state."""
    if team.workflow_session_state is None:
        team.workflow_session_state = {}

    if "steps" not in team.workflow_session_state:
        team.workflow_session_state["steps"] = []

    step = {
        "name": step_name,
        "assignee": assignee,
        "status": "pending",
        "priority": priority,
        "created_at": "now",
    }
    team.workflow_session_state["steps"].append(step)

    result = f"✅ Successfully added step '{step_name}' assigned to {assignee} (priority: {priority}). Total steps: {len(team.workflow_session_state['steps'])}"
    return result


def delete_step(team: Team, step_name: str) -> str:
    """Delete a step from the team's workflow session state."""
    if (
        team.workflow_session_state is None
        or "steps" not in team.workflow_session_state
    ):
        return "❌ No steps found to delete"

    steps = team.workflow_session_state["steps"]
    for i, step in enumerate(steps):
        if step["name"] == step_name:
            deleted_step = steps.pop(i)
            result = f"✅ Successfully deleted step '{step_name}' (was assigned to {deleted_step['assignee']}). Remaining steps: {len(steps)}"
            return result

    result = f"❌ Step '{step_name}' not found in the list"
    return result


# === AGENT TOOLS FOR STATUS MANAGEMENT ===
def update_step_status(
    agent: Agent, step_name: str, new_status: str, notes: str = ""
) -> str:
    """Update the status of a step in the workflow session state."""
    if (
        agent.workflow_session_state is None
        or "steps" not in agent.workflow_session_state
    ):
        return "❌ No steps found in workflow session state"

    steps = agent.workflow_session_state["steps"]
    for step in steps:
        if step["name"] == step_name:
            old_status = step["status"]
            step["status"] = new_status
            if notes:
                step["notes"] = notes
            step["last_updated"] = "now"

            result = f"✅ Updated step '{step_name}' status from '{old_status}' to '{new_status}'"
            if notes:
                result += f" with notes: {notes}"

            return result

    result = f"❌ Step '{step_name}' not found in the list"
    return result


def assign_step(agent: Agent, step_name: str, new_assignee: str) -> str:
    """Reassign a step to a different person."""
    if (
        agent.workflow_session_state is None
        or "steps" not in agent.workflow_session_state
    ):
        return "❌ No steps found in workflow session state"

    steps = agent.workflow_session_state["steps"]
    for step in steps:
        if step["name"] == step_name:
            old_assignee = step["assignee"]
            step["assignee"] = new_assignee
            step["last_updated"] = "now"

            result = f"✅ Reassigned step '{step_name}' from {old_assignee} to {new_assignee}"
            return result

    result = f"❌ Step '{step_name}' not found in the list"
    return result


# === CREATE AGENTS ===
step_manager = Agent(
    name="StepManager",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are a precise step manager. Your ONLY job is to use the provided tools.",
        "When asked to add a step: ALWAYS use add_step(step_name, assignee, priority).",
        "When asked to delete a step: ALWAYS use delete_step(step_name).",
        "Do NOT create imaginary steps or lists.",
        "Do NOT provide explanations beyond what the tool returns.",
        "Be direct and use the tools immediately.",
    ],
)

step_coordinator = Agent(
    name="StepCoordinator",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You coordinate with the StepManager to ensure tasks are completed.",
        "Support the team by confirming actions and helping with coordination.",
        "Be concise and focus on the specific request.",
    ],
)

status_manager = Agent(
    name="StatusManager",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[update_step_status, assign_step],
    instructions=[
        "You manage step statuses and assignments using the provided tools.",
        "Use update_step_status(step_name, new_status, notes) to change step status.",
        "Use assign_step(step_name, new_assignee) to reassign steps.",
        "Valid statuses: 'pending', 'in_progress', 'completed', 'blocked', 'cancelled'.",
        "Be precise and only use the tools provided.",
    ],
)

# === CREATE TEAMS ===
management_team = Team(
    name="ManagementTeam",
    members=[step_manager, step_coordinator],
    tools=[add_step, delete_step],
    instructions=[
        "You are a step management team that ONLY uses the provided tools for adding and deleting steps.",
        "CRITICAL: Use add_step(step_name, assignee, priority) to add steps.",
        "CRITICAL: Use delete_step(step_name) to delete steps.",
        "IMPORTANT: You do NOT handle status updates - that's handled by the status manager in the next step.",
        "IMPORTANT: Do NOT delete steps when asked to mark them as completed - only delete when explicitly asked to delete.",
        "If asked to mark a step as completed, respond that status updates are handled by the status manager.",
        "Do NOT create fictional content or step lists.",
        "Execute only the requested add/delete actions using tools and report the result.",
    ],
    mode="coordinate",
    storage=None,
)

# === CREATE STEPS ===
manage_steps_step = Step(
    name="manage_steps",
    description="Management team uses tools to add/delete steps in the workflow session state",
    team=management_team,
)

update_status_step = Step(
    name="update_status",
    description="Status manager updates step statuses and assignments",
    agent=status_manager,
)

# === CREATE WORKFLOW ===
project_workflow = Workflow(
    name="Project Management Workflow",
    steps=[manage_steps_step, update_status_step],
    workflow_session_state={"steps": []},
)

# === HELPER FUNCTION TO DISPLAY CURRENT STATE ===


def print_current_steps(workflow):
    """Helper function to display current workflow state"""
    if (
        not workflow.workflow_session_state
        or "steps" not in workflow.workflow_session_state
    ):
        print("📋 No steps in workflow")
        return

    steps = workflow.workflow_session_state["steps"]
    if not steps:
        print("📋 Step list is empty")
        return

    print("📋 **Current Project Steps:**")
    for i, step in enumerate(steps, 1):
        status_emoji = {
            "pending": "⏳",
            "in_progress": "🔄",
            "completed": "✅",
            "blocked": "🚫",
            "cancelled": "❌",
        }.get(step["status"], "❓")

        priority_emoji = {"high": "🔥", "medium": "📝", "low": "💤"}.get(
            step.get("priority", "medium"), "📝"
        )

        print(
            f"  {i}. {status_emoji} {priority_emoji} **{step['name']}** (assigned to: {step['assignee']}, status: {step['status']})"
        )
        if "notes" in step:
            print(f"     💬 Notes: {step['notes']}")
    print()


if __name__ == "__main__":
    print("🚀 Starting Project Management Workflow Tests")
    print_current_steps(project_workflow)

    # Example 1: Add multiple steps with different priorities
    print("=" * 60)
    print("📝 Example 1: Add Multiple Steps")
    print("=" * 60)
    project_workflow.print_response(
        message="Add a high priority step called 'Setup Database' assigned to Alice, and a medium priority step called 'Create API' assigned to Bob"
    )
    print_current_steps(project_workflow)
    print(f"🔍 Workflow Session State: {project_workflow.workflow_session_state}")
    print()

    # Example 2: Update step status
    print("=" * 60)
    print("🔄 Example 2: Update Step Status")
    print("=" * 60)
    project_workflow.print_response(
        message="Mark 'Setup Database' as in_progress with notes 'Started database schema design'"
    )
    print_current_steps(project_workflow)
    print(f"🔍 Workflow Session State: {project_workflow.workflow_session_state}")
    print()

    # Example 3: Reassign and complete a step
    print("=" * 60)
    print("✅ Example 3: Reassign and Complete Step")
    print("=" * 60)
    project_workflow.print_response(
        message="Reassign 'Create API' to Charlie, then mark it as completed with notes 'API endpoints implemented and tested'"
    )
    print_current_steps(project_workflow)
    print(f"🔍 Workflow Session State: {project_workflow.workflow_session_state}")
    print()

    # Example 4: Add more steps and manage them
    print("=" * 60)
    print("📋 Example 4: Add and Manage More Steps")
    print("=" * 60)
    project_workflow.print_response(
        message="Add a low priority step 'Write Tests' assigned to Dave, then mark 'Setup Database' as completed"
    )
    print_current_steps(project_workflow)
    print(f"🔍 Workflow Session State: {project_workflow.workflow_session_state}")
    print()

    # Example 5: Delete a step
    print("=" * 60)
    print("🗑️ Example 5: Delete Step")
    print("=" * 60)
    project_workflow.print_response(
        message="Delete the 'Write Tests' step and add a high priority 'Deploy to Production' step assigned to Eve"
    )
    print_current_steps(project_workflow)
    print(f"🔍 Workflow Session State: {project_workflow.workflow_session_state}")



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/store_events_and_events_to_skip_in_a_workflow.py
================================================
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.response import (
    RunResponseContentEvent,
    ToolCallCompletedEvent,
    ToolCallStartedEvent,
)
from agno.run.v2.workflow import WorkflowRunEvent
from agno.storage.sqlite import SqliteStorage
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.parallel import Parallel
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents for different tasks
news_agent = Agent(
    name="News Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    instructions="You are a news researcher. Get the latest tech news and summarize key points.",
)

search_agent = Agent(
    name="Search Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    instructions="You are a search specialist. Find relevant information on given topics.",
)

analysis_agent = Agent(
    name="Analysis Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are an analyst. Analyze the provided information and give insights.",
)

summary_agent = Agent(
    name="Summary Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a summarizer. Create concise summaries of the provided content.",
)

research_step = Step(
    name="Research Step",
    agent=news_agent,
)

search_step = Step(
    name="Search Step",
    agent=search_agent,
)


def print_stored_events(workflow, example_name):
    """Helper function to print stored events in a nice format"""
    print(f"\n--- {example_name} - Stored Events ---")
    if workflow.run_response and workflow.run_response.events:
        print(f"Total stored events: {len(workflow.run_response.events)}")
        for i, event in enumerate(workflow.run_response.events, 1):
            print(f"  {i}. {event.event}")
    else:
        print("No events stored")
    print()


print("=== Simple Step Workflow with Event Storage ===")
step_workflow = Workflow(
    name="Simple Step Workflow",
    description="Basic workflow demonstrating step event storage",
    storage=SqliteStorage(
        table_name="workflow_v2_steps",
        db_file="tmp/workflow_v2_steps.db",
        mode="workflow_v2",
    ),
    steps=[research_step, search_step],
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.step_started,
        WorkflowRunEvent.workflow_completed,
    ],  # Skip step started events to reduce noise
)

print("Running Step workflow with streaming...")
for event in step_workflow.run(
    message="AI trends in 2024",
    stream=True,
    stream_intermediate_steps=True,
):
    # Filter out RunResponseContentEvent from printing to reduce noise
    if not isinstance(
        event, (RunResponseContentEvent, ToolCallStartedEvent, ToolCallCompletedEvent)
    ):
        print(
            f"Event: {event.event if hasattr(event, 'event') else type(event).__name__}"
        )

print(f"\nStep workflow completed!")
print(
    f"Total events stored: {len(step_workflow.run_response.events) if step_workflow.run_response and step_workflow.run_response.events else 0}"
)

# Print stored events in a nice format
print_stored_events(step_workflow, "Simple Step Workflow")

# ------------------------------------------------------------------------------------------------ #
# ------------------------------------------------------------------------------------------------ #

# Example 2: Parallel Primitive with Event Storage
print("=== 2. Parallel Example ===")
parallel_workflow = Workflow(
    name="Parallel Research Workflow",
    steps=[
        Parallel(
            Step(name="News Research", agent=news_agent),
            Step(name="Web Search", agent=search_agent),
            name="Parallel Research",
        ),
        Step(name="Combine Results", agent=analysis_agent),
    ],
    storage=SqliteStorage(
        table_name="workflow_v2_parallel",
        db_file="tmp/workflow_v2_parallel.db",
        mode="workflow_v2",
    ),
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.parallel_execution_started,
        WorkflowRunEvent.parallel_execution_completed,
    ],
)

print("Running Parallel workflow...")
for event in parallel_workflow.run(
    message="Research machine learning developments",
    stream=True,
    stream_intermediate_steps=True,
):
    # Filter out RunResponseContentEvent from printing
    if not isinstance(event, RunResponseContentEvent):
        print(
            f"Event: {event.event if hasattr(event, 'event') else type(event).__name__}"
        )

print(f"Parallel workflow stored {len(parallel_workflow.run_response.events)} events")
print_stored_events(parallel_workflow, "Parallel Workflow")
print()



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/structured_io_at_each_level_agent.py
================================================
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    response_model=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    response_model=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    response_model=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow
structured_workflow = Workflow(
    name="Structured Content Creation Pipeline",
    description="AI-powered content creation with structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Steps ===")

    # Test with simple string input
    structured_workflow.print_response(
        message="Latest developments in artificial intelligence and machine learning"
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/structured_io_at_each_level_agent_stream.py
================================================
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    response_model=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    response_model=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    response_model=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow
structured_workflow = Workflow(
    name="Structured Content Creation Pipeline",
    description="AI-powered content creation with structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Steps ===")

    # Test with simple string input
    structured_workflow.print_response(
        message="Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/structured_io_at_each_level_function_1.py
================================================
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step, StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


def data_analysis_function(step_input: StepInput) -> StepOutput:
    """
    Custom function to analyze the structured data received from previous step
    This will help us see what format the data is in when received
    """
    message = step_input.message
    previous_step_content = step_input.previous_step_content

    print("\n" + "=" * 60)
    print("🔍 CUSTOM FUNCTION DATA ANALYSIS")
    print("=" * 60)

    print(f"\n📝 Input Message Type: {type(message)}")
    print(f"📝 Input Message Value: {message}")

    print(f"\n📊 Previous Step Content Type: {type(previous_step_content)}")

    # Try to parse if it's structured data
    analysis_results = []

    if previous_step_content:
        print(f"\n🔍 Previous Step Content Preview:")
        print("Topic: ", previous_step_content.topic, "\n")
        print("Key Insights: ", previous_step_content.key_insights, "\n")
        print(
            "Trending Technologies: ", previous_step_content.trending_technologies, "\n"
        )

        analysis_results.append("✅ Received structured data (BaseModel)")

        # If it's a BaseModel, try to access its fields
        analysis_results.append(
            f"✅ BaseModel type: {type(previous_step_content).__name__}"
        )
        try:
            model_dict = previous_step_content.model_dump()
            analysis_results.append(f"✅ Model fields: {list(model_dict.keys())}")

            # If it's ResearchFindings, extract specific data
            if hasattr(previous_step_content, "topic"):
                analysis_results.append(
                    f"✅ Research Topic: {previous_step_content.topic}"
                )
            if hasattr(previous_step_content, "confidence_score"):
                analysis_results.append(
                    f"✅ Confidence Score: {previous_step_content.confidence_score}"
                )

        except Exception as e:
            analysis_results.append(f"❌ Error accessing BaseModel: {e}")

    # Create enhanced analysis
    enhanced_analysis = f"""
        ## Data Flow Analysis Report

        **Input Analysis:**
        - Message Type: {type(message).__name__}
        - Previous Content Type: {type(previous_step_content).__name__}

        **Structure Analysis:**
        {chr(10).join(analysis_results)}

        **Recommendations for Next Step:**
        Based on the data analysis, the content planning step should receive this processed information.
    """.strip()

    print(f"\n📋 Analysis Results:")
    for result in analysis_results:
        print(f"   {result}")

    print("=" * 60)

    return StepOutput(content=enhanced_analysis, success=True)


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    response_model=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    response_model=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    response_model=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

# Custom function step to analyze data flow
analysis_step = Step(
    name="data_analysis",
    executor=data_analysis_function,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow with custom function in between
structured_workflow = Workflow(
    name="Structured Content Creation Pipeline with Analysis",
    description="AI-powered content creation with data flow analysis",
    steps=[research_step, analysis_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow with Custom Function Analysis ===")

    # Test with simple string input
    structured_workflow.print_response(
        message="Latest developments in artificial intelligence and machine learning",
        # stream=True,
        # stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/structured_io_at_each_level_function_2.py
================================================
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step, StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class AnalysisReport(BaseModel):
    """Enhanced analysis report with BaseModel output"""

    analysis_type: str = Field(description="Type of analysis performed")
    input_data_type: str = Field(description="Type of input data received")
    structured_data_detected: bool = Field(
        description="Whether structured data was found"
    )
    key_findings: List[str] = Field(description="Key findings from the analysis")
    recommendations: List[str] = Field(description="Recommendations for next steps")
    confidence_score: float = Field(
        description="Analysis confidence (0.0-1.0)", ge=0.0, le=1.0
    )
    data_quality_score: float = Field(
        description="Quality of input data (0.0-1.0)", ge=0.0, le=1.0
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


def enhanced_analysis_function(step_input: StepInput) -> StepOutput:
    """
    Enhanced custom function that returns a BaseModel instead of just a string.
    This demonstrates the new capability of StepOutput.content supporting structured data.
    """
    message = step_input.message
    previous_step_content = step_input.previous_step_content

    print("\n" + "=" * 60)
    print("🔍 ENHANCED CUSTOM FUNCTION WITH STRUCTURED OUTPUT")
    print("=" * 60)

    print(f"\n📝 Input Message Type: {type(message)}")
    print(f"📝 Input Message Value: {message}")

    print(f"\n📊 Previous Step Content Type: {type(previous_step_content)}")

    # Analysis results
    key_findings = []
    recommendations = []
    structured_data_detected = False
    confidence_score = 0.8
    data_quality_score = 0.9

    if previous_step_content:
        print(f"\n🔍 Previous Step Content Analysis:")

        if isinstance(previous_step_content, ResearchFindings):
            structured_data_detected = True
            print("✅ Detected ResearchFindings BaseModel")
            print(f"   Topic: {previous_step_content.topic}")
            print(
                f"   Key Insights: {len(previous_step_content.key_insights)} insights"
            )
            print(f"   Confidence: {previous_step_content.confidence_score}")

            # Extract findings from the structured data
            key_findings.extend(
                [
                    f"Research topic identified: {previous_step_content.topic}",
                    f"Found {len(previous_step_content.key_insights)} key insights",
                    f"Identified {len(previous_step_content.trending_technologies)} trending technologies",
                    f"Research confidence level: {previous_step_content.confidence_score}",
                    f"Market impact assessment available",
                ]
            )

            recommendations.extend(
                [
                    "Leverage high-confidence research findings for content strategy",
                    "Focus on trending technologies identified in research",
                    "Use market impact insights for audience targeting",
                    "Build content around key insights with strong evidence",
                ]
            )

            confidence_score = previous_step_content.confidence_score
            data_quality_score = 0.95  # High quality due to structured input

        else:
            key_findings.append(
                "Received unstructured data - converted to string format"
            )
            recommendations.append(
                "Consider implementing structured data models for better processing"
            )
            confidence_score = 0.6
            data_quality_score = 0.7

    else:
        key_findings.append("No previous step content available")
        recommendations.append("Ensure data flow between steps is properly configured")
        confidence_score = 0.4
        data_quality_score = 0.5

    # Create structured analysis report using BaseModel
    analysis_report = AnalysisReport(
        analysis_type="Structured Data Flow Analysis",
        input_data_type=type(previous_step_content).__name__,
        structured_data_detected=structured_data_detected,
        key_findings=key_findings,
        recommendations=recommendations,
        confidence_score=confidence_score,
        data_quality_score=data_quality_score,
    )

    print(f"\n📋 Analysis Results (BaseModel):")
    print(f"   Analysis Type: {analysis_report.analysis_type}")
    print(f"   Structured Data: {analysis_report.structured_data_detected}")
    print(f"   Confidence: {analysis_report.confidence_score}")
    print(f"   Data Quality: {analysis_report.data_quality_score}")
    print("=" * 60)

    # Return StepOutput with BaseModel content
    return StepOutput(content=analysis_report, success=True)


def simple_data_processor(step_input: StepInput) -> StepOutput:
    """
    Simple function that demonstrates accessing different content types
    """
    print(f"\n🔧 SIMPLE DATA PROCESSOR")
    print(f"Previous step content type: {type(step_input.previous_step_content)}")

    # Access the structured data directly
    if isinstance(step_input.previous_step_content, AnalysisReport):
        report = step_input.previous_step_content
        print(f"Processing analysis report with confidence: {report.confidence_score}")

        summary = {
            "processor": "simple_data_processor",
            "input_confidence": report.confidence_score,
            "input_quality": report.data_quality_score,
            "processed_findings": len(report.key_findings),
            "processed_recommendations": len(report.recommendations),
            "status": "processed_successfully",
        }

        return StepOutput(content=summary, success=True)
    else:
        return StepOutput(
            content="Unable to process - expected AnalysisReport", success=False
        )


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    response_model=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    response_model=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    response_model=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

# Custom function step that returns a BaseModel
analysis_step = Step(
    name="enhanced_analysis",
    executor=enhanced_analysis_function,
)

# Another custom function that processes the BaseModel
processor_step = Step(
    name="data_processor",
    executor=simple_data_processor,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow with custom functions that demonstrate structured output
enhanced_workflow = Workflow(
    name="Enhanced Structured Content Creation Pipeline",
    description="AI-powered content creation with BaseModel outputs from custom functions",
    steps=[research_step, analysis_step, processor_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    # Test with simple string input
    enhanced_workflow.print_response(
        message="Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/structured_io_at_each_level_team.py
================================================
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


# Create individual agents for teams
research_specialist = Agent(
    name="Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Find and analyze the latest AI trends and developments",
    instructions=[
        "Search for recent AI developments using available tools",
        "Focus on breakthrough technologies and market trends",
        "Provide detailed analysis with credible sources",
    ],
)

data_analyst = Agent(
    name="Data Analyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Analyze research data and extract key insights",
    instructions=[
        "Process research findings to identify patterns",
        "Quantify market impact and confidence levels",
        "Structure insights for strategic planning",
    ],
)

content_strategist = Agent(
    name="Content Strategist",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Develop content strategies based on research insights",
    instructions=[
        "Create comprehensive content strategies",
        "Focus on audience targeting and engagement",
        "Recommend optimal posting schedules and content pillars",
    ],
)

marketing_expert = Agent(
    name="Marketing Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Provide marketing insights and hashtag recommendations",
    instructions=[
        "Suggest effective hashtags and engagement tactics",
        "Analyze target audience preferences",
        "Recommend proven marketing strategies",
    ],
)

project_manager = Agent(
    name="Project Manager",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed project plans and timelines",
    instructions=[
        "Develop comprehensive implementation plans",
        "Set realistic timelines and budget estimates",
        "Identify potential risks and mitigation strategies",
    ],
)

budget_analyst = Agent(
    name="Budget Analyst",
    model=OpenAIChat(id="gpt-4o"),
    role="Analyze costs and provide budget recommendations",
    instructions=[
        "Estimate project costs and resource requirements",
        "Provide budget ranges and cost optimization suggestions",
        "Consider ROI and success metrics",
    ],
)

# Create teams with structured outputs
research_team = Team(
    name="AI Research Team",
    members=[research_specialist, data_analyst],
    mode="collaborate",
    model=OpenAIChat(id="gpt-4o"),
    description="A collaborative team that researches AI trends and extracts structured insights",
    response_model=ResearchFindings,
    instructions=[
        "Work together to research the given topic thoroughly",
        "Combine research findings with data analysis",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
    ],
)

strategy_team = Team(
    name="Content Strategy Team",
    members=[content_strategist, marketing_expert],
    mode="collaborate",
    model=OpenAIChat(id="gpt-4o"),
    description="A strategic team that creates comprehensive content strategies",
    response_model=ContentStrategy,
    instructions=[
        "Analyze the research findings from the previous step",
        "Collaborate to create a comprehensive content strategy",
        "Focus on audience engagement and brand building",
        "Combine content strategy with marketing expertise",
    ],
)

planning_team = Team(
    name="Content Planning Team",
    members=[project_manager, budget_analyst],
    mode="collaborate",
    model=OpenAIChat(id="gpt-4o"),
    description="A planning team that creates detailed implementation plans",
    response_model=FinalContentPlan,
    instructions=[
        "Use the content strategy to create a detailed implementation plan",
        "Combine project management with budget analysis",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
    ],
)

# Define steps using teams
research_step = Step(
    name="Research Insights",
    team=research_team,  # Using team instead of agent
)

strategy_step = Step(
    name="Content Strategy",
    team=strategy_team,  # Using team instead of agent
)

planning_step = Step(
    name="Final Planning",
    team=planning_team,  # Using team instead of agent
)

# Create workflow with teams
structured_workflow = Workflow(
    name="Team-Based Structured Content Creation Pipeline",
    description="AI-powered content creation with teams and structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Teams ===")

    structured_workflow.print_response(
        message="Latest developments in artificial intelligence and machine learning",
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/structured_io_at_each_level_team_stream.py
================================================
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


# Create individual agents for teams
research_specialist = Agent(
    name="Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Find and analyze the latest AI trends and developments",
    instructions=[
        "Search for recent AI developments using available tools",
        "Focus on breakthrough technologies and market trends",
        "Provide detailed analysis with credible sources",
    ],
)

data_analyst = Agent(
    name="Data Analyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Analyze research data and extract key insights",
    instructions=[
        "Process research findings to identify patterns",
        "Quantify market impact and confidence levels",
        "Structure insights for strategic planning",
    ],
)

content_strategist = Agent(
    name="Content Strategist",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Develop content strategies based on research insights",
    instructions=[
        "Create comprehensive content strategies",
        "Focus on audience targeting and engagement",
        "Recommend optimal posting schedules and content pillars",
    ],
)

marketing_expert = Agent(
    name="Marketing Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Provide marketing insights and hashtag recommendations",
    instructions=[
        "Suggest effective hashtags and engagement tactics",
        "Analyze target audience preferences",
        "Recommend proven marketing strategies",
    ],
)

project_manager = Agent(
    name="Project Manager",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed project plans and timelines",
    instructions=[
        "Develop comprehensive implementation plans",
        "Set realistic timelines and budget estimates",
        "Identify potential risks and mitigation strategies",
    ],
)

budget_analyst = Agent(
    name="Budget Analyst",
    model=OpenAIChat(id="gpt-4o"),
    role="Analyze costs and provide budget recommendations",
    instructions=[
        "Estimate project costs and resource requirements",
        "Provide budget ranges and cost optimization suggestions",
        "Consider ROI and success metrics",
    ],
)

# Create teams with structured outputs
research_team = Team(
    name="AI Research Team",
    members=[research_specialist, data_analyst],
    mode="route",
    model=OpenAIChat(id="gpt-4o"),
    description="A collaborative team that researches AI trends and extracts structured insights",
    response_model=ResearchFindings,
    instructions=[
        "Work together to research the given topic thoroughly",
        "Combine research findings with data analysis",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
    ],
)

strategy_team = Team(
    name="Content Strategy Team",
    members=[content_strategist, marketing_expert],
    mode="coordinate",
    model=OpenAIChat(id="gpt-4o"),
    description="A strategic team that creates comprehensive content strategies",
    response_model=ContentStrategy,
    instructions=[
        "Analyze the research findings from the previous step",
        "Collaborate to create a comprehensive content strategy",
        "Focus on audience engagement and brand building",
        "Combine content strategy with marketing expertise",
    ],
)

planning_team = Team(
    name="Content Planning Team",
    members=[project_manager, budget_analyst],
    mode="collaborate",
    model=OpenAIChat(id="gpt-4o"),
    description="A planning team that creates detailed implementation plans",
    response_model=FinalContentPlan,
    instructions=[
        "Use the content strategy to create a detailed implementation plan",
        "Combine project management with budget analysis",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
    ],
)

# Define steps using teams
research_step = Step(
    name="Research Insights",
    team=research_team,  # Using team instead of agent
)

strategy_step = Step(
    name="Content Strategy",
    team=strategy_team,  # Using team instead of agent
)

planning_step = Step(
    name="Final Planning",
    team=planning_team,  # Using team instead of agent
)

# Create workflow with teams
structured_workflow = Workflow(
    name="Team-Based Structured Content Creation Pipeline",
    description="AI-powered content creation with teams and structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Teams ===")

    structured_workflow.print_response(
        message="Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/workflow_metrics_on_run_response.py
================================================
import json

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.v2.workflow import WorkflowRunResponse
from agno.storage.sqlite import SqliteStorage
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    mode="coordinate",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        storage=SqliteStorage(
            table_name="workflow_v2",
            db_file="tmp/workflow_v2.db",
            mode="workflow_v2",
        ),
        steps=[research_step, content_planning_step],
    )
    workflow_run_response: WorkflowRunResponse = content_creation_workflow.run(
        message="AI trends in 2024",
    )

    # Print workflow metrics
    if workflow_run_response.workflow_metrics:
        print("\n" + "-" * 60)
        print("WORKFLOW METRICS")
        print("-" * 60)
        print(json.dumps(workflow_run_response.workflow_metrics.to_dict(), indent=2))
    else:
        print("\nNo workflow metrics available")



================================================
FILE: cookbook/workflows_2/sync/06_workflows_advanced_concepts/workflow_with_image_input.py
================================================
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.v2.step import Step
from agno.workflow.v2.workflow import Workflow

# Define agents
image_analyzer = Agent(
    name="Image Analyzer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Analyze the provided image and extract key details, objects, and context.",
)

news_researcher = Agent(
    name="News Researcher",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search for latest news and information related to the analyzed image content.",
)

# Define steps
analysis_step = Step(
    name="Image Analysis Step",
    agent=image_analyzer,
)

research_step = Step(
    name="News Research Step",
    agent=news_researcher,
)

# Create workflow with media input
media_workflow = Workflow(
    name="Image Analysis and Research Workflow",
    description="Analyze an image and research related news",
    steps=[analysis_step, research_step],
    storage=SqliteStorage(
        table_name="workflow_v2",
        db_file="tmp/workflow_v2.db",
        mode="workflow_v2",
    ),
)

# Run workflow with image input
if __name__ == "__main__":
    media_workflow.print_response(
        message="Please analyze this image and find related news",
        images=[
            Image(
                url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
            )
        ],
        markdown=True,
    )


