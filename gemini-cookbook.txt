Directory structure:
└── google-gemini-cookbook/
    ├── README.md
    ├── CONTRIBUTING.md
    ├── LICENSE
    ├── conferences/
    │   └── README.md
    ├── examples/
    │   ├── README.md
    │   ├── Agents_Function_Calling_Barista_Bot.ipynb
    │   ├── Analyze_a_Video_Historic_Event_Recognition.ipynb
    │   ├── Apollo_11.ipynb
    │   ├── CONTRIBUTING.md
    │   ├── document_search.ipynb
    │   ├── Entity_Extraction.ipynb
    │   ├── fastrtc_ui.py
    │   ├── gradio_audio.py
    │   ├── Object_detection.ipynb
    │   ├── Opossum_search.ipynb
    │   ├── Pdf_structured_outputs_on_invoices_and_forms.ipynb
    │   ├── Search_Wikipedia_using_ReAct.ipynb
    │   ├── Talk_to_documents_with_embeddings.ipynb
    │   ├── Upload_files_to_Colab.ipynb
    │   ├── Voice_memos.ipynb
    │   ├── Apps_script_and_Workspace_codelab/
    │   │   ├── README.md
    │   │   ├── CollegeExpenses.xlsx
    │   │   ├── Gemini-blog.txt
    │   │   ├── main.gs
    │   │   └── utils.gs
    │   ├── chromadb/
    │   │   ├── README.md
    │   │   └── Vectordb_with_chroma.ipynb
    │   ├── google-adk/
    │   │   ├── README.md
    │   │   └── Getting_started_with_ADK.ipynb
    │   ├── iot/
    │   │   └── esp32/
    │   │       └── voice_led_controller/
    │   │           ├── README.md
    │   │           └── led_controller.ino
    │   ├── json_capabilities/
    │   │   ├── README.md
    │   │   ├── Entity_Extraction_JSON.ipynb
    │   │   ├── Sentiment_Analysis.ipynb
    │   │   ├── Text_Classification.ipynb
    │   │   └── Text_Summarization.ipynb
    │   ├── langchain/
    │   │   ├── README.md
    │   │   ├── Chat_with_SQL_using_langchain.ipynb
    │   │   ├── Code_analysis_using_Gemini_LangChain_and_DeepLake.ipynb
    │   │   ├── Gemini_LangChain_QA_Chroma_WebLoad.ipynb
    │   │   ├── Gemini_LangChain_QA_Pinecone_WebLoad.ipynb
    │   │   └── Gemini_LangChain_Summarization_WebLoad.ipynb
    │   ├── llamaindex/
    │   │   ├── README.md
    │   │   └── Gemini_LlamaIndex_QA_Chroma_WebPageReader.ipynb
    │   ├── mlflow/
    │   │   ├── README.md
    │   │   └── MLflow_Observability.ipynb
    │   ├── prompting/
    │   │   ├── README.md
    │   │   ├── Adding_context_information.ipynb
    │   │   ├── Basic_Classification.ipynb
    │   │   ├── Basic_Code_Generation.ipynb
    │   │   ├── Basic_Evaluation.ipynb
    │   │   ├── Basic_Information_Extraction.ipynb
    │   │   ├── Basic_Reasoning.ipynb
    │   │   ├── Chain_of_thought_prompting.ipynb
    │   │   ├── Few_shot_prompting.ipynb
    │   │   ├── Providing_base_cases.ipynb
    │   │   ├── Role_prompting.ipynb
    │   │   ├── Self_ask_prompting.ipynb
    │   │   └── Zero_shot_prompting.ipynb
    │   ├── qdrant/
    │   │   ├── README.md
    │   │   └── Qdrant_similarity_search.ipynb
    │   └── weaviate/
    │       ├── README.md
    │       ├── docker-compose.yml
    │       └── query-agent-as-a-tool.ipynb
    ├── quickstarts/
    │   ├── README.md
    │   ├── Asynchronous_requests.ipynb
    │   ├── Authentication.ipynb
    │   ├── Authentication_with_OAuth.ipynb
    │   ├── Caching.ipynb
    │   ├── Error_handling.ipynb
    │   ├── Function_calling_config.ipynb
    │   ├── Get_started_LearnLM.ipynb
    │   ├── Get_started_LiveAPI.py
    │   ├── Get_started_LiveAPI_NativeAudio.py
    │   ├── Get_started_LyriaRealTime.py
    │   ├── JSON_mode.ipynb
    │   ├── Models.ipynb
    │   ├── New_in_002.ipynb
    │   ├── Safety.ipynb
    │   ├── Streaming.ipynb
    │   ├── System_instructions.ipynb
    │   ├── Template.ipynb
    │   ├── Video.ipynb
    │   ├── file-api/
    │   │   ├── README.md
    │   │   ├── package.json
    │   │   ├── requirements.txt
    │   │   ├── sample.js
    │   │   ├── sample.py
    │   │   └── sample.sh
    │   ├── rest/
    │   │   ├── README.md
    │   │   ├── Audio_REST.ipynb
    │   │   ├── Caching_REST.ipynb
    │   │   ├── Embeddings_REST.ipynb
    │   │   ├── Function_calling_config_REST.ipynb
    │   │   ├── Function_calling_REST.ipynb
    │   │   ├── JSON_mode_REST.ipynb
    │   │   ├── Models_REST.ipynb
    │   │   ├── Safety_REST.ipynb
    │   │   ├── Search_Grounding.ipynb
    │   │   ├── Streaming_REST.ipynb
    │   │   ├── System_instructions_REST.ipynb
    │   │   └── Video_REST.ipynb
    │   └── websockets/
    │       ├── README.md
    │       ├── Get_started_LiveAPI.py
    │       └── shell_websockets.sh
    ├── quickstarts-js/
    │   ├── README.md
    │   ├── Get_Started.js
    │   └── Image_out.js
    ├── .devcontainer/
    │   ├── devcontainer.json
    │   └── welcome.txt
    ├── .gemini/
    │   ├── config.yaml
    │   └── styleguide.md
    └── .github/
        ├── header-checker-lint.yml
        ├── labeler.yml
        └── workflows/
            ├── issue-metrics.yml
            ├── labeler.yml
            ├── new_examples_links_in_table_of_content.yml
            ├── notebooks.yaml
            └── stale.yml

================================================
FILE: README.md
================================================
# Welcome to the Gemini API Cookbook

This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.

**For comprehensive API documentation, visit [ai.google.dev](https://ai.google.dev/gemini-api/docs).**
<br><br>

## Navigating the Cookbook

This cookbook is organized into two main categories:

1.  **[Quick Starts](https://github.com/google-gemini/cookbook/tree/main/quickstarts/):**  Step-by-step guides covering both introductory topics ("[Get Started](./quickstarts/Get_started.ipynb)") and specific API features.
2.  **[Examples](https://github.com/google-gemini/cookbook/tree/main/examples/):** Practical use cases demonstrating how to combine multiple features.

We also showcase **Demos** in separate repositories, illustrating end-to-end applications of the Gemini API.
<br><br>

## What's New?

Here are the recent additions and updates to the Gemini API and the Cookbook: 

* **Gemini 2.5 models:** Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the [Get Started Guide](./quickstarts/Get_started.ipynb) and the [thinking guide](./quickstarts/Get_started_thinking.ipynb) as they'll all be thinking ones.
* **Imagen and Veo**: Get started with our media generation model with this brand new [Veo guide](./quickstarts/Get_started_Veo.ipynb) and [Imagen guide](./quickstarts/Get_started_imagen.ipynb)!
* **Lyria and TTS**: Get started with podcast and music generation with the [TTS](./quickstarts/Get_started_TTS.ipynb) and [Lyria RealTime](./quickstarts/Get_started_LyriaRealTime.ipynb) models.
* **LiveAPI**: Get started with the [multimodal Live API](./quickstarts/Get_started_LiveAPI.ipynb) and unlock new interactivity with Gemini.
* **Recently Added Guides:**
  * [Image-out](./quickstarts/Image_out.ipynb): Use Gemini's native image generation capabilities to edit images with high consistency or generate visual stories.
  * [Grounding](./quickstarts/Grounding.ipynb): Discover different ways to ground Gemini's answer using different tools, from Google Search to Youtube and the new **url context** tool.
  * [Batch-mode](./quickstarts/Batch_mode.ipynb): Use Batch-mode to send large volume of non-time-sensitive requests to the model and get a 50% discount.

  
<br><br>

## 1. Quick Starts

The [quickstarts section](https://github.com/google-gemini/cookbook/tree/main/quickstarts/) contains step-by-step tutorials to get you started with Gemini and learn about its specific features.

**To begin, you'll need:**

1.  A Google account.
2.  An API key (create one in [Google AI Studio](https://aistudio.google.com/app/apikey)).
<br><br>

We recommend starting with the following:

*   [Authentication](./quickstarts/Authentication.ipynb): Set up your API key for access.
*   [**Get started**](./quickstarts/Get_started.ipynb): Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input.
<br><br>

Then, explore the other quickstarts tutorials to learn about individual features:
*  [Get started with Live API](./quickstarts/Get_started_LiveAPI.ipynb): Get started with the live API with this comprehensive overview of its capabilities
*  [Get started with Veo](./quickstarts/Get_started_Veo.ipynb): Get started with our video generation capabilities 
*  [Get started with Imagen](./quickstarts/Get_started_imagen.ipynb) and [Image-out](./quickstarts/Image_out.ipynb): Get started with our image generation capabilities 
*  [Grounding](./quickstarts/Search_Grounding.ipynb): use Google Search for grounded responses
*  [Code execution](./quickstarts/Code_Execution.ipynb): Generating and running Python code to solve complex tasks and even ouput graphs
*  And [many more](https://github.com/google-gemini/cookbook/tree/main/quickstarts/)
<br><br>

## 2. Examples (Practical Use Cases)

These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications.
*  [Browser as a tool](./examples/Browser_as_a_tool.ipynb): Use a web browser for live and internal (intranet) web interactions
*  [Illustrate a book](./examples/Book_illustration.ipynb): Use Gemini and Imagen to create illustration for an open-source book
*  [Animated Story Generation](./examples/Animated_Story_Video_Generation_gemini.ipynb): Create animated videos by combining Gemini's story generation, Imagen, and audio synthesis
*  [Plotting and mapping Live](./examples/LiveAPI_plotting_and_mapping.ipynb): Mix *Live API* and *Code execution* to solve complex tasks live
*  [3D Spatial understanding](./examples/Spatial_understanding_3d.ipynb): Use Gemini *3D spatial* abilities to understand 3D scenes
*  [Gradio and live API](./examples/gradio_audio.py): Use gradio to deploy your own instance of the *Live API*
*  And [many many more](https://github.com/google-gemini/cookbook/tree/main/examples/)
<br><br>

## 3. Demos (End-to-End Applications)

These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios. 

*   [Gemini API quickstart](https://github.com/google-gemini/gemini-api-quickstart): Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini's multi-modal capabilities
*   [Multimodal Live API Web Console](https://github.com/google-gemini/multimodal-live-api-web-console): React-based starter app for using the Multimodal Live API over a websocket
*   [Google AI Studio Starter Applets](https://github.com/google-gemini/starter-applets): A collection of small apps that demonstrate how Gemini can be used to create interactive experiences
<br><br>


## Official SDKs

The Gemini API is a REST API. You can call it directly using tools like `curl` (see [REST examples](https://github.com/google-gemini/cookbook/tree/main/quickstarts/rest/) or the great [Postman workspace](https://www.postman.com/ai-on-postman/google-gemini-apis/overview)), or use one of our official SDKs:
* [Python](https://github.com/googleapis/python-genai)
* [Go](https://github.com/google/generative-ai-go)
* [Node.js](https://github.com/google/generative-ai-js)
* [Dart (Flutter)](https://github.com/google/generative-ai-dart)
* [Android](https://github.com/google/generative-ai-android)
* [Swift](https://github.com/google/generative-ai-swift)
<br><br>


## Important: Migration

With Gemini 2 we are offering a [new SDK](https://github.com/googleapis/python-genai)
(<code>[google-genai](https://pypi.org/project/google-genai/)</code>,
<code>v1.0</code>). The updated SDK is fully compatible with all Gemini API
models and features, including recent additions like the
[live API](https://aistudio.google.com/live) (audio + video streaming),
improved tool usage (
[code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python),
[function calling](https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python) and integrated
[Google search grounding](https://ai.google.dev/gemini-api/docs/grounding?lang=python)),
and media generation ([Imagen](https://ai.google.dev/gemini-api/docs/imagen) and [Veo](https://ai.google.dev/gemini-api/docs/video)).
This SDK allows you to connect to the Gemini API through either
[Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp) or
[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2).

The <code>[google-generativeai](https://pypi.org/project/google-generativeai)</code>
package will continue to support the original Gemini models.
It <em>can</em> also be used with Gemini 2 models, just with a limited feature
set. All new features will be developed in the new Google GenAI SDK.

See the [migration guide](https://ai.google.dev/gemini-api/docs/migrate) for details.
<br><br>

## Get Help

Ask a question on the [Google AI Developer Forum](https://discuss.ai.google.dev/).

## The Gemini API on Google Cloud Vertex AI

For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See [this repo](https://github.com/GoogleCloudPlatform/generative-ai) for examples.

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

Thank you for developing with the Gemini API! We're excited to see what you create.



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to the Gemini API Cookbook

We would love to accept your patches and contributions to the Gemini API Cookbook. We are excited that you are considering donating some of your time, and this guide will help us be respectful of that time.

# Before you send anything

## Sign our contributor agreement

All contributions to this project must be accompanied by a [Contributor License Agreement](https://cla.developers.google.com/about) (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project.

If you or your current employer have already signed the Google CLA (even if it was for a different project), you probably don't need to do it again.

Visit [https://cla.developers.google.com/](https://cla.developers.google.com/) to see your current agreements or to sign a new one.

## Style guides

Before you start writing, take a look at the [technical writing style guide](https://developers.google.com/style). You don’t need to fully digest the whole document, but do read the [highlights](https://developers.google.com/style/highlights) so you can anticipate the most common feedback.

Also check out the relevant [style guide](https://google.github.io/styleguide/) for the language you will be using. These apply strictly to raw code files (e.g. *.py, *.js), though code fragments in documentation (such as markdown files or notebooks) tend to favor readability over strict adherence.

For Python notebooks (*.ipynb files), consider running `pyink` over your notebook. It is not required, but it will avoid style-related nits.

See below for more detailed guidelines specific to writing notebooks and guides.

# Making changes

## Small fixes

Small fixes, such as typos or bug fixes, can be submitted directly via a pull request.

## Content submission

Before you send a PR, or even write a single line, please file an [issue](https://github.com/google-gemini/cookbook/issues). There we can discuss the request and provide guidance about how to structure any content you write.

Adding a new guide often involves lots of detailed reviews and we want to make sure that your idea is fully formed and has full support before you start writing anything. If you want to port an existing guide across (e.g. if you have a guide for Gemini on your own GitHub), feel free to link to it in the issue.

When you're ready, start by using the [notebook
template](./quickstarts/Template.ipynb) and following the guidance within.

Before submitting your notebook, it's recommended to run linting and formatting tools locally to ensure consistency and adherence to style guidelines.

1. Install Dependencies:

First, install the necessary packages using pip:

```bash
pip install -U tensorflow-docs
```

2. Format the Notebook:

Use the nbfmt tool from tensorflow-docs to automatically format your notebook:

```
python -m tensorflow_docs.tools.nbfmt path/to/notebook
```

Replace `path/to/notebook` with the actual path to your notebook file.

3. Lint the Notebook:

Use the nblint tool to check for style and consistency issues:

```
python -m tensorflow_docs.tools.nblint \
            --styles=google,tensorflow \
            --arg=repo:google-gemini/cookbook \
            --arg=branch:main \
            --exclude_lint=tensorflow::button_download \
            --exclude_lint=tensorflow::button_website \
            --arg=base_url:https://ai.google.dev/ \
            --exclude_lint=tensorflow::button_github \
            path/to/notebook
```

Replace `path/to/notebook` with the actual path to your notebook file.

## Things we consider

When accepting a new guide, we want to balance a few aspects.
* Originality - e.g. Is there another guide that does the same thing?
* Pedagogy - e.g. Does this guide teach something useful? Specifically for a Gemini API feature?
* Quality - e.g. Does this guide contain clear, descriptive prose? Is the code easy to understand?

It is not crucial for a submission to be strong along all of these dimensions, but the stronger the better. Old submissions may be replaced in favor of newer submissions that exceed these properties.

## Attribution
If you have authored a new guide from scratch, you are welcome to include a byline at the top of the document with your name and GitHub username.

# Detailed Coding and Notebook Guidelines
## Notebook Style

* Include the collapsed license at the top (uses the Colab "Form" mode to hide the cells).
* Save the notebook with the table of contents open.
* Use one `H1` header (`#` in Markdown) for the title.
* Include the "Open in Colab" button immediately after the `H1`. It should look like:
    ```html
    <a target="_blank" href="URL"><img src="[https://colab.research.google.com/assets/colab-badge.svg](https://colab.research.google.com/assets/colab-badge.svg)" height=30/></a>
    ```
    where `URL` should be `https://colab.research.google.com/github/google-gemini/cookbook/blob/main/` followed by the notebook location in the cookbook.
* Include an overview section before any code.
* Put all your installs (using `%pip` instead of `!pip`) and imports in a dedicated setup section near the beginning.
* Keep code and text cells as brief as possible.
* Break text cells at headings.
* Break code cells between distinct logical steps, such as "building" and "running", or between processing/printing different results.
* Necessary but uninteresting code (like helper functions) should be hidden in a toggleable code cell by putting `# @title` as the first line.

## Code Style

* Notebooks are for people. Write code optimized for clarity.
* Use the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html), where applicable. Code formatted by [`pyink`](https://github.com/google/pyink) will always be accepted.
* **Use 4 spaces per indentation level.** (PEP 8 recommendation)
* In particular, defining functions and variables takes extra spaces around the `=` sign, while function parameters don't:
    ```python
    var = value

    function(
        parameter=value
    )
    ```
* When a function has multiple parameters, expand it onto multiple lines with proper indentation for better readability:
    ```python
    response = client.models.generate_content(
        model=MODEL_ID,
        contents="Here's my prompt",
        config={
            "response_mime_type": "application/json",
            "response_schema": Schema
        }
    )
    ```
    *Notice the line break after the opening parenthesis and before the closing parenthesis.*
* Long text variables should use triple double quotes and proper indentation for better readability:
    ```python
    long_prompt = """
        Cupcake ipsum dolor. Sit amet marshmallow topping cheesecake muffin.
        Halvah croissant candy canes bonbon candy. Apple pie jelly beans topping carrot cake danish tart cake cheesecake.
        Muffin danish chocolate soufflé pastry icing bonbon oat cake. Powder cake jujubes oat cake.
        Lemon drops tootsie roll marshmallow halvah carrot cake.
    """
    ```
    *Notice the line break after the opening triple quotes and before the closing triple quotes.*
* Demonstrate small parts before combining them into something more complex.
* If you define a function, ideally run it and show its output immediately before using it in another function or a more complex block.
* Only use helper functions when necessary (e.g., for code reuse or complexity management). If a piece of logic is only a couple of lines and used once, it's often clearer to write it inline so readers don't have to look up the function definition. Hide helper function definitions using `# @title`.
* Keep examples quick and concise. Do not add extra options or parameters (like `temperature`) without explaining them; focus on what you want to showcase.
* If you *must* use extra parameters, explain *why* they are needed and the reasoning behind the specific value the first time you use them.
* When selecting a model, use a Colab form selector for easier maintainability:
    ```python
    MODEL_ID="gemini-2.5-flash" # @param ["gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-pro"] {"allow-input":true, isTemplate: true}
    ```
* Some notebooks can benefit from having a form to update the prompt:
    ```python
    prompt = "Detect the 2d bounding boxes of the cupcakes (with 'label' as topping description')"  # @param {type:"string"}
    ```
    or a list of prompts:
    ```python
    prompt = "Draw a square around the fox' shadow"  # @param ["Find the two origami animals.", "Where are the origamis' shadows?","Draw a square around the fox' shadow"] {"allow-input":true}
    ```
* To ensure notebook text remains accurate, present model metadata (like context window size) by executing code, not by hardcoding it in Markdown.
    * Example: Instead of writing "This model has a 1M token context window", display the output of `client.models.get('models/gemini-2.5-pro').input_token_limit`.

## Naming Conventions

* **Variables:** Use lowercase with underscores (snake\_case): `user_name`, `total_count`
* **Constants:** Use uppercase with underscores: `MAX_VALUE`, `DATABASE_NAME`
* **Functions:** Use lowercase with underscores (snake\_case): `calculate_total()`, `process_data()`
* **Classes:** Use CapWords (CamelCase): `UserManager`, `PaymentProcessor`
* **Modules:** Use lowercase with underscores (snake\_case): `user_utils`, `payment_gateway`

## Comments

* Write clear and concise comments: Explain the "why" behind the code, not just the "what".
* Comment sparingly: Well-written code should be self-documenting where possible.
* Use complete sentences: Start comments with a capital letter and use proper punctuation.

## Outputs

* Whenever possible, simply use `print()` for basic output.
* When needed, use `display(Markdown())` for formatted Markdown text, `print(json.dumps(json_string, indent=4))` for readable JSON, or `display(Image())` for images.

## Text

* Use an imperative style: "Run a prompt using the API."
* Use sentence case in titles/headings: "Download the data", "Call the API", "Process the results".
* Use short titles/headings.
* Use the [Google developer documentation style guide](https://developers.google.com/style/highlights).
* Use [second person](https://developers.google.com/style/person): "you" rather than "we". (You will fail the lint check otherwise).
* Explain what you are doing and the features you are using. Link to relevant documentation or other notebooks for more details where appropriate.

## GitHub Workflow

* Be consistent about how you save your notebooks (e.g., with ToC open, potentially omitting outputs) to keep JSON diffs manageable. Tools like [`nbfmt` and `nblint`](https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/tools/README.md) can help enforce consistency.
* Consider setting the "Omit code cell output when saving this notebook" option if outputs (like inline images) make diffs too large for GitHub.
* [ReviewNB.com](http://reviewnb.com) can assist with reviewing notebook diffs in pull requests.
* Use the [Open in Colab](https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo) browser extension to easily open GitHub notebooks in Colab.
* The easiest way to edit a notebook tracked in GitHub is often:
    1.  Open the notebook in Colab directly from the GitHub branch you intend to edit.
    2.  Make your changes in Colab.
    3.  Use Colab's "File" -> "Save a copy in GitHub" menu option to save it back to the same branch.
* For Pull Requests (PRs), it's helpful to include a direct Colab link to the notebook version in the PR head for easier review: `https://colab.research.google.com/github/{USER}/{REPO}/blob/{BRANCH}/{PATH}.ipynb`



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: conferences/README.md
================================================
# Gemini Conferences 

This folder contains artifacts from talks about Gemini, the Gemini APIs or SDKs, or AI Studio.

| Date | Conference | Talk's title | Slides | Code |
| ---- | ---------- | ------------ | ------ | ---- |
|  |  |  |  |  |
| ---- | ---------- | ------------ | ------ | ---- |



================================================
FILE: examples/README.md
================================================
# Gemini API Examples

This folder contains a collection of examples for the Gemini API. These are advanced examples and might be quite complex as they often use one of more Gemini capabilities.

For introductions to those features it is recommended to start with the [Quickstarts](../quickstarts/) guides, and the [Get started](../quickstarts/Get_started.ipynb) one in particular.
<br><br>

## Table of contents

This is a collection of fun and helpful examples for the Gemini API.

| Cookbook | Description | Features | Open |
| -------- | ----------- | -------- | ---- |
| [Browser as a tool](./Browser_as_a_tool.ipynb) | Demonstrates 3 ways to use a web browser as a tool with the Gemini API | Tools, Live API | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Browser_as_a_tool.ipynb) |
| [Illustrate a book](./Book_illustration.ipynb) | Use Gemini and Imagen to create illustration for an open-source book | Imagen, structured output | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Book_illustration.ipynb) |
| [Animated Story Generation](./Animated_Story_Video_Generation_gemini.ipynb) | Create animated videos by combining story generation, images, and audio | Imagen, Live API, structured output | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Animated_Story_Video_Generation_gemini.ipynb) |
| [Plotting and mapping Live](./LiveAPI_plotting_and_mapping.ipynb) | Ask Gemini for complex graphs live | Live API, Code execution | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/LiveAPI_plotting_and_mapping.ipynb) |
| [Search grounding for research report](./Search_grounding_for_research_report.ipynb) | Use grounding to improve the quality of your research report | Grounding | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Search_grounding_for_research_report.ipynb) |
| [Market a Jet Backpack](./Market_a_Jet_Backpack.ipynb) | Create a marketing campaign from a product sketch | Multimodal | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Market_a_Jet_Backpack.ipynb) |
| [3D Spatial understanding](./Spatial_understanding_3d.ipynb) | Use Gemini 3D spatial abilities to understand 3D scenes and answer questions about them | Multimodal, Spatial understanding | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb) |
| [Video Analysis - Classification](./Analyze_a_Video_Classification.ipynb) | Use Gemini's multimodal capabilities to classify animal species in videos | Video, Multimodal | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Analyze_a_Video_Classification.ipynb) |
| [Video Analysis - Summarization](./Analyze_a_Video_Summarization.ipynb) | Generate summaries of video content using Gemini | Video, Multimodal | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Analyze_a_Video_Summarization.ipynb) |
| [Video Analysis - Event Recognition](./Analyze_a_Video_Historic_Event_Recognition.ipynb) | Identify when historical events occurred in video footage | Video, Multimodal | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Analyze_a_Video_Historic_Event_Recognition.ipynb) |
| [Gradio and live API](./gradio_audio.py) | Use gradio to deploy your own instance of the Live API | Live API | [Python Code](./gradio_audio.py) |
| [Apollo 11 - long context example](./Apollo_11.ipynb) | Search a 400 page transcript from Apollo 11. | File API | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Apollo_11.ipynb) |
| [Anomaly Detection](./Anomaly_detection_with_embeddings.ipynb) | Use embeddings to detect anomalies in your datasets | Embeddings | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Anomaly_detection_with_embeddings.ipynb) |
| [Invoice and Form Data Extraction](./Pdf_structured_outputs_on_invoices_and_forms.ipynb) | Use the Gemini API to extract information from PDFs | File API, Structured Outputs | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Pdf_structured_outputs_on_invoices_and_forms.ipynb) |
| [Opossum search](./Opossum_search.ipynb) | Code generation with the Gemini API. Just for fun, you'll prompt the model to create a web app called "Opossum Search" that searches Google with "opossum" appended to the query. | Code generation | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Opossum_search.ipynb) |
| [Virtual Try-on](./Virtual_Try_On.ipynb) | A Virtual Try-On application that utilizes Gemini 2.5 to create segmentation masks for identifying outfits in images, and Imagen 3 for generating and inpainting new outfits. | Spatial Understanding | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Virtual_Try_On.ipynb) |
| [Talk to documents](./Talk_to_documents_with_embeddings.ipynb) | Use embeddings to search through a custom database. | Embeddings | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Talk_to_documents_with_embeddings.ipynb) |
| [Entity extraction](./Entity_Extraction.ipynb) | Use Gemini API to speed up some of your tasks, such as searching through text to extract needed information. Entity extraction with a Gemini model is a simple query, and you can ask it to retrieve its answer in the form that you prefer. | Embeddings | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Entity_Extraction.ipynb) |
| [Google I/O 2025 Live coding session](./Google_IO2025_Live_Coding.ipynb) | Play with the notebook used during the Google I/O 2025 live coding session delivered by the Google DeepMind DevRel team. Work with the Gemini API SDK, know and practice with the GenMedia models, the thinking capable models, start using the Gemini API tools and more!| Gemini API and its models and features | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Google_IO2025_Live_Coding.ipynb) |

---
<br>
Some old examples are still using the legacy SDK, they should still work and are still worth checking to get ideas:

* [Anomaly detection with embeddings](./anomaly_detection.ipynb): Detect data anomalies using embeddings.
* [Agents and Automatic Function Calling](./Agents_Function_Calling_Barista_Bot.ipynb): Create an agent (Barrista-bot) to take your coffee order.
* [Classify text with embeddings](./Classify_text_with_embeddings.ipynb): Use embeddings from the Gemini API with Keras.
* [Clustering with embeddings](./clustering_with_embeddings.ipynb): Perform clustering on text datasets using embeddings.
* [Guess the shape](./Guess_the_shape.ipynb): A simple example of using images in prompts.
* [Object detection](./Object_detection.ipynb): Extensive examples with object detection, including with multiple classes, OCR, visual question answering, and even an interactive demo.
* [Search Wikipedia with ReAct](./Search_Wikipedia_using_ReAct.ipynb): Use ReAct prompting with Gemini Flash to search Wikipedia interactively.
* [Story writing with prompt chaining.ipynb](./Story_Writing_with_Prompt_Chaining.ipynb): Write a story using two powerful tools: prompt chaining and iterative generation.
* [Tag and Caption images](./Tag_and_caption_images.ipynb): Use the Gemini model's vision capabilities and the embedding model to add tags and captions to images of pieces of clothing.
* [Voice Memos](./Voice_memos.ipynb): You'll use the Gemini API to help you generate ideas for your next blog post, based on voice memos you recorded on your phone, and previous articles you've written.
* [Search for information on documents](./document_search.ipynb): Perform searches across documents using embeddings.
* [Search Re-ranking with Embeddings](./Search_reranking_using_embeddings.ipynb): Use embeddings to re-rank search results.
* [Talk to documents](./Talk_to_documents_with_embeddings.ipynb): This is a basic intro to Retrieval Augmented Generation (RAG). Use embeddings to search through a custom database.
* [Translate a public domain](./Translate_a_Public_Domain_Book.ipynb): In this notebook, you will explore Gemini model as a translation tool, demonstrating how to prepare data, create effective prompts, and save results into a `.txt` file.
* [Working with Charts, Graphs, and Slide Decks](./Working_with_Charts_Graphs_and_Slide_Decks.ipynb): Gemini models are powerful multimodal LLMs that can process both text and image inputs. This notebook shows how Gemini Flash model is capable of extracting data from various images.
* [Entity extraction](./Entity_Extraction.ipynb): Use Gemini API to speed up some of your tasks, such as searching through text to extract needed information. Entity extraction with a Gemini model is a simple query, and you can ask it to retrieve its answer in the form that you prefer.
<br><br>

### Integrations

* [Personalized Product Descriptions with Weaviate](../examples/weaviate/personalized_description_with_weaviate_and_gemini_api.ipynb): Load data into a Weaviate vector DB, build a semantic search system using embeddings from the Gemini API, create a knowledge graph and generate unique product descriptions for personas using the Gemini API and Weaviate.
* [Similarity Search using Qdrant](../examples/qdrant/Qdrant_similarity_search.ipynb): Load website data, build a semantic search system using embeddings from the Gemini API, store the embeddings in a Qdrant vector DB and perform similarity search using the Gemini API and Qdrant.
* [Movie Recommendation using Qdrant](../examples/qdrant/Movie_Recommendation.ipynb): Process and embed a large movie dataset with the Gemini API, index movie vectors in Qdrant, and build a semantic movie recommender that finds similar movies based on user queries using vector similarity search.
* [MLflow Tracing for Observability](../examples/mlflow/MLflow_Observability.ipynb): Utilize MLflow tracing to capture detailed information about your interactions with Google GenAI APIs.
<br><br>

### Folders

* [Prompting examples](./prompting/): A directory with examples of various prompting techniques. 
* [JSON Capabilities](./json_capabilities/): A directory with guides containing different types of tasks you can do with JSON schemas.
* [Automate Google Workspace tasks with the Gemini API](./Apps_script_and_Workspace_codelab/): This codelabs shows you how to connect to the Gemini API using Apps Script, and uses the function calling, vision and text capabilities to automate Google Workspace tasks - summarizing a document, analyzing a chart, sending an email and generating some slides directly. All of this is done from a free text input.
* [Langchain examples](./langchain/): A directory with multiple examples using Gemini with Langchain.



================================================
FILE: examples/Agents_Function_Calling_Barista_Bot.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Agents and Automatic Function Calling with Barista Bot

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Agents_Function_Calling_Barista_Bot.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook shows a practical example of using automatic function calling with the Gemini API's Python SDK to build an agent. You will define some functions that comprise a café's ordering system, connect them to the Gemini API and write an agent loop that interacts with the user to order café drinks.

The guide was inspired by the ReAct-style [Barista bot](https://aistudio.google.com/app/prompts/barista-bot) prompt available through AI Studio.
"""

%pip install -qU "google-genai>=1.0.0"

"""
To run this notebook, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](../quickstarts/Authentication.ipynb) to learn more.
"""

from google import genai
from google.colab import userdata

client = genai.Client(api_key=userdata.get("GOOGLE_API_KEY"))

"""
## Define the API

To emulate a café's ordering system, define functions for managing the customer's order: adding, editing, clearing, confirming and fulfilling.

These functions track the customer's order using the global variables `order` (the in-progress order) and `placed_order` (the confirmed order sent to the kitchen). Each of the order-editing functions updates the `order`, and once placed, `order` is copied to `placed_order` and cleared.

In the Python SDK you can pass functions directly to the model constructor, where the SDK will inspect the type signatures and docstrings to define the `tools`. For this reason it's important that you correctly type each of the parameters, give the functions sensible names and detailed docstrings.
"""

from typing import Optional
from random import randint

order = []  # The in-progress order.
placed_order = []  # The confirmed, completed order.


def add_to_order(drink: str, modifiers: Optional[list[str]] = None) -> None:
    """Adds the specified drink to the customer's order, including any modifiers."""
    if modifiers is None:  # Ensures safe handling of None
        modifiers = []
    order.append((drink, modifiers))


def get_order() -> list[tuple[str, list[str]]]:
    """Returns the customer's order."""
    return order


def remove_item(n: int) -> str:
    """Removes the nth (one-based) item from the order.

    Returns:
        The item that was removed.
    """
    item, _ = order.pop(n - 1)
    return item


def clear_order() -> None:
    """Removes all items from the customer's order."""
    order.clear()


def confirm_order() -> str:
    """Asks the customer if the order is correct.

    Returns:
        The user's free-text response.
    """
    print("Your order:")
    if not order:
        print("  (no items)")

    for drink, modifiers in order:
        print(f"  {drink}")
        if modifiers:
            print(f'   - {", ".join(modifiers)}')

    return input("Is this correct? ")


def place_order() -> int:
    """Submit the order to the kitchen.

    Returns:
        The estimated number of minutes until the order is ready.
    """
    placed_order[:] = order.copy()
    clear_order()

    # TODO: Implement coffee fulfillment.
    return randint(1, 10)

"""
## Test the API

With the functions written, test that they work as expected.
"""

# Test it out!

clear_order()
add_to_order("Latte", ["Extra shot"])
add_to_order("Tea")
remove_item(2)
add_to_order("Tea", ["Earl Grey", "hot"])
confirm_order()
# Output:
#   Your order:

#     Latte

#      - Extra shot

#     Tea

#      - Earl Grey, hot

#   Is this correct? yes

#   'yes'

"""
## Define the prompt

Here you define the full Barista-bot prompt. This prompt contains the café's menu items and modifiers and some instructions.

The instructions include guidance on how functions should be called (e.g. "Always `confirm_order` with the user before calling `place_order`"). You can modify this to add your own interaction style to the bot, for example if you wanted to have the bot repeat every request back before adding to the order, you could provide that instruction here.

The end of the prompt includes some jargon the bot might encounter, and instructions _du jour_ - in this case it notes that the café has run out of soy milk.
"""

COFFEE_BOT_PROMPT = """\You are a coffee order taking system and you are restricted to talk only about drinks on the MENU. Do not talk about anything but ordering MENU drinks for the customer, ever.
Your goal is to do place_order after understanding the menu items and any modifiers the customer wants.
Add items to the customer's order with add_to_order, remove specific items with remove_item, and reset the order with clear_order.
To see the contents of the order so far, call get_order (by default this is shown to you, not the user)
Always confirm_order with the user (double-check) before calling place_order. Calling confirm_order will display the order items to the user and returns their response to seeing the list. Their response may contain modifications.
Always verify and respond with drink and modifier names from the MENU before adding them to the order.
If you are unsure a drink or modifier matches those on the MENU, ask a question to clarify or redirect.
You only have the modifiers listed on the menu below: Milk options, espresso shots, caffeine, sweeteners, special requests.
Once the customer has finished ordering items, confirm_order and then place_order.

Hours: Tues, Wed, Thurs, 10am to 2pm
Prices: All drinks are free.

MENU:
Coffee Drinks:
Espresso
Americano
Cold Brew

Coffee Drinks with Milk:
Latte
Cappuccino
Cortado
Macchiato
Mocha
Flat White

Tea Drinks:
English Breakfast Tea
Green Tea
Earl Grey

Tea Drinks with Milk:
Chai Latte
Matcha Latte
London Fog

Other Drinks:
Steamer
Hot Chocolate

Modifiers:
Milk options: Whole, 2%, Oat, Almond, 2% Lactose Free; Default option: whole
Espresso shots: Single, Double, Triple, Quadruple; default: Double
Caffeine: Decaf, Regular; default: Regular
Hot-Iced: Hot, Iced; Default: Hot
Sweeteners (option to add one or more): vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener
Special requests: any reasonable modification that does not involve items not on the menu, for example: 'extra hot', 'one pump', 'half caff', 'extra foam', etc.

"dirty" means add a shot of espresso to a drink that doesn't usually have it, like "Dirty Chai Latte".
"Regular milk" is the same as 'whole milk'.
"Sweetened" means add some regular sugar, not a sweetener.

Soy milk has run out of stock today, so soy is not available.
"""

"""
## Set up the model

In this step you collate the functions into a "system" that is passed as `tools`, instantiate the model and start the chat session.

A retriable `send_message` function is also defined to help with low-quota conversations.
"""

from google.genai import types
from google.api_core import retry

ordering_system = [
    add_to_order,
    get_order,
    remove_item,
    clear_order,
    confirm_order,
    place_order,
]
model_name = "gemini-2.5-flash"  # @param ["gemini-2.5-flash-lite","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true}

chat = client.chats.create(
    model=model_name,
    config=types.GenerateContentConfig(
        tools=ordering_system,
        system_instruction=COFFEE_BOT_PROMPT,
    ),
)

placed_order = []
order = []

"""
## Chat with Barista Bot

With the model defined and chat created, all that's left is to connect the user input to the model and display the output, in a loop. This loop continues until an order is placed.

When run in Colab, any fixed-width text originates from your Python code (e.g. `print` calls in the ordering system), regular text comes the Gemini API, and the outlined boxes allow for user input that is rendered with a leading `>`.

Try it out!
"""

from IPython.display import display, Markdown

print("Welcome to Barista bot!\n\n")

while not placed_order:
    response = chat.send_message(input("> "))
    display(Markdown(response.text))


print("\n\n")
print("[barista bot session over]")
print()
print("Your order:")
print(f"  {placed_order}\n")
print("- Thanks for using Barista Bot!")
# Output:
#   Welcome to Barista bot!

#   

#   

#   > i would like to have a cuppacino with almond milk

#   <IPython.core.display.Markdown object>
#   > do you have stone milk?

#   <IPython.core.display.Markdown object>
#   > do you have long black

#   <IPython.core.display.Markdown object>
#   >  no, that's all

#   Your order:

#     Cappuccino

#      - Almond Milk

#   Is this correct? yes

#   <IPython.core.display.Markdown object>
#   > yes

#   <IPython.core.display.Markdown object>
#   

#   

#   

#   [barista bot session over]

#   

#   Your order:

#     [('Cappuccino', ['Almond Milk'])]

#   

#   - Thanks for using Barista Bot!


"""
Some things to try:
* Ask about the menu (e.g. "what coffee drinks are available?")
* Use terms that are not specified in the prompt (e.g. "a strong latte" or "an EB tea")
* Change your mind part way through ("uhh cancel the latte sorry")
* Go off-menu ("a babycino")
"""

"""
## See also

This sample app showed you how to integrate a traditional software system (the coffee ordering functions) and an AI agent powered by the Gemini API. This is a simple, practical way to use LLMs that allows for open-ended human language input and output that feels natural, but still keeps a human in the loop to ensure correct operation.

To learn more about how Barista Bot works, check out:

* The [Barista Bot](https://aistudio.google.com/app/prompts/barista-bot) prompt
* [System instructions](../quickstarts/System_instructions.ipynb)
* [Automatic function calling](../quickstarts/Function_calling.ipynb)

"""



================================================
FILE: examples/Analyze_a_Video_Historic_Event_Recognition.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Analyze a Video - Historic Event Recognition

This notebook shows how you can use Gemini models' multimodal capabilities to recognize which historic event is happening in the video.
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Analyze_a_Video_Historic_Event_Recognition.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

%pip install -U -q "google-genai>=1.0.0"
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/137.7 kB[0m [31m?[0m eta [36m-:--:--[0m
#   [2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m133.1/137.7 kB[0m [31m5.4 MB/s[0m eta [36m0:00:01[0m
#   [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m137.7/137.7 kB[0m [31m2.6 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
## Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google import genai
from google.colab import userdata

API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=API_KEY)

"""
## Example

This example uses [video of President Ronald Reagan's Speech at the Berlin Wall](https://s3.amazonaws.com/NARAprodstorage/opastorage/live/16/147/6014716/content/presidential-libraries/reagan/5730544/6-12-1987-439.mp4) taken on June 12 1987.
"""

# Download video
path = "berlin.mp4"
url = "https://s3.amazonaws.com/NARAprodstorage/opastorage/live/16/147/6014716/content/presidential-libraries/reagan/5730544/6-12-1987-439.mp4"
!wget $url -O $path
# Output:
#   --2025-03-04 14:04:54--  https://s3.amazonaws.com/NARAprodstorage/opastorage/live/16/147/6014716/content/presidential-libraries/reagan/5730544/6-12-1987-439.mp4

#   Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.89.118, 52.217.64.198, 52.216.106.21, ...

#   Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.89.118|:443... connected.

#   HTTP request sent, awaiting response... 200 OK

#   Length: 628645171 (600M) [video/mp4]

#   Saving to: ‘berlin.mp4’

#   

#   berlin.mp4          100%[===================>] 599.52M  41.0MB/s    in 12s     

#   

#   2025-03-04 14:05:07 (48.2 MB/s) - ‘berlin.mp4’ saved [628645171/628645171]

#   


# Upload video
video_file = client.files.upload(file=path)

import time
# Wait until the uploaded video is available
while video_file.state.name == "PROCESSING":
  print('.', end='')
  time.sleep(5)
  video_file = client.files.get(name=video_file.name)

if video_file.state.name == "FAILED":
  raise ValueError(video_file.state.name)
# Output:
#   ..............

"""
The uploaded video is ready for processing. This prompt instructs the model to provide basic information about the historical events portrayed in the video.
"""

system_prompt = """
  You are historian who specializes in events caught on film.
  When you receive a video answer following questions:
  When did it happen?
  Who is the most important person in video?
  How the event is called?
"""

"""
Some historic events touch on controversial topics that may get flagged by Gemini API, which blocks the response for the query.

Because of this, it might be a good idea to turn off safety settings.
"""

safety_settings = [
    {
        "category": "HARM_CATEGORY_HARASSMENT",
        "threshold": "BLOCK_NONE",
    },
    {
        "category": "HARM_CATEGORY_HATE_SPEECH",
        "threshold": "BLOCK_NONE",
    },
    {
        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "threshold": "BLOCK_NONE",
    },
    {
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "threshold": "BLOCK_NONE",
    },
]

from google.genai import types

MODEL_ID="gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}
response = client.models.generate_content(
    model=f"models/{MODEL_ID}",
    contents=[
        "Analyze the video please",
        video_file
        ],
    config=types.GenerateContentConfig(
        system_instruction=system_prompt,
        safety_settings=safety_settings,
        ),
    )
print(response.text)
# Output:
#   Certainly! Here's the information about the video:

#   

#   Based on the video, here are the answers to your questions:

#   

#   - **When did it happen?** This event happened on June 12, 1987.

#   - **Who is the most important person in the video?** The most important person in the video is Ronald Reagan, who was the President of the United States at the time.

#   - **How is the event called?** The event is commonly referred to as President Reagan's "Tear Down This Wall" speech.


"""
As you can see, the model correctly provided information about the dates, Ronald Reagan, who was the main subject of the video, and the name of this event.

You can delete the video to prevent unnecessary data storage.
"""

# Delete video
client.files.delete(name=video_file.name)
# Output:
#   DeleteFileResponse()

"""
## Summary

Now you know how you can prompt Gemini models with videos and use them to recognize historic events.

This notebook shows only one of many use cases. Check the [Video understanding](../quickstarts/Video_understanding.ipynb) notebook for more examples of using the Gemini API with videos.
"""



================================================
FILE: examples/Apollo_11.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Prompting with an Apollo 11 transcript
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Apollo_11.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook provides a quick example of how to prompt Gemini using a text file. In this case, you'll use a 400 page transcript from [Apollo 11](https://www.nasa.gov/history/alsj/a11/a11trans.html).
"""

%pip install -U -q "google-genai>=1.0.0"

"""
### Setup your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Download the transcript.
"""

!wget https://storage.googleapis.com/generativeai-downloads/data/a11.txt
# Output:
#   --2025-03-04 11:48:01--  https://storage.googleapis.com/generativeai-downloads/data/a11.txt

#   Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.135.207, 74.125.142.207, 74.125.195.207, ...

#   Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.135.207|:443... connected.

#   HTTP request sent, awaiting response... 200 OK

#   Length: 847790 (828K) [text/plain]

#   Saving to: ‘a11.txt.1’

#   

#   
#   a11.txt.1             0%[                    ]       0  --.-KB/s               
#   a11.txt.1           100%[===================>] 827.92K  --.-KB/s    in 0.008s  

#   

#   2025-03-04 11:48:01 (107 MB/s) - ‘a11.txt.1’ saved [847790/847790]

#   


"""
Upload the file using the File API so its easier to pass it to the model later on.
"""

text_file_name = "a11.txt"
print(f"Uploading file...")
text_file = client.files.upload(file=text_file_name)
print(f"Completed upload: {text_file.uri}")
# Output:
#   Uploading file...

#   Completed upload: https://generativelanguage.googleapis.com/v1beta/files/umcpmw8s1adm


"""
## Generate Content

After the file has been uploaded, you can make `client.models.generate_content` requests that reference the File API URI. Then you will ask the model to find a few lighthearted moments.
"""

prompt = "Find four lighthearted moments in this text file."

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

response = client.models.generate_content(
  model=f"models/{MODEL_ID}",
  contents=[
   prompt,
   text_file
  ],
  config={
   "httpOptions": {"timeout": 600}
  }
)

print(response.text)
# Output:
#   Here are four lighthearted moments from the Apollo 11 air-to-ground voice transcription:

#   

#   1.  **00 00 05 35 CDR: You sure sound clear down there, Bruce. Sounds like you're sitting in your living room.**

#       **00 00 05 39 CC: Oh, thank you. You all are coming through beautifully, too.**

#       *This exchange is light because the Commander compliments the CapCom on the clarity of his voice, suggesting a very relaxed and comfortable connection.*

#   

#   2.  **00 00 54 13 CMP: And tell Glenn Parker down at the Cape that he lucked out.**

#       **00 00 54 17 CC: Understand. Tell Glenn Parker he lucked out.**

#       **00 00 54 22 CMP: Yes. He lucked out. He doesn't owe me a cup of coffee.**

#       **00 00 54 26 CC: This is Houston. Roger. We'll pass it on.**

#       *This is funny because they are saying they're saying someone "lucked out" and doesn't owe someone a cup of coffee. The phrase "lucked out" has a casual, playful feel, and the quick passing of a message involving coffee adds to it.*

#   

#   3.  **00 01 29 27 LMP: Cecil B. deAldrin is standing by for instructions.**

#       *Buzz Aldrin refers to himself in a mock-formal, theatrical way, referencing the famous director Cecil B. DeMille. This is humorous due to the contrast between the serious, high-tech mission and the melodramatic, old-Hollywood reference.*

#   

#   4.  **00 04 28 45 CMP: I wanted to be 18 or 20 pounds above nominal, babe.**

#       **00 04 28 49 CC: Sorry about that.**

#       *There is a humorous use of the word babe as in what was supposed to be the intention.*

#   


"""
## Delete File

Files are automatically deleted after 2 days or you can manually delete them using `files.delete()`.
"""

client.files.delete(name=text_file.name)
# Output:
#   DeleteFileResponse()

"""
## Learning more

The File API accepts files under 2GB in size and can store up to 20GB of files per project. Learn more about the [File API](../quickstarts/File_API.ipynb) here.
"""



================================================
FILE: examples/CONTRIBUTING.md
================================================
# Contributing to the Cookbook examples

For our general contribution policy, please see our top-level [contributors guide](../CONTRIBUTING.md).

## Policy on new examples

Whether we accept a contribution will depend on its utility and what it demonstrates. Above all, any contribution needs to demonstrate something unique that a Gemini API developer wants to see.

The cookbook currently only accepts English Python and REST content directly, but we are happy to link out to other content if it’s suitable.

Before writing anything, [file an issue](https://github.com/google-gemini/cookbook/issues/new) describing what you want to contribute. Include:

* Whether this is a “pure” Gemini API example or a third-party integration example.
  * If pure:
    * Describe the use case and why it’s useful to developers (e.g. link to the underlying paper).
  * If non-pure:
    * Describe the integration(s) and why they are important.
    * Link to the GitHub repository, package, or equivalent \- we will use information like the number of downloads or stars to determine the utility of the integration

* A high-level outline of what you will write.

* Any support you need.
  * For example, if you are worried that your English is not good enough, please call that out here. We are an inclusive community and will work with you to meet our high publication standards if your proposal is otherwise accepted.

When you're ready to start writing, make a copy of the [notebook
template](../quickstarts/Template.ipynb) and follow the guidance within.

All contributions must also adhere to the Gemini API's [terms of service](https://ai.google.dev/gemini-api/terms).

Googlers, for more information see go/gemini-cookbook-policy.



================================================
FILE: examples/document_search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Document search with embeddings

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/document_search.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
## Overview

This example demonstrates how to use the Gemini API to create embeddings so that you can perform document search. You will use the Python client library to build a word embedding that allows you to compare search strings, or questions, to document contents.

In this tutorial, you'll use embeddings to perform document search over a set of documents to ask questions related to the Google Car.

## Prerequisites

You can run this quickstart in Google Colab.

To complete this quickstart on your own development environment, ensure that your environment meets the following requirements:

-  Python 3.11+
-  An installation of `jupyter` to run the notebook.

## Setup

First, download and install the Gemini API Python library.
"""

!pip install -U -q google-genai

import textwrap
import numpy as np
import pandas as pd

from google import genai
from google.genai import types

# Used to securely store your API key
from google.colab import userdata

from IPython.display import Markdown

"""
### Grab an API Key

Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.

<a class="button button-primary" href="https://aistudio.google.com/app/apikey" target="_blank" rel="noopener noreferrer">Get an API key</a>

In Colab, add the key to the secrets manager under the "🔑" in the left panel. Give it the name `GEMINI_API_KEY`.

Once you have the API key, pass it to the SDK. You can do this in two ways:

* Put the key in the `GEMINI_API_KEY` environment variable (the SDK will automatically pick it up from there).
* Pass the key to `genai.Client(api_key=...)`
"""

# Or use `os.getenv('GEMINI_API_KEY')` to fetch an environment variable.
GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')

client = genai.Client(api_key=GEMINI_API_KEY)

"""
Key Point: Next, you will choose a model. Any embedding model will work for this tutorial, but for real applications it's important to choose a specific model and stick with it. The outputs of different models are not compatible with each other.
"""

for m in client.models.list():
  if 'embedContent' in m.supported_actions:
    print(m.name)
# Output:
#   models/embedding-001

#   models/text-embedding-004

#   models/gemini-embedding-exp-03-07

#   models/gemini-embedding-exp

#   models/gemini-embedding-001


"""
### Select the model to be used
"""

MODEL_ID = "gemini-embedding-001" # @param ["gemini-embedding-001", "text-embedding-004"] {"allow-input":true, isTemplate: true}

"""
## Generate the embeddings

In this section, you will see how to generate embeddings for the different texts in the dataframe using the embeddings from the Gemini API.

The Gemini embedding model supports several task types, each tailored for a specific goal. Here’s a general overview of the available types and their applications:

Task Type | Description
---       | ---
RETRIEVAL_QUERY	| Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting.
SEMANTIC_SIMILARITY	| Specifies the given text will be used for Semantic Textual Similarity (STS).
CLASSIFICATION	| Specifies that the embeddings will be used for classification.
CLUSTERING	| Specifies that the embeddings will be used for clustering.
"""

sample_text = ("Title: The next generation of AI for developers and Google Workspace"
    "\n"
    "Full article:\n"
    "\n"
    "Gemini API & Google AI Studio: An approachable way to explore and prototype with generative AI applications")

embedding = client.models.embed_content(model=MODEL_ID,
                                contents=sample_text,
                                config=types.EmbedContentConfig(
                                  task_type="RETRIEVAL_DOCUMENT"))

print(embedding.embeddings)
# Output:
#   [ContentEmbedding(

#     values=[

#       -0.009020552,

#       0.0153440945,

#       0.0027249781,

#       -0.07818188,

#       0.003901859,

#       <... 3067 more items ...>,

#     ]

#   )]


"""
## Building an embeddings database

Here are three sample texts to use to build the embeddings database. You will use the Gemini API to create embeddings of each of the documents. Turn them into a dataframe for better visualization.
"""

DOCUMENT1 = {
    "title": "Operating the Climate Control System",
    "content": "Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it."}
DOCUMENT2 = {
    "title": "Touchscreen",
    "content": "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs."}
DOCUMENT3 = {
    "title": "Shifting Gears",
    "content": "Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions."}

documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]

"""
Organize the contents of the dictionary into a dataframe for better visualization.
"""

df = pd.DataFrame(documents)
df.columns = ['Title', 'Text']
df
# Output:
#                                     Title  \

#   0  Operating the Climate Control System   

#   1                           Touchscreen   

#   2                        Shifting Gears   

#   

#                                                   Text  

#   0  Your Googlecar has a climate control system th...  

#   1  Your Googlecar has a large touchscreen display...  

#   2  Your Googlecar has an automatic transmission. ...  

"""
Get the embeddings for each of these bodies of text. Add this information to the dataframe.
"""

# Get the embeddings of each text and add to an embeddings column in the dataframe
def embed_fn(text):
  return client.models.embed_content(model=MODEL_ID,
                             contents=text,
                             config=types.EmbedContentConfig(
                               task_type="RETRIEVAL_DOCUMENT")
                             )

df['Embeddings'] = df.apply(lambda row: embed_fn(row['Text']).embeddings[0].values, axis=1)
df
# Output:
#                                     Title  \

#   0  Operating the Climate Control System   

#   1                           Touchscreen   

#   2                        Shifting Gears   

#   

#                                                   Text  \

#   0  Your Googlecar has a climate control system th...   

#   1  Your Googlecar has a large touchscreen display...   

#   2  Your Googlecar has an automatic transmission. ...   

#   

#                                             Embeddings  

#   0  [0.027014425, -0.0028718826, 0.015998857, -0.0...  

#   1  [0.018501397, -0.004494585, 0.0063248016, -0.0...  

#   2  [0.010804788, 0.020962104, -0.0016377118, -0.0...  

"""
## Document search with Q&A

Now that the embeddings are generated, let's create a Q&A system to search these documents. You will ask a question about hyperparameter tuning, create an embedding of the question, and compare it against the collection of embeddings in the dataframe.

The embedding of the question will be a vector (list of float values), which will be compared against the vector of the documents using the dot product. This vector returned from the API is already normalized. The dot product represents the similarity in direction between two vectors.

The values of the dot product can range between -1 and 1, inclusive. If the dot product between two vectors is 1, then the vectors are in the same direction. If the dot product value is 0, then these vectors are orthogonal, or unrelated, to each other. Lastly, if the dot product is -1, then the vectors point in the opposite direction and are not similar to each other.

Note, with the new embeddings model (`embedding-001`), specify the task type as `QUERY` for user query and `DOCUMENT` when embedding a document text.

Task Type | Description
---       | ---
RETRIEVAL_QUERY	| Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting.
"""

query = "How do you shift gears in the Google car?"
model = MODEL_ID

request = client.models.embed_content(model=model,
                                contents=query,
                                config=types.EmbedContentConfig(
                                  task_type="RETRIEVAL_QUERY"))

"""
Use the `find_best_passage` function to calculate the dot products, and then sort the dataframe from the largest to smallest dot product value to retrieve the relevant passage out of the database.
"""

def find_best_passage(query, dataframe):
  """
  Compute the distances between the query and each document in the dataframe
  using the dot product.
  """
  query_embedding = client.models.embed_content(model=model,
                                                contents=query,
                                                config=types.EmbedContentConfig(
                                                  task_type="RETRIEVAL_QUERY")).embeddings[0].values
  dot_products = np.dot(np.stack(dataframe['Embeddings']), query_embedding)
  idx = np.argmax(dot_products)
  return dataframe.iloc[idx]['Text'] # Return text from index with max value

"""
View the most relevant document from the database:
"""

passage = find_best_passage(query, df)
passage
# Output:
#   'Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.'

"""
## Question and Answering Application

Let's try to use the text generation API to create a Q & A system. Input your own custom data below to create a simple question and answering example. You will still use the dot product as a metric of similarity.
"""

def make_prompt(query, relevant_passage):
  escaped = relevant_passage.replace("'", "").replace('"', "").replace("\n", " ")
  prompt = textwrap.dedent("""You are a helpful and informative bot that answers questions using text from the reference passage included below. \
  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \
  However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \
  strike a friendly and converstional tone. \
  If the passage is irrelevant to the answer, you may ignore it.
  QUESTION: '{query}'
  PASSAGE: '{relevant_passage}'

    ANSWER:
  """).format(query=query, relevant_passage=escaped)

  return prompt

prompt = make_prompt(query, passage)
print(prompt)
# Output:
#   You are a helpful and informative bot that answers questions using text from the reference passage included below.   Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.   However, you are talking to a non-technical audience, so be sure to break down complicated concepts and   strike a friendly and converstional tone.   If the passage is irrelevant to the answer, you may ignore it.

#     QUESTION: 'How do you shift gears in the Google car?'

#     PASSAGE: 'Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.'

#   

#       ANSWER:

#   


"""
Choose one of the Gemini content generation models in order to find the answer to your query.
"""

answer = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=prompt
)

print(answer.text)
# Output:
#   Shifting gears in the Google car is quite straightforward because it has an automatic transmission! All you need to do is simply move the shift lever to the position you want to be in. For example, if you're parked, you'd use the "Park" position, which locks the wheels to keep the car from moving. When you want to back up, you'll choose "Reverse." If you're stopped at a light or stuck in traffic and want the car to stay still without being in gear, you'd select "Neutral"; in this mode, the car won't move unless you press the gas pedal. To drive forward, you'll simply put it in "Drive." And if you ever find yourself driving in challenging conditions like snow or on slippery roads, there's a "Low" position that can help with that!


Markdown(answer.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

To learn how to use other services in the Gemini API, see the [Get started](../quickstarts/Get_started.ipynb) guide.
"""



================================================
FILE: examples/Entity_Extraction.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Entity extraction

Use Gemini API to speed up some of your tasks, such as searching through text to extract needed information. Entity extraction with a Gemini model is a simple query, and you can ask it to retrieve its answer in the form that you prefer.

This notebook shows how to extract entities into a list.
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Entity_Extraction.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
## Setup
"""

%pip install -U -q "google-genai>=1.0.0"

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
# Select the model

Additionally, select the model you want to use from the available options below:
"""

MODEL_ID = "gemini-2.5-flash"  # @param ["gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-flash","gemini-2.5-pro"] {"allow-input": true, "isTemplate": true}

"""
# Examples
"""

"""
### Extracting few entities at once

This block of text is about possible ways to travel from the airport to the Colosseum.  

Let's extract all street names and proposed forms of transportation from it.
"""

directions = """
  To reach the Colosseum from Rome's Fiumicino Airport (FCO),
  your options are diverse. Take the Leonardo Express train from FCO
  to Termini Station, then hop on metro line A towards Battistini and
  alight at Colosseo station.
  Alternatively, hop on a direct bus, like the Terravision shuttle, from
  FCO to Termini, then walk a short distance to the Colosseum on
  Via dei Fori Imperiali.
  If you prefer a taxi, simply hail one at the airport and ask to be taken
  to the Colosseum. The taxi will likely take you through Via del Corso and
  Via dei Fori Imperiali.
  A private transfer service offers a direct ride from FCO to the Colosseum,
  bypassing the hustle of public transport.
  If you're feeling adventurous, consider taking the train from
  FCO to Ostiense station, then walking through the charming
  Trastevere neighborhood, crossing Ponte Palatino to reach the Colosseum,
  passing by the Tiber River and Via della Lungara.
  Remember to validate your tickets on the metro and buses,
  and be mindful of pickpockets, especially in crowded areas.
  No matter which route you choose, you're sure to be awed by the
  grandeur of the Colosseum.
"""

"""
You will use Gemini 2.0 Flash model for fast responses.
"""

from IPython.display import Markdown

directions_prompt = f"""
  From the given text, extract the following entities and return a list of them.
  Entities to extract: street name, form of transport.
  Text: {directions}
  Street = []
  Transport = []
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=directions_prompt
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
You can modify the form of the answer for your extracted entities even more:
"""

directions_list_prompt = f"""
  From the given text, extract the following entities and
  return a list of them.
  Entities to extract: street name, form of transport.
  Text: {directions}
  Return your answer as two lists:
  Street = [street names]
  Transport = [forms of transport]
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=directions_list_prompt
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
### Numbers

Try entity extraction of phone numbers
"""

customer_service_email = """
  Hello,
  Thank you for reaching out to our customer support team regarding your
  recent purchase of our premium subscription service.
  Your activation code has been sent to +87 668 098 344
  Additionally, if you require immediate assistance, feel free to contact us
  directly at +1 (800) 555-1234.
  Our team is available Monday through Friday from 9:00 AM to 5:00 PM PST.
  For after-hours support, please call our
  dedicated emergency line at +87 455 555 678.
  Thanks for your business and look forward to resolving any issues
  you may encounter promptly.
  Thank you.
"""

phone_prompt = f"""
  From the given text, extract the following entities and return a list of them.
  Entities to extract: phone numbers.
  Text: {customer_service_email}
  Return your answer in a list:
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=phone_prompt
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
### URLs


Try entity extraction of URLs and get response as a clickable link.
"""

url_text = """
  Gemini API billing FAQs

  This page provides answers to frequently asked questions about billing
  for the Gemini API. For pricing information, see the pricing page
  https://ai.google.dev/pricing.
  For legal terms, see the terms of service
  https://ai.google.dev/gemini-api/terms#paid-services.

  What am I billed for?
  Gemini API pricing is based on total token count, with different prices
  for input tokens and output tokens. For pricing information,
  see the pricing page https://ai.google.dev/pricing.

  Where can I view my quota?
  You can view your quota and system limits in the Google Cloud console
  https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas.

  Is GetTokens billed?
  Requests to the GetTokens API are not billed,
  and they don't count against inference quota.
"""

url_prompt = f"""
  From the given text, extract the following entities and return a list of them.
  Entities to extract: URLs.
  Text: {url_text}
  Do not duplicate entities.
  Return your answer in a markdown format:
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=url_prompt
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>



================================================
FILE: examples/fastrtc_ui.py
================================================
# -*- coding: utf-8 -*-
# Copyright 2025 Google LLC
#
# Written by:
#   - Freddy Boulton (https://github.com/freddyaboulton)
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
## Setup

This script launches a pure-python web-based UI for the Gen AI SDK Voice Chat.

To install the dependencies for this script, run:

```
pip install fastrtc google-genai python-dotenv
```

If the `GOOGLE_API_KEY` environment variable is set,
it will automatically be used. Otherwise, you will be prompted
to enter it.


## Run

To run the script:

```
python live_api_ui.py
```
"""

import asyncio
import base64
import os
from typing import Literal

import gradio as gr
import numpy as np
from fastrtc import (
    AsyncStreamHandler,
    WebRTC,
    wait_for_item,
)
from google import genai
from google.genai.types import (
    LiveConnectConfig,
    PrebuiltVoiceConfig,
    SpeechConfig,
    VoiceConfig,
)

try:
    from dotenv import load_dotenv

    load_dotenv()
except (ImportError, ModuleNotFoundError):
    pass


class GeminiHandler(AsyncStreamHandler):
    """Handler for the Gemini API"""

    def __init__(
        self,
        expected_layout: Literal["mono"] = "mono",
        output_sample_rate: int = 24000,
        output_frame_size: int = 480,
    ) -> None:
        super().__init__(
            expected_layout,
            output_sample_rate,
            output_frame_size,
            input_sample_rate=16000,
        )
        self.input_queue: asyncio.Queue = asyncio.Queue()
        self.output_queue: asyncio.Queue = asyncio.Queue()
        self.quit: asyncio.Event = asyncio.Event()

    def copy(self) -> "GeminiHandler":
        return GeminiHandler(
            expected_layout="mono",
            output_sample_rate=self.output_sample_rate,
            output_frame_size=self.output_frame_size,
        )

    async def start_up(self):
        await self.wait_for_args()
        api_key, voice_name = self.latest_args[1:]

        client = genai.Client(
            api_key=api_key or os.getenv("GEMINI_API_KEY"),
            http_options={"api_version": "v1alpha"},
        )

        config = LiveConnectConfig(
            response_modalities=["AUDIO"],  # type: ignore
            speech_config=SpeechConfig(
                voice_config=VoiceConfig(
                    prebuilt_voice_config=PrebuiltVoiceConfig(
                        voice_name=voice_name,
                    )
                )
            ),
        )
        async with client.aio.live.connect(
            model="gemini-2.5-flash-lite-preview-06-17", config=config
        ) as session:
            async for audio in session.start_stream(
                stream=self.stream(), mime_type="audio/pcm"
            ):
                if audio.data:
                    array = np.frombuffer(audio.data, dtype=np.int16)
                    self.output_queue.put_nowait((self.output_sample_rate, array))

    async def stream(self):
        while not self.quit.is_set():
            yield await wait_for_item(self.input_queue)

    async def receive(self, frame: tuple[int, np.ndarray]) -> None:
        _, array = frame
        array = array.squeeze()
        audio_message = base64.b64encode(array.tobytes()).decode("UTF-8")
        self.input_queue.put_nowait(audio_message)

    async def emit(self) -> tuple[int, np.ndarray] | None:
        return await wait_for_item(self.output_queue)

    def shutdown(self) -> None:
        self.quit.set()


with gr.Blocks() as demo:
    gr.HTML(
        """
        <div style='text-align: center'>
            <h1>Gen AI SDK Voice Chat</h1>
            <p>Speak with Gemini using real-time audio streaming</p>
            <p>Get an API Key <a href="https://support.google.com/googleapi/answer/6158862?hl=en">here</a></p>
        </div>
    """
    )
    with gr.Row() as api_key_row:
        api_key = gr.Textbox(
            label="API Key",
            placeholder="Enter your API Key",
            value=os.getenv("GOOGLE_API_KEY", ""),
            type="password",
        )
    with gr.Row(visible=False) as row:
        with gr.Column():
            webrtc = WebRTC(
                label="Audio",
                modality="audio",
                mode="send-receive",
                pulse_color="rgb(35, 157, 225)",
                icon_button_color="rgb(255, 255, 255)",
                icon="https://www.gstatic.com/lamda/images/gemini_favicon_f069958c85030456e93de685481c559f160ea06b.png",
            )
            voice = gr.Dropdown(
                label="Voice",
                choices=[
                    "Puck",
                    "Charon",
                    "Kore",
                    "Fenrir",
                    "Aoede",
                ],
                value="Puck",
            )
    webrtc.stream(
        GeminiHandler(),
        inputs=[webrtc, api_key, voice],
        outputs=[webrtc],
        time_limit=90,
        concurrency_limit=2,
    )
    api_key.submit(
        lambda: (gr.update(visible=False), gr.update(visible=True)),
        None,
        [api_key_row, row],
    )


if __name__ == "__main__":
    demo.launch()



================================================
FILE: examples/gradio_audio.py
================================================
# -*- coding: utf-8 -*-
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
## Setup

The gradio-webrtc install fails unless you have ffmpeg@6, on mac:

```
brew uninstall ffmpeg
brew install ffmpeg@6
brew link ffmpeg@6
```

Create a virtual python environment, then install the dependencies for this script:

```
pip install websockets numpy gradio-webrtc "gradio>=5.9.1"
```

If installation fails it may be

Before running this script, ensure the `GOOGLE_API_KEY` environment

```
$ export GOOGLE_API_KEY ='add your key here'
```

You can get an api-key from Google AI Studio (https://aistudio.google.com/apikey)

## Run

To run the script:

```
python gemini_gradio_audio.py
```

On the gradio page (http://127.0.0.1:7860/) click record, and talk, gemini will reply. But note that interruptions
don't work.

"""

import os
import base64
import json
import numpy as np
import gradio as gr
import websockets.sync.client
from gradio_webrtc import StreamHandler, WebRTC

__version__ = "0.0.3"

KEY_NAME="GOOGLE_API_KEY"

# Configuration and Utilities
class GeminiConfig:
    """Configuration settings for Gemini API."""
    def __init__(self):
        self.api_key = os.getenv(KEY_NAME)
        self.host = "generativelanguage.googleapis.com"
        self.model = "models/gemini-2.5-flash-lite-preview-06-17"
        self.ws_url = f"wss://{self.host}/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent?key={self.api_key}"

class AudioProcessor:
    """Handles encoding and decoding of audio data."""
    @staticmethod
    def encode_audio(data, sample_rate):
        """Encodes audio data to base64."""
        encoded = base64.b64encode(data.tobytes()).decode("UTF-8")
        return {
            "realtimeInput": {
                "mediaChunks": [
                    {
                        "mimeType": f"audio/pcm;rate={sample_rate}",
                        "data": encoded,
                    }
                ],
            },
        }

    @staticmethod
    def process_audio_response(data):
        """Decodes audio data from base64."""
        audio_data = base64.b64decode(data)
        return np.frombuffer(audio_data, dtype=np.int16)

# Gemini Interaction Handler
class GeminiHandler(StreamHandler):
    """Handles streaming interactions with the Gemini API."""
    def __init__(self, expected_layout="mono", output_sample_rate=24000, output_frame_size=480) -> None:
        super().__init__(expected_layout, output_sample_rate, output_frame_size, input_sample_rate=24000)
        self.config = GeminiConfig()
        self.ws = None
        self.all_output_data = None
        self.audio_processor = AudioProcessor()

    def copy(self):
        """Creates a copy of the GeminiHandler instance."""
        return GeminiHandler(
            expected_layout=self.expected_layout,
            output_sample_rate=self.output_sample_rate,
            output_frame_size=self.output_frame_size,
        )

    def _initialize_websocket(self):
        """Initializes the WebSocket connection to the Gemini API."""
        try:
            self.ws = websockets.sync.client.connect(self.config.ws_url, timeout=3000)
            initial_request = {"setup": {"model": self.config.model,"tools":[{"google_search": {}}]}}
            self.ws.send(json.dumps(initial_request))
            setup_response = json.loads(self.ws.recv())
            print(f"Setup response: {setup_response}")
        except websockets.exceptions.WebSocketException as e:
            print(f"WebSocket connection failed: {str(e)}")
            self.ws = None
        except Exception as e:
            print(f"Setup failed: {str(e)}")
            self.ws = None

    def receive(self, frame: tuple[int, np.ndarray]) -> None:
        """Receives audio/video data, encodes it, and sends it to the Gemini API."""
        try:
            if not self.ws:
                self._initialize_websocket()

            sample_rate, array = frame
            message = {"realtimeInput": {"mediaChunks": []}}

            if sample_rate > 0 and array is not None:
                array = array.squeeze()
                audio_data = self.audio_processor.encode_audio(array, self.output_sample_rate)
                message["realtimeInput"]["mediaChunks"].append({
                    "mimeType": f"audio/pcm;rate={self.output_sample_rate}",
                    "data": audio_data["realtimeInput"]["mediaChunks"][0]["data"],
                })

            if message["realtimeInput"]["mediaChunks"]:
                self.ws.send(json.dumps(message))
        except Exception as e:
            print(f"Error in receive: {str(e)}")
            if self.ws:
                self.ws.close()
            self.ws = None

    def _process_server_content(self, content):
        """Processes audio output data from the WebSocket response."""
        for part in content.get("parts", []):
            data = part.get("inlineData", {}).get("data", "")
            if data:
                audio_array = self.audio_processor.process_audio_response(data)
                if self.all_output_data is None:
                    self.all_output_data = audio_array
                else:
                    self.all_output_data = np.concatenate((self.all_output_data, audio_array))

                while self.all_output_data.shape[-1] >= self.output_frame_size:
                    yield (self.output_sample_rate, self.all_output_data[: self.output_frame_size].reshape(1, -1))
                    self.all_output_data = self.all_output_data[self.output_frame_size :]

    def generator(self):
        """Generates audio output from the WebSocket stream."""
        while True:
            if not self.ws:
                print("WebSocket not connected")
                yield None
                continue

            try:
                message = self.ws.recv(timeout=30)
                msg = json.loads(message)
                if "serverContent" in msg:
                    content = msg["serverContent"].get("modelTurn", {})
                    yield from self._process_server_content(content)
            except TimeoutError:
                print("Timeout waiting for server response")
                yield None
            except Exception as e:
                yield None

    def emit(self) -> tuple[int, np.ndarray] | None:
        """Emits the next audio chunk from the generator."""
        if not self.ws:
            return None
        if not hasattr(self, "_generator"):
            self._generator = self.generator()
        try:
            return next(self._generator)
        except StopIteration:
            self.reset()
            return None

    def reset(self) -> None:
        """Resets the generator and output data."""
        if hasattr(self, "_generator"):
            delattr(self, "_generator")
        self.all_output_data = None

    def shutdown(self) -> None:
        """Closes the WebSocket connection."""
        if self.ws:
            self.ws.close()

    def check_connection(self):
        """Checks if the WebSocket connection is active."""
        try:
            if not self.ws or self.ws.closed:
                self._initialize_websocket()
            return True
        except Exception as e:
            print(f"Connection check failed: {str(e)}")
            return False

# Main Gradio Interface
def registry(
        name: str,
        token: str | None = None,
        **kwargs
):
    """Sets up and returns the Gradio interface."""
    api_key = token or os.environ.get(KEY_NAME)
    if not api_key:
        raise ValueError(f"{KEY_NAME} environment variable is not set.")

    interface = gr.Blocks()
    with interface:
        with gr.Tabs():
            with gr.TabItem("Voice Chat"):
                gr.HTML(
                    """
                    <div style='text-align: left'>
                        <h1>Gemini API Voice Chat</h1>
                    </div>
                    """
                )
                gemini_handler = GeminiHandler()
                with gr.Row():
                    audio = WebRTC(label="Voice Chat", modality="audio", mode="send-receive")

                audio.stream(
                    gemini_handler,
                    inputs=[audio],
                    outputs=[audio],
                    time_limit=600,
                    concurrency_limit=10
                )
    return interface

# Launch the Gradio interface
gr.load(
    name='gemini-2.5-flash-lite-preview-06-17',
    src=registry,
).launch()



================================================
FILE: examples/Object_detection.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Object detection with Gemini 1.5 Flash
"""

"""
### This notebook has been replaced by the new [spatial understanding](../quickstarts/Spatial_understanding.ipynb) notebook highliting the latest Gemini capabilities.

You can also check the [Spatial demo](https://aistudio.google.com/starter-apps/spatial) on AI Studio.

And if you still need it, you can find an archived version of the notebook [here](../../gemini-1.5-archive/examples/Object_detection.ipynb).
"""



================================================
FILE: examples/Opossum_search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Opossum search

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Opossum_search.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook contains a simple example of generating code with the Gemini API and Gemini Flash. Just for fun, you'll prompt the model to create a web app called "Opossum Search" that searches Google with "opossum" appended to the query.
"""

"""
<img src="https://storage.googleapis.com/generativeai-downloads/images/opossum_search.jpg" alt="An image of the opossum search web app running in a browser" width="500"/>

> The opossum image above is from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Opossum_2.jpg), and shared under a CC BY-SA 2.5 license.
"""

%pip install -q -U "google-genai>=1.0.0"

"""
## Set up your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google import genai
from google.genai.types import GenerateContentConfig
from google.colab import userdata

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Prompt the model to generate the web app.
"""

instruction = """
    You are a coding expert that specializes in creating web pages based on a user request.
    You create correct and simple code that is easy to understand.
    You implement all the functionality requested by the user.
    You ensure your code works properly, and you follow best practices for HTML programming.
"""

prompt = """
    Create a web app called Opossum Search:
    1. Every time you make a search query, it should redirect you to a Google search
    with the same query, but with the word opossum before it.
    2. It should be visually similar to Google search.
    3. Instead of the google logo, it should have a picture of this opossum:
    https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Opossum_2.jpg/292px-Opossum_2.jpg.
    4. It should be a single HTML file, with no separate JS or CSS files.
    5. It should say Powered by opossum search in the footer.
    6. Do not use any unicode characters.
    Thank you!
"""

from IPython.display import display, Markdown
MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config=GenerateContentConfig(
        system_instruction=instruction
    )
)
display(Markdown(response.text))
# Output:
#   <IPython.core.display.Markdown object>

"""
## Run the output locally

You can start a web server as follows.

* Save the HTML output to a file called `search.html`
* In your terminal run `python3 -m http.server 8000`
* Open your web browser, and point it to `http://localhost:8000/search.html`
"""

"""
## Display the output in IPython

Like all LLMs, the output may not always be correct. You can experiment by rerunning the prompt, or by writing an improved one (and/or better system instructions). Have fun!
"""

import IPython
code = response.text.split('```')[1][len('html'):]
IPython.display.HTML(code)
# Output:
#   <IPython.core.display.HTML object>



================================================
FILE: examples/Pdf_structured_outputs_on_invoices_and_forms.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Automated Invoice and Form Data Extraction with Gemini API & Pydantic

This notebook demonstrates how you can convert a PDF file so that it can be read by the Gemini API.

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Pdf_structured_outputs_on_invoices_and_forms.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
## 1. Set up Environment and create inference Client

The first task is to install the `google-genai` [Python SDK](https://googleapis.github.io/python-genai/) and obtain an API key. If you don”t have a can get one from Google AI Studio: [Get a Gemini API key](https://aistudio.google.com/app/apikey). If you are new to Google Colab checkout the [quickstart](../quickstarts/Authentication.ipynb)).

"""

%pip install "google-genai>=1"

"""
Once you have the SDK and API key, you can create a client and define the model you are going to use the new Gemini 2.0 Flash model, which is available via [free tier](https://ai.google.dev/pricing#2_0flash) with 1,500 request per day (at 2025-02-06). 
"""

from google import genai
from google.colab import userdata
api_key = userdata.get("GOOGLE_API_KEY") # If you are not using Colab you can set the API key directly

# Create a client
client = genai.Client(api_key=api_key)

# Define the model you are going to use
model_id =  "gemini-2.5-flash" # or "gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-pro"

"""
*Note: If you want to use Vertex AI see [here](https://googleapis.github.io/python-genai/#create-a-client) how to create your client*
"""

"""
## 2. Work with PDFs and other files

Gemini models are able to process [images and videos](https://ai.google.dev/gemini-api/docs/vision?lang=python#image-input), which can used with base64 strings or using the `files`api. After uploading the file you can include the file uri in the call directly. The Python API includes a [upload](https://googleapis.github.io/python-genai/#upload) and [delete](https://googleapis.github.io/python-genai/#delete) method. 

For this example you have 2 PDFs samples, one basic invoice and on form with and written values. 

"""

!wget -q -O handwriting_form.pdf https://storage.googleapis.com/generativeai-downloads/data/pdf_structured_outputs/handwriting_form.pdf
!wget -q -O invoice.pdf https://storage.googleapis.com/generativeai-downloads/data/pdf_structured_outputs/invoice.pdf

"""
You can now upload the files using our client with the `upload` method. Let's try this for one of the files.

"""

invoice_pdf = client.files.upload(file="invoice.pdf", config={'display_name': 'invoice'})

"""
_Note: The File API lets you store up to 20 GB of files per project, with a per-file maximum size of 2 GB. Files are stored for 48 hours. They can be accessed in that period with your API key, but they cannot be downloaded. File uploads are available at no cost._

After a file is uploaded you can check to how many tokens it got converted. This not only help us understand the context you are working with it also helps to keep track of the cost. 
"""

file_size = client.models.count_tokens(model=model_id,contents=invoice_pdf)
print(f'File: {invoice_pdf.display_name} equals to {file_size.total_tokens} tokens')
# File: invoice equals to 821 tokens
# Output:
#   File: invoice equals to 821 tokens


"""
## 3. Structured outputs with Gemini 2.0 and Pydantic

Structured Outputs is a feature that ensures Gemini always generate responses that adhere to a predefined format, such as JSON Schema. This means you have more control over the output and how to integrate it into our application as it is guaranteed to return a valid JSON object with the schema you define. 

Gemini 2.0 currenlty supports 3 dfferent types of how to define a JSON schemas:
- A single python type, as you would use in a [typing annotation](https://docs.python.org/3/library/typing.html).
- A Pydantic [BaseModel](https://docs.pydantic.dev/latest/concepts/models/)
- A dict equivalent of [genai.types.Schema](https://googleapis.github.io/python-genai/genai.html#genai.types.Schema) / [Pydantic BaseModel](https://docs.pydantic.dev/latest/concepts/models/)

Lets look at quick text-based example.
"""

from pydantic import BaseModel, Field

# Define a Pydantic model
# Use the Field class to add a description and default value to provide more context to the model
class Topic(BaseModel):
    name: str = Field(description="The name of the topic")

class Person(BaseModel):
    first_name: str = Field(description="The first name of the person")
    last_name: str = Field(description="The last name of the person")
    age: int = Field(description="The age of the person, if not provided please return 0")
    work_topics: list[Topic] = Field(description="The fields of interest of the person, if not provided please return an empty list")


# Define the prompt
prompt = "Philipp Schmid is a Senior AI Developer Relations Engineer at Google DeepMind working on Gemini, Gemma with the mission to help every developer to build and benefit from AI in a responsible way.  "

# Generate a response using the Person model
response = client.models.generate_content(model=model_id, contents=prompt, config={'response_mime_type': 'application/json', 'response_schema': Person})

# print the response as a json string
print(response.text)

# sdk automatically converts the response to the pydantic model
philipp: Person = response.parsed

# access an attribute of the json response
print(f"First name is {philipp.first_name}")
# Output:
#   {

#     "age": 0,

#     "first_name": "Philipp",

#     "last_name": "Schmid",

#     "work_topics": [

#       {

#         "name": "AI"

#       },

#       {

#         "name": "Gemini"

#       },

#       {

#         "name": "Gemma"

#       }

#     ]

#   }

#   First name is Philipp


"""
## 4. Extract Structured data from PDFs using Gemini 2.0

Now, let's combine the File API and structured output to extract information from our PDFs. You can create a simple method that accepts a local file path and a pydantic model and return the structured data for us. The method will:

1. Upload the file to the File API
2. Generate a structured response using the Gemini API
3. Convert the response to the pydantic model and return it

"""

def extract_structured_data(file_path: str, model: BaseModel):
    # Upload the file to the File API
    file = client.files.upload(file=file_path, config={'display_name': file_path.split('/')[-1].split('.')[0]})
    # Generate a structured response using the Gemini API
    prompt = f"Extract the structured data from the following PDF file"
    response = client.models.generate_content(model=model_id, contents=[prompt, file], config={'response_mime_type': 'application/json', 'response_schema': model})
    # Convert the response to the pydantic model and return it
    return response.parsed

"""
In our Example every PDF is a different to each other. So you want to define unique Pydantic models for each PDF to show the performance of the Gemini 2.0. If you have very similar PDFs and want to extract the same information you can use the same model for all of them. 

- `Invoice.pdf` : Extract the invoice number, date and all list items with description, quantity and gross worth and the total gross worth
- `handwriting_form.pdf` : Extract the form number, plan start date and the plan liabilities beginning of the year and end of the year

_Note: Using Pydantic features you can add more context to the model to make it more accurate as well as some validation to the data. Adding a comprehensive description can significantly improve the performance of the model. Libraries like [instructor](https://python.useinstructor.com/) added automatic retries based on validation errors, which can be a great help, but come at the cost of additional requests._


### Invoice.pdf

![Invoice.pdf](https://storage.googleapis.com/generativeai-downloads/data/pdf_structured_outputs/invoice.png)
"""

from pydantic import BaseModel, Field

class Item(BaseModel):
    description: str = Field(description="The description of the item")
    quantity: float = Field(description="The Qty of the item")
    gross_worth: float = Field(description="The gross worth of the item")

class Invoice(BaseModel):
    """Extract the invoice number, date and all list items with description, quantity and gross worth and the total gross worth."""
    invoice_number: str = Field(description="The invoice number e.g. 1234567890")
    date: str = Field(description="The date of the invoice e.g. 2024-01-01")
    items: list[Item] = Field(description="The list of items with description, quantity and gross worth")
    total_gross_worth: float = Field(description="The total gross worth of the invoice")


result = extract_structured_data("invoice.pdf", Invoice)
print(type(result))
print(f"Extracted Invoice: {result.invoice_number} on {result.date} with total gross worth {result.total_gross_worth}")
for item in result.items:
    print(f"Item: {item.description} with quantity {item.quantity} and gross worth {item.gross_worth}")
# Output:
#   <class '__main__.Invoice'>

#   Extracted Invoice: 27301261 on 10/09/2012 with total gross worth 544.46

#   Item: Lilly Pulitzer dress Size 2 with quantity 5.0 and gross worth 247.5

#   Item: New ERIN Erin Fertherston Straight Dress White Sequence Lining Sleeveless SZ 10 with quantity 1.0 and gross worth 65.99

#   Item: Sequence dress Size Small with quantity 3.0 and gross worth 115.5

#   Item: fire los angeles dress Medium with quantity 3.0 and gross worth 21.45

#   Item: Eileen Fisher Women's Long Sleeve Fleece Lined Front Pockets Dress XS Gray with quantity 3.0 and gross worth 52.77

#   Item: Lularoe Nicole Dress Size Small Light Solid Grey/ White Ringer Tee Trim with quantity 2.0 and gross worth 8.25

#   Item: J.Crew Collection Black & White sweater Dress sz S with quantity 1.0 and gross worth 33.0


"""
Fantastic! The model did a great job extracting the information from the invoice. 

### handwriting_form.pdf

![handwriting_form.pdf](https://storage.googleapis.com/generativeai-downloads/data/pdf_structured_outputs/handwriting_form.jpg)
"""

class Form(BaseModel):
    """Extract the form number, fiscal start date, fiscal end date, and the plan liabilities beginning of the year and end of the year."""
    form_number: str = Field(description="The Form Number")
    start_date: str = Field(description="Effective Date")
    beginning_of_year: float = Field(description="The plan liabilities beginning of the year")
    end_of_year: float = Field(description="The plan liabilities end of the year")

result = extract_structured_data("handwriting_form.pdf", Form)

print(f'Extracted Form Number: {result.form_number} with start date {result.start_date}. \nPlan liabilities beginning of the year {result.beginning_of_year} and end of the year {result.end_of_year}')
# Output:
#   Extracted Form Number: CA530082 with start date 02/05/2022. 

#   Plan liabilities beginning of the year 40000.0 and end of the year 55000.0


"""
## Learning more

If you want to learn more about the File API, Structured Outputs and how to use it to process images, audio, and video files, check out the following resources:

* Learn more about the [File API](../quickstarts/File_API.ipynb) with the quickstart.
* Learn more about prompting with [media files](https://ai.google.dev/gemini-api/docs/file-prompting-strategies) in the docs, including the supported formats and maximum length.
* Learn more about [Structured Outputs](https://ai.google.dev/gemini-api/docs/structured-output?lang=python) in the docs.

"""



================================================
FILE: examples/Search_Wikipedia_using_ReAct.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Search Wikipedia using ReAct
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook is a minimal implementation of [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) with the Google `gemini-2.0-flash` model. You'll use ReAct prompting to configure a model to search Wikipedia to find the answer to a user's question.

"""

"""
In this walkthrough, you will learn how to:


1.   Set up your development environment and API access to use Gemini.
2.   Use a ReAct few-shot prompt.
3.   Use the newly prompted model for multi-turn conversations (chat).
4.   Connect the model to the **Wikipedia API**.
5.  Have conversations with the model (try asking it questions like "how tall is the Eiffel Tower?") and watch it search Wikipedia.

"""

"""
> Note: The non-source code materials on this page are licensed under Creative Commons - Attribution-ShareAlike CC-BY-SA 4.0, https://creativecommons.org/licenses/by-sa/4.0/legalcode.
"""

"""
### Background

  

"""

"""
[ReAct](https://arxiv.org/abs/2210.03629) is a prompting method which allows language models to create a trace of their thinking processes and the steps required to answer a user's questions. This improves human interpretability and trustworthiness. ReAct prompted models generate Thought-Action-Observation triplets for every iteration, as you'll soon see. Let's get started!
"""

"""
## Setup

"""

%pip install -q "google-generativeai>=0.7.2"

%pip install -q wikipedia

"""
Note: The [`wikipedia` package](https://pypi.org/project/wikipedia/) notes that it was "designed for ease of use and simplicity, not for advanced use", and that production or heavy use should instead "use [Pywikipediabot](http://www.mediawiki.org/wiki/Manual:Pywikipediabot) or one of the other more advanced [Python MediaWiki API wrappers](http://en.wikipedia.org/wiki/Wikipedia:Creating_a_bot#Python)".
"""

import re
import os

import wikipedia
from wikipedia.exceptions import DisambiguationError, PageError

import google.generativeai as genai

"""
To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)

"""
## The ReAct prompt
"""

"""
The prompts used in the paper are available at [https://github.com/ysymyth/ReAct/tree/master/prompts](https://github.com/ysymyth/ReAct/tree/master/prompts)

Here, you will be working with the following ReAct prompt with a few minor adjustments.
"""

"""
> Note: The prompt and in-context examples used here are borrowed from [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct) which is published under a [MIT license](https://opensource.org/licenses/MIT).
"""

model_instructions = """Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, Observation is understanding relevant information from an Action's output and Action can be of three types:
(1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search and you can try to search the information from those topics.
(2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches, so keep your searches short.
(3) <finish>answer</finish>, which returns the answer and finishes the task.
"""

"""
### Few-shot prompting to enable in-context learning with Gemini

"""

"""
While large language models show good understanding of the instructions they are prompted with, they still may perform poorly on complex tasks in a zero-shot setting. Hence, you will now provide a few examples along with your prompt to steer the model's output according to your needs. This in-context learning improves the model's performance significantly.
"""

examples = """
Here are some examples.

Question
What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?

Thought 1
I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.

Action 1
<search>Colorado orogeny</search>

Observation 1
The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.

Thought 2
It does not mention the eastern sector. So I need to look up eastern sector.

Action 2
<lookup>eastern sector</lookup>

Observation 2
The eastern sector extends into the High Plains and is called the Central Plains orogeny.

Thought 3
The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.

Action 3
<search>High Plains</search>

Observation 3
High Plains refers to one of two distinct land regions

Thought 4
I need to instead search High Plains (United States).

Action 4
<search>High Plains (United States)</search>

Observation 4
The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).

Thought 5
High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.

Action 5
<finish>1,800 to 7,000 ft</finish>

Question
Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who?

Thought 1
The question simplifies to "The Simpsons" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.

Action 1
<search>Milhouse</search>

Observation 1
Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.

Thought 2
The paragraph does not tell who Milhouse is named after, maybe I can look up "named after".

Action 2
<lookup>named after</lookup>

Observation 2
Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.

Thought 3
Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.

Action 3
<finish>Richard Nixon</finish>

Question
Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?

Thought 1
I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.

Action 1
<search>Adam Clayton Powell</search>

Observation 1
Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr. State Office Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell (film)’, ’Giancarlo Esposito’].

Thought 2
To find the documentary, I can search Adam Clayton Powell (film).

Action 2
<search>Adam Clayton Powell (film)</search>

Observation 2
Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.

Thought 3
Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.

Action 3
<finish>The Saimaa Gesture</finish>

Question
What profession does Nicholas Ray and Elia Kazan have in common?

Thought 1
I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.

Action 1
<search>Nicholas Ray</search>

Observation 1
Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.

Thought 2
Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.

Action 2
<search>Elia Kazan</search>

Observation 2
Elia Kazan was an American film and theatre director, producer, screenwriter and actor.

Thought 3
Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.

Action 3
<finish>director, screenwriter, actor</finish>

Question
Which magazine was started first Arthur’s Magazine or First for Women?

Thought 1
I need to search Arthur’s Magazine and First for Women, and find which was started first.

Action 1
<search>Arthur’s Magazine</search>

Observation 1
Arthur’s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.

Thought 2
Arthur’s Magazine was started in 1844. I need to search First for Women next.

Action 2
<search>First for Women</search>

Observation 2
First for Women is a woman’s magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.

Thought 3
First for Women was started in 1989. 1844 (Arthur’s Magazine) < 1989 (First for Women), so Arthur’s Magazine was started first.

Action 3
<finish>Arthur’s Magazine</finish>

Question
Were Pavel Urysohn and Leonid Levin known for the same type of work?

Thought 1
I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.

Action 1
<search>Pavel Urysohn</search>

Observation 1
Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.

Thought 2
Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.

Action 2
<search>Leonid Levin</search>

Observation 2
Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.

Thought 3
Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.

Action 3
<finish>yes</finish>

Question
{question}"""

"""
Copy the instructions along with examples in a file called `model_instructions.txt`
"""

ReAct_prompt = model_instructions + examples
with open('model_instructions.txt', 'w') as f:
  f.write(ReAct_prompt)

"""
## The Gemini-ReAct pipeline
"""

"""
### Setup
"""

"""
You will now build an end-to-end pipeline to facilitate multi-turn chat with the ReAct-prompted Gemini model.
"""

class ReAct:
  def __init__(self, model: str, ReAct_prompt: str | os.PathLike):
    """Prepares Gemini to follow a `Few-shot ReAct prompt` by imitating
    `function calling` technique to generate both reasoning traces and
    task-specific actions in an interleaved manner.

    Args:
        model: name to the model.
        ReAct_prompt: ReAct prompt OR path to the ReAct prompt.
    """
    self.model = genai.GenerativeModel(model)
    self.chat = self.model.start_chat(history=[])
    self.should_continue_prompting = True
    self._search_history: list[str] = []
    self._search_urls: list[str] = []

    try:
      # try to read the file
      with open(ReAct_prompt, 'r') as f:
        self._prompt = f.read()
    except FileNotFoundError:
      # assume that the parameter represents prompt itself rather than path to the prompt file.
      self._prompt = ReAct_prompt

  @property
  def prompt(self):
    return self._prompt

  @classmethod
  def add_method(cls, func):
    setattr(cls, func.__name__, func)

  @staticmethod
  def clean(text: str):
    """Helper function for responses."""
    text = text.replace("\n", " ")
    return text

"""
### Define tools

"""

"""
As instructed by the prompt, the model will be generating **Thought-Action-Observation** traces, where every **Action** trace could be one of the following tokens:


1.   </search/> : Perform a Wikipedia search via external API.
2.   </lookup/> : Lookup for specific information on a page with the Wikipedia API.
3.   </finish/> : Stop the execution of the model and return the answer.

If the model encounters any of these tokens, the model should make use of the `tools` made available to the model. This understanding of the model to leverage acquired toolsets to collect information from the external world is often referred to as **function calling**. Therefore, the next goal is to imitate this function calling technique in order to allow ReAct prompted Gemini model to access the external groundtruth.

The Gemini API supports function calling and you could use this feature to set up your tools. However, for this tutorial, you will learn to simulate it using `stop_sequences` parameter.


Define the tools:
"""

"""
#### Search
Define a method to perform Wikipedia searches
"""

@ReAct.add_method
def search(self, query: str):
    """Perfoms search on `query` via Wikipedia api and returns its summary.

    Args:
        query: Search parameter to query the Wikipedia API with.

    Returns:
        observation: Summary of Wikipedia search for `query` if found else
        similar search results.
    """
    observation = None
    query = query.strip()
    try:
      # try to get the summary for requested `query` from the Wikipedia
      observation = wikipedia.summary(query, sentences=4, auto_suggest=False)
      wiki_url = wikipedia.page(query, auto_suggest=False).url
      observation = self.clean(observation)

      # if successful, return the first 2-3 sentences from the summary as model's context
      observation = self.model.generate_content(f'Retun the first 2 or 3 \
      sentences from the following text: {observation}')
      observation = observation.text

      # keep track of the model's search history
      self._search_history.append(query)
      self._search_urls.append(wiki_url)
      print(f"Information Source: {wiki_url}")

    # if the page is ambiguous/does not exist, return similar search phrases for model's context
    except (DisambiguationError, PageError) as e:
      observation = f'Could not find ["{query}"].'
      # get a list of similar search topics
      search_results = wikipedia.search(query)
      observation += f' Similar: {search_results}. You should search for one of those instead.'

    return observation

"""
#### Lookup
Look for a specific phrase on the Wikipedia page.
"""

@ReAct.add_method
def lookup(self, phrase: str, context_length=200):
    """Searches for the `phrase` in the lastest Wikipedia search page
    and returns number of sentences which is controlled by the
    `context_length` parameter.

    Args:
        phrase: Lookup phrase to search for within a page. Generally
        attributes to some specification of any topic.

        context_length: Number of words to consider
        while looking for the answer.

    Returns:
        result: Context related to the `phrase` within the page.
    """
    # get the last searched Wikipedia page and find `phrase` in it.
    page = wikipedia.page(self._search_history[-1], auto_suggest=False)
    page = page.content
    page = self.clean(page)
    start_index = page.find(phrase)

    # extract sentences considering the context length defined
    result = page[max(0, start_index - context_length):start_index+len(phrase)+context_length]
    print(f"Information Source: {self._search_urls[-1]}")
    return result

"""
#### Finish
Instruct the pipline to terminate its execution.
"""

@ReAct.add_method
def finish(self, _):
  """Finishes the conversation on encountering <finish> token by
  setting the `self.should_continue_prompting` flag to `False`.
  """
  self.should_continue_prompting = False
  print(f"Information Sources: {self._search_urls}")

"""
### Stop tokens and function calling imitation
"""

"""
Now that you are all set with function definitions, the next step is to instruct the model to interrupt its execution upon encountering any of the action tokens. You will make use of the `stop_sequences` parameter from [`genai.GenerativeModel.GenerationConfig`](https://ai.google.dev/api/python/google/generativeai/GenerationConfig) class to instruct the model when to stop. Upon encountering an action token, the pipeline will simply extract what specific token from the `stop_sequences` argument terminated the model's execution, and then call the appropriate **tool** (function).

The function's response will be added to model's chat history for continuing the context link.
"""

@ReAct.add_method
def __call__(self, user_question, max_calls: int=8, **generation_kwargs):
  """Starts multi-turn conversation with the chat models with function calling

  Args:
      max_calls: max calls made to the model to get the final answer.

      generation_kwargs: Same as genai.GenerativeModel.GenerationConfig
              candidate_count: (int | None) = None,
              stop_sequences: (Iterable[str] | None) = None,
              max_output_tokens: (int | None) = None,
              temperature: (float | None) = None,
              top_p: (float | None) = None,
              top_k: (int | None) = None

  Raises:
      AssertionError: if max_calls is not between 1 and 8
  """

  # hyperparameter fine-tuned according to the paper
  assert 0 < max_calls <= 8, "max_calls must be between 1 and 8"

  if len(self.chat.history) == 0:
    model_prompt = self.prompt.format(question=user_question)
  else:
    model_prompt = user_question

  # stop_sequences for the model to immitate function calling
  callable_entities = ['</search>', '</lookup>', '</finish>']

  generation_kwargs.update({'stop_sequences': callable_entities})

  self.should_continue_prompting = True
  for idx in range(max_calls):

    self.response = self.chat.send_message(content=[model_prompt],
              generation_config=generation_kwargs, stream=False)

    for chunk in self.response:
      print(chunk.text, end=' ')

    response_cmd = self.chat.history[-1].parts[-1].text

    try:
      # regex to extract <function name writen in between angular brackets>
      cmd = re.findall(r'<(.*)>', response_cmd)[-1]
      print(f'</{cmd}>')
      # regex to extract param
      query = response_cmd.split(f'<{cmd}>')[-1].strip()
      # call to appropriate function
      observation = self.__getattribute__(cmd)(query)

      if not self.should_continue_prompting:
        break

      stream_message = f"\nObservation {idx + 1}\n{observation}"
      print(stream_message)
      # send function's output as user's response
      model_prompt = f"<{cmd}>{query}</{cmd}>'s Output: {stream_message}"

    except (IndexError, AttributeError) as e:
      model_prompt = "Please try to generate thought-action-observation traces \
      as instructed by the prompt."

"""
### Test ReAct prompted Gemini model
"""

gemini_ReAct_chat = ReAct(model='gemini-2.0-flash', ReAct_prompt='model_instructions.txt')
# Note: try different combinations of generational_config parameters for variational results
gemini_ReAct_chat("What are the total of ages of the main trio from the new Percy Jackson and the Olympians TV series in real life?", temperature=0.2)
# Output:
#   Thought 1

#   I need to find the ages of the main trio from the new Percy Jackson and the Olympians TV series in real life, then add them together.

#   

#   Action 1

#   <search>Percy Jackson and the Olympians TV series </search>

#   

#   Observation 1

#   Could not find ["Percy Jackson and the Olympians TV series"]. Similar: ['Percy Jackson and the Olympians (TV series)', 'Percy Jackson & the Olympians', 'Percy Jackson (film series)', 'Percy Jackson & the Olympians: The Lightning Thief', 'Percy Jackson (disambiguation)', 'Percy Jackson', 'List of characters in mythology novels by Rick Riordan', 'The Lightning Thief', 'The Heroes of Olympus', 'Brandon T. Jackson']. You should search for one of those instead.

#   Thought 2:

#   The search for the exact phrase "Percy Jackson and the Olympians TV series" failed. I should try searching for one of the suggested alternatives. "Percy Jackson and the Olympians (TV series)" seems like the most likely match.

#   

#   Action 2:

#   <search>Percy Jackson and the Olympians (TV series) </search>

#   Information Source: https://en.wikipedia.org/wiki/Percy_Jackson_and_the_Olympians_(TV_series)

#   

#   Observation 2

#   Percy Jackson and the Olympians is an American fantasy television series created by Rick Riordan and Jonathan E. Steinberg for Disney+, based on the book series of the same name by Riordan.  Walker Scobell stars as Percy Jackson, alongside Leah Sava Jeffries as Annabeth Chase and Aryan Simhadri as Grover Underwood. 

#   

#   Thought 3:

#   The output provides the names of the actors playing the main trio: Walker Scobell, Leah Sava Jeffries, and Aryan Simhadri. I need to find their ages.

#   

#   Action 3:

#   <search>Walker Scobell age </search>

#   

#   Observation 3

#   Could not find ["Walker Scobell age"]. Similar: ['Walker Scobell', 'The Adam Project', 'Percy Jackson', 'Aryan Simhadri', 'Hiro Kanagawa', 'Maximum Effort', 'Michael Douglas on stage and screen', 'List of awards and nominations received by Ryan Reynolds', 'List of American current child actors', 'Oliver Cromwell']. You should search for one of those instead.

#   Thought 4:

#   The search for "Walker Scobell age" failed. I should try searching for "Walker Scobell" and see if his age is mentioned in the Wikipedia page.

#   

#   Action 4:

#   <search>Walker Scobell </search>

#   Information Source: https://en.wikipedia.org/wiki/Walker_Scobell

#   

#   Observation 4

#   Walker Scobell (born January 5, 2009) is an American actor. He has starred in the 2022 action comedy films The Adam Project and Secret Headquarters. 

#   

#   Thought 5:

#   The output tells us Walker Scobell was born on January 5, 2009. To find his age, I need to calculate the difference between his birth year and the current year.

#   

#   Action 5:

#   <finish>14 </finish>

#   Information Sources: ['https://en.wikipedia.org/wiki/Percy_Jackson_and_the_Olympians_(TV_series)', 'https://en.wikipedia.org/wiki/Walker_Scobell']


"""
Now, try asking the same question to `gemini-2.0-flash` model without the ReAct prompt.
"""

gemini_ReAct_chat.model.generate_content("What is the total of ages of the main trio from the new Percy Jackson and the Olympians TV series in real life?").text
# Output:
#   "It is impossible to determine the total age of the main trio from the new Percy Jackson and the Olympians TV series in real life. This is because:\n\n* **The ages of the actors are not publicly known.** While some websites may have guesses or estimates, these are unreliable. \n* **The characters' ages are different from the actors' ages.**  The characters are teenagers in the series, while the actors are likely older. \n\nTherefore, without knowing the actors' actual birthdates, it is impossible to calculate their combined age. \n"

"""
## Summary

The ReAct prompted Gemini model is grounded by external information sources and hence is less prone to hallucination. Furthermore, Thought-Action-Observation  traces generated by the model enhance human interpretability and trustworthiness by allowing users to witness the model's reasoning process for answering the user's query.

"""

"""
## Next steps

"""

"""
Head over to this [Streamlit app](https://mayochat.streamlit.app/) to interact with a ReAct prompted Gemini bot built with this code.
"""



================================================
FILE: examples/Talk_to_documents_with_embeddings.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Document search with embeddings
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Talk_to_documents_with_embeddings.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
## Overview

This example demonstrates how to use the Gemini API to create embeddings so that you can perform document search. You will use the Python client library to build a word embedding that allows you to compare search strings, or questions, to document contents.

In this tutorial, you'll use embeddings to perform document search over a set of documents to ask questions related to the Google Car.

"""

"""
## Setup
"""

%pip install -U -q "google-genai>=1.0.0"

"""
To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google import  genai
from google.colab import userdata

GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')
client = genai.Client(api_key=GEMINI_API_KEY)

"""
## Embedding generation

In this section, you will see how to generate embeddings for a piece of text using the embeddings from the Gemini API.

See the [Embeddings quickstart](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb) to learn more about the `task_type` parameter used below.
"""

from google.genai import types

title = "The next generation of AI for developers and Google Workspace"
sample_text = """
    Title: The next generation of AI for developers and Google Workspace
    Full article:
    Gemini API & Google AI Studio: An approachable way to explore and
    prototype with generative AI applications
"""

EMBEDDING_MODEL_ID = MODEL_ID = "gemini-embedding-001"  # @param ["gemini-embedding-001", "text-embedding-004"] {"allow-input": true, "isTemplate": true}
embedding = client.models.embed_content(
        model=EMBEDDING_MODEL_ID,
        contents=sample_text,
        config=types.EmbedContentConfig(
            task_type="retrieval_document",
            title=title
    ))

print(embedding)
# Output:
#   embeddings=[ContentEmbedding(

#     values=[

#       -0.019380787,

#       0.015025399,

#       0.006310311,

#       -0.057478663,

#       0.011998727,

#       <... 3067 more items ...>,

#     ]

#   )] metadata=None


"""
## Building an embeddings database

Here are three sample texts to use to build the embeddings database. You will use the Gemini API to create embeddings of each of the documents. Turn them into a dataframe for better visualization.
"""

DOCUMENT1 = {
    "title": "Operating the Climate Control System",
    "content": "Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it."}
DOCUMENT2 = {
    "title": "Touchscreen",
    "content": "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs."}
DOCUMENT3 = {
    "title": "Shifting Gears",
    "content": "Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions."}

documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]

"""
Organize the contents of the dictionary into a dataframe for better visualization.
"""

import pandas as pd

df = pd.DataFrame(documents)
df.columns = ['Title', 'Text']
df
# Output:
#                                     Title  \

#   0  Operating the Climate Control System   

#   1                           Touchscreen   

#   2                        Shifting Gears   

#   

#                                                   Text  

#   0  Your Googlecar has a climate control system th...  

#   1  Your Googlecar has a large touchscreen display...  

#   2  Your Googlecar has an automatic transmission. ...  

"""
Get the embeddings for each of these bodies of text. Add this information to the dataframe.
"""

# Get the embeddings of each text and add to an embeddings column in the dataframe
def embed_fn(title, text):
  response = client.models.embed_content(
        model=EMBEDDING_MODEL_ID,
        contents=text,
        config=types.EmbedContentConfig(
            task_type="retrieval_document",
            title=title
        )
    )

  return response.embeddings[0].values

df['Embeddings'] = df.apply(lambda row: embed_fn(row['Title'], row['Text']), axis=1)
df
# Output:
#                                     Title  \

#   0  Operating the Climate Control System   

#   1                           Touchscreen   

#   2                        Shifting Gears   

#   

#                                                   Text  \

#   0  Your Googlecar has a climate control system th...   

#   1  Your Googlecar has a large touchscreen display...   

#   2  Your Googlecar has an automatic transmission. ...   

#   

#                                             Embeddings  

#   0  [0.02483931, -0.003871694, 0.013593362, -0.031...  

#   1  [0.008149438, -0.0013574613, -0.0029458047, -0...  

#   2  [0.009464946, 0.022619268, -0.0036155856, -0.0...  

"""
## Document search with Q&A

Now that the embeddings are generated, let's create a Q&A system to search these documents. You will ask a question about hyperparameter tuning, create an embedding of the question, and compare it against the collection of embeddings in the dataframe.

The embedding of the question will be a vector (list of float values), which will be compared against the vector of the documents using the dot product. This vector returned from the API is already normalized. The dot product represents the similarity in direction between two vectors.

The values of the dot product can range between -1 and 1, inclusive. If the dot product between two vectors is 1, then the vectors are in the same direction. If the dot product value is 0, then these vectors are orthogonal, or unrelated, to each other. Lastly, if the dot product is -1, then the vectors point in the opposite direction and are not similar to each other.

Note, with the new embeddings model (`gemini-embedding-001`), specify the task type as `QUERY` for user query and `DOCUMENT` when embedding a document text.

Task Type | Description
---       | ---
RETRIEVAL_QUERY	| Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting.
"""

query = "How to shift gears in the Google car?"

request = client.models.embed_content(
    model=EMBEDDING_MODEL_ID,
    contents=query,
    config=types.EmbedContentConfig(
        task_type="RETRIEVAL_DOCUMENT",
        )
)

"""
Use the `find_best_passage` function to calculate the dot products, and then sort the dataframe from the largest to smallest dot product value to retrieve the relevant passage out of the database.
"""

import numpy as np

def find_best_passage(query, dataframe):
  """
  Compute the distances between the query and each document in the dataframe
  using the dot product.
  """
  query_embedding = client.models.embed_content(
      model=EMBEDDING_MODEL_ID,
      contents=query,
      config=types.EmbedContentConfig(
          task_type="retrieval_document",
          )
  )

  dot_products = np.dot(
      np.stack(dataframe['Embeddings']),
      query_embedding.embeddings[0].values
  )
  idx = np.argmax(dot_products)
  return dataframe.iloc[idx]['Text'] # Return text from index with max value

"""
View the most relevant document from the database:
"""

from IPython.display import Markdown

passage = find_best_passage(query, df)
Markdown(passage)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Question and Answering Application

Let's try to use the text generation API to create a Q & A system. Input your own custom data below to create a simple question and answering example. You will still use the dot product as a metric of similarity.
"""

import textwrap

def make_prompt(query, relevant_passage):
  escaped = (
      relevant_passage
      .replace("'", "")
      .replace('"', "")
      .replace("\n", " ")
  )
  prompt = textwrap.dedent("""
    You are a helpful and informative bot that answers questions using text
    from the reference passage included below. Be sure to respond in a
    complete sentence, being comprehensive, including all relevant
    background information.

    However, you are talking to a non-technical audience, so be sure to
    break down complicated concepts and strike a friendly and conversational
    tone. If the passage is irrelevant to the answer, you may ignore it.

    QUESTION: '{query}'
    PASSAGE: '{relevant_passage}'

    ANSWER:
  """).format(query=query, relevant_passage=escaped)


  return prompt

prompt = make_prompt(query, passage)
Markdown(prompt)
# Output:
#   <IPython.core.display.Markdown object>

"""
Choose one of the Gemini content generation models in order to find the answer to your query.
"""

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-pro"] {"allow-input":true, isTemplate: true}
answer = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

Markdown(answer.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

Check out the [embeddings quickstart](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb) to learn more, and browse the cookbook for more [examples](https://github.com/google-gemini/cookbook/tree/main/examples).
"""



================================================
FILE: examples/Upload_files_to_Colab.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Upload files to Google Colab

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Upload_files_to_Colab.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
You can upload files to Google Colab to quickly experiment with your own data. For example, you can upload video or image files to use with the Files API, or a upload a text file to read in with a long context model like Gemini Flash. This example shows you how to upload files to the Colab runtime and use them in your code.

First, download the following file to your local machine:

*  [a11.txt](https://storage.googleapis.com/generativeai-downloads/data/a11.txt)

It contains a transcript of transmissions from the Apollo 11 mission, originally from https://www.nasa.gov/history/alsj/a11/a11trans.html.

Next, upload the file to Google Colab. To do so, first click **Files** on the left sidebar, then click the **Upload** button:

<img width=400 src="https://ai.google.dev/tutorials/images/colab_upload.png">

You're now able to use the file in Colab!
"""

with open('a11.txt') as file:
  text_data = file.read()

# Print first 10 lines
for line in text_data.splitlines()[:10]:
  print(line)
# Output:
#   INTRODUCTION

#   

#   This is the transcription of the Technical Air-to-Ground Voice Transmission (GOSS NET 1) from the Apollo 11 mission.

#   

#   Communicators in the text may be identified according to the following list.

#   

#   Spacecraft:

#   CDR	Commander	Neil A. Armstrong

#   CMP	Command module pilot   	Michael Collins

#   LMP	Lunar module pilot	Edwin E. ALdrin, Jr.


"""
This makes it simple to use the file with the Gemini API.
"""

%pip install -U -q "google-genai>=1.0.0"
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/137.7 kB[0m [31m?[0m eta [36m-:--:--[0m
#   [2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m133.1/137.7 kB[0m [31m3.9 MB/s[0m eta [36m0:00:01[0m
#   [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m137.7/137.7 kB[0m [31m3.1 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}
response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        'What is this transcript?',
        text_data
    ]
)
print(response.text)
# Output:
#   Based on the provided text, this is a transcript of air-to-ground voice communications from the Apollo 11 mission. It includes:

#   

#   *   **Introduction:** Explains the document is a transcription of GOSS NET 1 (Technical Air-to-Ground Voice Transmission) for Apollo 11. It lists abbreviations used for different speakers (Commander, Command Module Pilot, Lunar Module Pilot, various ground control and recovery personnel).

#   

#   *   **Abbreviations Key:** Provides a key to understand who the different communicators are (e.g., CDR = Commander, CMP = Command Module Pilot, CC = Capsule Communicator).

#   

#   *   **Air-to-Ground Voice Transcription:** The main body of the document, which is the transcribed dialogue between the Apollo 11 astronauts and mission control in Houston, as well as other ground stations. The text is segmented by timecode and location (e.g., MILA, GRAND BAHAMA ISLANDS, CANARY). It covers various aspects of the mission from launch to initial post-splashdown communications.

#   

#   In short, this is the official record of what was said between the Apollo 11 crew and ground control during the mission, providing a detailed account of procedures, observations, and conversations.

#   




================================================
FILE: examples/Voice_memos.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Voice memos
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Voice_memos.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook provides a quick example of how to work with audio and text files in the same prompt. You'll use the Gemini API to help you generate ideas for your next blog post, based on voice memos you recorded on your phone, and previous articles you've written.
"""

%pip install -U -q "google-generativeai>=0.7.2"

import google.generativeai as genai

"""
### Setup your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)

"""
Install PDF processing tools.
"""

!apt install poppler-utils

"""
## Upload your audio and text files

"""

!wget https://storage.googleapis.com/generativeai-downloads/data/Walking_thoughts_3.m4a
!wget https://storage.googleapis.com/generativeai-downloads/data/A_Possible_Future_for_Online_Content.pdf
!wget https://storage.googleapis.com/generativeai-downloads/data/Unanswered_Questions_and_Endless_Possibilities.pdf

audio_file_name = "Walking_thoughts_3.m4a"
audio_file = genai.upload_file(path=audio_file_name)

"""
## Extract text from the PDFs
"""

!pdftotext A_Possible_Future_for_Online_Content.pdf
!pdftotext Unanswered_Questions_and_Endless_Possibilities.pdf

blog_file_name = "A_Possible_Future_for_Online_Content.txt"
blog_file = genai.upload_file(path=blog_file_name)

blog_file_name2 = "Unanswered_Questions_and_Endless_Possibilities.txt"
blog_file2 = genai.upload_file(path=blog_file_name2)

"""
## System instructions

Write a detailed system instruction to configure the model.
"""

si="""Objective: Transform raw thoughts and ideas into polished, engaging blog posts that capture a writers unique style and voice.
Input:
Example Blog Posts (1-5): A user will provide examples of blog posts that resonate with their desired style and tone. These will guide you in understanding the preferences for word choice, sentence structure, and overall voice.
Audio Clips: A user will share a selection of brainstorming thoughts and key points through audio recordings. They will talk freely and openly, as if they were explaining their ideas to a friend.
Output:
Blog Post Draft: A well-structured first draft of the blog post, suitable for platforms like Substack or LinkedIn.
The draft will include:
Clear and engaging writing: you will strive to make the writing clear, concise, and interesting for the target audience.
Tone and style alignment: The language and style will closely match the examples provided, ensuring consistency with the desired voice.
Logical flow and structure: The draft will be organized with clear sections based on the content of the post.
Target word count: Aim for 500-800 words, but this can be adjusted based on user preferences.
Process:
Style Analysis: Carefully analyze the example blog posts provided by the user to identify key elements of their preferred style, including:
Vocabulary and word choice: Formal vs. informal, technical terms, slang, etc.
Sentence structure and length: Short and impactful vs. longer and descriptive sentences.
Tone and voice: Humorous, serious, informative, persuasive, etc.
Audio Transcription and Comprehension: Your audio clips will be transcribed with high accuracy. you will analyze them to extract key ideas, arguments, and supporting points.
Draft Generation: Using the insights from the audio and the style guidelines from the examples, you will generate a first draft of the blog post. This draft will include all relevant sections with supporting arguments or evidence, and a great ending that ties everything together and makes the reader want to invest in future readings.
"""

"""
## Generate Content
"""

prompt = "Draft my next blog post based on my thoughts in this audio file and these two previous blog posts I wrote."

model = genai.GenerativeModel(model_name="models/gemini-2.5-flash", system_instruction=si)

response = model.generate_content([prompt, blog_file, blog_file2, audio_file],
                                  request_options={"timeout": 600})
print(response.text)
# Output:
#   ## The Throwaway Work That Makes You Better

#   

#   Early in my career, I spent a lot of time working on visions, roadmaps, and ideas. Some of them ended up not happening at all, or were essentially thrown away.  It was frustrating, especially coming straight out of school with the idea that you’re given an assignment, you do it, and then you’re graded on it.  There's no "takesies-backsies" in school.  You get the assignment, you produce the work, and that’s that. 

#   

#   The real world is a lot different. 

#   

#   It's a constant adjustment, where priorities change, markets shift, and sometimes the work you do doesn't get used or even goes nowhere.  It felt like a colossal waste of time!

#   

#   It took me a while to get over it, but I don’t think I truly appreciated this until I joined my current team. They have a "right to think" culture, and that changed everything. 

#   

#   Suddenly, I realized that the work you produce, the content you create, is part of the process of making you better at what you do in the future.  It’s about honing your skills and getting better over time. 

#   

#   The "right to think" culture isn’t simply about accepting that priorities change and things shift; it's about recognizing that what you produce, even if it's not the final product, is valuable.  It's about learning and iterating.

#   

#   This "right to think" idea ties in nicely with iterative processes and learning by doing.  It's a reframing of the mindset around throwaway work.  There's no such thing as throwaway work.  It's all helping you to hone your skills and get better over time. 

#   

#   I'm constantly talking about this reframing, and I think it’s important to call out the importance of writing to think. It's about being willing to scrap things and move on once they’ve served their purpose.  Write more, write earlier, write often. Don’t worry about the final product. Just get it out there.

#   

#   In the end, the work you produce, even if it's ultimately discarded, is still part of the process that makes you a better thinker, a better creator, and a better professional. So, don’t be afraid to throw things away. It might just make you better in the long run. 

#   


"""
## Learning more

* Learn more about the [File API](https://github.com/google-gemini/cookbook/blob/main/quickstarts/File_API.ipynb) with the quickstart.
"""



================================================
FILE: examples/Apps_script_and_Workspace_codelab/README.md
================================================
# Gemini API and Google Workspace Codelab

These are the final, accompanying files for the [Automate Google Workspace tasks with the Gemini API
](https://codelabs.developers.google.com/codelabs/gemini-workspace) codelab. This codelabs shows you how to
connect to the Gemini API using Apps Script, and uses the function calling, vision and text capabilities to automate
Google Workspace tasks - summarising a document, analyzing a chart, sending an email and generating some slides directly. All of this is done from a free text input.

Please read and follow along with the main codelab, and if you get stuck you can load these files directly.

This workshop was featured at [Google I/O 2024](https://io.google/2024/).



================================================
FILE: examples/Apps_script_and_Workspace_codelab/CollegeExpenses.xlsx
================================================
[Binary file]


================================================
FILE: examples/Apps_script_and_Workspace_codelab/Gemini-blog.txt
================================================
Our next-generation model: Gemini 1.5
Feb 15, 2024

8 min read

The model delivers dramatically enhanced performance, with a breakthrough in long-context understanding across modalities.

Sundar Pichai
CEO of Google and Alphabet
Demis Hassabis
CEO of Google DeepMind

A note from Google and Alphabet CEO Sundar Pichai:

Last week, we rolled out our most capable model, Gemini 1.0 Ultra, and took a significant step forward in making Google products more helpful, starting with Gemini Advanced. Today, developers and Cloud customers can begin building with 1.0 Ultra too — with our Gemini API in AI Studio and in Vertex AI.

Our teams continue pushing the frontiers of our latest models with safety at the core. They are making rapid progress. In fact, we’re ready to introduce the next generation: Gemini 1.5. It shows dramatic improvements across a number of dimensions and 1.5 Pro achieves comparable quality to 1.0 Ultra, while using less compute.

This new generation also delivers a breakthrough in long-context understanding. We’ve been able to significantly increase the amount of information our models can process — running up to 1 million tokens consistently, achieving the longest context window of any large-scale foundation model yet.

Longer context windows show us the promise of what is possible. They will enable entirely new capabilities and help developers build much more useful models and applications. We’re excited to offer a limited preview of this experimental feature to developers and enterprise customers. Demis shares more on capabilities, safety and availability below.

— Sundar

Introducing Gemini 1.5
By Demis Hassabis, CEO of Google DeepMind, on behalf of the Gemini team

This is an exciting time for AI. New advances in the field have the potential to make AI more helpful for billions of people over the coming years. Since introducing Gemini 1.0, we’ve been testing, refining and enhancing its capabilities.

Today, we’re announcing our next-generation model: Gemini 1.5.

Gemini 1.5 delivers dramatically enhanced performance. It represents a step change in our approach, building upon research and engineering innovations across nearly every part of our foundation model development and infrastructure. This includes making Gemini 1.5 more efficient to train and serve, with a new Mixture-of-Experts (MoE) architecture.

The first Gemini 1.5 model we’re releasing for early testing is Gemini 1.5 Pro. It’s a mid-size multimodal model, optimized for scaling across a wide-range of tasks, and performs at a similar level to 1.0 Ultra, our largest model to date. It also introduces a breakthrough experimental feature in long-context understanding.

Gemini 1.5 Pro comes with a standard 128,000 token context window. But starting today, a limited group of developers and enterprise customers can try it with a context window of up to 1 million tokens via AI Studio and Vertex AI in private preview.

As we roll out the full 1 million token context window, we’re actively working on optimizations to improve latency, reduce computational requirements and enhance the user experience. We’re excited for people to try this breakthrough capability, and we share more details on future availability below.

These continued advances in our next-generation models will open up new possibilities for people, developers and enterprises to create, discover and build using AI.

Context lengths of leading foundation models

Highly efficient architecture
Gemini 1.5 is built upon our leading research on Transformer and MoE architecture. While a traditional Transformer functions as one large neural network, MoE models are divided into smaller "expert” neural networks.

Depending on the type of input given, MoE models learn to selectively activate only the most relevant expert pathways in its neural network. This specialization massively enhances the model’s efficiency. Google has been an early adopter and pioneer of the MoE technique for deep learning through research such as Sparsely-Gated MoE, GShard-Transformer, Switch-Transformer, M4 and more.

Our latest innovations in model architecture allow Gemini 1.5 to learn complex tasks more quickly and maintain quality, while being more efficient to train and serve. These efficiencies are helping our teams iterate, train and deliver more advanced versions of Gemini faster than ever before, and we’re working on further optimizations.

Greater context, more helpful capabilities
An AI model’s “context window” is made up of tokens, which are the building blocks used for processing information. Tokens can be entire parts or subsections of words, images, videos, audio or code. The bigger a model’s context window, the more information it can take in and process in a given prompt — making its output more consistent, relevant and useful.

Through a series of machine learning innovations, we’ve increased 1.5 Pro’s context window capacity far beyond the original 32,000 tokens for Gemini 1.0. We can now run up to 1 million tokens in production.

This means 1.5 Pro can process vast amounts of information in one go — including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we’ve also successfully tested up to 10 million tokens.

Complex reasoning about vast amounts of information
1.5 Pro can seamlessly analyze, classify and summarize large amounts of content within a given prompt. For example, when given the 402-page transcripts from Apollo 11’s mission to the moon, it can reason about conversations, events and details found across the document.

Reasoning across a 402-page transcript: Gemini 1.5 Pro Demo
1:53
Gemini 1.5 Pro can understand, reason about and identify curious details in the 402-page transcripts from Apollo 11’s mission to the moon.

Better understanding and reasoning across modalities
1.5 Pro can perform highly-sophisticated understanding and reasoning tasks for different modalities, including video. For instance, when given a 44-minute silent Buster Keaton movie, the model can accurately analyze various plot points and events, and even reason about small details in the movie that could easily be missed.

Multimodal prompting with a 44-minute movie: Gemini 1.5 Pro Demo
1:59
Gemini 1.5 Pro can identify a scene in a 44-minute silent Buster Keaton movie when given a simple line drawing as reference material for a real-life object.

Relevant problem-solving with longer blocks of code
1.5 Pro can perform more relevant problem-solving tasks across longer blocks of code. When given a prompt with more than 100,000 lines of code, it can better reason across examples, suggest helpful modifications and give explanations about how different parts of the code works.

Problem solving across 100,633 lines of code | Gemini 1.5 Pro Demo
3:15
Gemini 1.5 Pro can reason across 100,000 lines of code giving helpful solutions, modifications and explanations.

Enhanced performance
When tested on a comprehensive panel of text, code, image, audio and video evaluations, 1.5 Pro outperforms 1.0 Pro on 87% of the benchmarks used for developing our large language models (LLMs). And when compared to 1.0 Ultra on the same benchmarks, it performs at a broadly similar level.

Gemini 1.5 Pro maintains high levels of performance even as its context window increases. In the Needle In A Haystack (NIAH) evaluation, where a small piece of text containing a particular fact or statement is purposely placed within a long block of text, 1.5 Pro found the embedded text 99% of the time, in blocks of data as long as 1 million tokens.

Gemini 1.5 Pro also shows impressive “in-context learning” skills, meaning that it can learn a new skill from information given in a long prompt, without needing additional fine-tuning. We tested this skill on the Machine Translation from One Book (MTOB) benchmark, which shows how well the model learns from information it’s never seen before. When given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person learning from the same content.

As 1.5 Pro’s long context window is the first of its kind among large-scale models, we’re continuously developing new evaluations and benchmarks for testing its novel capabilities.

For more details, see our Gemini 1.5 Pro technical report.

Extensive ethics and safety testing
In line with our AI Principles and robust safety policies, we’re ensuring our models undergo extensive ethics and safety tests. We then integrate these research learnings into our governance processes and model development and evaluations to continuously improve our AI systems.

Since introducing 1.0 Ultra in December, our teams have continued refining the model, making it safer for a wider release. We’ve also conducted novel research on safety risks and developed red-teaming techniques to test for a range of potential harms.

In advance of releasing 1.5 Pro, we've taken the same approach to responsible deployment as we did for our Gemini 1.0 models, conducting extensive evaluations across areas including content safety and representational harms, and will continue to expand this testing. Beyond this, we’re developing further tests that account for the novel long-context capabilities of 1.5 Pro.

Build and experiment with Gemini models
We’re committed to bringing each new generation of Gemini models to billions of people, developers and enterprises around the world responsibly.

Starting today, we’re offering a limited preview of 1.5 Pro to developers and enterprise customers via AI Studio and Vertex AI. Read more about this on our Google for Developers blog and Google Cloud blog.

We’ll introduce 1.5 Pro with a standard 128,000 token context window when the model is ready for a wider release. Coming soon, we plan to introduce pricing tiers that start at the standard 128,000 context window and scale up to 1 million tokens, as we improve the model.

Early testers can try the 1 million token context window at no cost during the testing period, though they should expect longer latency times with this experimental feature. Significant improvements in speed are also on the horizon.

Developers interested in testing 1.5 Pro can sign up now in AI Studio, while enterprise customers can reach out to their Vertex AI account team.

Learn more about Gemini’s capabilities and see how it works.


================================================
FILE: examples/Apps_script_and_Workspace_codelab/main.gs
================================================
/**
 * Copyright 2025 Google LLC
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 *     http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

function main() {
  // const userQuery = "Set up a meeting at 5PM with Helen to discuss the news in the Gemini-1.5-blog.txt file.";  
  // const userQuery = "Draft an email for Mary with insights from the chart in the CollegeExpenses sheet.";
  const userQuery = "Help me put together a deck about water conservation.";

  var tool_use = callGeminiWithTools(userQuery, WORKSPACE_TOOLS);
  Logger.log(tool_use);
  
  if(tool_use['name'] == "setupMeeting") {
    setupMeeting(tool_use['args']['time'], tool_use['args']['recipient'], tool_use['args']['filename']);
    Logger.log("Your meeting has been set up.");
  }
  else if(tool_use['name'] == "draftEmail") {
    draftEmail(tool_use['args']['sheet_name'], tool_use['args']['recipient']);
    Logger.log("Check your Gmail to review the draft");
  }
  else if(tool_use['name'] == 'createDeck') {
    deckURL = createDeck(tool_use['args']['topic']);
    Logger.log("Deck URL: " + deckURL);
  }
  else
    Logger.log("no proper tool found");
}




================================================
FILE: examples/Apps_script_and_Workspace_codelab/utils.gs
================================================
/**
 * Copyright 2025 Google LLC
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 *     http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

const properties = PropertiesService.getScriptProperties().getProperties();
const geminiApiKey = properties['GOOGLE_API_KEY'];
const geminiEndpoint = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${geminiApiKey}`;

const NUM_SLIDES = 3;

const WORKSPACE_TOOLS = {
 "function_declarations": [
   {
     "name": "setupMeeting",
     "description": "Sets up a meeting in Google Calendar.",
     "parameters": {
       "type": "object",
       "properties": {
         "time": {
           "type": "string",
           "description": "The time of the meeting."
         },
         "recipient": {
           "type": "string",
           "description": "The name of the recipient."
         },   
         "filename": {
           "type": "string",
           "description": "The name of the file."
         },                     
       },
       "required": [
         "time",
         "recipient",
         "filename"
       ]
     }
   },
      {
        "name": "draftEmail",
        "description": "Write an email by analyzing data or charts in a Google Sheets file.",
        "parameters": {
          "type": "object",
          "properties": {
            "sheet_name": {
              "type": "string",
              "description": "The name of the sheet to analyze."
            },
            "recipient": {
              "type": "string",
              "description": "The name of the recipient."
            },            
          },
          "required": [
            "sheet_name",
            "recipient"
          ]
        }
      },   
      {
        "name": "createDeck",
        "description": "Build a simple presentation deck with Google Slides and return the URL.",
        "parameters": {
          "type": "object",
          "properties": {
            "topic": {
              "type": "string",
              "description": "The topic that the presentation is about."
            },
          },
          "required": [
            "topic"
          ]
        }
      },

   // You add tools here.        
 ]
};

function callGemini(prompt, temperature=0) {
  const payload = {
    "contents": [
      {
        "parts": [
          {
            "text": prompt
          },
        ]
      }
    ], 
    "generationConfig":  {
      "temperature": temperature,
    },
  };

  const options = { 
    'method' : 'post',
    'contentType': 'application/json',
    'payload': JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(geminiEndpoint, options);
  const data = JSON.parse(response);
  const content = data["candidates"][0]["content"]["parts"][0]["text"];
  return content;
}

function testGemini() {
  const prompt = "The best thing since sliced bread is";
  const output = callGemini(prompt);
  console.log(prompt, output);
}


function callGeminiProVision(prompt, image, temperature=0) {
  const imageData = Utilities.base64Encode(image.getAs('image/png').getBytes());

  const payload = {
    "contents": [
      {
        "parts": [
          {
            "text": prompt
          },
          {
            "inlineData": {
              "mimeType": "image/png",
              "data": imageData
            }
          }          
        ]
      }
    ], 
    "generationConfig":  {
      "temperature": temperature,
    },
  };

  const options = { 
    'method' : 'post',
    'contentType': 'application/json',
    'payload': JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(geminiEndpoint, options);
  const data = JSON.parse(response);
  const content = data["candidates"][0]["content"]["parts"][0]["text"];
  return content;
}


function testGeminiVision() {
  const prompt = "Provide a fun fact about this object.";
  const image = UrlFetchApp.fetch('https://storage.googleapis.com/generativeai-downloads/images/instrument.jpg').getBlob();
  const output = callGeminiProVision(prompt, image);
  console.log(prompt, output);
}

function callGeminiWithTools(prompt, tools, temperature=0) {
  const payload = {
    "contents": [
      {
        "parts": [
          {
            "text": prompt
          },
        ]
      }
    ], 
    "tools" : tools,
    "generationConfig":  {
      "temperature": temperature,
    },    
  };

  const options = { 
    'method' : 'post',
    'contentType': 'application/json',
    'payload': JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(geminiEndpoint, options);
  const data = JSON.parse(response);
  const content = data["candidates"][0]["content"]["parts"][0]["functionCall"];
  return content;
}

function testGeminiTools() {
  const prompt = "Tell me how many days there are left in this month.";
  const tools = {
    "function_declarations": [
      {
        "name": "datetime",
        "description": "Returns the current date and time as a formatted string.",
        "parameters": {
          "type": "string"
        }
      }
    ]
  };
  const output = callGeminiWithTools(prompt, tools);
  console.log(prompt, output);
}

function attachFileToMeeting(event, file, fileName) {
  // Get the iCal ID for the event.
  const iCalEventId = event.getId();

  // Log the ID and title for debugging.
  console.log(`iCal event ID: ${iCalEventId}`);
  console.log(`event Title: ${event.getTitle()}`);

  // Set up the options for listing the event with the advanced Google Calendar service.
  const options = {
      iCalUID: iCalEventId,
    };

  // Use the primary calendar as the calendar ID to list events.
  const calendarId = 'primary';

  // Use the advanced Google Calendar service to list the event.
  const calEvents = Calendar.Events.list(calendarId, options);

  // Get the Calendar ID used by the advanced Google Calendar service.
  const eventId = calEvents.items[0].id;

  // Get the file URL for the attachment.
  const fileUrl = file.getUrl();

    // Set up the patch options to add the file.
    var patch = {
      attachments: [{
        'fileUrl': fileUrl,
        'title': fileName
      }]
    };

    // Patch the event to add the file as an attachment.
    Calendar.Events.patch(patch, 'primary', eventId, {"supportsAttachments": true});  
}

function setupMeeting(time, recipient, filename) {
  const files = DriveApp.getFilesByName(filename);
  const file = files.next();
  const blogContent = file.getAs("text/*").getDataAsString();
  
  var geminiOutput = callGemini("Give me a really short title of this blog and a summary with less than three sentences. Please return the result as a JSON with two fields: title and summary. \n" +  blogContent);

  // The Gemini model likes to enclose the JSON with ```json and ```
  geminiOutput = JSON.parse(geminiOutput.replace(/```(?:json|)/g, ""));  
  const title = geminiOutput['title'];
  const fileSummary = geminiOutput['summary'];

  const event = CalendarApp.getDefaultCalendar().createEventFromDescription(`meet ${recipient} at ${time} to discuss "${title}"`); 
  event.setDescription(fileSummary);
  attachFileToMeeting(event, file, filename);
}

function draftEmail(sheet_name, recipient) {
  
  const prompt = `Compose the email body for ${recipient} with your insights for this chart. Use information in this chart only and do not do historical comparisons. Be concise.`;

  var files = DriveApp.getFilesByName(sheet_name);
  var sheet = SpreadsheetApp.openById(files.next().getId()).getSheetByName("Sheet1");
  var expenseChart = sheet.getCharts()[0];

  var chartFile = DriveApp.createFile(expenseChart.getBlob().setName("ExpenseChart.png"));
  var emailBody = callGeminiProVision(prompt, expenseChart);
  GmailApp.createDraft(recipient+"@demo-email-provider.com", "College expenses", emailBody, {
      attachments: [chartFile.getAs(MimeType.PNG)],
      name: 'myname'
  });
}

function createDeck(topic) {
  const prompt = `I'm preparing a ${NUM_SLIDES}-slide deck to discuss ${topic}. Please help me brainstorm and generate main bullet points for each slide. Keep the title of each slide short. Please produce the result as a valid JSON so that I can pass it to other APIs.`;
  
  var geminiOutput = callGemini(prompt, 0.4);
  // The Gemini model likes to enclose the JSON with ```json and ```
  geminiOutput = geminiOutput.replace(/```(?:json|)/g, "");
  const bulletPoints = JSON.parse(geminiOutput);
    
  // Create a Google Slides presentation.
  const presentation = SlidesApp.create("My New Presentation");

  // Set up the opening slide.
  var slide = presentation.getSlides()[0]; 
  var shapes = slide.getShapes();
  shapes[0].getText().setText(topic);

  var body;
  for (var i = 0; i < NUM_SLIDES; i++) {
      slide = presentation.appendSlide(SlidesApp.PredefinedLayout.TITLE_AND_BODY);
      shapes = slide.getShapes();
      // Set title.
      shapes[0].getText().setText(bulletPoints['slides'][i]['title']);
  
      // Set body.
      body = "";
      for (var j = 0; j < bulletPoints['slides'][i]['bullets'].length; j++) {
        // Logger.log(j);
        body += '* ' + bulletPoints['slides'][i]['bullets'][j] + '\n';
      }
      shapes[1].getText().setText(body);
  } 

  return presentation.getUrl();
}




================================================
FILE: examples/chromadb/README.md
================================================
# ChromaDB integration

This [tutorial](./Vectordb_with_chroma.ipynb) demonstrates how to use the Gemini API to create a vector database and retrieve answers to questions from the database using [ChromaDB](https://docs.trychroma.com/){:.external}, an open-source Python tool that creates embedding databases.


================================================
FILE: examples/chromadb/Vectordb_with_chroma.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Document Q&A with ChromaDB
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/chromadb/Vectordb_with_chroma.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
## Overview

This tutorial demonstrates how to use the Gemini API to create a vector database and retrieve answers to questions from the database. Moreover, you will use [ChromaDB](https://docs.trychroma.com/){:.external}, an open-source Python tool that creates embedding databases. ChromaDB allows you to:

* Store embeddings as well as their metadata
* Embed documents and queries
* Search through the database of embeddings

In this tutorial, you'll use embeddings to retrieve an answer from a database of vectors created with ChromaDB.
"""

"""
## Setup

First, download and install ChromaDB and the Gemini API Python library.
"""

%pip install -U -q "google-genai>=1.0.0" chromadb

"""
Then import the modules you'll use in this tutorial.
"""

import textwrap
import chromadb
import numpy as np
import pandas as pd

from IPython.display import Markdown
from chromadb import Documents, EmbeddingFunction, Embeddings

"""
## Configure your API key

Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.

<a class="button button-primary" href="https://aistudio.google.com/app/apikey" target="_blank" rel="noopener noreferrer">Get an API key</a>

In Colab, add the key to the secrets manager under the "🔑" in the left panel. Give it the name `GEMINI_API_KEY`.

Once you have the API key, pass it to the SDK. You can do this in two ways:

* Put the key in the `GEMINI_API_KEY` environment variable (the SDK will automatically pick it up from there).
* Pass the key to `genai.Client(api_key=...)`
"""

from google import genai
from google.colab import userdata


GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')
client = genai.Client(api_key=GEMINI_API_KEY)

"""
Key Point: Next, you will choose a model. Any embedding model will work for this tutorial, but for real applications it's important to choose a specific model and stick with it. The outputs of different models are not compatible with each other.

**Note**: At this time, the Gemini API is [only available in certain regions](https://ai.google.dev/available_regions).
"""

for m in client.models.list():
  if 'embedContent' in m.supported_actions:
    print(m.name)
# Output:
#   models/embedding-001

#   models/text-embedding-004

#   models/gemini-embedding-exp-03-07

#   models/gemini-embedding-exp

#   models/gemini-embedding-001


"""
### Data

Here is a small set of documents you will use to create an embedding database:
"""

DOCUMENT1 = """
  Operating the Climate Control System  Your Googlecar has a climate control
  system that allows you to adjust the temperature and airflow in the car.
  To operate the climate control system, use the buttons and knobs located on
  the center console.  Temperature: The temperature knob controls the
  temperature inside the car. Turn the knob clockwise to increase the
  temperature or counterclockwise to decrease the temperature.
  Airflow: The airflow knob controls the amount of airflow inside the car.
  Turn the knob clockwise to increase the airflow or counterclockwise to
  decrease the airflow. Fan speed: The fan speed knob controls the speed
  of the fan. Turn the knob clockwise to increase the fan speed or
  counterclockwise to decrease the fan speed.
  Mode: The mode button allows you to select the desired mode. The available
  modes are: Auto: The car will automatically adjust the temperature and
  airflow to maintain a comfortable level.
  Cool: The car will blow cool air into the car.
  Heat: The car will blow warm air into the car.
  Defrost: The car will blow warm air onto the windshield to defrost it.
"""
DOCUMENT2 = """
  Your Googlecar has a large touchscreen display that provides access to a
  variety of features, including navigation, entertainment, and climate
  control. To use the touchscreen display, simply touch the desired icon.
  For example, you can touch the \"Navigation\" icon to get directions to
  your destination or touch the \"Music\" icon to play your favorite songs.
"""
DOCUMENT3 = """
  Shifting Gears Your Googlecar has an automatic transmission. To
  shift gears, simply move the shift lever to the desired position.
  Park: This position is used when you are parked. The wheels are locked
  and the car cannot move.
  Reverse: This position is used to back up.
  Neutral: This position is used when you are stopped at a light or in traffic.
  The car is not in gear and will not move unless you press the gas pedal.
  Drive: This position is used to drive forward.
  Low: This position is used for driving in snow or other slippery conditions.
"""

documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]

"""
## Creating the embedding database with ChromaDB

You will create a [custom function](https://docs.trychroma.com/embeddings#custom-embedding-functions){:.external} for performing embedding using the Gemini API. By inputting a set of documents into this custom function, you will receive vectors, or embeddings of the documents.

"""

"""
### API changes to Embeddings with model gemini-embedding-001

For the new embeddings model, embedding-001, there is a new task type parameter and the optional title (only valid with task_type=`RETRIEVAL_DOCUMENT`).

These new parameters apply only to the newest embeddings models.The task types are:

Task Type | Description
---       | ---
RETRIEVAL_QUERY	| Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting.
SEMANTIC_SIMILARITY	| Specifies the given text will be used for Semantic Textual Similarity (STS).
CLASSIFICATION	| Specifies that the embeddings will be used for classification.
CLUSTERING	| Specifies that the embeddings will be used for clustering.
"""

from google.genai import types

class GeminiEmbeddingFunction(EmbeddingFunction):
  def __call__(self, input: Documents) -> Embeddings:
    EMBEDDING_MODEL_ID = "gemini-embedding-001"  # @param ["gemini-embedding-001", "text-embedding-004"] {"allow-input": true, "isTemplate": true}
    title = "Custom query"
    response = client.models.embed_content(
        model=EMBEDDING_MODEL_ID,
        contents=input,
        config=types.EmbedContentConfig(
          task_type="retrieval_document",
          title=title
        )
    )

    return response.embeddings[0].values

"""
Now you will create the vector database. In the `create_chroma_db` function, you will instantiate a [Chroma client](https://docs.trychroma.com/getting-started). From there, you will create a collection, which is where you store your embeddings, documents, and any metadata. Note that the embedding function from above is passed as an argument to the `create_collection`.

Next, you use the `add` method to add the documents to the collection.
"""

def create_chroma_db(documents, name):
  chroma_client = chromadb.Client()
  db = chroma_client.create_collection(
      name=name,
      embedding_function=GeminiEmbeddingFunction()
  )

  for i, d in enumerate(documents):
    db.add(
      documents=d,
      ids=str(i)
    )
  return db

# Set up the DB
db = create_chroma_db(documents, "google-car-db")
# Output:
#   /tmp/ipykernel_83887/781126645.py:5: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.

#     embedding_function=GeminiEmbeddingFunction()


"""
Confirm that the data was inserted by looking at the database:
"""

sample_data = db.get(include=['documents', 'embeddings'])

df = pd.DataFrame({
    "IDs": sample_data['ids'][:3],
    "Documents": sample_data['documents'][:3],
    "Embeddings": [str(emb)[:50] + "..." for emb in sample_data['embeddings'][:3]]  # Truncate embeddings
})

df
# Output:
#     IDs                                          Documents  \

#   0   0  \n  Operating the Climate Control System  Your...   

#   1   1  \n  Your Googlecar has a large touchscreen dis...   

#   2   2  \n  Shifting Gears Your Googlecar has an autom...   

#   

#                                             Embeddings  

#   0  [ 0.00971627 -0.00177013  0.00590323 ...  0.00...  

#   1  [ 0.00388563 -0.00036349 -0.00230268 ...  0.01...  

#   2  [-0.00264773  0.010808   -0.00854844 ...  0.00...  

"""
## Getting the relevant document

`db` is a Chroma collection object. You can call `query` on it to perform a nearest neighbors search to find similar embeddings or documents.

"""

def get_relevant_passage(query, db):
  passage = db.query(query_texts=[query], n_results=1)['documents'][0][0]
  return passage

# Perform embedding search
passage = get_relevant_passage("touch screen features", db)
Markdown(passage)
# Output:
#   <IPython.core.display.Markdown object>

"""
Now that you have found the relevant passage in your set of documents, you can use it make a prompt to pass into the Gemini API.
"""

def make_prompt(query, relevant_passage):
  escaped = relevant_passage.replace("'", "").replace('"', "").replace("\n", " ")
  prompt = ("""
    You are a helpful and informative bot that answers questions using
    text from the reference passage included below.
    Be sure to respond in a complete sentence, being comprehensive,
    including all relevant background information.
    However, you are talking to a non-technical audience, so be sure to
    break down complicated concepts and strike a friendly
    and converstional tone. If the passage is irrelevant to the answer,
    you may ignore it.
    QUESTION: '{query}'
    PASSAGE: '{relevant_passage}'

    ANSWER:
  """).format(query=query, relevant_passage=escaped)

  return prompt

"""
Pass a query to the prompt:
"""

query = "How do you use the touchscreen in the Google car?"
prompt = make_prompt(query, passage)
Markdown(prompt)
# Output:
#   <IPython.core.display.Markdown object>

"""
Now use the `generate_content` method to to generate a response from the model.
"""

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-pro"] {"allow-input":true, isTemplate: true}
answer = client.models.generate_content(
    model = MODEL_ID,
    contents = prompt
)
Markdown(answer.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

To learn more about how you can use the embeddings, check out the [examples](https://ai.google.dev/examples?keywords=embed) available. To learn how to use other services in the Gemini API, visit the [Python quickstart](https://ai.google.dev/gemini-api/docs/get-started/python).
"""



================================================
FILE: examples/google-adk/README.md
================================================
# Gemini API Google Agent Development Kit (ADK) Examples
<html>
    <h2 align="center">
      <img src="https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png" width="256"/>
    </h2>
    <h3 align="center">
      An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.
    </h3>
    <h3 align="center">
      Important Links:
      <a href="https://github.com/google/adk-python/">Repo</a>,
      <a href="https://google.github.io/adk-docs/">Docs</a> &
      <a href="https://github.com/google/adk-samples">Samples</a>.
    </h3>
</html>

Agent Development Kit (ADK) is designed for developers seeking fine-grained
control and flexibility when building advanced AI agents that are tightly
integrated with services in Google Cloud. It allows you to define agent
behavior, orchestration, and tool use directly in code, enabling robust
debugging, versioning, and deployment anywhere – from your laptop to the cloud.

---

## Table of contents

This folder contains examples demonstrating how to use the Google Application Development Kit (ADK) with the Gemini API to build more robust, stateful, and manageable applications.

*   **[Getting_started_with_ADK](./Getting_started_with_ADK.ipynb)**: Learn the basics of using ADK for state management around a simple Gemini API call, showcasing workflow transitions (e.g., start, processing, end).

Explore other folders in this cookbook for more examples focusing directly on the Gemini API or integrations with other frameworks and tools.



================================================
FILE: examples/google-adk/Getting_started_with_ADK.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# ADK Simple Demo: Stateful Echo Agent with Gemini

"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/google-adk/Getting_started_with_ADK.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook provides a basic, introductory example of using `Gemini` in the Google Agent Development Kit (ADK).

**Goal:** Demonstrate how ADK orchestrates a simple workflow involving state transitions (`START` -> `PROCESSING` -> `END`) around a core interaction with the Gemini API.

**Scenario:**
You will build a "Stateful Echo Agent". This agent's primary task is to echo the user's input. However, it will use ADK components to manage its internal state throughout the process:
1.  It starts in a `START` state.
2.  Upon receiving input, it uses an ADK Tool to transition to `PROCESSING`.
3.  It prepares the echo response (implicitly using the Gemini model configured in the Agent).
4.  It uses the ADK Tool again to transition to the `END` state.
5.  It delivers the final echo response.

This example highlights ADK's role in managing structured workflows and state, even for simple tasks.
"""

"""
<!-- Community Contributor Badge -->
<table>
  <tr>
    <!-- Author Avatar Cell -->
    <td bgcolor="#d7e6ff">
      <a href="https://github.com/andycandy" target="_blank" title="View Anand Roy's profile on GitHub">
        <img src="https://github.com/andycandy.png?size=100"
             alt="andycandy's GitHub avatar"
             width="100"
             height="100">
      </a>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#d7e6ff">
      <h2><font color='black'>This notebook was contributed by <a href="https://github.com/andycandy" target="_blank"><font color='#217bfe'><strong>Anand Roy</strong></font></a>.</font></h2>
      <h5><font color='black'><a href="https://www.linkedin.com/in/anand-roy-61a2b529b"><font color="#078efb">LinkedIn</font></a> - See <a href="https://github.com/andycandy" target="_blank"><font color="#078efb"><strong>Anand</strong></font></a> other notebooks <a href="https://github.com/search?q=repo%3Agoogle-gemini%2Fcookbook%20%22Anand%20Roy%22&type=code" target="_blank"><font color="#078efb">here</font></a>.</h5></font><br>
      <!-- Footer -->
      <font color='black'><small><em>Have a cool Gemini example? Feel free to <a href="https://github.com/google-gemini/cookbook/blob/main/CONTRIBUTING.md" target="_blank"><font color="#078efb">share it too</font></a>!</em></small></font>
    </td>
  </tr>
</table>
"""

"""
## Setup
"""

%pip install -q google-adk google-genai python-dotenv

"""
## 1. Configure Google API Key

To power the `Agent` with Gemini, access to the Google Generative AI API is required. The next code cell configures your API key.

**Important:** This example uses Colab Secrets (`userdata.get('GOOGLE_API_KEY')`). Make sure you have stored your key named `GOOGLE_API_KEY` in the Colab Secrets manager (View -> Secrets).
"""

from google.colab import userdata
import os
from dotenv import load_dotenv

api_key = userdata.get('GOOGLE_API_KEY')
os.environ['GOOGLE_API_KEY'] = api_key
load_dotenv()
# Output:
#   False

"""
## 2. Core ADK Components in this Demo

This example uses the following key ADK components:

*   **`Agent`**: The agent powered by the Gemini model. It understands instructions, decides when to use tools, and generates responses.
*   **`FunctionTool`**: A custom capability provided to the agent. In this case, it's a tool to update the workflow status.
*   **`ToolContext`**: An object automatically passed to our tool, allowing it to access and modify the `Session State`.
*   **`SessionService` (`InMemorySessionService`)**: Manages the conversation's state (`workflow_status`). `InMemory` means the state exists only while this script runs.
*   **`Runner`**: Orchestrates the entire interaction: passes user input to the agent, handles tool calls, manages the state via the `SessionService`, and delivers the final response.
*   **`Session State`**: A dictionary holding data for the current conversation (session). Here, you use it to store `{'workflow_status': '...'}`.
"""

from google.adk.agents import Agent
from google.adk.tools import FunctionTool, ToolContext
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from google.adk.sessions import Session

"""
## 3. Define ADK Components (Tool, Agent, Services)

Now, let's define the core ADK components for our Stateful Echo Agent:

1.  **Tool (`set_workflow_state`):** A Python function wrapped as an ADK `FunctionTool`. This function will modify the `workflow_status` in the session state when called by the agent.
2.  **Agent (`echo_agent`):** An `LlmAgent` configured with the Gemini model, specific instructions on *when* to call the `state_tool`, and the tool itself.
3.  **Services (`session_service`, `runner`):** The `InMemorySessionService` to hold state and the `Runner` to execute the agent.
4.  **Session:** Used to create a specific session instance with an initial state `{'workflow_status': 'START'}`.
"""

async def set_workflow_state(state_name: str, tool_context: ToolContext) -> dict:
    """Sets the current workflow state in the session state.

    Use this tool to mark progress through the workflow stages:
    - Call with 'PROCESSING' before handling the user input.
    - Call with 'END' after handling the user input.

    Args:
        state_name: The state to set (e.g., 'PROCESSING', 'END').
        tool_context: Injected context to access session state.

    Returns:
        A dictionary confirming the status update.
    """
    try:
        tool_context.state['workflow_status'] = state_name
        return {'status': 'success', 'message': f'Workflow state set to {state_name}'}
    except Exception as e:
        return {'status': 'error', 'message': f'Failed to set state: {e}'}

# Create the function tool
state_tool = FunctionTool(func=set_workflow_state)

GEMINI_MODEL = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

echo_agent = Agent(
  name="EchoAgent",
  description="An agent that echoes input while tracking workflow state.",
  model=GEMINI_MODEL,
  instruction="""
    You are a simple echo agent. You also manage a workflow status stored in the session state under the key 'workflow_status'.
    The workflow states are: START, PROCESSING, END.

    Your Workflow:
    1. The workflow starts in the 'START' state (this is set externally).
    2. When you receive user input:
        a. FIRST, use the 'set_workflow_state' tool to change the status to 'PROCESSING'.
        b. THEN, simply repeat the user's exact input back to them in your response text.
        c. AFTER preparing the echo response text, use the 'set_workflow_state' tool AGAIN to change the status to 'END'.
        d. FINALLY, provide only the echo response text to the user.
  """,
  tools=[state_tool],
)

session_service = InMemorySessionService()
runner = Runner(
    agent=echo_agent,
    session_service=session_service,
    app_name="EchoAgentDemo"
)

APP_NAME="EchoAgentDemo"
USER_ID="1"
ID="session_01"

session = session_service.create_session(
    app_name=APP_NAME,
    user_id=USER_ID,
    session_id=ID,
    state={'workflow_status': 'START'}
)

"""
## 4. Run the Interaction

Now send a simple message ("Hello ADK!") to the `echo_agent` via the `Runner`. The `Runner` will manage the execution flow according to the agent's instructions.

**Expected Flow:**
1.  Agent receives "Hello ADK!".
2.  Agent calls `set_workflow_state` tool (state -> `PROCESSING`).
3.  Agent calls `set_workflow_state` tool (state -> `END`).
4.  Agent responds with the text "Hello ADK!".

The next cell initiates the `run_async` call and processes the stream of events generated during execution, logging the steps.
"""

from google.genai.types import Content, Part

user_input_text = "Hello ADK!"
user_message = Content(role='user', parts=[Part(text=user_input_text)])

final_agent_response_text = None
async def process_interaction_events():
    """Helper async function to process events."""
    global final_agent_response_text
    event_count = 0
    async for event in runner.run_async(session_id=ID, new_message=user_message, user_id=USER_ID):
        event_count += 1
        print(f"\n[Event {event_count}] Type: {type(event).__name__}")

        if event.content:
            part = event.content.parts[0]
            role = event.content.role
            print(f"  Role: {role}")
            if part.text:
                print(f"  Text: '{part.text}'")
                if role == 'model':
                    final_agent_response_text = part.text
            elif part.function_call:
                print(f"  >>> Function Call <<<")
                print(f"      Name: {part.function_call.name}")
                print(f"      Args: {part.function_call.args}")
            elif part.function_response:
                print(f"  <<< Function Response >>>")
                print(f"      Name: {part.function_response.name}")
                print(f"      Data: {part.function_response.response}")

await process_interaction_events()
# Output:
#   WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.

#   

#   [Event 1] Type: Event

#     Role: model

#     >>> Function Call <<<

#         Name: set_workflow_state

#         Args: {'state_name': 'PROCESSING'}

#   

#   [Event 2] Type: Event

#     Role: user

#     <<< Function Response >>>

#         Name: set_workflow_state

#         Data: {'status': 'success', 'message': 'Workflow state set to PROCESSING'}

#   WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.

#   

#   [Event 3] Type: Event

#     Role: model

#     >>> Function Call <<<

#         Name: set_workflow_state

#         Args: {'state_name': 'END'}

#   

#   [Event 4] Type: Event

#     Role: user

#     <<< Function Response >>>

#         Name: set_workflow_state

#         Data: {'status': 'success', 'message': 'Workflow state set to END'}

#   

#   [Event 5] Type: Event

#     Role: model

#     Text: 'Hello ADK!

#   '


"""
## 5. Analyze the Results

Examine the "Agent Event Log" printed above. You should clearly see the sequence reflecting the agent's instructions:

1.  `Function Call` event targeting `set_workflow_state` with `args={'state_name': 'PROCESSING'}`.
2.  `Function Response` event confirming the first tool execution.
3.  `Function Call` event targeting `set_workflow_state` with `args={'state_name': 'END'}`.
4.  `Function Response` event confirming the second tool execution.
5.  final event containing the agent's text response (the echoed message).

The next cell verifies this outcome by checking the final state stored in the session.
"""

final_session = session_service.get_session(
    session_id=session.id,
    user_id=USER_ID,
    app_name=APP_NAME
)

if final_session:
    final_state = final_session.state
    workflow_status = final_state.get('workflow_status')
    print(f"Final workflow status: {workflow_status}")

if final_agent_response_text:
    print(f"Agent response: {final_agent_response_text}")
# Output:
#   Final workflow status: END

#   Agent response: Hello ADK!

#   


"""
## Next Steps & Further Learning

This notebook demonstrated the basic structure of an ADK application, including:

*   Defining an `Agent` powered by Gemini.
*   Creating a simple `FunctionTool` to modify state.
*   Using `SessionService` and `ToolContext` for state management.
*   Orchestrating the flow with the `Runner`.

To dive deeper into the capabilities of the Google Agent Development Kit:

1.  **Explore the Official Documentation:** For detailed explanations of all components (Agents, Tools, Sessions, Callbacks, Multi-Agent systems, etc.), visit the [**Google ADK Documentation site**](https://google.github.io/adk-docs/).
2. **Try the Getting Started Notebook:** Explore the [**official ADK tutorial notebook on Colab**](https://colab.sandbox.google.com/github/google/adk-docs/blob/main/examples/python/tutorial/agent_team/adk_tutorial.ipynb) for a hands-on introduction to building your first agent.
3.  **Discover More Examples:** Check out the [**Google ADK GitHub repository**](https://github.com/google/adk-python)  for a wider range of examples, including more complex workflows, integrations, and advanced agent patterns.

Consider exploring concepts like:

*   **Workflow Agents** (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) for structured process control.
*   **Multi-Agent Systems** for building collaborative agent teams.
*   Other **Tool Types** (OpenAPI, Google Cloud Tools, Built-in Tools) for broader integrations.
"""



================================================
FILE: examples/iot/esp32/voice_led_controller/README.md
================================================
# Voice-Controlled LED Ring with the ESP32

This Arduino-based application demonstrates how to capture audio from a microphone, send it to the Gemini API for processing, and control an LED ring based on the API's response.

## Prerequisites

* Arduino IDE
* ESP32 development board
* MAX9814 Microphone module
* Push button
* LED (connected to `LED_PIN`)
* Adafruit NeoPixel ring (connected to `NEOPIXEL_PIN`)
* SD card module (connected to `SD_CS`)
* WiFi network
* [Google Gemini API key](http://aistudio.google.com/app/apikey)

## Hardware Setup

![Wiring Diagram](wiring-diagram.png)

* **MAX9814 Microphone Module -> ESP32**
    * GND -> GND
    * VDD + GAIN (connected together) -> 3.3V
    * OUT -> IO34

* **SD Card Adapter -> ESP32**
    * CS -> IO5
    * SCK -> IO18
    * MOSI -> IO23
    * MISO -> IO19
    * VCC -> 3.3V
    * GND -> GND

* **Button -> ESP32**
    * One end to GND
    * One end split with 10k resistor to 3.3V and IO32

* **LED -> ESP32**
    * Connect the LED's anode (longer leg) to IO33 via a current-limiting resistor (e.g., 10k ohms).
    * Connect the LED's cathode (shorter leg) to GND.

* **Adafruit NeoPixel ring -> ESP32**
    * Connect the NeoPixel's data input pin to IO15.
    * Connect the NeoPixel's VCC pin to 5V.
    * Connect the NeoPixel's GND pin to GND.

* **ESP32 Power:** Ensure the ESP32 is powered via USB or an external 5V source.

## Software Setup

1.  Install the Arduino IDE and the ESP32 board support package.
2.  Install the following Arduino libraries:
    * WiFi
    * FS
    * SD
    * HTTPClient
    * ArduinoJson
    * Adafruit NeoPixel
3.  Load the provided code.
4.  Configure the following variables in the code:
    * `SSID`: Your WiFi network SSID.
    * `PASSWORD`: Your WiFi network password.
    * `API_KEY`: Your Google Gemini API key.
5.  Upload the code to your ESP32 board.

## Usage

1.  Power on the ESP32 board.
2.  Press the push button.
3.  Speak a sentence into the microphone.
4.  The LED will turn on while recording and turn off when finished.
5.  The application will record the audio, convert it to base64, create a JSON request, send it to the Gemini API, and process the response.
6.  The Gemini API will analyze the audio and respond with instructions to control the LEDs.
7.  The NeoPixel ring will either toggle on/off or change colors based on the spoken request.

## Functionality

* **Audio Recording:** The application records audio for a specified duration when the button is pressed and saves it as a WAV file on the SD card.
* **Base64 Encoding:** The audio data is encoded in base64 format for transmission to the Gemini API.
* **Gemini API Interaction:** The application sends a JSON request to the Gemini API with the base64 encoded audio data and instructions.
* **LED Control:** The application interprets the API's response and controls the LED and NeoPixel ring accordingly.
* **Function Calling:** The Gemini api utilizes function calling to allow for the control of the LED's.

## Code Structure

* `setup()`: Initializes the hardware, WiFi, and serial communication.
* `setupWifi()`: Connects to the WiFi network.
* `toggleLights()`: Turns the NeoPixel ring on or off while also handling color changes.
* `recordAudio()`: Records audio from the microphone and saves it to the SD card.
* `writeWavHeader()`: Writes the WAV file header.
* `base64Encode()`: Encodes binary data to base64.
* `createAudioJsonRequest()`: Creates the JSON request for the Gemini API.
* `sendAudio()`: Sends the JSON request to the Gemini API and processes the response.
* `saveAudioString()`: Saves the base64 encoded audio to the SD card.

## Notes

* Ensure that your WiFi network has internet access.
* Verify that your Gemini API key is valid.
* Adjust the `RECORD_DURATION` constant to change the recording length. Longer durations may be too much for the limited memory of the ESP32.
* Adjust the `NEOPIXEL_COUNT` constant to match the amount of neopixels you have.
* The `jsonBufferSize` may need to be adjusted depending on the length of the audio recording.
* The application uses an insecure WiFi client for simplicity. For production use, consider using a secure connection.
* The application disables brownout detection and watchdog timers. This is for testing purposes and may not be suitable for production environments.



================================================
FILE: examples/iot/esp32/voice_led_controller/led_controller.ino
================================================
/*
Copyright 2025 Google LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

#include <Arduino.h>
#include <WiFi.h>
#include <FS.h>
#include <SD.h>
#include <HTTPClient.h>
#include <WiFiClientSecure.h>
#include <ArduinoJson.h>
#include "soc/soc.h"
#include "soc/rtc_cntl_reg.h"
#include <Adafruit_NeoPixel.h>

// Pins
const int SD_CS = 5;
const int AUDIO_PIN = 34;
const int BUTTON_PIN = 32;
const int LED_PIN = 33;
const int NEOPIXEL_PIN = 15

// Configs for LED ring
const int NEOPIXEL_COUNT = 24;
int red = 255;
int green = 255;
int blue = 255;

// Configuration for audio recording
const int SAMPLE_RATE = 8000;
const int BIT_DEPTH = 16;
const int RECORD_DURATION = 2;

Adafruit_NeoPixel pixels(NEOPIXEL_COUNT, NEOPIXEL_PIN, NEO_GRB + NEO_KHZ800);

// WIFI connection
String SSID = "";
String PASSWORD = "";

// Gemini API key
String API_KEY = "";

void setupWifi() {
  WiFi.begin(SSID, PASSWORD);
  while (WiFi.status()!= WL_CONNECTED) {
    delay(1000);
    Serial.print("...");
  }
  Serial.print("IP address: ");
  Serial.println(WiFi.localIP());
}

void toggleLights(bool on) {
  if( on ) {
    Serial.println("Turning on lights");
    int color = 255;
    for (int i = 0; i < NEOPIXEL_COUNT; i++) {
     pixels.setPixelColor(i, pixels.Color(red, green, blue));
     pixels.setBrightness(255);
    }
    pixels.show();
  } else {
    Serial.println("Turning off lights");
    pixels.clear();
    pixels.show(); 
  }
}

void recordAudio() {
  if (!SD.begin(SD_CS, SPI, 1000000)) {
    Serial.println("SD card initialization failed!");
    while (1);
  } else {
    Serial.println("SD card initialized!");
  }

  if (SD.exists("/tmp.wav")) {
    if (SD.remove("/tmp.wav")) {
      Serial.println("Previous audio file deleted.");
    } else {
      Serial.println("Failed to delete previous audio file.");
      return;
    }
  } else {
    Serial.println("No previous audio file detected, starting new");
  }

  File audioFile = SD.open("/tmp.wav", FILE_WRITE);
  if (!audioFile) {
    Serial.println("Failed to create audio file.");
    return;
  }

  Serial.println("Start recording");
  writeWavHeader(audioFile, SAMPLE_RATE, BIT_DEPTH, 1);

  int numSamples = SAMPLE_RATE * RECORD_DURATION;
  for (int i = 0; i < numSamples; i++) {
    int rawValue = analogRead(AUDIO_PIN);
    int16_t sample = map(rawValue, 0, 4095, -32768, 32767);
    audioFile.write((uint8_t*)&sample, 2);
    delayMicroseconds(1000000 / SAMPLE_RATE);
  }

  audioFile.close();
  Serial.println("Audio recorded to /tmp.wav");
}

void writeWavHeader(File& file, int sampleRate, int bitDepth, int channels) {
  uint32_t byteRate = sampleRate * channels * bitDepth / 8;
  uint16_t blockAlign = channels * bitDepth / 8;

  file.write((const uint8_t*)"RIFF", 4);
  uint32_t fileSize = 36 + RECORD_DURATION * byteRate;
  file.write((uint8_t*)&fileSize, 4); 
  file.write((const uint8_t*)"WAVE", 4);
  file.write((const uint8_t*)"fmt ", 4);
  uint32_t subchunk1Size = 16;
  file.write((uint8_t*)&subchunk1Size, 4);
  uint16_t audioFormat = 1;
  file.write((uint8_t*)&audioFormat, 2);
  file.write((uint8_t*)&channels, 2);
  file.write((uint8_t*)&sampleRate, 4);
  file.write((uint8_t*)&byteRate, 4);
  file.write((uint8_t*)&blockAlign, 2);
  file.write((uint8_t*)&bitDepth, 2);
  file.write((const uint8_t*)"data", 4);
  uint32_t subchunk2Size = RECORD_DURATION * byteRate;
  file.write((uint8_t*)&subchunk2Size, 4);
}

String base64Encode(const uint8_t* data, size_t length) {
  const char* b64_alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";
  String encodedString = "";
  uint32_t i = 0;
  uint8_t b1, b2, b3;

  while (i < length) {
    b1 = data[i++];
    encodedString += b64_alphabet[b1 >> 2];
    if (i < length) {
      b2 = data[i++];
      encodedString += b64_alphabet[((b1 & 0x03) << 4) | (b2 >> 4)];
    } else {
      encodedString += b64_alphabet[(b1 & 0x03) << 4];
      encodedString += "==";
      break;
    }
    if (i < length) {
      b3 = data[i++];
      encodedString += b64_alphabet[((b2 & 0x0F) << 2) | (b3 >> 6)];
      encodedString += b64_alphabet[b3 & 0x3F];
    } else {
      encodedString += b64_alphabet[(b2 & 0x0F) << 2];
      encodedString += '=';
      break;
    }
  }
  return encodedString;
}

void createAudioJsonRequest() {
  if (SD.exists("/request-tmp.json")) {
    if (SD.remove("/request-tmp.json")) {
      Serial.println("Previous request file deleted.");
    } else {
      Serial.println("Failed to delete previous request file.");
      return;
    }
  } else {
    Serial.println("No previous request file detected, starting new");
  }

  File stringFile = SD.open("/audiostring.txt", FILE_READ);
  if (!stringFile) {
    Serial.println("Failed to open audiostring.txt for reading");
    return;
  }

  // Read the base64 encoded audio data from the file
  String base64EncodedData = stringFile.readString();
  stringFile.close();

  // Create the JSON document
  const size_t jsonBufferSize = 1024 * 64; // Adjust as needed
  DynamicJsonDocument doc(jsonBufferSize);

  // Set up REST call to call custom functions based on sent audio clip
  JsonArray contents = doc.createNestedArray("contents");
  JsonObject content = contents.createNestedObject();
  JsonArray parts = content.createNestedArray("parts");

  JsonObject textPart = parts.createNestedObject();
  textPart["text"] = "Trigger a function based on this audio input.";

  JsonObject audioPart = parts.createNestedObject();
  JsonObject inlineData = audioPart.createNestedObject("inline_data");
  inlineData["mime_type"] = "audio/x-wav";
  inlineData["data"] = base64EncodedData; // Use the data read from the file

  JsonArray tools = doc.createNestedArray("tools");
  JsonObject tool = tools.createNestedObject();
  JsonArray functionDeclarations = tool.createNestedArray("function_declarations");

  JsonObject changeColor = functionDeclarations.createNestedObject();
  changeColor["name"] = "changeColor";
  changeColor["description"] = "Change the default color for the lights in an RGB format. Example: Green would be 0 255 0.";

  JsonObject parametersChangeColor = changeColor.createNestedObject("parameters");
  parametersChangeColor["type"] = "object";
  JsonObject propertiesChangeColor = parametersChangeColor.createNestedObject("properties");

  JsonObject red = propertiesChangeColor.createNestedObject("red");
  red["type"] = "integer";
  red["description"] = "A value from 0 to 255 for the color RED in an RGB color code";

  JsonObject green = propertiesChangeColor.createNestedObject("green");
  green["type"] = "integer";
  green["description"] = "A value from 0 to 255 for the color GREEN in an RGB color code";

  JsonObject blue = propertiesChangeColor.createNestedObject("blue");
  blue["type"] = "integer";
  blue["description"] = "A value from 0 to 255 for the color BLUE in an RGB color code";

  JsonArray requiredChangeColor = parametersChangeColor.createNestedArray("required");
  requiredChangeColor.add("red");
  requiredChangeColor.add("green");
  requiredChangeColor.add("blue");

  JsonObject toggleLights = functionDeclarations.createNestedObject();
  toggleLights["name"] = "toggleLights";
  toggleLights["description"] = "Turn on or off the lights";

  JsonObject parametersToggleLights = toggleLights.createNestedObject("parameters");
  parametersToggleLights["type"] = "object";
  JsonObject propertiesToggleLights = parametersToggleLights.createNestedObject("properties");

  JsonObject toggle = propertiesToggleLights.createNestedObject("toggle");
  toggle["type"] = "boolean";
  toggle["description"] = "Determine if the lights should be turned on or off.";

  JsonArray requiredToggleLights = parametersToggleLights.createNestedArray("required");
  requiredToggleLights.add("toggle");

  // Open a file on the SD card for writing the JSON request
  File jsonFile = SD.open("/request-tmp.json", FILE_WRITE);
  if (!jsonFile) {
    Serial.println("Failed to open JSON file for writing");
    return;
  }

  // Serialize the JSON document to the file
  serializeJson(doc, jsonFile);
  jsonFile.close();

  Serial.println("JSON request saved to /request-tmp.json");
}

void sendAudio() {
  WiFiClientSecure client;
  client.setInsecure();
  HTTPClient http;

  if (http.begin(client, "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=" + API_KEY)) {
    http.addHeader("Content-Type", "application/json");

    File file = SD.open("/request-tmp.json", FILE_READ);
    if (!file) {
      Serial.println("Failed to open file for reading from SD card");
      return;
    }

    const int BUFFER_SIZE = 64;
    uint8_t fileBuffer[BUFFER_SIZE];

    const int JSON_STRING_SIZE = 65536; // Allocate 64kb for the audio file request. Likely smaller.
    char *jsonString = (char *)malloc(JSON_STRING_SIZE); 
    if (jsonString == NULL) {
      Serial.println("Failed to allocate memory for JSON string");
      file.close();
      return;
    }
    int jsonStringIndex = 0;

    while (file.available()) {
      int bytesRead = file.read(fileBuffer, BUFFER_SIZE);
      for (int i = 0; i < bytesRead && jsonStringIndex < JSON_STRING_SIZE - 1; i++) {
        jsonString[jsonStringIndex++] = fileBuffer[i];
      }
    }
    jsonString[jsonStringIndex] = '\0';

    file.close();
    SD.end(); // Close the SD connection after reading the file
    
    int httpCode = http.POST(jsonString);
    free(jsonString);
    Serial.print(F("Http code: "));
    Serial.println(httpCode);

    if (httpCode == HTTP_CODE_OK) {
      String payload = http.getString();
      DynamicJsonDocument doc(1024);
      DeserializationError error = deserializeJson(doc, payload);

      if (error) {
        Serial.print(F("deserializeJson() failed: "));
        Serial.println(error.c_str());
        return;
      }

      if (doc["candidates"][0]["content"]["parts"][0].containsKey("functionCall") &&
                    doc["candidates"][0]["content"]["parts"][0]["functionCall"].is<JsonObject>()) {

        JsonObject functionCall =
            doc["candidates"][0]["content"]["parts"][0]["functionCall"].as<JsonObject>();

        if (functionCall.containsKey("name")) {
          String functionName = functionCall["name"].as<String>();

          if( functionName == "toggleLights") {
            if (functionCall.containsKey("args") && functionCall["args"].is<JsonObject>()) {
                JsonObject args = functionCall["args"].as<JsonObject>();
                if (args.containsKey("toggle")) {
                  bool toggleValue = args["toggle"].as<bool>();
                  toggleLights(toggleValue);
                } else {
                  Serial.println("Toggle argument not found.");
                }
            } else {
              Serial.println("Args not found in function call.");
            }
          } else if( functionName == "changeColor") {
            if (functionCall.containsKey("args") && functionCall["args"].is<JsonObject>()) {
                JsonObject args = functionCall["args"].as<JsonObject>();
                red = args["red"].as<int>();
                green = args["green"].as<int>();
                blue = args["blue"].as<int>();
                toggleLights(true);
            } else {
              Serial.println("Args not found in function call.");
            }
          } 
        } else {
          Serial.println("Function name not found.");
        }
      } else {
        Serial.println("Function call not found.");
      }
      
    } else {
      Serial.println("HTTP POST request failed");
    }
    http.end();
  } else {
    Serial.println("HTTP begin failed");
  }
}

void saveAudioString() {
  File audioFile = SD.open("/tmp.wav", FILE_READ);
  if (!audioFile) {
    Serial.println("Failed to open audio file for reading");
    return;
  }

  size_t fileSize = audioFile.size();
  uint8_t* audioData = (uint8_t*)malloc(fileSize);
  if (audioData == NULL) {
    Serial.println("Failed to allocate memory for audio data");
    audioFile.close();
    return;
  }
  audioFile.read(audioData, fileSize);
  audioFile.close();

  String base64AudioData = base64Encode(audioData, fileSize);
  free(audioData);

  File stringFile = SD.open("/audiostring.txt", FILE_WRITE);
  if (!stringFile) {
    Serial.println("Failed to open audiostring.txt for writing");
    return;
  }
  stringFile.print(base64AudioData);
  stringFile.close();

  Serial.println("Audio base64 string saved to /audiostring.txt");
}

void setup() {
  WRITE_PERI_REG(RTC_CNTL_BROWN_OUT_REG, 0);
  WRITE_PERI_REG(RTC_CNTL_WDTCONFIG0_REG, 0);

  pinMode(BUTTON_PIN, INPUT_PULLUP);
  pinMode(LED_PIN, OUTPUT);
  pixels.begin();
  pixels.show();

  Serial.begin(115200);
  WiFi.mode(WIFI_STA);
  WiFi.disconnect();
  while (!Serial);
  
  setupWifi();
}

void loop() {
  if (digitalRead(BUTTON_PIN) == LOW) {
    digitalWrite(LED_PIN, HIGH);
    
    // This delay is to debounce the button and allow time to speak
    delay(500); 

    recordAudio();
    digitalWrite(LED_PIN, LOW);
    saveAudioString();
    createAudioJsonRequest();
    sendAudio();
  }
}


================================================
FILE: examples/json_capabilities/README.md
================================================
# Gemini JSON Capabilities

This directory provides a collection of examples demonstrating how to leverage the Gemini API's powerful JSON capabilities for various text processing tasks. Each notebook focuses on a specific application, providing practical code and explanations to help you get started.

**Here's a summary of the available notebooks:**

*   **[Entity Extraction](./Entity_Extraction_JSON.ipynb):** Learn how to extract structured data from text by identifying and classifying entities like people, companies, locations, and more. You define a custom JSON schema to specify the categories you're interested in, and the API returns the extracted entities in a structured JSON format.

*   **[Sentiment Analysis](./Sentiment_Analysis.ipynb):** Discover how to analyze the sentiment of text, such as customer reviews or social media posts, and quantify the positive, negative, and neutral sentiment scores. The notebook demonstrates defining a JSON schema for the sentiment scores and using the API to obtain these scores in a structured JSON response.

*   **[Text Classification](./Text_Classification.ipynb):** Explore how to categorize text into predefined topics or classes. This notebook shows you how to define a JSON schema to represent the topics and their relevance scores, enabling you to automatically classify documents, articles, or any other textual data.

*   **[Text Summarization](./Text_Summarization.ipynb):** Learn how to generate concise summaries of long texts while extracting key information such as characters, locations, and genres. You'll see how to define a comprehensive JSON schema that includes a synopsis, lists of genres, locations, and characters, and use the API to obtain a structured JSON summary of the text.

**Getting Started:**

Each notebook provides step-by-step instructions and runnable code examples. Simply follow the instructions in each notebook to experiment with different texts and schemas.

**Key Benefits of Using JSON with the Gemini API:**

*   **Structured Output:** Obtain results in a well-defined, predictable JSON format, making it easy to integrate with other applications and systems.
*   **Customizable Schemas:** Define your own JSON schemas to tailor the API's output to your specific needs and data requirements.
*   **Simplified Data Processing:** Streamline data extraction and analysis by working with structured data instead of raw text.

This cookbook is designed to help you unlock the full potential of the Gemini API's JSON capabilities and build powerful text processing solutions. Happy coding!



================================================
FILE: examples/json_capabilities/Entity_Extraction_JSON.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Entity Extraction
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/json_capabilities/Entity_Extraction_JSON.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
You will use Gemini to extract all fields that fit one of the predefined classes and label them.
"""

%pip install -U -q "google-genai>=1.0.0"

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Example
"""

from enum import Enum
from typing_extensions import TypedDict # in python 3.12 replace typing_extensions with typing
from google.genai import types

entity_recognition_text = "John Johnson, the CEO of the Oil Inc. and Coal Inc. companies, has unveiled plans to build a new factory in Houston, Texas."
prompt = f"""
Generate list of entities in text based on the following Python class structure:

class CategoryEnum(str, Enum):
    Person = 'Person'
    Company = 'Company'
    State = 'State'
    City = 'City'

class Entity(TypedDict):
  name: str
  category: CategoryEnum

class Entities(TypedDict):
  entities: list[Entity]

{entity_recognition_text}"""
MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config=types.GenerateContentConfig(
        temperature=0,
        response_mime_type="application/json"
    )
  )

import json

print(json.dumps(json.loads(response.text), indent=4))
# Output:
#   {

#       "entities": [

#           {

#               "name": "John Johnson",

#               "category": "Person"

#           },

#           {

#               "name": "Oil Inc.",

#               "category": "Company"

#           },

#           {

#               "name": "Coal Inc.",

#               "category": "Company"

#           },

#           {

#               "name": "Houston",

#               "category": "City"

#           },

#           {

#               "name": "Texas",

#               "category": "State"

#           }

#       ]

#   }


"""
## Summary
You have used the Gemini API to extract entities of predefined categories with their labels. You extracted every person, company, state, and country. You are not limited to these categories, as this should work with any category of your choice.

Please see the other notebooks in this directory to learn more about how you can use the Gemini API for other JSON related tasks.

"""



================================================
FILE: examples/json_capabilities/Sentiment_Analysis.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Sentiment Analysis
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/json_capabilities/Sentiment_Analysis.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
You will use the Gemini to extract sentiment scores of reviews.
"""

%pip install -U -q "google-genai>=1.0.0"

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Example
"""

"""
Start by defining how you want your JSON to be returned and which categories you would like to classify an item by. After that, go ahead and define some examples. In this case, you are trying to classify reviews as positive, neutral, or negative.
"""

import enum
from typing_extensions import TypedDict


class Magnitude(enum.Enum):
  WEAK = "weak"
  STRONG = "strong"


class Sentiment(TypedDict):
  positive_sentiment_score: Magnitude
  negative_sentiment_score: Magnitude
  neutral_sentiment_score: Magnitude


system_instruct = """
Generate each sentiment score probability (positive, negative, or neutral) for the whole text.
"""

negative_review = "This establishment is an insult to the culinary arts, with inedible food that left me questioning the chef's sanity and the health inspector's judgment."
positive_review = "This restaurant is a true gem with impeccable service and a menu that tantalizes the taste buds. Every dish is a culinary masterpiece, crafted with fresh ingredients and bursting with flavor."
neutral_review = "The restaurant offers a decent dining experience with average food and service, making it a passable choice for a casual meal."

"""
Take a look at each of the probabilities returned to see how each of these reviews would be classified by the Gemini model.
"""

"""
Helper function to generate content from sentiment llm:
"""

from google.genai import types

def generate_content(review):
    MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}
    return client.models.generate_content(
        model=MODEL_ID,
        contents=review,
        config=types.GenerateContentConfig(
            system_instruction=system_instruct,
            response_mime_type="application/json",
            response_schema=Sentiment,
        )
    )

from pprint import pprint

response = generate_content(negative_review)
pprint(response.parsed)
# Output:
#   {'negative_sentiment_score': <Magnitude.STRONG: 'strong'>,

#    'neutral_sentiment_score': <Magnitude.WEAK: 'weak'>,

#    'positive_sentiment_score': <Magnitude.WEAK: 'weak'>}


response = generate_content(positive_review)
pprint(response.parsed)
# Output:
#   {'negative_sentiment_score': <Magnitude.WEAK: 'weak'>,

#    'neutral_sentiment_score': <Magnitude.WEAK: 'weak'>,

#    'positive_sentiment_score': <Magnitude.STRONG: 'strong'>}


response = generate_content(neutral_review)
pprint(response.parsed)
# Output:
#   {'negative_sentiment_score': <Magnitude.WEAK: 'weak'>,

#    'neutral_sentiment_score': <Magnitude.STRONG: 'strong'>,

#    'positive_sentiment_score': <Magnitude.WEAK: 'weak'>}


"""
## Summary
You have now used the Gemini API to analyze the sentiment of restaurant reviews using structured data. Try out other types of texts, such as comments under a video or emails.

Please see the other notebooks in this directory to learn more about how you can use the Gemini API for other JSON related tasks.
"""



================================================
FILE: examples/json_capabilities/Text_Classification.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Text Classification

---

"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/json_capabilities/Text_Classification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>

"""

"""
You will use the Gemini API to classify what topics are relevant in the text.
"""

"""
## Install dependencies
"""

%pip install -U -q "google-genai>=1.0.0"

"""
## Set up your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY = userdata.get("GOOGLE_API_KEY")
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Example
"""

from IPython.display import Markdown

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}
response = client.models.generate_content(
    model=MODEL_ID,
    contents="Generate a 5 paragraph article about Sports, include one other topic",
)
article = response.text
Markdown(article)
# Output:
#   <IPython.core.display.Markdown object>

import enum
from typing_extensions import TypedDict  # in python 3.12 replace typing_extensions with typing

from google.genai import types


class Relevance(enum.Enum):
  WEAK = "weak"
  STRONG = "strong"

class Topic(TypedDict):
  topic: str
  relevance: Relevance


sys_int = """
Generate topics from text. Ensure that topics are general e.g. "Health".
Strong relevance is obtained when the topic is a core tenent of the content
and weak relevance reflects one or two mentions.
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=article,
    config=types.GenerateContentConfig(
        system_instruction=sys_int,
        response_mime_type="application/json",
        response_schema=list[Topic],
    )
)

from pprint import pprint

pprint(response.parsed)
# Output:
#   [{'relevance': <Relevance.STRONG: 'strong'>, 'topic': 'Sports'},

#    {'relevance': <Relevance.STRONG: 'strong'>, 'topic': 'Health'},

#    {'relevance': <Relevance.WEAK: 'weak'>, 'topic': 'Economics'},

#    {'relevance': <Relevance.STRONG: 'strong'>, 'topic': 'Social Issues'},

#    {'relevance': <Relevance.WEAK: 'weak'>, 'topic': 'Art'}]


"""
## Summary
Now, you know how to classify text into different categories. Feel free to experiment with other texts, or provide a specific set of possible topics.

Please see the other notebooks in this directory to learn more about how you can use the Gemini API for other JSON related tasks.
"""



================================================
FILE: examples/json_capabilities/Text_Summarization.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Text Summarization

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/json_capabilities/Text_Summarization.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
You will use Gemini API's JSON capabilities to extract characters, locations, and summary of the plot from a story.
"""

%pip install -U -q "google-genai>=1.0.0"
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/141.0 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m133.1/141.0 kB[0m [31m85.0 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m141.0/141.0 kB[0m [31m3.4 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Example
"""

from IPython.display import Markdown

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-1.5-flash-latest","gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true}
prompt = "Generate a 10 paragraph fantasy story. Include at least 2 named characters and 2 named locations. Give as much detail in the story as possible."
response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt
    )
story = response.text
Markdown(story)
# Output:
#   <IPython.core.display.Markdown object>

from typing_extensions import TypedDict  # in python 3.12 replace typing_extensions with typing

class Character(TypedDict):
  name: str
  description: str
  alignment: str

class Location(TypedDict):
  name: str
  description: str

class TextSummary(TypedDict):
  synopsis: str
  genres: list[str]
  locations: list[Location]
  characters: list[Character]

prompt = f"""
Generate summary of the story. With a list of genres locations and characters.

{story}"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config={
        "response_mime_type": "application/json",
        "response_schema": TextSummary
        }
    )

from pprint import pprint

pprint(response.parsed)
# Output:
#   {'characters': [{'alignment': 'Good',

#                    'description': 'A cartographer and adventurer seeking the '

#                                   'lost city of Eldoria.',

#                    'name': 'Elara'},

#                   {'alignment': 'Neutral',

#                    'description': 'A creature of stone and shadow tasked with '

#                                   'protecting the secrets of Eldoria.',

#                    'name': 'Guardian of the Hidden Passage'}],

#    'genres': ['Fantasy', 'Adventure'],

#    'locations': [{'description': 'A treacherous mountain range with jagged '

#                                  'peaks, dangerous paths, and a hidden passage '

#                                  'to Eldoria.',

#                   'name': "Dragon's Tooth Mountains"},

#                  {'description': 'A legendary, vanished civilization rumored to '

#                                  'hold unimaginable riches and powerful magic.',

#                   'name': 'Eldoria (Lost City)'},

#                  {'description': 'A village where tales of Eldoria are whispered '

#                                  'in taverns.',

#                   'name': 'Whisperwind Village'}],

#    'synopsis': 'Elara, a cartographer and adventurer, seeks the lost city of '

#                "Eldoria, following an ancient map through the Dragon's Tooth "

#                'mountains. She faces treacherous terrain and a guardian '

#                'creature, ultimately using music to connect with the guardian '

#                "and realizing that Eldoria's true value lies in its wisdom, not "

#                'material riches. She chooses to leave Eldoria undisturbed, '

#                'gaining newfound courage and respect for legends.'}


"""
## Summary

In this example, you used the Gemini API to extract key information from a story. This information could be fed into a structured database or used as a prompt for other writers to create their own versions.

This technique of converting large open-ended text to structured data works across other formats too, not just stories.

Please see the other notebooks in this directory to learn more about how you can use the Gemini API for other JSON related tasks.
"""



================================================
FILE: examples/langchain/README.md
================================================
# Gemini API LangChain Examples

## Table of contents

This is a collection of fun examples for the Gemini API used with [LangChain](https://python.langchain.com/v0.2/docs/introduction/). 

* [Summarize large documents using LangChain](https://github.com/google-gemini/cookbook/blob/main/examples/langchain/Gemini_LangChain_Summarization_WebLoad.ipynb): Use Gemini large context window to summarize large documents without using RAG. 
* [Chat with SQL using LangChain](https://github.com/google-gemini/cookbook/blob/main/examples/langchain/Chat_with_SQL_using_langchain.ipynb): Querry your database using Gemini API and LangChain. 
* [Code analysis using LangChain and DeepLake](https://github.com/google-gemini/cookbook/blob/main/examples/langchain/Code_analysis_using_Gemini_LangChain_and_DeepLake.ipynb): Use Gemini large context window to analyze all the code of a github prject. 
* [Question Answering using LangChain and Chroma](https://github.com/google-gemini/cookbook/blob/main/examples/langchain/Gemini_LangChain_QA_Chroma_WebLoad.ipynb): Learn how to interface Gemini, LangChain and [Chroma](https://docs.trychroma.com/). 
* [Question Answering using LangChain and Pinecone](https://github.com/google-gemini/cookbook/blob/main/examples/langchain/Gemini_LangChain_QA_Pinecone_WebLoad.ipynb): Learn how to interface Gemini, LangChain and [Pinecone](https://app.pinecone.io/). 

There are even more examples in the [examples](https://github.com/google-gemini/cookbook/tree/main/examples) and [quickstarts](https://github.com/google-gemini/cookbook/tree/main/quickstarts) folders.



================================================
FILE: examples/langchain/Chat_with_SQL_using_langchain.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Chat with SQL using LangChain
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/langchain/Chat_with_SQL_using_langchain.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
Reading an SQL database can be challenging for humans. However, with accurate prompts, Gemini models can generate answers based on the data. Through the use of the Gemini API, you will be able retrieve necessary information by chatting with a SQL database.
"""

%pip install -U -q "google-genai>=1.7.0" langchain langchain-community langchain-google-genai

import sqlite3

from langchain.chains import create_sql_query_chain, LLMChain
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_community.utilities import SQLDatabase
from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool
from operator import itemgetter
from langchain_core.runnables import RunnablePassthrough
from google import genai
from IPython.display import Markdown

"""
## Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.

"""

import os
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

"""
## Setting up the database
To query a database, you first need to set one up.

1. **Load the California Housing Dataset:** Load the dataset from sklearn.datasets and extract it into a DataFrame.

"""

from sklearn.datasets import fetch_california_housing

california_housing_bunch = fetch_california_housing(as_frame=True)
california_housing_df = california_housing_bunch.frame

"""
2. **Connect to the SQLite database:** The database will be stored in the specified file.
"""

conn = sqlite3.connect("mydatabase.db")

# Write the DataFrame to a SQL table named 'housing'.
california_housing_df.to_sql("housing", conn, index=False)
# Output:
#   20640

# Create an SQLDatabase object
db = SQLDatabase.from_uri("sqlite:///mydatabase.db")

"""
## Question to query
With the database connection established, the `SQLDatabase` object now contains information about our database, which the model can access.

You can now start asking the LLM to generate queries.

"""

# you can see what information is available
Markdown(db.get_table_info())
# Output:
#   <IPython.core.display.Markdown object>

# Define query chain
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
write_query_chain = create_sql_query_chain(llm, db)

"""
You use `create_sql_query_chain` that fits our database. It provides default prompts for various types of SQL including Oracle, Google SQL, MySQL and more.


In this case, default prompt is suitable for the task. However, feel free to experiment with writing this part of our chain yourself to suit your preferences.
"""

Markdown(write_query_chain.get_prompts()[0].template)
# Output:
#   <IPython.core.display.Markdown object>

response = write_query_chain.invoke({"question": "What is the total population?"})
display(Markdown(response))
# Output:
#   <IPython.core.display.Markdown object>

db.run('SELECT SUM("Population") FROM housing')
# Output:
#   '[(29421840.0,)]'

"""
Great! The SQL query is correct, but it needs proper formatting before it can be executed directly by the database.

"""

"""
## Validating the query
You will pass the output of the previous query to a model that will extract just the SQL query and ensure its validity.
"""

validate_prompt = PromptTemplate(
    input_variables=["not_formatted_query"],
    template="""
        You are going to receive a text that contains a SQL query. Extract that query.
        Make sure that it is a valid SQL command that can be passed directly to the Database.
        Avoid using Markdown for this task.
        Text: {not_formatted_query}
    """
)

validate_chain = write_query_chain | validate_prompt | llm | StrOutputParser()
validate_chain.invoke({"question": "What is the total population?"})
# Output:
#   'SELECT sum("Population") FROM housing'

"""
## Automatic querying
Now, let's automate the process of querying the database using *QuerySQLDataBaseTool*. This tool can receive text from previous parts of the chain, execute the query, and return the answer.

"""

execute_query = QuerySQLDataBaseTool(db=db)
execute_chain = validate_chain | execute_query
execute_chain.invoke({"question": "What is the total population?"})
# Output:
#   <ipython-input-16-580ecc1223c9>:1: LangChainDeprecationWarning: The class `QuerySQLDataBaseTool` was deprecated in LangChain 0.3.12 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-community package and should be used instead. To use it run `pip install -U :class:`~langchain-community` and import as `from :class:`~langchain_community.tools import QuerySQLDatabaseTool``.

#     execute_query = QuerySQLDataBaseTool(db=db)

#   '[(29421840.0,)]'

"""
## Generating answer
You are almost done!

To enhance our output, you'll use LLM not only to get the number but to get properly formatted and natural language response.
"""

answer_prompt = PromptTemplate.from_template("""
    You are going to receive a original user question, generated SQL query, and result of said query. You should use this information to answer the original question. Use only information provided to you.

    Original Question: {question}
    SQL Query: {query}
    SQL Result: {result}
    Answer: """
)

answer_chain = (
    RunnablePassthrough.assign(query=validate_chain).assign(
        result=itemgetter("query") | execute_query
    )
    | answer_prompt | llm | StrOutputParser()
)

answer_chain.invoke({"question": "What is the total population?"})
# Output:
#   'The total population is 29,421,840.'

"""
## Next steps

Congratulations! You've successfully created a functional chain to interact with SQL. Now, feel free to explore further by asking different questions.
"""



================================================
FILE: examples/langchain/Code_analysis_using_Gemini_LangChain_and_DeepLake.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Code analysis using LangChain and DeepLake
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/langchain/Code_analysis_using_Gemini_LangChain_and_DeepLake.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
This notebook shows how to use Gemini API with [Langchain](https://python.langchain.com/v0.2/docs/introduction/) and [DeepLake](https://www.deeplake.ai/) for code analysis. The notebook will teach you:
- loading and splitting files
- creating a Deeplake database with embedding information
- setting up a retrieval QA chain
"""

"""
### Load dependencies
"""

%pip install -q -U langchain-google-genai langchain-deeplake langchain langchain-text-splitters langchain-community

from glob import glob
from IPython.display import Markdown, display

from langchain.document_loaders import TextLoader
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain_deeplake.vectorstores import DeeplakeVectorStore

"""
### Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GEMINI_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../../quickstarts/Authentication.ipynb) for an example.

"""

import os
from google.colab import userdata
GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')

os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY

"""
## Prepare the files
"""

"""
First, download a [langchain-google](https://github.com/langchain-ai/langchain-google) repository. It is the repository you will analyze in this example.

It contains code integrating Gemini API, VertexAI, and other Google products with langchain.
"""

!git clone https://github.com/langchain-ai/langchain-google

"""
This example will focus only on the integration of Gemini API with langchain and ignore the rest of the codebase.
"""

repo_match = "langchain-google/libs/genai/langchain_google_genai**/*.py"

"""
Each file with a matching path will be loaded and split by `RecursiveCharacterTextSplitter`.
In this example, it is specified, that the files are written in Python. It helps split the files without having documents that lack context.
"""

docs = []
for file in glob(repo_match, recursive=True):
  loader = TextLoader(file, encoding='utf-8')
  splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=2000, chunk_overlap=0)
  docs.extend(loader.load_and_split(splitter))

"""
`Language` Enum provides common separators used in most popular programming languages, it lowers the chances of classes or functions being split in the middle.
"""

# common seperators used for Python files
RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)
# Output:
#   ['\nclass ', '\ndef ', '\n\tdef ', '\n\n', '\n', ' ', '']

"""
## Create the database
The data will be loaded into the memory since the database doesn't need to be permanent in this case and is small enough to fit.

The type of storage used is specified by prefix in the path, in this case by `mem://`.

Check out other types of storage [here](https://docs.activeloop.ai/setup/storage-and-creds/storage-options).
"""

# define path to database
dataset_path = 'mem://deeplake/langchain_google'

# define the embedding model
embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

"""
Everything needed is ready, and now you can create the database. It should not take longer than a few seconds.
"""

db = DeeplakeVectorStore.from_documents(
    dataset_path=dataset_path,
    embedding=embeddings,
    documents=docs,
    overwrite=True
)

"""
## Question Answering
"""

"""
Set-up the document retriever.
"""

retriever = db.as_retriever()
retriever.search_kwargs['distance_metric'] = 'cos'
retriever.search_kwargs['k'] = 20 # number of documents to return

# define the chat model
llm = ChatGoogleGenerativeAI(model = "gemini-2.5-flash")

"""
Now, you can create a chain for Question Answering. In this case, `RetrievalQA` chain will be used.

If you want to use the chat option instead, use `ConversationalRetrievalChain`.
"""

qa = RetrievalQA.from_llm(llm, retriever=retriever)

"""
The chain is ready to answer your questions.

NOTE: `Markdown` is used for improved formatting of the output.
"""

# a helper function for calling retrival chain
def call_qa_chain(prompt):
  response = qa.invoke(prompt)
  display(Markdown(response["result"]))

call_qa_chain("Show hierarchy for _BaseGoogleGenerativeAI. Do not show content of classes.")
# Output:
#   <IPython.core.display.Markdown object>

call_qa_chain("What is the return type of embedding models.")
# Output:
#   <IPython.core.display.Markdown object>

call_qa_chain("What classes are related to Attributed Question and Answering.")
# Output:
#   <IPython.core.display.Markdown object>

call_qa_chain("What are the dependencies of the GenAIAqa class?")
# Output:
#   <IPython.core.display.Markdown object>

"""
## Summary

Gemini API works great with Langchain. The integration is seamless and provides an easy interface for:
- loading and splitting files
- creating DeepLake database with embeddings
- answering questions based on context from files
"""

"""
## What's next?

This notebook showed only one possible use case for langchain with Gemini API. You can find many more [here](../../examples/langchain).
"""



================================================
FILE: examples/langchain/Gemini_LangChain_QA_Chroma_WebLoad.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Question Answering using LangChain and Chroma
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/langchain/Gemini_LangChain_QA_Chroma_WebLoad.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
## Overview

[Gemini](https://ai.google.dev/models/gemini) is a family of generative AI models that lets developers generate content and solve problems. These models are designed and trained to handle both text and images as input.

[LangChain](https://www.langchain.com/) is a data framework designed to make integration of Large Language Models (LLM) like Gemini easier for applications.

[Chroma](https://docs.trychroma.com/) is an open-source embedding database focused on simplicity and developer productivity. Chroma allows users to store embeddings and their metadata, embed documents and queries, and search the embeddings quickly.

In this notebook, you'll learn how to create an application that answers questions using data from a website with the help of Gemini, LangChain, and Chroma.
"""

"""
## Setup

First, you must install the packages and set the necessary environment variables.

### Installation

Install LangChain's Python library, `langchain` and LangChain's integration package for Gemini, `langchain-google-genai`. Next, install Chroma's Python client SDK, `chromadb`.
"""

%pip install --quiet langchain-core==0.1.23
%pip install --quiet langchain==0.1.1
%pip install --quiet langchain-google-genai==0.0.6
%pip install --quiet -U langchain-community==0.0.20
%pip install --quiet chromadb
%pip install --quiet bs4

from langchain import PromptTemplate
from langchain import hub
from langchain.docstore.document import Document
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain.schema.prompt_template import format_document
from langchain.schema.runnable import RunnablePassthrough
from langchain.vectorstores import Chroma

"""
## Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.

"""

import os
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

"""
## Basic steps
LLMs are trained offline on a large corpus of public data. Hence they cannot answer questions based on custom or private data accurately without additional context.

If you want to make use of LLMs to answer questions based on private data, you have to provide the relevant documents as context alongside your prompt. This approach is called Retrieval Augmented Generation (RAG).

You will use this approach to create a question-answering assistant using the Gemini text model integrated through LangChain. The assistant is expected to answer questions about the Gemini model. To make this possible you will add more context to the assistant using data from a website.

In this tutorial, you'll implement the two main components in an RAG-based architecture:

1. Retriever

    Based on the user's query, the retriever retrieves relevant snippets that add context from the document. In this tutorial, the document is the website data.
    The relevant snippets are passed as context to the next stage - "Generator".

2. Generator

    The relevant snippets from the website data are passed to the LLM along with the user's query to generate accurate answers.

You'll learn more about these stages in the upcoming sections while implementing the application.
"""

"""
## Retriever

In this stage, you will perform the following steps:

1. Read and parse the website data using LangChain.

2. Create embeddings of the website data.

    Embeddings are numerical representations (vectors) of text. Hence, text with similar meaning will have similar embedding vectors. You'll make use of Gemini's embedding model to create the embedding vectors of the website data.

3. Store the embeddings in Chroma's vector store.
    
    Chroma is a vector database. The Chroma vector store helps in the efficient retrieval of similar vectors. Thus, for adding context to the prompt for the LLM, relevant embeddings of the text matching the user's question can be retrieved easily using Chroma.

4. Create a Retriever from the Chroma vector store.

    The retriever will be used to pass relevant website embeddings to the LLM along with user queries.
"""

"""
### Read and parse the website data

LangChain provides a wide variety of document loaders. To read the website data as a document, you will use the `WebBaseLoader` from LangChain.

To know more about how to read and parse input data from different sources using the document loaders of LangChain, read LangChain's [document loaders guide](https://python.langchain.com/docs/integrations/document_loaders).
"""

loader = WebBaseLoader("https://blog.google/technology/ai/google-gemini-ai/")
docs = loader.load()

"""
If you only want to select a specific portion of the website data to add context to the prompt, you can use regex, text slicing, or text splitting.

In this example, you'll use Python's `split()` function to extract the required portion of the text. The extracted text should be converted back to LangChain's `Document` format.
"""

# Extract the text from the website data document
text_content = docs[0].page_content

# The text content between the substrings "code, audio, image and video." to
# "Cloud TPU v5p" is relevant for this tutorial. You can use Python's `split()`
# to select the required content.
text_content_1 = text_content.split("code, audio, image and video.",1)[1]
final_text = text_content_1.split("Cloud TPU v5p",1)[0]

# Convert the text to LangChain's `Document` format
docs = [Document(page_content=final_text, metadata={"source": "local"})]

"""
### Initialize Gemini's embedding model

To create the embeddings from the website data, you'll use Gemini's embedding model, **gemini-embedding-001** which supports creating text embeddings.

To use this embedding model, you have to import `GoogleGenerativeAIEmbeddings` from LangChain. To know more about the embedding model, read Google AI's [language documentation](https://ai.google.dev/models/gemini).
"""

from langchain_google_genai import GoogleGenerativeAIEmbeddings

gemini_embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

"""
### Store the data using Chroma

To create a Chroma vector database from the website data, you will use the `from_documents` function of `Chroma`. Under the hood, this function creates embeddings from the documents created by the document loader of LangChain using any specified embedding model and stores them in a Chroma vector database.  

You have to specify the `docs` you created from the website data using LangChain's `WebBasedLoader` and the `gemini_embeddings` as the embedding model when invoking the `from_documents` function to create the vector database from the website data. You can also specify a directory in the `persist_directory` argument to store the vector store on the disk. If you don't specify a directory, the data will be ephemeral in-memory.

"""

# Save to disk
vectorstore = Chroma.from_documents(
                     documents=docs,                 # Data
                     embedding=gemini_embeddings,    # Embedding model
                     persist_directory="./chroma_db" # Directory to save data
                     )

"""
### Create a retriever using Chroma

You'll now create a retriever that can retrieve website data embeddings from the newly created Chroma vector store. This retriever can be later used to pass embeddings that provide more context to the LLM for answering user's queries.


To load the vector store that you previously stored in the disk, you can specify the name of the directory that contains the vector store in `persist_directory` and the embedding model in the `embedding_function` arguments of Chroma's initializer.

You can then invoke the `as_retriever` function of `Chroma` on the vector store to create a retriever.
"""

# Load from disk
vectorstore_disk = Chroma(
                        persist_directory="./chroma_db",       # Directory of db
                        embedding_function=gemini_embeddings   # Embedding model
                   )
# Get the Retriever interface for the store to use later.
# When an unstructured query is given to a retriever it will return documents.
# Read more about retrievers in the following link.
# https://python.langchain.com/docs/modules/data_connection/retrievers/
#
# Since only 1 document is stored in the Chroma vector store, search_kwargs `k`
# is set to 1 to decrease the `k` value of chroma's similarity search from 4 to
# 1. If you don't pass this value, you will get a warning.
retriever = vectorstore_disk.as_retriever(search_kwargs={"k": 1})

# Check if the retriever is working by trying to fetch the relevant docs related
# to the word 'MMLU' (Massive Multitask Language Understanding). If the length is greater than zero, it means that
# the retriever is functioning well.
print(len(retriever.get_relevant_documents("MMLU")))
# Output:
#   1


"""
## Generator

The Generator prompts the LLM for an answer when the user asks a question. The retriever you created in the previous stage from the Chroma vector store will be used to pass relevant embeddings from the website data to the LLM to provide more context to the user's query.

You'll perform the following steps in this stage:

1. Chain together the following:
    * A prompt for extracting the relevant embeddings using the retriever.
    * A prompt for answering any question using LangChain.
    * An LLM model from Gemini for prompting.
    
2. Run the created chain with a question as input to prompt the model for an answer.

"""

"""
### Initialize Gemini

You must import `ChatGoogleGenerativeAI` from LangChain to initialize your model.
 In this example, you will use **gemini-2.0-flash**, as it supports text summarization. To know more about the text model, read Google AI's [language documentation](https://ai.google.dev/models/gemini).

You can configure the model parameters such as ***temperature*** or ***top_p***,  by passing the appropriate values when initializing the `ChatGoogleGenerativeAI` LLM.  To learn more about the parameters and their uses, read Google AI's [concepts guide](https://ai.google.dev/docs/concepts#model_parameters).
"""

from langchain_google_genai import ChatGoogleGenerativeAI

# To configure model parameters use the `generation_config` parameter.
# eg. generation_config = {"temperature": 0.7, "topP": 0.8, "topK": 40}
# If you only want to set a custom temperature for the model use the
# "temperature" parameter directly.

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

"""
### Create prompt templates

You'll use LangChain's [PromptTemplate](https://python.langchain.com/docs/how_to/#prompt-templates) to generate prompts to the LLM for answering questions.

In the `llm_prompt`, the variable `question` will be replaced later by the input question, and the variable `context` will be replaced by the relevant text from the website retrieved from the Chroma vector store.
"""

# Prompt template to query Gemini
llm_prompt_template = """You are an assistant for question-answering tasks.
Use the following context to answer the question.
If you don't know the answer, just say that you don't know.
Use five sentences maximum and keep the answer concise.\n
Question: {question} \nContext: {context} \nAnswer:"""

llm_prompt = PromptTemplate.from_template(llm_prompt_template)

print(llm_prompt)
# Output:
#   input_variables=['context', 'question'] template="You are an assistant for question-answering tasks.\nUse the following context to answer the question.\nIf you don't know the answer, just say that you don't know.\nUse five sentences maximum and keep the answer concise.\n\nQuestion: {question} \nContext: {context} \nAnswer:"


"""
### Create a stuff documents chain

LangChain provides [Chains](https://python.langchain.com/docs/modules/chains/) for chaining together LLMs with each other or other components for complex applications. You will create a **stuff documents chain** for this application. A stuff documents chain lets you combine all the relevant documents, insert them into the prompt, and pass that prompt to the LLM.

You can create a stuff documents chain using the [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language).

To learn more about different types of document chains, read LangChain's [chains guide](https://python.langchain.com/docs/modules/chains/document/).

The stuff documents chain for this application retrieves the relevant website data and passes it as the context to an LLM prompt along with the input question.
"""

# Combine data from documents to readable string format.
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Create stuff documents chain using LCEL.
#
# This is called a chain because you are chaining together different elements
# with the LLM. In the following example, to create the stuff chain, you will
# combine the relevant context from the website data matching the question, the
# LLM model, and the output parser together like a chain using LCEL.
#
# The chain implements the following pipeline:
# 1. Extract the website data relevant to the question from the Chroma
#    vector store and save it to the variable `context`.
# 2. `RunnablePassthrough` option to provide `question` when invoking
#    the chain.
# 3. The `context` and `question` are then passed to the prompt where they
#    are populated in the respective variables.
# 4. This prompt is then passed to the LLM (`gemini-2.0-flash`).
# 5. Output from the LLM is passed through an output parser
#    to structure the model's response.
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | llm_prompt
    | llm
    | StrOutputParser()
)

"""
### Prompt the model

You can now query the LLM by passing any question to the `invoke()` function of the stuff documents chain you created previously.
"""

from IPython.display import Markdown

Markdown(rag_chain.invoke("What is Gemini?"))
# Output:
#   <IPython.core.display.Markdown object>

"""
# Conclusion

That's it. You have successfully created an LLM application that answers questions using data from a website with the help of Gemini, LangChain, and Chroma.
"""



================================================
FILE: examples/langchain/Gemini_LangChain_QA_Pinecone_WebLoad.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Question Answering using LangChain and Pinecone
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/langchain/Gemini_LangChain_QA_Pinecone_WebLoad.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
## Overview

[Gemini](https://ai.google.dev/models/gemini) is a family of generative AI models that lets developers generate content and solve problems. These models are designed and trained to handle both text and images as input.

[LangChain](https://www.langchain.com/) is a data framework designed to make integration of Large Language Models (LLM) like Gemini easier for applications.

[Pinecone](https://www.pinecone.io/) is a cloud-first vector database that allows users to search across billions of embeddings with ultra-low query latency.

In this notebook, you'll learn how to create an application that answers questions using data from a website with the help of Gemini, LangChain, and Pinecone.
"""

"""
## Setup

First, you must install the packages and set the necessary environment variables.

### Installation

Install LangChain's Python library, `langchain` and LangChain's integration package for Gemini, `langchain-google-genai`. Next, install LangChain's integration package for the new version of Pinecone, `langchain-pinecone` and the `pinecone-client`, which is Pinecone's Python SDK. Finally, install `langchain-community` to access the `WebBaseLoader` module later.
"""

%pip install --quiet -U langchain
%pip install --quiet -U langchain-google-genai
%pip install --quiet -U langchain-pinecone
%pip install --quiet -U pinecone
%pip install --quiet -U langchain-community
%pip install --quiet -U bs4

"""
## Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.

"""

import os
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

"""
### Setup Pinecone

To use Pinecone in your application, you must have an API key. To create an API key you have to set up a Pinecone account. Visit [Pinecone's app page](https://app.pinecone.io/), and Sign up/Log in to your account. Then navigate to the "API Keys" section and copy your API key.

For more detailed instructions on getting the API key, you can read Pinecone's [Quickstart documentation](https://docs.pinecone.io/docs/quickstart#2-get-your-api-key).

Set the environment variable `PINECONE_API_KEY` to configure Pinecone to use your API key.

"""

PINECONE_API_KEY=userdata.get('PINECONE_API_KEY')

os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY

"""
## Basic steps
LLMs are trained offline on a large corpus of public data. Hence they cannot answer questions based on custom or private data accurately without additional context.

If you want to make use of LLMs to answer questions based on private data, you have to provide the relevant documents as context alongside your prompt. This approach is called Retrieval Augmented Generation (RAG).

You will use this approach to create a question-answering assistant using the Gemini text model integrated through LangChain. The assistant is expected to answer questions about Gemini model. To make this possible you will add more context to the assistant using data from a website.

In this tutorial, you'll implement the two main components in an RAG-based architecture:

1. Retriever

    Based on the user's query, the retriever retrieves relevant snippets that add context from the document. In this tutorial, the document is the website data.
    The relevant snippets are passed as context to the next stage - "Generator".

2. Generator

    The relevant snippets from the website data are passed to the LLM along with the user's query to generate accurate answers.

You'll learn more about these stages in the upcoming sections while implementing the application.
"""

"""
## Import the required libraries
"""

from langchain import hub
from langchain import PromptTemplate
from langchain.docstore.document import Document
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain.schema.prompt_template import format_document
from langchain.schema.runnable import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone as pc
from pinecone import ServerlessSpec

"""
## Retriever

In this stage, you will perform the following steps:

1. Read and parse the website data using LangChain.

2. Create embeddings of the website data.

    Embeddings are numerical representations (vectors) of text. Hence, text with similar meaning will have similar embedding vectors. You'll make use of Gemini's embedding model to create the embedding vectors of the website data.

3. Store the embeddings in Pinecone's vector store.
    
    Pinecone is a vector database. The Pinecone vector store helps in the efficient retrieval of similar vectors. Thus, for adding context to the prompt for the LLM, relevant embeddings of the text matching the user's question can be retrieved easily using Pinecone.

4. Create a Retriever from the Pinecone vector store.

    The retriever will be used to pass relevant website embeddings to the LLM along with user queries.
"""

"""
### Read and parse the website data

LangChain provides a wide variety of document loaders. To read the website data as a document, you will use the `WebBaseLoader` from LangChain.

To know more about how to read and parse input data from different sources using the document loaders of LangChain, read LangChain's [document loaders guide](https://python.langchain.com/docs/integrations/document_loaders).
"""

loader = WebBaseLoader("https://blog.google/technology/ai/google-gemini-ai/")
docs = loader.load()

"""
If you only want to select a specific portion of the website data to add context to the prompt, you can use regex, text slicing, or text splitting.

In this example, you'll use Python's `split()` function to extract the required portion of the text. The extracted text should be converted back to LangChain's `Document` format.
"""

# Extract the text from the website data document
text_content = docs[0].page_content
# The text content between the substrings "code, audio, image and video." to
# "Cloud TPU v5p" is relevant for this tutorial. You can use Python's `split()`
# to select the required content.
text_content_1 = text_content.split("code, audio, image and video.",1)[1]
final_text = text_content_1.split("Cloud TPU v5p",1)[0]

# Convert the text to LangChain's `Document` format
docs = [Document(page_content=final_text, metadata={"source": "local"})]

"""
### Initialize Gemini's embedding model

To create the embeddings from the website data, you'll use Gemini's embedding model, **gemini-embedding-001** which supports creating text embeddings.

To use this embedding model, you have to import `GoogleGenerativeAIEmbeddings` from LangChain. To know more about the embedding model, read Google AI's [language documentation](https://ai.google.dev/models/gemini).
"""

from langchain_google_genai import GoogleGenerativeAIEmbeddings

gemini_embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

"""
### Store the data using Pinecone


To create a Pinecone vector database, first, you have to initialize your Pinecone client connection using the API key you set previously.

In Pinecone, vector embeddings have to be stored in indexes. An index represents the vector data's top-level organizational unit. The vectors in any index must have the same dimensionality and distance metric for calculating similarity. You can read more about indexes in [Pinecone's Indexes documentation](https://docs.pinecone.io/docs/indexes).

First, you'll create an index using Pinecone's `create_index` function. Pinecone allows you to create two types of indexes, Serverless indexes and Pod-based indexes. Pinecone's free starter plan lets you create only one project and one pod-based starter index with sufficient resources to support 100,000 vectors. For this tutorial, you have to create a pod-based starter index. To know more about different indexes and how they can be created, read Pinecone's [create indexes guide](https://docs.pinecone.io/docs/new-api#creating-indexes).


Next, you'll insert the documents you extracted earlier from the website data into the newly created index using LangChain's `Pinecone.from_documents`. Under the hood, this function creates embeddings from the documents created by the document loader of LangChain using any specified embedding model and inserts them into the specified index in a Pinecone vector database.  

You have to specify the `docs` you created from the website data using LangChain's `WebBasedLoader` and the `gemini_embeddings` as the embedding model when invoking the `from_documents` function to create the vector database from the website data.
"""

# Initialize Pinecone client

pine_client= pc(
    api_key = os.getenv("PINECONE_API_KEY"),  # API key from app.pinecone.io
    )
index_name = "langchain-demo"

# First, check if the index already exists. If it doesn't, create a new one.
if index_name not in pine_client.list_indexes().names():
    # Create a new index.
    # https://docs.pinecone.io/docs/new-api#creating-a-starter-index
    print("Creating index")
    pine_client.create_index(name=index_name,
                      # `cosine` distance metric compares different documents
                      # for similarity.
                      # Read more about different distance metrics from
                      # https://docs.pinecone.io/docs/indexes#distance-metrics.
                      metric="cosine",
                      # The Gemini embedding model `gemini-embedding-001` uses
                      # 3072 dimensions.
                      dimension=3072,
                      # The `pod_type` is the type of pod to use.
                      # Read more about different pod types from
                      # https://docs.pinecone.io/docs/pod-types.
                      # Specify the pod details.
                      spec=ServerlessSpec(
                        cloud="aws",
                        region="us-east-1"
                        ),
    )
    print(pine_client.describe_index(index_name))

vectorstore = PineconeVectorStore.from_documents(docs,
                      gemini_embeddings, index_name=index_name)

# Output:
#   Creating index

#   {'deletion_protection': 'disabled',

#    'dimension': 3072,

#    'host': 'langchain-demo-u4710cy.svc.aped-4627-b74a.pinecone.io',

#    'metric': 'cosine',

#    'name': 'langchain-demo',

#    'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},

#    'status': {'ready': True, 'state': 'Ready'},

#    'tags': None,

#    'vector_type': 'dense'}


"""
### Create a retriever using Pinecone

You'll now create a retriever that can retrieve website data embeddings from the newly created Pinecone vector store. This retriever can be later used to pass embeddings that provide more context to the LLM for answering user's queries.

Invoke the `as_retriever` function of the vector store you initialized in the last step, to create a retriever.
"""

retriever = vectorstore.as_retriever()
# Check if the retriever is working by trying to fetch the relevant docs related
# to the word 'MMLU'(Massive Multitask Language Understanding). If the length is
# greater than zero, it means that the retriever is functioning well.
print(len(retriever.invoke("MMLU")))
# Output:
#   1


"""
## Generator

The Generator prompts the LLM for an answer when the user asks a question. The retriever you created in the previous stage from the Pinecone vector store will be used to pass relevant embeddings from the website data to the LLM to provide more context to the user's query.

You'll perform the following steps in this stage:

1. Chain together the following:
    * A prompt for extracting the relevant embeddings using the retriever.
    * A prompt for answering any question using LangChain.
    * An LLM model from Gemini for prompting.
    
2. Run the created chain with a question as input to prompt the model for an answer.

"""

"""
### Initialize Gemini

You must import `ChatGoogleGenerativeAI` from LangChain to initialize your model.
 In this example, you will use **gemini-2.0-flash**, as it supports text summarization. To know more about the text model, read Google AI's [language documentation](https://ai.google.dev/models/gemini).

You can configure the model parameters such as ***temperature*** or ***top_p***,  by passing the appropriate values when initializing the `ChatGoogleGenerativeAI` LLM.  To learn more about the parameters and their uses, read Google AI's [concepts guide](https://ai.google.dev/docs/concepts#model_parameters).
"""

from langchain_google_genai import ChatGoogleGenerativeAI

# To configure model parameters use the `generation_config` parameter.
# eg. generation_config = {"temperature": 0.7, "topP": 0.8, "topK": 40}
# If you only want to set a custom temperature for the model use the
# "temperature" parameter directly.

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

"""
### Create prompt templates

You'll use LangChain's [PromptTemplate](https://python.langchain.com/docs/how_to/#prompt-templates) to generate prompts to the LLM for answering questions.

In the `llm_prompt`, the variable `question` will be replaced later by the input question, and the variable `context` will be replaced by the relevant text from the website retrieved from the Pinecone vector store.
"""

# Prompt template to query Gemini
llm_prompt_template = """You are an assistant for question-answering tasks.
Use the following context to answer the question.
If you don't know the answer, just say that you don't know.
Use five sentences maximum and keep the answer concise.

Question: {question}
Context: {context}
Answer:"""

llm_prompt = PromptTemplate.from_template(llm_prompt_template)

print(llm_prompt)
# Output:
#   input_variables=['context', 'question'] input_types={} partial_variables={} template="You are an assistant for question-answering tasks.\nUse the following context to answer the question.\nIf you don't know the answer, just say that you don't know.\nUse five sentences maximum and keep the answer concise.\n\nQuestion: {question}\nContext: {context}\nAnswer:"


"""
### Create a stuff documents chain

LangChain provides [Chains](https://python.langchain.com/docs/modules/chains/) for chaining together LLMs with each other or other components for complex applications. You will create a **stuff documents chain** for this application. A stuff documents chain lets you combine all the relevant documents, insert them into the prompt, and pass that prompt to the LLM.

You can create a stuff documents chain using the [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language).

To learn more about different types of document chains, read LangChain's [chains guide](https://python.langchain.com/docs/modules/chains/document/).

The stuff documents chain for this application retrieves the relevant website data and passes it as the context to an LLM prompt along with the input question.
"""

# Combine data from documents to readable string format.
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Create stuff documents chain using LCEL.
# This is called a chain because you are chaining
# together different elements with the LLM.
# In the following example, to create a stuff chain,
# you will combine content, prompt, LLM model, and
# output parser together like a chain using LCEL.
#
# The chain implements the following pipeline:
# 1. Extract data from documents and save to the variable `context`.
# 2. Use the `RunnablePassthrough` option to provide question during invoke.
# 3. The `context` and `question` are then passed to the prompt and
#    input variables in the prompt are populated.
# 4. The prompt is then passed to the LLM (`gemini-2.0-flash`).
# 5. Output from the LLM is passed through an output parser
#    to structure the model response.
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | llm_prompt
    | llm
    | StrOutputParser()
)

"""
### Prompt the model

You can now query the LLM by passing any question to the `invoke()` function of the stuff documents chain you created previously.
"""

from IPython.display import Markdown

Markdown(rag_chain.invoke("What is Gemini?"))
# Output:
#   <IPython.core.display.Markdown object>

"""
## Summary

Gemini API works great with Langchain. The integration is seamless and provides an easy interface for:
- loading and splitting files
- creating Pinecone database with embeddings
- answering questions based on context from files
"""

"""
## What's next?

This notebook showed only one possible use case for langchain with Gemini API. You can find many more [here](../../examples/langchain).
"""



================================================
FILE: examples/langchain/Gemini_LangChain_Summarization_WebLoad.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Summarize large documents using LangChain
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/langchain/Gemini_LangChain_Summarization_WebLoad.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
## Overview

The [Gemini](https://ai.google.dev/models/gemini) models are a family of generative AI models that allow developers generate content and solve problems. These models are designed and trained to handle both text and images as input.

[LangChain](https://www.langchain.com/) is a framework designed to make integration of Large Language Models (LLM) like Gemini easier for applications.

In this notebook, you'll learn how to create an application to summarize large documents using the Gemini API and LangChain.

"""

"""
## Setup

First, you must install the packages and set the necessary environment variables.

### Installation

Install LangChain's Python library, `langchain` and LangChain's integration package for the Gemini API, `langchain-google-genai`. Installing `langchain-community` allows you to use the `WebBaseLoader` tool shown later in this example.
"""

%pip install --quiet langchain-core==0.1.23
%pip install --quiet langchain==0.1.1
%pip install --quiet langchain-google-genai==0.0.6
%pip install --quiet -U langchain-community==0.0.20
# Output:
#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m241.2/241.2 kB[0m [31m2.9 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m55.4/55.4 kB[0m [31m2.9 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m53.0/53.0 kB[0m [31m3.8 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m802.4/802.4 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.0/2.0 MB[0m [31m42.2 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m49.2/49.2 kB[0m [31m3.4 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m302.9/302.9 kB[0m [31m19.9 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.0/2.0 MB[0m [31m48.7 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.0/2.0 MB[0m [31m53.4 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.0/2.0 MB[0m [31m67.6 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.9/1.9 MB[0m [31m39.5 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.9/1.9 MB[0m [31m51.3 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.9/1.9 MB[0m [31m37.3 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.9/1.9 MB[0m [31m40.1 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.9/1.9 MB[0m [31m45.7 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.8/1.8 MB[0m [31m56.8 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.8/1.8 MB[0m [31m56.0 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.8/1.8 MB[0m [31m60.9 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.8/1.8 MB[0m [31m78.1 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.8/1.8 MB[0m [31m80.7 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/1.7 MB[0m [31m76.1 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/1.7 MB[0m [31m80.6 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/1.7 MB[0m [31m69.3 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/1.7 MB[0m [31m76.3 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/1.7 MB[0m [31m83.8 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m146.9/146.9 kB[0m [31m2.9 MB/s[0m eta [36m0:00:00[0m

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m598.7/598.7 kB[0m [31m6.7 MB/s[0m eta [36m0:00:00[0m

#   [?25h

from langchain import PromptTemplate
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain.schema.prompt_template import format_document

"""
## Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.

"""

import os
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

"""
## Summarize text

In this tutorial, you are going to summarize the text from a website using the Gemini model integrated through LangChain.

You'll perform the following steps to achieve the same:
1. Read and parse the website data using LangChain.
2. Chain together the following:
    * A prompt for extracting the required input data from the parsed website data.
    * A prompt for summarizing the text using LangChain.
    * An LLM model (such as the Gemini model) for prompting.

3. Run the created chain to prompt the model for the summary of the website data.
"""

"""
### Read and parse the website data

LangChain provides a wide variety of document loaders. To read the website data as a document, you will use the `WebBaseLoader` from LangChain.

To know more about how to read and parse input data from different sources using the document loaders of LangChain, read LangChain's [document loaders guide](https://python.langchain.com/docs/integrations/document_loaders).
"""

loader = WebBaseLoader("https://blog.google/technology/ai/google-gemini-ai/#sundar-note")
docs = loader.load()

"""
### Initialize the Gemini model

You must import the `ChatGoogleGenerativeAI` LLM from LangChain to initialize your model.

In this example you will use Gemini 2.0 Flash, (`gemini-2.0-flash`), as it supports text summarization. To know more about this model and the other models availabe, read Google AI's [language documentation](https://ai.google.dev/models/gemini).

You can configure the model parameters such as `temperature` or `top_p`,  by passing the appropriate values when creating the `ChatGoogleGenerativeAI` LLM.  To learn more about the parameters and their uses, read Google AI's [concepts guide](https://ai.google.dev/docs/concepts#model_parameters).
"""

from langchain_google_genai import ChatGoogleGenerativeAI

# To configure model parameters use the `generation_config` parameter.
# eg. generation_config = {"temperature": 0.7, "topP": 0.8, "topK": 40}
# If you only want to set a custom temperature for the model use the
# "temperature" parameter directly.

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

"""
### Create prompt templates

You'll use LangChain's [`PromptTemplate`](https://python.langchain.com/docs/how_to/#prompt-templates) to generate prompts for summarizing the text.

To summarize the text from the website, you will need the following prompts.
1. Prompt to extract the data from the output of `WebBaseLoader`, named `doc_prompt`
2. Prompt for the Gemini model to summarize the extracted text, named `llm_prompt`.

In the `llm_prompt`, the variable `text` will be replaced later by the text from the website.
"""

# To extract data from WebBaseLoader
doc_prompt = PromptTemplate.from_template("{page_content}")

# To query Gemini
llm_prompt_template = """Write a concise summary of the following:
"{text}"
CONCISE SUMMARY:"""
llm_prompt = PromptTemplate.from_template(llm_prompt_template)

print(llm_prompt)
# Output:
#   input_variables=['text'] template='Write a concise summary of the following:\n"{text}"\nCONCISE SUMMARY:'


"""
### Create a Stuff documents chain

LangChain provides [Chains](https://python.langchain.com/docs/modules/chains/) for chaining together LLMs with each other or other components for complex applications. You will create a **Stuff documents chain** for this application. A **Stuff documents chain** lets you combine all the documents, insert them into the prompt and pass that prompt to the LLM.

You can create a Stuff documents chain using the [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language).

To learn more about different types of document chains, read LangChain's [chains guide](https://python.langchain.com/docs/modules/chains/document/).
"""

# Create Stuff documents chain using LCEL.
# This is called a chain because you are chaining
# together different elements with the LLM.
# In the following example, to create stuff chain,
# you will combine content, prompt, LLM model and
# output parser together like a chain using LCEL.
#
# The chain implements the following pipeline:
# 1. Extract data from documents and save to variable `text`.
# 2. This `text` is then passed to the prompt and input variable
#    in prompt is populated.
# 3. The prompt is then passed to the LLM (Gemini).
# 4. Output from the LLM is passed through an output parser
#    to structure the model response.

stuff_chain = (
    # Extract data from the documents and add to the key `text`.
    {
        "text": lambda docs: "\n\n".join(
            format_document(doc, doc_prompt) for doc in docs
        )
    }
    | llm_prompt         # Prompt for Gemini
    | llm                # Gemini API function
    | StrOutputParser()  # output parser
)

"""
### Prompt the model

To generate the summary of the  the website data, pass the documents extracted using the `WebBaseLoader` (`docs`) to `invoke()`.
"""

stuff_chain.invoke(docs)
# Output:
#   "Google has introduced Gemini, its most capable AI model yet. Gemini is multimodal, meaning it can understand and interact with various forms of information, including text, code, audio, images, and video. It comes in three sizes: Ultra (for complex tasks), Pro (for a wide range of tasks), and Nano (for on-device tasks). Gemini surpasses existing models in performance benchmarks across various domains, including natural language understanding, reasoning, and coding. \n\nGoogle emphasizes Gemini's safety and responsibility features, including comprehensive bias and toxicity evaluation, adversarial testing, and collaboration with external experts.  \n\nGemini is being integrated into various Google products, such as Bard, Pixel, Search, and Ads, and will be available to developers through APIs. \n\nThe release of Gemini marks a significant milestone in AI development, opening up new possibilities for innovation and enhancing human capabilities in various areas. \n"

"""
# Conclusion

That's it. You have successfully created an LLM application to summarize text using LangChain and the Gemini API.
"""



================================================
FILE: examples/llamaindex/README.md
================================================
# LlamaIndex Integration with Gemini

This example demonstrates how to leverage LlamaIndex alongside Gemini models. LlamaIndex is a versatile data framework that simplifies the process of connecting Large Language Models (LLMs) to your custom data sources.

**[Gemini_LlamaIndex_QA_Chroma_WebPageReader.ipynb](./Gemini_LlamaIndex_QA_Chroma_WebPageReader.ipynb)** provides a concrete demonstration of how to build a question answering system.  In this specific example, the notebook utilizes LlamaIndex to ingest and process data from a website, enabling you to ask questions about its content using the power of Gemini.

This example offers a glimpse into how LlamaIndex streamlines the process of integrating external knowledge into LLM applications, opening up a wide range of possibilities for building intelligent systems.



================================================
FILE: examples/llamaindex/Gemini_LlamaIndex_QA_Chroma_WebPageReader.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Question Answering LlamaIndex and Chroma
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/llamaindex/Gemini_LlamaIndex_QA_Chroma_WebPageReader.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
## Overview

[Gemini](https://ai.google.dev/models/gemini) is a family of generative AI models that lets developers generate content and solve problems. These models are designed and trained to handle both text and images as input.

[LlamaIndex](https://www.llamaindex.ai/) is a simple, flexible data framework that can be used by Large Language Model(LLM) applications to connect custom data sources to LLMs.

[Chroma](https://docs.trychroma.com/) is an open-source embedding database focused on simplicity and developer productivity. Chroma allows users to store embeddings and their metadata, embed documents and queries, and search the embeddings quickly.

In this notebook, you'll learn how to create an application that answers questions using data from a website with the help of Gemini, LlamaIndex, and Chroma.
"""

"""
## Setup

First, you must install the packages and set the necessary environment variables.

### Installation

Install LlamaIndex's Python library, `llama-index`. Install LlamaIndex's integration package for Gemini, `llama-index-llms-gemini` and the integration package for Gemini embedding model, `llama-index-embeddings-gemini`. Next, install LlamaIndex's web page reader, `llama-index-readers-web`. Finally, install ChromaDB's Python client SDK, `chromadb` and
"""

%pip install -q -U llama-index
%pip install -q -U llama-index-llms-google-genai
%pip install -q -U llama-index-embeddings-google-genai
%pip install -q -U llama-index-readers-web
%pip install -q -U llama-index-vector-stores-chroma
%pip install -q -U chromadb
%pip install -q -U bs4

"""
## Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.

"""

import os
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

"""
## Basic steps
LLMs are trained offline on a large corpus of public data. Hence they cannot answer questions based on custom or private data accurately without additional context.

If you want to make use of LLMs to answer questions based on private data, you have to provide the relevant documents as context alongside your prompt. This approach is called Retrieval Augmented Generation (RAG).

You will use this approach to create a question-answering assistant using the Gemini text model integrated through LlamaIndex. The assistant is expected to answer questions about Google's Gemini model. To make this possible you will add more context to the assistant using data from a website.

In this tutorial, you'll implement the two main components in a RAG-based architecture:

1. Retriever

    Based on the user's query, the retriever retrieves relevant snippets that add context from the document. In this tutorial, the document is the website data.
    The relevant snippets are passed as context to the next stage - "Generator".

2. Generator

    The relevant snippets from the website data are passed to the LLM along with the user's query to generate accurate answers.

You'll learn more about these stages in the upcoming sections while implementing the application.
"""

"""
## Import the required libraries
"""

from bs4 import BeautifulSoup
from IPython.display import Markdown, display
from llama_index.core import Document
from llama_index.core import Settings
from llama_index.core import SimpleDirectoryReader
from llama_index.core import StorageContext
from llama_index.core import VectorStoreIndex
from llama_index.readers.web import SimpleWebPageReader
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

import chromadb
import re

"""
## 1. Retriever

In this stage, you will perform the following steps:

1. Read and parse the website data using LlamaIndex.

2. Create embeddings of the website data.

    Embeddings are numerical representations (vectors) of text. Hence, text with similar meaning will have similar embedding vectors. You'll make use of Gemini's embedding model to create the embedding vectors of the website data.

3. Store the embeddings in Chroma's vector store.
    
    Chroma is a vector database. The Chroma vector store helps in the efficient retrieval of similar vectors. Thus, for adding context to the prompt for the LLM, relevant embeddings of the text matching the user's question can be retrieved easily using Chroma.

4. Create a Retriever from the Chroma vector store.

    The retriever will be used to pass relevant website embeddings to the LLM along with user queries.
"""

"""
### Read and parse the website data

LlamaIndex provides a wide variety of data loaders. To read the website data as a document, you will use the `SimpleWebPageReader` from LlamaIndex.

To know more about how to read and parse input data from different sources using the data loaders of LlamaIndex, read LlamaIndex's [loading data guide](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html).
"""

web_documents = SimpleWebPageReader().load_data(
    ["https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"]
)

# Extract the content from the website data document
html_content = web_documents[0].text

"""
You can use variety of HTML parsers to extract the required text from the html content.

In this example, you'll use Python's `BeautifulSoup` library to parse the website data. After processing, the extracted text should be converted back to LlamaIndex's `Document` format.
"""

# Parse the data.
soup = BeautifulSoup(html_content, 'html.parser')
p_tags = soup.find_all('p')
text_content = ""
for each in p_tags:
    text_content += each.text + "\n"

# Convert back to Document format
documents = [Document(text=text_content)]

"""
### Initialize Gemini's embedding model

To create the embeddings from the website data, you'll use Gemini's embedding model, **gemini-embedding-001** which supports creating text embeddings.

To use this embedding model, you have to import `GeminiEmbedding` from LlamaIndex. To know more about the embedding model, read Google AI's [language documentation](https://ai.google.dev/models/gemini).
"""

gemini_embedding_model = GoogleGenAIEmbedding(model_name="models/gemini-embedding-001")

"""
### Initialize Gemini

You must import `Gemini` from LlamaIndex to initialize your model.
 In this example, you will use **gemini-2.0-flash**, as it supports text summarization. To know more about the text model, read Google AI's [model documentation](https://ai.google.dev/models/gemini).

You can configure the model parameters such as ***temperature*** or ***top_p***,  using the  ***generation_config*** parameter when initializing the `Gemini` LLM.  To learn more about the model parameters and their uses, read Google AI's [concepts guide](https://ai.google.dev/docs/concepts#model_parameters).
"""

from llama_index.llms.google_genai import GoogleGenAI

# To configure model parameters use the `generation_config` parameter.
# eg. generation_config = {"temperature": 0.7, "topP": 0.8, "topK": 40}
# If you only want to set a custom temperature for the model use the
# "temperature" parameter directly.

llm = GoogleGenAI(model_name="models/gemini-2.5-flash")

"""
### Store the data using Chroma

 Next, you'll store the embeddings of the website data in Chroma's vector store using LlamaIndex.

 First, you have to initiate a Python client in `chromadb`. Since the plan is to save the data to the disk, you will use the `PersistentClient`. You can read more about the different clients in Chroma in the [client reference guide](https://docs.trychroma.com/reference/Client).

After initializing the client, you have to create a Chroma collection. You'll then initialize the `ChromaVectorStore` class in LlamaIndex using the collection created in the previous step.

Next, you have to set `Settings` and create storage contexts for the vector store.

`Settings` is a collection of commonly used resources that are utilized during the indexing and querying phase in a LlamaIndex pipeline. You can specify the LLM, Embedding model, etc that will be used to create the application in the `Settings`. To know more about `Settings`, read the [module guide for Settings](https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings.html).

`StorageContext` is an abstraction offered by LlamaIndex around different types of storage. To know more about storage context, read the [storage context API guide](https://docs.llamaindex.ai/en/stable/api_reference/storage.html).

The final step is to load the documents and build an index over them. LlamaIndex offers several indices that help in retrieving relevant context for a user query. Here you'll use the `VectorStoreIndex` since the website embeddings have to be stored in a vector store.

To create the index you have to pass the storage context along with the documents to the `from_documents` function of `VectorStoreIndex`.
The `VectorStoreIndex` uses the embedding model specified in the `Settings` to create embedding vectors from the documents and stores these vectors in the vector store specified in the storage context. To know more about the
`VectorStoreIndex` you can read the [Using VectorStoreIndex guide](https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index.html).
"""

# Create a client and a new collection
client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = client.get_or_create_collection("quickstart")

# Create a vector store
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

# Create a storage context
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# Set Global settings
Settings.llm = llm
Settings.embed_model = gemini_embedding_model

# Create an index from the documents and save it to the disk.
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)

"""
### Create a retriever using Chroma

You'll now create a retriever that can retrieve data embeddings from the newly created Chroma vector store.

First, initialize the `PersistentClient` with the same path you specified while creating the Chroma vector store. You'll then retrieve the collection `"quickstart"` you created previously from Chroma. You can use this collection to initialize the `ChromaVectorStore` in which you store the embeddings of the website data. You can then use the `from_vector_store` function of `VectorStoreIndex` to load the index.
"""

from IPython.display import Markdown

# Load from disk
load_client = chromadb.PersistentClient(path="./chroma_db")

# Fetch the collection
chroma_collection = load_client.get_collection("quickstart")

# Fetch the vector store
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

# Get the index from the vector store
index = VectorStoreIndex.from_vector_store(
    vector_store
)

# Check if the retriever is working by trying to fetch the relevant docs related
# to the phrase 'MMLU' (Multimodal Machine Learning Understanding).
# If the length is greater than zero, it means that the retriever is
# functioning well.
# You can ask questions about your data using a generic interface called
# a query engine. You have to use the `as_query_engine` function of the
# index to create a query engine and use the `query` function of query engine
# to inquire the index.
test_query_engine = index.as_query_engine()
response = test_query_engine.query("AIME")
Markdown(response.response)
# Output:
#   <IPython.core.display.Markdown object>

"""
## 2. Generator

The Generator prompts the LLM for an answer when the user asks a question. The retriever you created in the previous stage from the Chroma vector store will be used to pass relevant embeddings from the website data to the LLM to provide more context to the user's query.

You'll perform the following steps in this stage:

1. Create a prompt for answering any question using LlamaIndex.
    
2. Use a query engine to ask a question and prompt the model for an answer.
"""

"""
### Create prompt templates

You'll use LlamaIndex's [PromptTemplate](https://docs.llamaindex.ai/en/stable/module_guides/models/prompts.html) to generate prompts to the LLM for answering questions.

In the `llm_prompt`, the variable `query_str` will be replaced later by the input question, and the variable `context_str` will be replaced by the relevant text from the website retrieved from the Chroma vector store.
"""

from llama_index.core import PromptTemplate

template = (
    """ You are an assistant for question-answering tasks.
Use the following context to answer the question.
If you don't know the answer, just say that you don't know.
Use five sentences maximum and keep the answer concise.\n
Question: {query_str} \nContext: {context_str} \nAnswer:"""
)
llm_prompt = PromptTemplate(template)

"""
### Prompt the model using Query Engine

You will use the `as_query_engine` function of the `VectorStoreIndex` to create a query engine from the index using the `llm_prompt` passed as the value for the `text_qa_template` argument. You can then use the `query` function of the query engine to prompt the LLM. To know more about custom prompting in LlamaIndex, read LlamaIndex's [prompts usage pattern documentation](https://docs.llamaindex.ai/en/stable/module_guides/models/prompts/usage_pattern.html#defining-a-custom-prompt).
"""

# Query data from the persisted index
query_engine = index.as_query_engine(text_qa_template=llm_prompt)
response = query_engine.query("What is Gemini?")
Markdown(response.response)
# Output:
#   <IPython.core.display.Markdown object>

"""
## What's next?

This notebook showed only one possible use case for langchain with Gemini API. You can find many more [here](../../examples/langchain).
"""



================================================
FILE: examples/mlflow/README.md
================================================
# MLflow Integration with Gemini
[MLflow](https://mlflow.org/) is an open-source tool for comprehensive management of the Machine Learning Lifecycle. Here's the foundational components of MLflow:

- Tracking: MLflow Tracking provides both an API and UI dedicated to the logging of parameters, code versions, metrics, and artifacts during the ML process. This centralized repository captures details such as parameters, metrics, artifacts, data, and environment configurations, giving teams insight into their models’ evolution over time. Whether working in standalone scripts, notebooks, or other environments, Tracking facilitates the logging of results either to local files or a server, making it easier to compare multiple runs across different users.
- Model Registry: A systematic approach to model management, the Model Registry assists in handling different versions of models, discerning their current state, and ensuring smooth productionization. It offers a centralized model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, including model lineage, versioning, aliasing, tagging, and annotations.
- MLflow Tracing: MLflow provides a tracing feature that enhances observability in your AI applications by capturing detailed information about the execution of your application's services. Tracing provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to easily pinpoint the source of bugs and unexpected behaviors.
- Evaluate: Designed for in-depth model analysis, this set of tools facilitates objective model comparison, be it traditional ML algorithms or cutting-edge LLMs.

MLflow provides a native tracing integration with Google Gen AI SDK that automatically generate traces for your interactions with Gemini models. Please refer to [MLflow Gemini integration](https://mlflow.org/docs/latest/tracing/integrations/gemini) for more details.


================================================
FILE: examples/mlflow/MLflow_Observability.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2024 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: LLM Observability with MLflow
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/mlflow/MLflow_Observability.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>

"""

"""
## Overview
"""

"""
[MLflow](https://mlflow.org/) is an open-source platform to assist machine learning practitioners and teams in handling the complexities of the machine learning process.

It provides [MLflow Tracing](https://mlflow.org/docs/latest/tracing/) that enhances LLM observability in your Generative AI applications by capturing detailed information about the execution of your application’s services. Tracing provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to easily pinpoint the source of bugs and unexpected behaviors.

MLflow provides a built-in integration with Google Gen AI SDK that enables you to instrument your Gemini calls easily. This cookbook describes the basic usage of the MLflow tracing integration with the `google-genai` package.
"""

"""
<!-- Community Contributor Badge -->
<table>
  <tr>
    <!-- Author Avatar Cell -->
    <td bgcolor="#d7e6ff">
      <a href="https://github.com/TomeHirata" target="_blank" title="View Tomu Hirata's profile on GitHub">
        <img src="https://github.com/TomeHirata.png?size=100"
             alt="TomeHirata's GitHub avatar"
             width="100"
             height="100">
      </a>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#d7e6ff">
      <h2><font color='black'>This notebook was contributed by <a href="https://github.com/TomeHirata" target="_blank"><font color='#217bfe'><strong>Tomu Hirata</strong></font></a>.</font></h2>
      <!-- Footer -->
      <font color='black'><small><em>Have a cool Gemini example? Feel free to <a href="https://github.com/google-gemini/cookbook/blob/main/CONTRIBUTING.md" target="_blank"><font color="#078efb">share it too</font></a>!</em></small></font>
    </td>
  </tr>
</table>
"""

"""
## Installation
Install Google Gen AI SDK (`google-genai`) and MLflow (`mlflow`). See [troubleshooting for MLFlow installation](https://mlflow.org/docs/latest/quickstart_drilldown/#quickstart_drilldown_install) for other install options.
"""

# Integration with google-genai is supported mlflow >= 2.20.3
%pip install -q google-genai "mlflow>=2.20.3"
# Output:
#   

#   [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m24.3.1[0m[39;49m -> [0m[32;49m25.0.1[0m

#   [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m

#   Note: you may need to restart the kernel to use updated packages.


"""
## Create Gemini Client with your API key

Let's create an API client and pass your API key. If you do not have API ket yet, visit [AI Studio](https://aistudio.google.com/app/apikey) to create one.
"""

import google.genai as genai
from google.colab import userdata

client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))

"""
## Tracking Server
There are several options to run MLflow tracking server: local tracking server, Databricks Free Trial, and production Databricks. See [our documentation](https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html) for the comparison. In this example, you use [Databricks Free Trial](https://mlflow.org/docs/latest/getting-started/databricks-trial/) which allows easy connection from Colab notebooks and enable you to use managed MLflow for free. Follow the steps below to create an account and generate a Personal Access Token (PAT) to connect to your workspace.

* Go to the [Databricks Trial Signup Page](http://signup.databricks.com/) and create your account
* Follow the steps in [this guide](https://docs.databricks.com/aws/en/dev-tools/auth/pat) to create a PAT for your Databricks workspace user
* You need following information to connect to your workspace from your notebook
  * Databricks Host: Use "https://\<your workspace host\>.cloud.databricks.com/
  * Token: Your personal access token for your Databricks Workspace. 
"""

WORKSPACE_EMAIL = userdata.get("WORKSPACE_EMAIL") # your email
WORKSPACE_PAT = userdata.get("WORKSPACE_PAT") # your databricks host https://<your workspace host>.cloud.databricks.com/
WORKSPACE_URL = userdata.get("WORKSPACE_URL") # your PAT

"""
Then, set the tracking server uri and experiment name.
"""

import os
import mlflow
mlflow.set_tracking_uri('databricks')
mlflow.set_registry_uri('databricks-uc')
os.environ['DATABRICKS_HOST'] = WORKSPACE_URL
os.environ['DATABRICKS_TOKEN'] = WORKSPACE_PAT
mlflow.login()

# Create Experiment
mlflow.create_experiment(
    f"/Users/{WORKSPACE_EMAIL}/gemini",
    artifact_location="dbfs:/Volumes/workspace/default/gemini",
)
mlflow.set_experiment(f"/Users/{WORKSPACE_EMAIL}/gemini")

"""
## Enable AutoLogging
MLflow Tracing provides automatic tracing capability for Google Gemini. By enabling auto tracing for Gemini by calling the `mlflow.gemini.autolog()` function, MLflow will capture nested traces and log them to the active MLflow Experiment upon invocation of Gemini Python SDK.

MLflow trace automatically captures the following information about Gemini calls:
- Prompts and completion responses
- Latencies
- Model name
- Additional metadata such as temperature, max_tokens, if specified.
- Function calling if returned in the response
- Any exception if raised
"""

import mlflow

# Turn on auto tracing for Gemini
mlflow.gemini.autolog()

"""
## Call Simple Content Generation
Let's run `client.models.generate_content` to try a simple text generation use case with MLflow tracing. For Jupyter Notebook users, MLflow provides a convenient way to see the generated traces on your notebook. See [this blog](https://mlflow.org/blog/mlflow-tracing-in-jupyter) for more information. For users who use other platforms, visit "http://localhost:5000" to see MLflow UI.
"""

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

result = client.models.generate_content(
    model=MODEL_ID, contents="The opposite of hot is"
)
result.text
# Output:
#   'cold\n'

"""
<img src="https://i.imgur.com/xWGZXtZ.png" />
"""

"""
## Multi-Turn Chat Interactions
MLflow tracing captures the structure of your interactions with the Gemini API. Run the following cell to try multi-turn chat and see how MLflow captures the interaction.
"""

with mlflow.start_span(name="multi-turn"):
    chat = client.chats.create(model=MODEL_ID)
    response = chat.send_message("In one sentence, explain how a computer works to a young child.")
    print(response.text)
    response = chat.send_message("Okay, how about a more detailed explanation to a high schooler?")
    print(response.text)
# Output:
#   A computer follows instructions, like a recipe, to do things with numbers and pictures.

#   

#   A computer works by executing a sequence of instructions, called a program, written in a language it understands. These instructions manipulate binary data (0s and 1s) representing information, performing calculations, storing and retrieving data from memory and storage devices, and interacting with input and output devices like the keyboard, screen, and network to complete tasks.

#   


"""
<img src="https://i.imgur.com/ftJ3CEp.png">
"""

"""
## Function Call
If your application uses [function calling](https://ai.google.dev/gemini-api/docs/function-calling) of the Gemini API, the function definition, function call and function response are captured by MLflow automatically.
"""

def add(a: float, b: float):
    """returns a + b."""
    return a + b


def subtract(a: float, b: float):
    """returns a - b."""
    return a - b


def multiply(a: float, b: float):
    """returns a * b."""
    return a * b


def divide(a: float, b: float):
    """returns a / b."""
    return a / b


operation_tools = [add, subtract, multiply, divide]

chat = client.chats.create(
    model = MODEL_ID,
    config = {
        "tools": operation_tools,
    }
)

response = chat.send_message(
    "I have 57 cats, each owns 44 mittens, how many mittens is that in total?"
)
response.text
# Output:
#   '2508\n'

"""
<img src="https://i.imgur.com/mJeryzg.png"/>
"""

"""
## Conclusion

That's all for this cookbook. MLflow tracing provides many features that are not included in this notebook and actively releases new features. Visit [MLflow Gemini tracing integration](https://mlflow.org/docs/latest/tracing/integrations/gemini) for more configurations and [MLflow Tracing Overview](https://mlflow.org/docs/latest/tracing/) for general offerings.
"""



================================================
FILE: examples/prompting/README.md
================================================
# Prompting Examples

This directory contains a collection of examples demonstrating various prompting techniques for use with the Gemini API. Each notebook focuses on a specific approach to guide the model's output and achieve desired results.

## Notebooks

Here's a breakdown of the notebooks available and the concepts they cover:

*   **[`Adding_context_information.ipynb`](./Adding_context_information.ipynb):** Shows how to provide context for more relevant and accurate responses.

*   **[`Basic_Classification.ipynb`](./Basic_Classification.ipynb):** Demonstrates how to use prompting for classification tasks, categorizing content.

*   **[`Basic_Code_Generation.ipynb`](./Basic_Code_Generation.ipynb):** Demonstrates basic code generation, including error handling and generating code snippets.

*   **[`Basic_Evaluation.ipynb`](./Basic_Evaluation.ipynb):** Shows how to use the LLM for evaluation, providing feedback and grading of text.

*   **[`Basic_Information_Extraction.ipynb`](./Basic_Information_Extraction.ipynb):** Demonstrates extracting information from text and returning it in a defined structure.

*   **[`Basic_Reasoning.ipynb`](./Basic_Reasoning.ipynb):** Demonstrates instructing the model to solve reasoning problems.

*   **[`Chain_of_thought_prompting.ipynb`](./Chain_of_thought_prompting.ipynb):** Guides the model through intermediate reasoning steps for complex problems.

*   **[`Few_shot_prompting.ipynb`](./Few_shot_prompting.ipynb):** Provides a few input-output examples to guide the model.

*   **[`Providing_base_cases.ipynb`](./Providing_base_cases.ipynb):** Shows how providing base cases can influence the output.

*   **[`Role_prompting.ipynb`](./Role_prompting.ipynb):** Demonstrates how to assign a specific role to the model to influence responses.

*   **[`Self_ask_prompting.ipynb`](./Self_ask_prompting.ipynb):** Demonstrates a technique where the model asks itself questions to help answer the query.

*   **[`Zero_shot_prompting.ipynb`](./Zero_shot_prompting.ipynb):** Demonstrates prompting the model *without* any specific examples.

## General Tips

*   **Experiment:** The best way to learn prompting is to experiment.
*   **Iterate:** Prompt engineering is often an iterative process.
*   **Explore Other Techniques:** The "Next steps" section of many of these notebooks encourages exploring other examples in the repository.



================================================
FILE: examples/prompting/Adding_context_information.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Adding context information
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Adding_context_information.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
While LLMs are trained extensively on various documents and data, the LLM does not know everything. New information or information that is not easily accessible cannot be known by the LLM, unless it was specifically added to its corpus of knowledge somehow. For this reason, it is sometimes necessary to provide the LLM, with information and context necessary to answer our queries by providing additional context.
"""

%pip install -U -q "google-genai>=1.0.0"
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/144.7 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m143.4/144.7 kB[0m [31m8.3 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m144.7/144.7 kB[0m [31m2.6 MB/s[0m eta [36m0:00:00[0m

#   [?25h

from google import genai

from IPython.display import Markdown

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Additionally, select the model you want to use from the available options below:
"""

MODEL_ID = "gemini-2.5-flash"  # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-flash","gemini-2.5-pro"] {"allow-input": true, "isTemplate": true}

"""
## Example

Let's say you provide some statistics from a recent Olympics competition, and this data wasn't used to train the LLM. Insert it into the prompt, and input the prompt to the model.
"""

# the list as of April 2024
prompt = """
  QUERY: provide a list of atheletes that competed in olympics exactly 9 times.
  CONTEXT:

  Table title: Olympic athletes and number of times they've competed
  Ian Millar, 10
  Hubert Raudaschl, 9
  Afanasijs Kuzmins, 9
  Nino Salukvadze, 9
  Piero d'Inzeo, 8
  Raimondo d'Inzeo, 8
  Claudia Pechstein, 8
  Jaqueline Mourão, 8
  Ivan Osiier, 7
  François Lafortune, Jr, 7

"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt
    )

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

While some information may be easily searchable online without the use of an LLM, consider data that is not found on the internet, such as private documentation, quickbooks, and forums. Use this code as a template to help you input that information into the Gemini model.

Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own data, or try some of the other prompting techniques such as few-shot prompting.
"""



================================================
FILE: examples/prompting/Basic_Classification.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Basic classification

This notebook demonstrates how to use prompting to perform classification tasks using the Gemini API's Python SDK.
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Basic_Classification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
LLMs can be used in tasks that require classifying content into predefined categories. This business case shows how it categorizes user messages under the blog topic. It can classify replies in the following categories: spam, abusive comments, and offensive messages.
"""

%pip install -U -q "google-genai"

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Examples
"""

from google.genai import types

classification_system_prompt = """
  As a social media moderation system, your task is to categorize user
  comments under a post. Analyze the comment related to the topic and
  classify it into one of the following categories:

  Abusive
  Spam
  Offensive

  If the comment does not fit any of the above categories,
  classify it as: Neutral.

  Provide only the category as a response without explanations.
"""

generation_config = types.GenerateContentConfig(
    temperature=0,
    system_instruction=classification_system_prompt
)

# Define a template that you will reuse in examples below
classification_template = """
  Topic: What can I do after highschool?
  Comment: You should do a gap year!
  Class: Neutral

  Topic: Where can I buy a cheap phone?
  Comment: You have just won an IPhone 15 Pro Max!!! Click the link to receive the prize!!!
  Class: Spam

  Topic: How long do you boil eggs?
  Comment: Are you stupid?
  Class: Offensive

  Topic: {topic}
  Comment: {comment}
  Class:
"""

from IPython.display import Markdown

spam_topic = """
  I am looking for a vet in our neighbourhood.
  Can anyone recommend someone good? Thanks.
"""
spam_comment = "You can win 1000$ by just following me!"
spam_prompt = classification_template.format(
    topic=spam_topic,
    comment=spam_comment
)

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=spam_prompt,
    config=generation_config
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

neutral_topic = "My computer froze. What should I do?"
neutral_comment = "Try turning it off and on."

neutral_prompt = classification_template.format(
    topic=neutral_topic,
    comment=neutral_comment
)

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=neutral_prompt,
    config=generation_config
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own datasets.
"""



================================================
FILE: examples/prompting/Basic_Code_Generation.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Basic code generation

This notebook demonstrates how to use prompting to perform basic code generation using the Gemini API's Python SDK. Two use cases are explored: error handling and code generation.
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Basic_Code_Generation.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
The Gemini API can be a great tool to save you time during the development process. Tasks such as code generation, debugging, or optimization can be done with the assistance of the Gemini model.
"""

%pip install -U -q "google-genai>=1.0.0"

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Additionally, select the model you want to use from the available options below:

"""

MODEL_ID = "gemini-2.5-flash-lite-preview-06-17"  # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-flash","gemini-2.5-pro"] {"allow-input": true, "isTemplate": true}

"""
## Examples
"""

"""
### Error handling
"""

"""
For code generation, you should prioritize accuracy over creativity.
A temperature of 0 ensures that the generated content is deterministic,
producing the most sensible output every time.
"""

from google.genai import types

error_handling_system_prompt =f"""
  Your task is to explain exactly why this error occurred and how to fix it.
"""

error_handling_model_config = types.GenerateContentConfig(
    temperature=0,
    system_instruction=error_handling_system_prompt
)

from IPython.display import Markdown

error_message = """
    1 my_list = [1,2,3]
  ----> 2 print(my_list[3])

  IndexError: list index out of range
"""

error_prompt = f"""
  You've encountered the following error message:
  Error Message: {error_message}
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=error_prompt,
    config=error_handling_model_config
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
### Code generation
"""

code_generation_system_prompt = f"""
  You are a coding assistant. Your task is to generate a code snippet that
  accomplishes a specific goal. The code snippet must be concise, efficient,
  and well-commented for clarity. Consider any constraints or requirements
  provided for the task.

  If the task does not specify a programming language, default to Python.
"""

code_generation_model_config = types.GenerateContentConfig(
    temperature= 0,
    system_instruction=code_generation_system_prompt
  )

code_generation_prompt = """
  Create a countdown timer that ticks down every second and prints
  "Time is up!" after 20 seconds
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=code_generation_prompt,
    config=code_generation_model_config
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
Let's check if generated code works.
"""

import re
matchFound = re.search(r"python\n(.*?)```", response.text, re.DOTALL)
if matchFound:
  code = matchFound.group(1)
  exec(code)
# Output:
#   20 seconds remaining...

#   19 seconds remaining...

#   18 seconds remaining...

#   17 seconds remaining...

#   16 seconds remaining...

#   15 seconds remaining...

#   14 seconds remaining...

#   13 seconds remaining...

#   12 seconds remaining...

#   11 seconds remaining...

#   10 seconds remaining...

#   9 seconds remaining...

#   8 seconds remaining...

#   7 seconds remaining...

#   6 seconds remaining...

#   5 seconds remaining...

#   4 seconds remaining...

#   3 seconds remaining...

#   2 seconds remaining...

#   1 seconds remaining...

#   Time is up!


"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try writing prompts around your own code as well using the examples in this notebook.
"""



================================================
FILE: examples/prompting/Basic_Evaluation.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Basic evaluation
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Basic_Evaluation.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
Gemini API's Python SDK can be used for various forms of evaluation, including:
- Providing feedback based on selected criteria
- Comparing multiple texts
- Assigning grades or confidence scores
- Identifying weak areas

Below is an example of using the LLM to enhance text quality through feedback and grading.
"""

%pip install -U -q "google-genai>=1.7.0"

from google import genai
from google.genai import types
from IPython.display import Markdown

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Example

Start by defining some system instructions for this problem. For demonstration purposes, the use case involves prompting the model to write an essay with some mistakes. Remember that for generation tasks like writing an essay, you can change the temperature of the model to get more creative answers.
"""

student_system_prompt = """
    You're a college student. Your job is to write an essay riddled with common mistakes and a few major ones.
    The essay should have mistakes regarding clarity, grammar, argumentation, and vocabulary.
    Ensure your essay includes a clear thesis statement. You should write only an essay, so do not include any notes.
"""

MODEL_ID="gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

student_response = client.models.generate_content(
    model=MODEL_ID,
    contents='Write an essay about the benefits of reading.',
    config=types.GenerateContentConfig(
        system_instruction=student_system_prompt
    ),
)

essay = student_response.text
Markdown(essay)
# Output:
#   <IPython.core.display.Markdown object>

teacher_system_prompt = """
    As a teacher, you are tasked with grading students' essays.
    Please follow these instructions for evaluation:

    1. Evaluate the essay on a scale of 1-5 based on the following criteria:
       - Thesis statement,
       - Clarity and precision of language,
       - Grammar and punctuation,
       - Argumentation

    2. Write a corrected version of the essay, addressing any identified issues
       in the original submission. Point out what changes were made.
"""
teacher_response = client.models.generate_content(
    model=MODEL_ID,
    contents=essay,
    config=types.GenerateContentConfig(
        system_instruction=teacher_system_prompt
    ),
)


Markdown(teacher_response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try writing your own prompts for evaluating texts.
"""



================================================
FILE: examples/prompting/Basic_Information_Extraction.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Basic information extraction
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Basic_Information_Extraction.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This example notebook shows how Gemini API's Python SDK can be used to extract information from a block of text and return it in defined structure.

In this notebook, the LLM is given a recipe and is asked to extract all the ingredients to create a shopping list. According to best practices, complex tasks will be executed better if divided into separate steps, such as:

1. First, the model will extract all the groceries into a list.

2. Then, you will prompt it to convert this list into a shopping list.

You can find more tips for writing prompts [here](https://ai.google.dev/gemini-api/docs/prompting-intro).

"""

%pip install -U -q "google-genai>=1.0.0"

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Additionally, select the model you want to use from the available options below:
"""

MODEL_ID = "gemini-2.5-flash"  # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-flash","gemini-2.5-pro"] {"allow-input": true, "isTemplate": true}

"""
## Example

First, start by extracting all the groceries. To dod this, set the system instructions when defining the model
"""

from google.genai import types

groceries_system_prompt = f"""
  Your task is to extract to a list all the groceries with its quantities based on the provided recipe.
  Make sure that groceries are in the order of appearance.
"""

grocery_extraction_config =  types.GenerateContentConfig(
    system_instruction=groceries_system_prompt
)

"""
Next, the recipe is defined. You will pass the recipe into `generate_content`, and see that the list of groceries was successfully extracted from the input.
"""

recipe = """
  Step 1:
  Grind 3 garlic cloves, knob of fresh ginger, roughly chopped, 3 spring onions to a paste in a food processor.
  Add 2 tbsp of clear honey, juice from one orange, 1 tbsp of light soy sauce and 2 tbsp of vegetable oil, then blend again.
  Pour the mixture over the cubed chicken from 4 small breast fillets and leave to marnate for at least 1hr.
  Toss in the 20 button mushrooms for the last half an hour so the take on some of the flavour, too.

  Step 2:
  Thread the chicken, 20 cherry tomatoes, mushrooms and 2 large red peppers onto 20 wooden skewers,
  then cook on a griddle pan for 7-8 mins each side or until the chicken is thoroughly cooked and golden brown.
  Turn the kebabs frequently and baste with the marinade from time to time until evenly cooked.
  Arrange on a platter, and eat with your fingers.
"""

grocery_list = client.models.generate_content(
    model=MODEL_ID,
    contents=recipe,
    config=grocery_extraction_config
)
print(grocery_list.text)
# Output:
#   - 3 garlic cloves

#   - knob of fresh ginger

#   - 3 spring onions

#   - 2 tbsp of clear honey

#   - 1 orange

#   - 1 tbsp of light soy sauce

#   - 2 tbsp of vegetable oil

#   - 4 small chicken breast fillets

#   - 20 button mushrooms

#   - 20 cherry tomatoes

#   - 2 large red peppers

#   


"""
The next step is to further format the shopping list based on the ingredients extracted.
"""

shopping_list_system_prompt = """
  You are given a list of groceries. Complete the following:
  - Organize groceries into categories for easier shopping.
  - List each item one under another with a checkbox [].
"""

shopping_list_config = types.GenerateContentConfig(
    system_instruction=shopping_list_system_prompt
)

"""
Now that you have defined the instructions, you can also decide how you want to format your grocery list. Give the prompt a couple examples, or perform few-shot prompting, so it understands how to format your grocery list.
"""

from IPython.display import Markdown

shopping_list_prompt = f"""
  LIST: 3 tomatoes, 1 turkey, 4 tomatoes
  OUTPUT:
  ## VEGETABLES
  - [ ] 7 tomatoes
  ## MEAT
  - [ ] 1 turkey

  LIST: {grocery_list.text}
  OUTPUT:
"""

shopping_list = client.models.generate_content(
    model=MODEL_ID,
    contents=shopping_list_prompt,
    config=shopping_list_config
)

Markdown(shopping_list.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try creating your own prompts for information extraction or adapt the ones provided in the notebook.
"""



================================================
FILE: examples/prompting/Basic_Reasoning.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Basic reasoning

This notebook demonstrates how to use prompting to perform reasoning tasks using the Gemini API's Python SDK. In this example, you will work through a mathematical word problem using prompting.
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Basic_Reasoning.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
The Gemini API can handle many tasks that involve indirect reasoning, such as solving mathematical or logical proofs.

In this example, you will see how the LLM explains given problems step by step.
"""

%pip install -U -q "google-genai>=1.0.0"
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/144.7 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m144.7/144.7 kB[0m [31m4.1 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Additionally, select the model you want to use from the available options below:
"""

MODEL_ID = "gemini-2.5-flash"  # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-flash","gemini-2.5-pro"] {"allow-input": true, "isTemplate": true}

"""
## Examples

Begin by defining some system instructions that will be include when you define and choose the model.
"""

from google.genai import types

system_prompt = """
  You are a teacher solving mathematical and logical problems. Your task:
  1. Summarize given conditions.
  2. Identify the problem.
  3. Provide a clear, step-by-step solution.
  4. Provide an explanation for each step.

  Ensure simplicity, clarity, and correctness in all steps of your explanation.
  Each of your task should be done in order and seperately.
"""

config = types.GenerateContentConfig(
    system_instruction=system_prompt
)

"""
Next, you can define a logical problem such as the one below.
"""

from IPython.display import Markdown

logical_problem = """
  Assume a world where 1 in 5 dice are weighted and have 100% to roll a 6.
  A person rolled a dice and rolled a 6.
  Is it more likely that the die was weighted or not?
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=logical_problem,
    config=config,
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

math_problem = """
  Given a triangle with base b=6 and height h=8, calculate its area
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=math_problem,
    config=config,
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try creating your own prompts that include instructions on how to solve basic reasoning problems, or use the prompt given in this notebook as a template.
"""



================================================
FILE: examples/prompting/Chain_of_thought_prompting.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Chain of thought prompting
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Chain_of_thought_prompting.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
Using chain of thought helps the LLM take a logical and arithmetic approach. Instead of outputting the answer immediately, the LLM uses smaller and easier steps to get to the answer.
"""

%pip install -U -q "google-genai>=1.0.0"

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Additionally, select the model you want to use from the available options below:
"""

MODEL_ID = "gemini-2.5-flash"  # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-flash","gemini-2.5-pro"] {"allow-input": true, "isTemplate": true}

"""
## Example

Sometimes LLMs can return non-satisfactory answers. To simulate that behavior, you can implement a phrase like "Return the answer immediately" in your prompt.

Without this, the model sometimes uses chain of thought by itself, but it is inconsistent and does not always result in the correct answer.
"""

from IPython.display import Markdown

prompt = """
  5 people can create 5 donuts every 5 minutes. How much time would it take
  25 people to make 100 donuts? Return the answer immediately.
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
To influence this you can implement chain of thought into your prompt and look at the difference in the response. Note the multiple steps within the prompt.
"""

prompt = """
  Question: 11 factories can make 22 cars per hour. How much time would it take 22 factories to make 88 cars?
  Answer: A factory can make 22/11=2 cars per hour. 22 factories can make 22*2=44 cars per hour. Making 88 cars would take 88/44=2 hours. The answer is 2 hours.
  Question: 5 people can create 5 donuts every 5 minutes. How much time would it take 25 people to make 100 donuts?
  Answer:
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own data, or try some of the other prompting techniques such as few-shot prompting.
"""



================================================
FILE: examples/prompting/Few_shot_prompting.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Few-shot prompting
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Few_shot_prompting.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
Some prompts may need a bit more information or require a specific output schema for the LLM to understand and accomplish the requested task. In such cases, providing example questions with answers to the model may greatly increase the quality of the response.
"""

%pip install -U -q "google-genai>=1.0.0"

from google import genai
from google.genai import types

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

client=genai.Client(api_key=GOOGLE_API_KEY)

"""
## Examples

Use Gemini Flash as your model to run through the following examples.
"""

MODEL_ID="gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

prompt = """
    Sort the animals from biggest to smallest.
    Question: Sort Tiger, Bear, Dog
    Answer: Bear > Tiger > Dog}
    Question: Sort Cat, Elephant, Zebra
    Answer: Elephant > Zebra > Cat}
    Question: Sort Whale, Goldfish, Monkey
    Answer:
"""

respose=client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

print(respose.text)
# Output:
#   Whale > Monkey > Goldfish

#   


prompt = """
    Extract cities from text, include country they are in.
    USER: I visited Mexico City and Poznan last year
    MODEL: {"Mexico City": "Mexico", "Poznan": "Poland"}
    USER: She wanted to visit Lviv, Monaco and Maputo
    MODEL: {"Minsk": "Ukraine", "Monaco": "Monaco", "Maputo": "Mozambique"}
    USER: I am currently in Austin, but I will be moving to Lisbon soon
    MODEL:
"""

respose=client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config=types.GenerateContentConfig(
        response_mime_type= 'application/json',
    ),
)

print(respose.text)
# Output:
#   {"Austin": "USA", "Lisbon": "Portugal"}


"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own data, or try some of the other prompting techniques such as zero-shot prompting.
"""



================================================
FILE: examples/prompting/Providing_base_cases.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Providing base cases
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Providing_base_cases.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
LLMs require specific instructions to provide the expected results. Because of this, it is vital to ensure that the model knows how it should behave when it lacks information or when it should not answer a given query and provide a default response instead.
"""

%pip install -U -q "google-generativeai>=0.7.2"

import google.generativeai as genai

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)

"""
## Examples
"""

"""
Let's go ahead and define the model, as well as give the model a template for how it should answer the question.
"""

instructions = """
You are an assistant that helps tourists around the world to plan their vacation. Your responsibilities are:
1. Helping book the hotel.
2. Recommending restaurants.
3. Warning about potential dangers.

If other request is asked return "I cannot help with this request."
"""

model = genai.GenerativeModel(model_name='gemini-2.0-flash', system_instruction=instructions)

print("ON TOPIC:", model.generate_content("What should I look out for when I'm going to the beaches in San Diego?").text)
print("OFF TOPIC:", model.generate_content("What bowling places do you recommend in Moscow?").text)
# Output:
#   ON TOPIC: Here are some things to look out for when visiting the beaches in San Diego:

#   

#   * **Strong currents:**  San Diego has beautiful beaches, but the Pacific Ocean can have strong currents.  Always swim near a lifeguard and be aware of your surroundings.

#   * **Sunburns:**  The sun is strong in San Diego, even on cloudy days.  Use sunscreen, wear a hat, and take breaks from the sun.

#   * **Rip currents:** These powerful currents can quickly pull swimmers out to sea. If caught in a rip current, don't fight against it. Swim parallel to the shore until you're out of the current, then swim back to shore.

#   * **Sea lions:**  You may encounter sea lions on the beach. They can be aggressive, so keep a safe distance.

#   * **Jellyfish:**  Jellyfish are common in San Diego waters, especially in the summer. Wear protective footwear in the water and avoid touching jellyfish.

#   

#   **Enjoy your time on the beaches of San Diego!** 

#   

#   OFF TOPIC: I cannot help with this request. 

#   


"""
Let's try another template.
"""

instructions = """
You are an assistant at a library. Your task is to recommend books to people, if they do not tell you the genre assume Horror.
"""

model = genai.GenerativeModel(model_name='gemini-2.0-flash', system_instruction=instructions)

print("## Specified genre:

", model.generate_content("Could you recommend me 3 books with hard magic system?").text, sep="\n")
print("## Not specified genre:

", model.generate_content("Could you recommend me 2 books?").text, sep="\n")
# Output:
#   ## Specified genre:


#   Of course! I'd be happy to recommend some books with hard magic systems. 

#   

#   Here are three recommendations:

#   

#   1. **Mistborn: The Final Empire** by Brandon Sanderson: This is a classic of the genre, with a meticulously crafted magic system based on "Allomancy," where people ingest metals to gain different powers. It's a thrilling story with a complex world and memorable characters.

#   

#   2. **The Lies of Locke Lamora** by Scott Lynch: This fantasy novel features a fascinating magic system based on the "The Gentleman Bastards," a group of con artists who use a mixture of wit, cunning, and magic to pull off elaborate heists. 

#   

#   3. **Jonathan Strange & Mr Norrell** by Susanna Clarke: This book features a unique and detailed magic system based on the "English magicians" who use a combination of rituals, incantations, and magical objects to perform their spells. It's a slow-burn read, but ultimately a rewarding one with a richly imagined world.

#   

#   Enjoy your reading! 

#   

#   ## Not specified genre:


#   Sure! Since you didn't specify a genre, I'll recommend two spine-chilling horror novels:

#   

#   1. **"The Haunting of Hill House" by Shirley Jackson:** This classic explores the psychological and supernatural terrors that haunt a group of paranormal investigators in a sprawling, sinister mansion. 

#   

#   2. **"The Ritual" by Adam Nevill:** This atmospheric thriller follows a group of friends on a hiking trip in the Swedish wilderness that turns into a terrifying fight for survival against an ancient evil. 

#   

#   Let me know if you'd like suggestions in another genre! 

#   


"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own data, or try some of the other prompting techniques such as few-shot prompting.
"""



================================================
FILE: examples/prompting/Role_prompting.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Role prompting
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Role_prompting.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
You can specify what role should the model perform, such as a critic, assistant, or teacher.

Doing so can both increase the accuracy of answers and style the response such as if a person of a specific background or occupation has answered the question.
"""

%pip install -U -q "google-genai>=1.7.0"

from google import genai
from google.genai import types

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Examples

Begin by defining a model, and go ahead and input the prompt below. The prompt sets the scene such that the LLM will generate a response with the perspective of being a music connoisseur with a particular interest in Mozart.
"""

system_prompt = """
    You are a highly regarded music connoisseur, you are a big fan of Mozart.
    You recently listened to Mozart's Requiem.
"""

prompt = 'Write a 2 paragraph long review of Requiem.'

MODEL_ID="gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

response = client.models.generate_content(
    model=MODEL_ID,
    config=types.GenerateContentConfig(system_instruction=system_prompt),
    contents=prompt
)

print(response.text)
# Output:
#   Mozart's Requiem is, without a doubt, a monumental work. Even shadowed by the mystery of its incompleteness and shrouded in the lore surrounding Mozart's premature death, the music itself speaks volumes. The sheer drama and emotional depth are captivating from the very first bars of the "Introitus." The soaring soprano lines in the "Dies Irae" are both terrifying and exhilarating, while the "Lacrimosa" is heartbreaking in its plea for mercy. What strikes me most is how Mozart manages to balance the stark realities of death with an underlying sense of hope and faith. This is not merely a lament; it's a profound exploration of the human condition, grappling with mortality, judgment, and the possibility of redemption.

#   

#   Despite its fragmented history and the contributions of Süssmayr, the Requiem possesses a remarkable unity of vision. Mozart's genius shines through in every phrase, and even the sections completed by others feel intrinsically connected to his initial conception. The orchestration is masterly, utilizing the chorus and soloists to create a powerful and evocative soundscape. It's a piece that lingers in the mind long after the final note has faded, prompting contemplation on the mysteries of life and death. For anyone seeking a profound and moving musical experience, Mozart's Requiem is an absolute must-listen.

#   


"""
Let's try another example, in which you are a German tour guide as per the prompt.
"""

system_prompt = """
    You are a German tour guide. Your task is to give recommendations to people visiting your country.
"""

prompt = 'Could you give me some recommendations on art museums in Berlin and Cologne?'

response = client.models.generate_content(
    model=MODEL_ID,
    config=types.GenerateContentConfig(system_instruction=system_prompt),
    contents=prompt
)

print(response.text)
# Output:
#   Ah, herzlich willkommen in Deutschland! Berlin and Cologne, two fantastic cities for art lovers! Allow me, your humble tour guide, to offer some excellent recommendations for your artistic journey:

#   

#   **Berlin:**

#   

#   *   **Pergamon Museum:** *The* museum to see. Famous for its monumental reconstructions of archaeological sites, like the Pergamon Altar and the Ishtar Gate of Babylon. It's simply breathtaking! Be warned: it can get crowded, so try to visit early in the morning or book tickets in advance.

#   

#   *   **Neues Museum:** Houses the iconic bust of Nefertiti, which is worth the visit alone! But it also has a fantastic collection of Egyptian and prehistoric artifacts. A truly historical treasure.

#   

#   *   **Alte Nationalgalerie:** A stunning building showcasing 19th-century art, including masterpieces by German Romantic painters like Caspar David Friedrich, as well as French Impressionists and Realists. Great for seeing the artistic spirit of that time.

#   

#   *   **Hamburger Bahnhof - Museum für Gegenwart:** If you're interested in contemporary art, this is the place to go! It's housed in a former railway station and features works by artists like Andy Warhol, Joseph Beuys, and many others. Very cutting-edge!

#   

#   *   **East Side Gallery:** While not strictly a museum, this is an open-air art gallery on a preserved section of the Berlin Wall. It's a powerful and moving experience, combining art with a poignant piece of history.

#   

#   **Cologne:**

#   

#   *   **Museum Ludwig:** My personal favorite in Cologne. This museum boasts an outstanding collection of modern and contemporary art, including a world-class collection of Pop Art (Warhol, Lichtenstein), Expressionism, and Picasso. A must-see!

#   

#   *   **Wallraf-Richartz Museum & Foundation Corboud:** For art from the Middle Ages to the early 19th century, this is the place to be. You'll find masterpieces by Cologne painters, Baroque masters, and French Impressionists. A good choice for a complete overview of the art history!

#   

#   *   **Kolumba (Archdiocesan Museum Cologne):** This is a unique museum, blending art, architecture, and history. The building itself is a modern masterpiece built on the ruins of a medieval church. The collection spans from late antiquity to the present, with a focus on religious art.

#   

#   *   **Museum Schnütgen:** If you are interested in medieval art, this museum is a true hidden gem. It is located in a former church and displays a magnificent collection of medieval religious art, including sculptures, textiles, and goldsmith work.

#   

#   **Important Tips for Your Visit:**

#   

#   *   **Book tickets in advance:** Especially for popular museums like the Pergamon Museum in Berlin. This will save you time and ensure you can get in.

#   *   **Check opening hours:** Opening hours can vary, especially on holidays. It's best to check the museum's website before you go.

#   *   **Consider a museum pass:** Both Berlin and Cologne offer museum passes that can save you money if you plan to visit several museums.

#   *   **Wear comfortable shoes:** You'll be doing a lot of walking!

#   *   **Take your time:** Don't try to see everything in one day. Choose a few museums that interest you and enjoy them at a leisurely pace.

#   *   **Take a break:** Art viewing can be tiring! Stop for a coffee and a piece of Kuchen (cake) to recharge.

#   

#   I hope these recommendations are helpful! Enjoy your art adventures in Berlin and Cologne! If you have any other questions, don't hesitate to ask! Viel Spaß! (Have fun!)

#   


"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own data, or try some of the other prompting techniques such as few-shot prompting.
"""



================================================
FILE: examples/prompting/Self_ask_prompting.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Self-ask prompting
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Self_ask_prompting.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
Self ask prompting is similar to chain of thought, but instead of going step by step as one answer, it asks itself questions that will help answer the query. Like the chain of thought, it helps the model to think analytically.
"""

%pip install -U -q "google-genai>=1.0.0"

"""
Additionally, select the model you want to use from the available options below:
"""

MODEL_ID = "gemini-2.5-flash"  # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-flash","gemini-2.5-pro"] {"allow-input": true, "isTemplate": true}

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Example
"""

from IPython.display import Markdown

prompt = """
  Question: Who was the president of the united states when Mozart died?
  Are follow up questions needed?: yes.
  Follow up: When did Mozart died?
  Intermediate answer: 1791.
  Follow up: Who was the president of the united states in 1791?
  Intermediate answer: George Washington.
  Final answer: When Mozart died George Washington was the president of the USA.

  Question: Where did the Emperor of Japan, who ruled the year Maria
  Skłodowska was born, die?
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
## Additional note
Self-ask prompting works well with function calling. Follow-up questions can be used as input to a function, which e.g. searches the internet. The question and answer from the function can be added back to the prompt. During the next query to the model, it can either create another function call or return the final answer.

For a related example, please see the [Search re-ranking using Gemini embeddings](https://github.com/google-gemini/cookbook/blob/22ba52659005defc53ce2d6717fb9fedf1d661f1/examples/Search_reranking_using_embeddings.ipynb) example in the Gemini Cookbook.
"""



================================================
FILE: examples/prompting/Zero_shot_prompting.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
<a href="https://colab.research.google.com/github/omkenge/cookbook/blob/add-new-example-for-prompting/examples/prompting/Zero_shot_prompting.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Zero-shot prompting
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Zero_shot_prompting.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
You can use the Gemini models to answer many queries without any additional context. Zero-shot prompting is useful for situations when your queries are not complicated and do not require a specific schema.
"""

%pip install -U -q "google-genai>=1.0.0"

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Examples

Here are a few examples with zero-shot prompting. Note that in each of these examples, you can simply provide the task, with zero examples.
"""

MODEL_ID="gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

from IPython.display import Markdown
prompt = """
    Sort following animals from biggest to smallest:
    fish, elephant, dog
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

prompt = """
    Classify sentiment of review as positive, negative or neutral:
    I go to this restaurant every week, I love it so much.
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

prompt = """
    Extract capital cities from the text:
    During the summer I visited many countries in Europe. First I visited Italy, specifically Sicily and Rome.
    Then I visited Cologne in Germany and the trip ended in Berlin.
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

prompt = """
    Find and fix the error in this Python code:
    def add_numbers(a, b):
        return a + b
    print(add_numbers(5, "10"))
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

prompt = """
    Solve this math problem:
    A train travels 120 km in 2 hours. What is its average speed?
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

prompt = """
    Identify the names of people, places, and countries in this text:
    Emmanuel Macron, the president of France, announced a AI partnership in collaboration with the United Arab Emirates.
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

display(Markdown(response.text))
# Output:
#   <IPython.core.display.Markdown object>

prompt = """
    Correct the grammar in this sentence:
    She don't like playing football but she enjoy to watch it.
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)

display(Markdown(response.text))
# Output:
#   <IPython.core.display.Markdown object>

"""
## Next steps

Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own data, or try some of the other prompting techniques such as [few-shot prompting](https://github.com/google-gemini/cookbook/blob/main/examples/prompting/Few_shot_prompting.ipynb).
"""



================================================
FILE: examples/qdrant/README.md
================================================
## Gemini API Qdrant Examples

### Table of Contents

This folder contains example notebooks demonstrating how to combine the **Gemini API** with the **Qdrant vector database** to enable semantic search and recommendation features using embeddings.

---

### Notebooks

* **[Similarity Search using Qdrant](./Qdrant_similarity_search.ipynb)**
  Load website data, build a semantic search system using embeddings from the Gemini API, store the embeddings in a Qdrant vector DB, and perform similarity search using Gemini-powered queries.

* **[Movie Recommendation using Qdrant](./Movie_Recommendation.ipynb)**
  Process and embed a large movie dataset with the Gemini API, index movie vectors in Qdrant, and build a semantic movie recommender that returns similar movies based on user input using vector similarity search.

---

These examples show how to:

* Embed unstructured text data using Gemini's embedding model.
* Store and search high-dimensional vectors in Qdrant.
* Use Gemini queries to semantically match user input to relevant content.

You can use these templates as a foundation for building search, recommendation, or AI assistant systems using Gemini and Qdrant.


================================================
FILE: examples/qdrant/Qdrant_similarity_search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Similarity Search using Qdrant
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/qdrant/Qdrant_similarity_search.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
## Overview
"""

"""
The [Gemini API](https://ai.google.dev/models/gemini) provides access to a family of generative AI models for generating content and solving problems. These models are designed and trained to handle both text and images as input.

[Qdrant](https://qdrant.tech/) is a vector similarity search engine that offers an easy-to-use API for managing, storing, and searching vectors, with an additional payload. It is a production-ready service.

In this notebook, you'll learn how to perform a similarity search on data from a website with the help of Gemini API and Qdrant.
"""

"""
## Setup

First, you must install the packages and set the necessary environment variables.

### Installation
"""

"""
Install google's python client SDK for the Gemini API, `google-genai`. Next, install Qdrant's Python client SDK, `qdrant-client`.
"""

%pip install -q "google-genai>=1.0.0"
%pip install -q protobuf==4.25.1 qdrant-client[fastembed]

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')
client = genai.Client(api_key=GEMINI_API_KEY)

"""
## Basic steps

Semantic search is the process using which search engines interpret and match keywords to a user's intent in organic search results. It goes beyond surface-level keyword matching. It uses the meaning of words, phrases, and context using advanced algorithms resulting in more relevant and user-friendly search experiences.

Semantic searches rely on vector embeddings which can best match the user query to the most similar result.

In this tutorial, you'll implement the three main components of semantic search:

1. Create an index

    Create and store the index for the data in the Qdrant vector store. You will use a Gemini API embedding model to create embedding vectors that can be stored in the Qdrant vector store.

2. Query the index

    Query the index using a query string to return the top `n` neighbors of the query.

You'll learn more about these stages in the upcoming sections while implementing the application.
"""

"""
## Import the required libraries
"""

from bs4 import BeautifulSoup
from qdrant_client import models, QdrantClient
from urllib.request import urlopen

"""
## 1. Create an index

In this stage, you will perform the following steps:

1. Read and parse the website data using Python's BeautifulSoup library.

2. Create embeddings of the website data.

3. Store the embeddings in Qdrant's vector database.
    
    Qdrant is a vector similarity search engine. Along with a convenient API to store, search, and manage points(i.e. vectors), it also provides an option to add an additional payload. The payloads are essentially extra bits of data that you can utilize to refine your search and obtain relevant information that you can then share with your users.
"""

"""
### Read and parse the website data

To read the website data as text, you will use the `BeautifulSoup` library from Python.
"""

url = "https://blog.google/outreach-initiatives/sustainability/"\
      "report-ai-sustainability-google-cop28/"
html = urlopen(url).read()
soup = BeautifulSoup(html, features="html.parser")

# Remove all script and style elements
for script in soup(["script", "style"]):
    script.extract()    # Self-destruct

# Get the text
text_content = soup.get_text()

"""
If you only want to select a specific portion of the website data to add context to the prompt, you can use regex, text slicing, or text splitting.

In this example, you'll use Python's `split()` function to extract the required portion of the text.
"""

# The text content between the substrings "Later this month at COP28" to
# "POSTED IN:" is relevant for this tutorial. You can use Python's `split()`
# to select the required content.
text_content_1 = text_content.split("Later this month at COP28",1)[1]
final_text = text_content_1.split("POSTED IN:",1)[0]

texts = final_text.split(".")

documents = []

# Convert text into a chunk of 3 sentences.
for i in range(0, len(texts), 3):
  documents.append({"content": " ".join(texts[i:i+3])})

"""
### Initialize the embedding model

To create the embeddings from the website data, you'll use the **gemini-embedding-001** model, which supports creating embeddings from text.

To use the embedding model, you have to use the `embed_content` function from the `google-genai` package. To learn more about the embedding model, read the [model documentation](https://ai.google.dev/gemini-api/docs/embeddings).

One of the arguments passed to the embedding function is `task_type`. Specifying the `task_type` parameter ensures the model produces appropriate embeddingsfor the expected task and inputs. It is a string that can take on one of the following values:

| task_type	  |  Description |
|---|---|
| `RETRIEVAL_QUERY` | Specifies the given text is a query in a search or retrieval setting. |
| `RETRIEVAL_DOCUMENT` | Specifies the given text is a document in a search or retrieval setting. |  
| `SEMANTIC_SIMILARITY` | Specifies the given text will be used for Semantic Textual Similarity (STS). |  
| `CLASSIFICATION` | Specifies that the embeddings will be used for classification. |
| `CLUSTERING` | Specifies that the embeddings will be used for clustering. |
"""

from google.genai import types

# Select embedding model
MODEL_ID = "gemini-embedding-001"  # @param ["gemini-embedding-001", "text-embedding-004"] {"allow-input": true, "isTemplate": true}


# Function to convert text to embeddings
def make_embed_text_fn(text, model=MODEL_ID,
                       task_type="retrieval_document"):
    embedding = client.models.embed_content(
        model=model,
        contents=text,
        config=types.EmbedContentConfig(
          task_type=task_type,
        )
    )
    return embedding.embeddings[0].values

"""
### Store the data using Qdrant

 Next, you'll store the embeddings of the website data in Qdrant's vector store.

 First, you have to initiate a Qdrant client by creating an instance of `QdrantClient`. In this tutorial, you will store the embeddings in memory. To create an in-memory Qdrant client specify `:memory:` for the `location` argument of the `QdrantClient` class initializer. You can read more about the different types of storage in Qdrant in the [storage reference guide](https://qdrant.tech/documentation/concepts/storage/).

After initializing the client, you have to create a Qdrant collection using the `recreate_collection` function of `QdrantClient`. You can specify your vector configuration inside the `recreate_collection` function. Pass an instance of `VectorParams` with the `size` set to `768` to match the embedding model and `distance` set to cosine.

**Note**: Since you will run the script several times during your experiments, `recreate_collection` is appropriate for this tutorial. `recreate_collection` will first try to remove an existing collection with the same name.
"""

# Initialize Qdrant client.
qdrant = QdrantClient(":memory:")

# Create a collection named "GeminiCollection".
qdrant.create_collection(
    collection_name="GeminiCollection",
    vectors_config=models.VectorParams(
        size=3072,  # Vector size of `gemini-embedding-001`
        distance=models.Distance.COSINE,
    ),
)
# Output:
#   True

"""
You will now insert the `documents` you parsed from the website data into the Qdrant collection you created earlier and index them using the `upsert` function of `QdrantClient`.

The `upsert` function takes the data to be stored and indexed as an array of `PointsStruct`s.

Points are the main entity in Qdrant operations. A point is a record consisting of a vector and an optional payload. You can perform a similarity search among the points in one collection. Read more about points in [Qdrant's points documentation](https://qdrant.tech/documentation/concepts/points/).

You'll create an array of points by enumerating over the documents you prepared earlier from the website data.
"""

# Qdrant uses batch loading of points to optimize performance.
# You can create a batch in two ways - record-oriented and column-oriented.
# Here you are using the record-oriented approach.

qdrant.upsert(
    collection_name="GeminiCollection",
    points=[
        # Use PointStruct function to intialize the point.
        models.PointStruct(
            # Use `make_embed_text_fn` to convert text to embeddings.
            # Pass the same data as payload for a refined search.
            id=idx, vector=make_embed_text_fn(doc["content"]), payload = doc
        )
        for idx, doc in enumerate(documents)
    ]
)
# Output:
#   UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)

"""
## 2. Query the index

You'll now query the Qdrant index you created earlier with a question related to the data contained in the website documents.
To query the index, you have to mention the collection name and the query vector. The query vector should be first converted to an embedding vector using the Gemini API embedding model you leveraged to create embedding vectors for the website data. Use the `make_embed_text_fn` you defined earlier for creating an embedding vector from your query. Since you are embedding a query string that is being used to search `retrieval_document` embeddings, the `task_type` must be set to `retrieval_query`.
"""

hits = qdrant.search(
    collection_name="GeminiCollection",
    query_vector=make_embed_text_fn("How can AI address climate challenges?",
                                    task_type="retrieval_query"),
    limit=3,
)
for hit in hits:
    print("score:", hit.score, "- content:", hit.payload.get("content").replace("\n", ""))
# Output:
#   score: 0.8047714249217505 - content:  Already, it is starting to address climate challenges in three key areas: providing people and organizations with better information to make more sustainable choices, delivering improved predictions to help adapt to climate change, and finding recommendations to optimize climate action for high-impact applications Here’s a look at how, at Google, we’ve used AI to address climate challenges:Providing helpful information: People are looking for information to reduce their environmental footprint  Fuel-efficient routing in Google Maps uses AI to suggest routes that have fewer hills, less traffic, and constant speeds with the same or similar ETA

#   score: 0.7745056851050607 - content: Managing the environmental impact of AIWhile scaling these applications of AI and finding new ways to use it to accelerate climate action is crucial, we need to build AI responsibly and manage the environmental impact associated with it As AI is at an inflection point, predicting the future growth of energy use and emissions from AI compute in our data centers is challenging  Historically, data center energy consumption has grown much more slowly than demand for computing power

#   score: 0.7709980934840739 - content: Promoting environmentally and socially responsible deployment of AI Together, we can boldly and responsibly develop more tools and products that harness the power of AI to accelerate the climate progress we need 

#   /tmp/ipykernel_47101/1026465469.py:1: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.

#     hits = qdrant.search(


"""
## Conclusion

That's it. You have successfully performed a similarity search using Qdrant with the help of a Gemini API embedding model.
"""



================================================
FILE: examples/weaviate/README.md
================================================
# Gemini API Weaviate Examples

## Table of contents

This is a collection of examples for the Gemini API used with [Weaviate](https://weaviate.io/). 

* [Personalized Product Descriptions with Weaviate and the Gemini API](./personalized_description_with_weaviate_and_gemini_api.ipynb): Learn how to embed your data, run a semantic search, make a generative call to the Gemini API and store the output in your vector database, and personalize the description based on the user profile.
* [Weaviate Query Agent with Gemini API](./query-agent-as-a-tool.ipynb): Learn how to define the Weaviate Query Agent as a tool through the Gemini API.



================================================
FILE: examples/weaviate/docker-compose.yml
================================================
---
version: '3.4'
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.25.10
    ports:
    - 8080:8080
    - 50051:50051
    restart: on-failure:0
    environment:
      PALM_APIKEY: 'sk-key'
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-palm'
      ENABLE_MODULES: 'text2vec-palm, generative-palm'
      CLUSTER_HOSTNAME: 'node1'
...



================================================
FILE: examples/weaviate/query-agent-as-a-tool.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
*Contributed by the Weaviate team*

## Weaviate Query Agent with Gemini API

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/weaviate/query-agent-as-a-tool.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>

"""

"""
This notebook will show you how to define the Weaviate Query Agent as a tool through the Gemini API.

### Requirements
1. Weaviate Cloud instance (WCD): The Weaviate Query Agent is only accessible through WCD at the moment. You can create a serverless cluster or a free 14-day sandbox [here](https://console.weaviate.cloud/).
2. Have a GCP project and Gemini API key (generate one [here](https://aistudio.google.com/))
3. Install the Google Gen AI SDK with `pip install --upgrade --quiet google-genai`
4. Install the Weaviate Python client and the agents sub-package with `pip install weaviate-client[agents]`
5. You'll need a Weaviate cluster with data. If you don't have one, check out [this notebook](integrations/Weaviate-Import-Example.ipynb) to import the Weaviate Blogs.

Connect with us and let us know if you have any questions!

Erika's accounts:
* [Follow on X](https://x.com/ecardenas300)
* [Connect on LinkedIn](https://www.linkedin.com/in/erikacardenas300/)

Patrick's accounts:
* [Follow on X](https://x.com/patloeber)
* [Connect on LinkedIn](https://www.linkedin.com/in/patrick-l%C3%B6ber-403022137/)

Connor's accounts:
* [LinkedIn](https://www.linkedin.com/in/connor-shorten-34923a178/)
* [X](https://x.com/CShorten30)
"""

"""
### Install libraries
"""

%pip install -U google-genai
%pip install -U "weaviate-client[agents]"


"""
### Import libraries and keys
"""

import os

import weaviate
from weaviate_agents.query import QueryAgent

from google import genai
from google.genai import types

"""
### Set you API keys and Weaviate URL
"""

os.environ["WEAVIATE_URL"] = ""
os.environ["WEAVIATE_API_KEY"] = ""
os.environ["GOOGLE_API_KEY"] = ""

"""
### Create API client
"""

client = genai.Client()

"""
### Define Query Agent function
"""

def query_agent_request(query: str) -> str:
    """
    Send a query to the database and get the response.

    Args:
        query (str): The question or query to search for in the database. This can be any natural language question related to the content stored in the database.

    Returns:
        str: The response from the database containing relevant information.
    """

    # connect to your Weaviate Cloud instance
    weaviate_client = weaviate.connect_to_weaviate_cloud(
        cluster_url=os.getenv("WEAVIATE_URL"),
        auth_credentials=weaviate.auth.AuthApiKey(os.getenv("WEAVIATE_API_KEY")),
        headers={  # add the API key to the model provider from your Weaviate collection, for example `headers={"X-Goog-Studio-Api-Key": os.getenv("GEMINI_API_KEY")}`
        }
    )

    # connect the query agent to your Weaviate collection(s)
    query_agent = QueryAgent(
        client=weaviate_client,
        collections=["WeaviateBlogChunks"]
    )
    return query_agent.run(query).final_answer

"""
### Configure Tool
"""

config = types.GenerateContentConfig(tools=[query_agent_request])

"""
### Query Time
"""

prompt = """
You are connected to a database that has a blog post on deploying Weaviate on Docker.
Can you answer how I can Weaviate with Docker?
"""

chat = client.chats.create(model='gemini-2.0-flash', config=config)
response = chat.send_message(prompt)
print(response.text)
# Output:
#   /usr/local/lib/python3.11/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:

#     Expected `enum` but got `str` with value `'STRING'` - serialized value may not be as expected

#     return self.__pydantic_serializer__.to_python(

#   To deploy Weaviate with Docker, you need to:

#   

#   1.  Install Docker and Docker Compose.

#   2.  Obtain the Weaviate Docker image using:

#       ```bash

#       docker pull cr.weaviate.io/semitechnologies/weaviate:latest

#       ```

#   3.  Prepare a `docker-compose.yml` file, which you can generate using the Weaviate configuration tool or example files from the documentation.

#   4.  Start Weaviate using either:

#       *   Directly with Docker:

#           ```bash

#           docker run -p 8080:8080 -p 50051:50051 cr.weaviate.io/semitechnologies/weaviate:latest

#           ```

#       *   Using Docker Compose:

#           ```bash

#           docker-compose up -d

#           ```

#   5.  Access Weaviate at `http://localhost:8080` and configure as needed.

#   6.  Check if Weaviate is ready by hitting the readiness endpoint:

#       ```bash

#       curl localhost:8080/v1/.well-known/ready

#       ```

#   

#   




================================================
FILE: quickstarts/README.md
================================================
# Gemini API Tutorials

This folder contains guides to help you explore all Gemini API features using complete end-to-end code examples.

When you're confident in your Gemini capabilities, the [examples](https://github.com/google-gemini/cookbook/tree/main/examples/) folder will be an endless source of inspiration on how to mix those capabilities together.
<br><br>

## Table of contents

If you're new to Gemini API, you should start with these two guides:

| Quickstart | Description | Open |
| -------- | ----------- | ---- |
| [Authentication](./Authentication.ipynb) | Start here to learn how you can set up your API key so you can get access to the Gemini API. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) |
| [Get Started](./Get_started.ipynb) | Learn how to make your first calls to the Gemini API and get a quick overview of everything it can do. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb) |
<br>

Then learn about how to **Get Started** with the other models that you can use with the API:

| Quickstart | Description | Open |
| -------- | ----------- | ---- |
| [Veo](./Get_started_Veo.ipynb) | Get started with video generation using the Veo models. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_Veo.ipynb) |
| [Imagen](./Get_started_imagen.ipynb) | Get started with our image generation model. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_imagen.ipynb) |
| [Imagen (REST)](./Get_started_imagen_rest.ipynb) | Get started with our image generation model (REST version). | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_imagen_rest.ipynb) |
| [Thinking models](./Get_started_thinking.ipynb) | The thinking models are, as their names imply, capable of deeper chains of thoughts than the classical models, this guide will show you how to use those thinking capabilities to solve complex problems. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb) |
| [Lyria RealTime](./Get_started_LyriaRealTime.ipynb) | The Lyria RealTime model let's your generate music and prompt the model in real-time to have it mis it for you live. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LyriaRealTime.ipynb) |
| [Text-to-speech](./Get_started_TTS.ipynb) | The TTS models let you generate speeches with one or even two speakers! | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_TTS.ipynb) |
<br>

There're multiple ways to call the models using the Gemini API, these other **Get Started** guides will then show you the other ways to call the model:

| Quickstart | Description | Open |
| -------- | ----------- | ---- |
| [Get started with Live API](./Get_started_LiveAPI.ipynb) | Get started with the live API with this comprehensive overview of its capabilities | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb) |
| [OpenAI compatibility](./Get_started_OpenAI_Compatibility.ipynb) | Did you know that you could use Gemini using the OpenAI SDK? | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_OpenAI_Compatibility.ipynb) |
<br>

Finally, these guides will deep-dive into specific capabilities of the Gemini models and API:

| Quickstart | Description | Open |
| -------- | ----------- | ---- |
| [Grounding](./Grounding.ipynb) | Learn how to use diffrent ways (Google Search, Youtube, url context) to ground your answers with external sources. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Grounding.ipynb) |
| [Search Grounding](./Search_Grounding.ipynb) | Deep-dive into the Google search grounding capabilities. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb) |
| [Batch-mode](./Batch_mode.ipynb) | Use Batch-mode to send large volume of non-time-sensitive requests to the model and get a 50% discount. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Batch_mode.ipynb) |
| [Function Calling](./Function_calling.ipynb) | Discover how to have Gemini call you own function and enhaced its capabilites. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Function_calling.ipynb) |
| [Image-out](./Image_out.ipynb) | Get to know how the Gemini model can directly output images and edit them through multi-turn discussion. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Image_out.ipynb) |
| [Spatial understanding](./Spatial_understanding.ipynb) | Learn how to use Gemini's spatial understanding capabilities to detect what's in your images, and reason about them | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb) |
| [Video understanding](./Video_understanding.ipynb) | Learn how to use Gemini's video understanding capabilities to analyze what's in your videos | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Video_understanding.ipynb) |
| [Get started with Live API tools](./Get_started_LiveAPI_tools.ipynb) | Now you know everything about the Live API, go to the next level and learn how to use tools with it! | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb) |
<br>

These guides will walk you through the various use cases of the Gemini API:

| Quickstart | Description | Open |
| -------- | ----------- | ---- |
| [Asynchronous requests](./Asynchronous_requests.ipynb) | Learn how to use Python's async/await API with the Gemini SDK to parallelize calls. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Asynchronous_requests.ipynb) |
| [Counting Tokens](./Counting_Tokens.ipynb) | Tokens are the basic inputs to the Gemini models. Through this | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Counting_Tokens.ipynb) |
| [Models](./Models.ipynb) | Learn about the different models and parameters available in the Gemini API. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Models.ipynb) |
| [Working with files](./File_API.ipynb) | Use the Gemini API to upload files (audio, video, images, code, text) and perform actions with them through the Gemini models. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/File_API.ipynb) |
| [Audio](./Audio.ipynb) | Learn how to use the Gemini API with audio files. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Audio.ipynb) |
| [JSON mode](./JSON_mode.ipynb) | Discover how to use JSON mode. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb) |
| [PDF files](./PDF_Files.ipynb) | Learn how to work with PDF files, and upload text and images. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/PDF_Files.ipynb) |
| [System Instructions](./System_instructions.ipynb) | Give models additional context on how to respond by setting system instructions. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/System_instructions.ipynb) |
| [Streaming](./Streaming.ipynb) | Learn how to use streaming for single interactions, and for chat. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Streaming.ipynb) |
| [Embeddings](./Embeddings.ipynb) | Create high quality and task-specific embeddings. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob.ipynb) |
| [Video](./Video_understanding.ipynb) | Upload a video to the Gemini API and use it in your prompt. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Video_understanding.ipynb) |
| [AI Tutors with LearnLM](./Get_started_LearnLM.ipynb) | Demonstrates how to craft AI tutoring experiences using system instructions aligned with learning science principles. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LearnLM.ipynb) |



================================================
FILE: quickstarts/Asynchronous_requests.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Asynchronous Python requests

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Asynchronous_requests.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook will show you how to make asynchronous and parallel requests using the Gemini API's Python SDK and Python 3's [`asyncio`](https://docs.python.org/3/library/asyncio.html) standard library.

The examples here run in Google Colab and use the implicit event loop supplied in Colab. You can also run these commands interactively using the `asyncio` REPL (invoked with `python -m asyncio`), or you can manage the [event loop](https://docs.python.org/3/library/asyncio-eventloop.html) yourself.
"""

%pip install -qU 'google-genai>=1.0.0' aiohttp
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/200.0 kB[0m [31m?[0m eta [36m-:--:--[0m
#   [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m200.0/200.0 kB[0m [31m14.5 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
## Set up your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY = userdata.get("GOOGLE_API_KEY")
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off).
"""

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.0-flash","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

"""
## Using local files

This simple example shows how can you use local files (presumed to load quickly) with the SDK's `async` API.
"""

prompt = "Describe this image in just 3 words."

img_filenames = ["firefighter.jpg", "elephants.jpeg", "jetpack.jpg"]
img_dir = "https://storage.googleapis.com/generativeai-downloads/images/"

"""
Start by downloading the files locally.
"""

!wget -nv {img_dir}{{{','.join(img_filenames)}}}
# Output:
#   2025-06-09 09:43:19 URL:https://storage.googleapis.com/generativeai-downloads/images/firefighter.jpg [547369/547369] -> "firefighter.jpg.1" [1]

#   2025-06-09 09:43:20 URL:https://storage.googleapis.com/generativeai-downloads/images/elephants.jpeg [224007/224007] -> "elephants.jpeg.1" [1]

#   2025-06-09 09:43:20 URL:https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg [357568/357568] -> "jetpack.jpg.1" [1]

#   FINISHED --2025-06-09 09:43:20--

#   Total wall clock time: 0.2s

#   Downloaded: 3 files, 1.1M in 0.02s (56.7 MB/s)


"""
The async code uses the `aio.models.generate_content` method to invoke the API. Most async API methods can be found in the [`aio`](https://googleapis.github.io/python-genai/genai.html#genai.client.AsyncClient) namespace.

Note that this code is not run in parallel. The async call indicates that the event loop *can* yield to other tasks, but there are no other tasks scheduled in this code. This may be sufficient, e.g. if you are running this in a web server request handler as it will allow the handler to yield to other tasks while waiting for the API response.
"""

import PIL

async def describe_local_images():

  for img_filename in img_filenames:

    img = PIL.Image.open(img_filename)
    r = await client.aio.models.generate_content(
        model=MODEL_ID,
        contents=[prompt, img]
    )
    print(r.text)


await describe_local_images()
# Output:
#   Boy, cat, tree.

#   Forest elephant family

#   Jetpack Backpack Concept


"""
## Downloading images asynchronously and in parallel

This example shows a more real-world case where an image is downloaded from an external source using the async HTTP library [`aiohttp`](https://pypi.org/project/aiohttp), and each image is processed in parallel.
"""

import io, aiohttp, asyncio

async def download_image(session: aiohttp.ClientSession, img_url: str) -> PIL.Image:
  """Returns a PIL.Image object from the provided URL."""
  async with session.get(img_url) as img_resp:
    buffer = io.BytesIO()
    buffer.write(await img_resp.read())
    return PIL.Image.open(buffer)


async def process_image(img_future: asyncio.Future[PIL.Image]) -> str:
  """Summarise the image using the Gemini API."""
  # This code uses a future so that it defers work as late as possible. Using a
  # concrete Image object would require awaiting the download task before *queueing*
  # this content generation task - this approach chains the futures together
  # so that the download only starts when the generation is scheduled.
  r = await client.aio.models.generate_content(
      model=MODEL_ID,
      contents=[prompt, await img_future]
  )
  return r.text

async def download_and_describe():

  async with aiohttp.ClientSession() as sesh:
    response_futures = []
    for img_filename in img_filenames:

      # Create the image download tasks (this does not schedule them yet).
      img_future = download_image(sesh, img_dir + img_filename)

      # Kick off the Gemini API request using the pending image download tasks.
      text_future = process_image(img_future)

      # Save the reference so they can be processed as they complete.
      response_futures.append(text_future)

    print(f"Download and content generation queued for {len(response_futures)} images.")

    # Process responses as they complete (may be a different order). The tasks are started here.
    for response in asyncio.as_completed(response_futures):
      print()
      print(await response)


await download_and_describe()
# Output:
#   Download and content generation queued for 3 images.

#   

#   Wild elephant family.

#   

#   Jetpack backpack concept

#   

#   Cat, person, tree.


"""
In the above example, a coroutine is created for each image that both downloads and then summarizes the image. The coroutines are executed in the final step, in the `as_completed` loop. To start them as early as possible without blocking the other work, you could wrap `download_image` in [`asyncio.ensure_future`](https://docs.python.org/3/library/asyncio-future.html#asyncio.ensure_future), but for this example the execution has been deferred to keep the creation and execution concerns separate.
"""

"""
## Next Steps

* Check out the [`AsyncClient`](https://googleapis.github.io/python-genai/genai.html#genai.client.AsyncClient) class in the Python SDK reference.
* Read more on Python's [`asyncio`](https://docs.python.org/3/library/asyncio.html) library
"""



================================================
FILE: quickstarts/Authentication.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Authentication Quickstart
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
The Gemini API uses API keys for authentication. This notebook walks you through creating an API key, and using it with the Python SDK or a command-line tool like `curl`.
"""

"""
## Create an API key

You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.  

Remember to treat your API key like a password. Don't accidentally save it in a notebook or source file you later commit to GitHub. This notebook shows you two ways you can securely store your API key.

* If you're using Google Colab, it's recommended to store your key in Colab Secrets.

* If you're using a different development environment (or calling the Gemini API through `cURL` in your terminal), it's recommended to store your key in an [environment variable](https://en.wikipedia.org/wiki/Environment_variable).

Let's start with Colab Secrets.
"""

"""
## Add your key to Colab Secrets

Add your API key to the Colab Secrets manager to securely store it.

1. Open your Google Colab notebook and click on the 🔑 **Secrets** tab in the left panel.
   
   <img src="https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg" alt="You can find the Secrets tab on the left panel." width=50%>

2. Create a new secret with the name `GOOGLE_API_KEY`.
3. Copy and paste your API key into the `Value` input box of `GOOGLE_API_KEY`.
4. Toggle the button on the left to allow all notebooks access to the secret.

"""

"""
## Install the Python SDK
"""

%pip install -qU 'google-genai>=1.0.0'
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/200.0 kB[0m [31m?[0m eta [36m-:--:--[0m
#   [2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m194.6/200.0 kB[0m [31m29.4 MB/s[0m eta [36m0:00:01[0m
#   [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m200.0/200.0 kB[0m [31m5.7 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
## Configure the SDK with your API key

You create a client using your API key, but instead of pasting your key into the notebook, you'll read it from Colab Secrets thanks to `userdata`.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Now choose a model. The Gemini API offers different models that are optimized for specific use cases. For more information check [Gemini models](https://ai.google.dev/gemini-api/docs/models)
"""

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.0-flash","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

"""
And that's it! Now you're ready to call the Gemini API.
"""

from IPython.display import Markdown

response = client.models.generate_content(
    model=MODEL_ID,
    contents="Please give me python code to sort a list."
)

display(Markdown(response.text))
# Output:
#   <IPython.core.display.Markdown object>

"""
## Store your key in an environment variable
"""

"""
If you're using a different development environment (or calling the Gemini API through `cURL` in your terminal), it's recommended to store your key in an environment variable.

To store your key in an environment variable, open your terminal and run:

```export GOOGLE_API_KEY="YOUR_API_KEY"```

If you're using Python, you can add these two lines to your notebook to read the key:

```
import os
client = genai.Client(api_key=os.environ['GOOGLE_API_KEY'])
```

Alternatively, if it isn't provided explicitly, the client will look for the API key.

```
client = genai.Client()
```

Or, if you're calling the API through your terminal using `cURL`, you can copy and paste this code to read your key from the environment variable.

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[{
          "text": "Please give me Python code to sort a list."
        }]
      }]
    }'
```

"""

"""
## Learning more

Now that you know how to manage your API key, you've everything to [get started](./Get_started.ipynb) with Gemini. Check all the [quickstart guides](https://github.com/google-gemini/cookbook/tree/main/quickstarts) from the Cookbook, and in particular the [Get started](./Get_started.ipynb) one.
"""



================================================
FILE: quickstarts/Authentication_with_OAuth.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: OAuth Quickstart
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Authentication_with_OAuth.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
Some parts of the Gemini API like model tuning and semantic retrieval use OAuth for authentication.

If you are a beginner, you should start by using [API keys](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb), and come back to this OAuth guide only when you need it for these features.

To help you get started with OAuth, this notebook shows a simplified approach that is appropriate
for a testing environment.

For a production environment, learn
about [authentication and authorization](https://developers.google.com/workspace/guides/auth-overview) before [choosing the access credentials](https://developers.google.com/workspace/guides/create-credentials#choose_the_access_credential_that_is_right_for_you) that are appropriate for your app.
"""

"""
## Prerequisites

To run this quickstart, you need:

*   The [Google Cloud CLI](https://cloud.google.com/sdk/docs/install-sdk) installed on your local machine.
*   [A Google Cloud project](https://developers.google.com/workspace/guides/create-project).

If you created an API key in Google AI Studio, a Google Cloud project was made for you. Go to [Google AI Studio](https://aistudio.google.com/app/apikey) and note the Google Cloud project name to use that project.
"""

"""
## Set up your Cloud project

To complete this quickstart, you first need to setup your Cloud project.

### 1. Enable the API

Before using Google APIs, you need to turn them on in a Google Cloud project.

*   In the Google Cloud console, [enable](https://console.cloud.google.com/flows/enableapi?apiid=generativelanguage.googleapis.com) the Google Generative Language API. If you created an API Key in AI Studio, this was done for you.<br>

### 2. Configure the OAuth consent screen

Next configure the project's OAuth consent screen and add yourself as a test user. If you've already completed this step for your Cloud project, skip to the next section.

1. In the Google Cloud console, go to the [OAuth consent screen](https://console.cloud.google.com/apis/credentials/consent), this can be found under **Menu** > **APIs & Services** > **OAuth
  consent screen**.

2. Select the user type **External** for your app, then click **Create**.

3. Complete the app registration form (you can leave most fields blank), then click **Save and Continue**.

4. For now, you can skip adding scopes and click **Save and Continue**. In the
   future, when you create an app for use outside of your Google Workspace
   organization, you must add and verify the authorization scopes that your
   app requires.

5. Add test users:
    1. Under **Test users**, click **Add users**.
    2. Enter your email address and any other authorized test users, then
       click **Save and Continue**.

6. Review your app registration summary. To make changes, click **Edit**. If
  the app registration looks OK, click **Back to Dashboard**.

### 3. Authorize credentials for a desktop application

To authenticate as an end user and access user data in your app, you need to
create one or more OAuth 2.0 Client IDs. A client ID is used to identify a
single app to Google's OAuth servers. If your app runs on multiple platforms,
you must create a separate client ID for each platform.

1. In the Google Cloud console, go to [Credentials](https://console.cloud.google.com/apis/credentials/consent), this can be found under **Menu** > **APIs & Services** >
   **Credentials**.

2. Click **Create Credentials** > **OAuth client ID**.
3. Click **Application type** > **Desktop app**.
4. In the **Name** field, type a name for the credential. This name is only
  shown in the Google Cloud console.
5. Click **Create**. The OAuth client created screen appears, showing your new
  Client ID and Client secret.
6. Click **OK**. The newly created credential appears under **OAuth 2.0 Client
  IDs.**
7. Click the download button to save the JSON file. It will be saved as
  `client_secret_<identifier>.json`.

"""

"""
## Set up application default credentials

In this quickstart you will use [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials) to authenticate.
"""

"""
### Add client secret to Colab secrets

If you need to use OAuth with the Gemini API in Google Colab frequently, it is easiest to add the contents of your `client_secret.json` file into Colab's Secrets manager.

1. Open your Google Colab notebook and click on the 🔑 **Secrets** tab in the left panel.
2. Create a new secret with the name `CLIENT_SECRET`.
3. Open your `client_secret.json` file in a text editor and copy/paste the content into the `Value` input box of `CLIENT_SECRET`.
4. Toggle the button on the left to allow notebook access to the secret.

Now you can programmatically create the file instead of uploading it every time. The client secret is also available in all your Google Colab notebooks after you allow access.
"""

from google.colab import userdata
import pathlib
pathlib.Path('client_secret.json').write_text(userdata.get('CLIENT_SECRET'))
# Output:
#   413

"""
### Set the application default credentials

To convert the `client_secret.json` file into usable credentials, pass its location the `gcloud auth application-default login` command's `--client-id-file` argument.

The simplified project setup in this tutorial triggers a **Google hasn't verified this app** dialog. This is normal, choose **Continue**.

You will need to do this step once for every new Google Colab notebook or runtime.

**Note**: Carefully follow the instructions the following command prints (don't just click the link). Also make sure your local `gcloud --version` is the [latest](https://cloud.google.com/sdk/docs/release-notes) to match the version pre-installed in Google Colab.

"""

!gcloud auth application-default login \
  --no-browser --client-id-file client_secret.json \
  --scopes https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.tuning,https://www.googleapis.com/auth/generative-language.retriever


"""
The specific `scopes` you need depends on the API you are using. For example, looking at the API reference for [`tunedModels.create`](https://ai.google.dev/api/rest/v1beta/tunedModels/create#authorization-scopes), you will see:

> Requires one of the following OAuth scopes:
>
> *   `https://www.googleapis.com/auth/generative-language.tuning`

This sample asks for all the scopes for tuning and semantic retrieval, but best practice is to use the smallest set of scopes for security and user confidence.
"""

"""
## Using the Python SDK with OAuth

The Python SDK will automatically find and use application default credentials.
"""

%pip install -U -q "google-generativeai>=0.7.2"
# Output:
#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m137.4/137.4 kB[0m [31m2.7 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m

#   [?25h

"""
Let's do a quick test. Note that you did not set an API key using `genai.configure()`!
"""

import google.generativeai as genai

print('Available base models:', [m.name for m in genai.list_models()])

"""
# Appendix
"""

"""
## Making authenticated REST calls from Colab

In general, you should use the Python SDK to interact with the Gemini API when possible. This example shows how to make OAuth authenticated REST calls from Python for debugging or testing purposes. It assumes you have already set application default credentials from the Quickstart.
"""

import requests

access_token = !gcloud auth application-default print-access-token

headers = {
    'Content-Type': 'application/json',
    'Authorization': f'Bearer {access_token[0]}',
}

response = requests.get('https://generativelanguage.googleapis.com/v1/models', headers=headers)
response_json = response.json()

# All the model names
for model in response_json['models']:
    print(model['name'])

"""
### Share a tuned model

Some beta API features may not be supported by the Python SDK yet. This example shows how to make a REST call to add a permission to a tuned model from Python.
"""

import requests

model_name = ''   # @param {type:"string"}
emailAddress = '' # @param {type:"string"}


access_token = !gcloud auth application-default print-access-token

headers = {
    'Content-Type': 'application/json',
    'Authorization': f'Bearer {access_token[0]}',
}

body = {
  'granteeType': 'USER',        # Or 'GROUP' or 'EVERYONE' https://ai.google.dev/api/rest/v1beta/tunedModels.permissions
  'emailAddress': emailAddress, # Optional if 'granteeType': 'EVERYONE'
  'role': 'READER'
}

response = requests.post(f'https://generativelanguage.googleapis.com/v1beta/tunedModels/{model_name}/permissions', json=body, headers=headers)
print(response.json())


"""
## Use a service account to authenticate

Google Cloud [service accounts](https://cloud.google.com/iam/docs/service-account-overview) are accounts that do not represent a human user. They provide a way to manage authentication and authorization when a human is not directly involved, such as your application calling the Gemini API to fulfill a user request, but not authenticated as the user. A simple way to use service accounts to authenticate with the Gemini API is to use a [service account key](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-key).

This guide briefly covers how to use service account keys in Google Colab.

**Important:** Service account keys can be a security risk! For more information, see [best practices for managing service account keys](https://cloud.google.com/iam/docs/best-practices-for-managing-service-account-keys).

### 1. Create a service account

Follow the instructions to [create a service account](https://cloud.google.com/iam/docs/service-accounts-create#creating). The **Console** instructions are easiest if you are doing this manually.

### 2. Create a service account key

Follow the instructions to [create a service account key]( https://cloud.google.com/iam/docs/keys-create-delete#creating). Note the name of the downloaded key.

### 3. Add the service account key to Colab

1. Open your Google Colab notebook and click on the 🔑 **Secrets** tab in the left panel.
2. Create a new secret with the name `SERVICE_ACCOUNT_KEY`.
3. Open your service account key file in a text editor and copy/paste the content into the `Value` input box of `SERVICE_ACCOUNT_KEY`.
4. Toggle the button on the left to allow notebook access to the secret.

### 4. Authenticate with the Python SDK by service account key
"""

import google.generativeai as genai
import pathlib
from google.colab import userdata
from google.oauth2 import service_account

pathlib.Path('service_account_key.json').write_text(userdata.get('SERVICE_ACCOUNT_KEY'))

credentials = service_account.Credentials.from_service_account_file('service_account_key.json')

# Adjust scopes as needed
scoped_credentials = credentials.with_scopes(
    ['https://www.googleapis.com/auth/cloud-platform', 'https://www.googleapis.com/auth/generative-language.retriever'])

genai.configure(credentials=scoped_credentials)

print('Available base models:', [m.name for m in genai.list_models()])



================================================
FILE: quickstarts/Caching.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Context Caching Quickstart

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Caching.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook introduces context caching with the Gemini API and provides examples of interacting with the Apollo 11 transcript using the Python SDK. For a more comprehensive look, check out [the caching guide](https://ai.google.dev/gemini-api/docs/caching?lang=python).
"""

"""
### Install dependencies
"""

%pip install -q -U "google-genai>=1.0.0"

"""
### Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Upload a file

A common pattern with the Gemini API is to ask a number of questions of the same document. Context caching is designed to assist with this case, and can be more efficient by avoiding the need to pass the same tokens through the model for each new request.

This example will be based on the transcript from the Apollo 11 mission.

Start by downloading that transcript.
"""

!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt
!head a11.txt
# Output:
#   INTRODUCTION

#   

#   This is the transcription of the Technical Air-to-Ground Voice Transmission (GOSS NET 1) from the Apollo 11 mission.

#   

#   Communicators in the text may be identified according to the following list.

#   

#   Spacecraft:

#   CDR	Commander	Neil A. Armstrong

#   CMP	Command module pilot   	Michael Collins

#   LMP	Lunar module pilot	Edwin E. ALdrin, Jr.


"""
Now upload the transcript using the [File API](../quickstarts/File_API.ipynb).
"""

document = client.files.upload(file="a11.txt")

"""
## Cache the prompt

Next create a [`CachedContent`](https://ai.google.dev/api/python/google/generativeai/protos/CachedContent) object specifying the prompt you want to use, including the file and other fields you wish to cache. In this example the [`system_instruction`](../quickstarts/System_instructions.ipynb) has been set, and the document was provided in the prompt.

Note that caches are model specific. You cannot use a cache made with a different model as their tokenization might be slightly different.
"""

# Note that caching requires a frozen model, e.g. one with a `-001` suffix.
MODEL_ID = "gemini-2.5-flash"  # @param ["gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"] {"allow-input":true, isTemplate: true}

apollo_cache = client.caches.create(
    model=MODEL_ID,
    config={
        'contents': [document],
        'system_instruction': 'You are an expert at analyzing transcripts.',
    },
)

apollo_cache
# Output:
#   CachedContent(

#     create_time=datetime.datetime(2025, 8, 6, 13, 48, 36, 419118, tzinfo=TzInfo(UTC)),

#     display_name='',

#     expire_time=datetime.datetime(2025, 8, 6, 14, 48, 36, 38936, tzinfo=TzInfo(UTC)),

#     model='models/gemini-2.5-flash',

#     name='cachedContents/0c5j38gpopx49ok6x7kedvbpy65d1bzkq8i5vldr',

#     update_time=datetime.datetime(2025, 8, 6, 13, 48, 36, 419118, tzinfo=TzInfo(UTC)),

#     usage_metadata=CachedContentUsageMetadata(

#       total_token_count=322698

#     )

#   )

from IPython.display import Markdown

display(Markdown(f"As you can see in the output, you just cached **{apollo_cache.usage_metadata.total_token_count}** tokens."))
# Output:
#   <IPython.core.display.Markdown object>

"""
## Manage the cache expiry

Once you have a `CachedContent` object, you can update the expiry time to keep it alive while you need it.
"""

from google.genai import types

client.caches.update(
    name=apollo_cache.name,
    config=types.UpdateCachedContentConfig(ttl="7200s")  # 2 hours in seconds
)

apollo_cache = client.caches.get(name=apollo_cache.name) # Get the updated cache
apollo_cache
# Output:
#   CachedContent(

#     create_time=datetime.datetime(2025, 8, 6, 13, 48, 36, 419118, tzinfo=TzInfo(UTC)),

#     display_name='',

#     expire_time=datetime.datetime(2025, 8, 6, 15, 48, 36, 651814, tzinfo=TzInfo(UTC)),

#     model='models/gemini-2.5-flash',

#     name='cachedContents/0c5j38gpopx49ok6x7kedvbpy65d1bzkq8i5vldr',

#     update_time=datetime.datetime(2025, 8, 6, 13, 48, 36, 691886, tzinfo=TzInfo(UTC)),

#     usage_metadata=CachedContentUsageMetadata(

#       total_token_count=322698

#     )

#   )

"""
## Use the cache for generation

As the `CachedContent` object refers to a specific model and parameters, you must create a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) using [`from_cached_content`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#from_cached_content). Then, generate content as you would with a directly instantiated model object.
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents='Find a lighthearted moment from this transcript',
    config=types.GenerateContentConfig(
        cached_content=apollo_cache.name,
    )
)

display(Markdown(response.text))
# Output:
#   <IPython.core.display.Markdown object>

"""
You can inspect token usage through `usage_metadata`. Note that the cached prompt tokens are included in `prompt_token_count`, but excluded from the `total_token_count`.
"""

response.usage_metadata
# Output:
#   GenerateContentResponseUsageMetadata(

#     cache_tokens_details=[

#       ModalityTokenCount(

#         modality=<MediaModality.TEXT: 'TEXT'>,

#         token_count=322698

#       ),

#     ],

#     cached_content_token_count=322698,

#     candidates_token_count=282,

#     prompt_token_count=322707,

#     prompt_tokens_details=[

#       ModalityTokenCount(

#         modality=<MediaModality.TEXT: 'TEXT'>,

#         token_count=322707

#       ),

#     ],

#     thoughts_token_count=4049,

#     total_token_count=327038

#   )

display(Markdown(f"""
  As you can see in the `usage_metadata`, the token usage is split between:
  *  {response.usage_metadata.cached_content_token_count} tokens for the cache,
  *  {response.usage_metadata.prompt_token_count} tokens for the input (including the cache, so {response.usage_metadata.prompt_token_count - response.usage_metadata.cached_content_token_count} for the actual prompt),
  *  {response.usage_metadata.thoughts_token_count} tokens for the thinking process,
  *  {response.usage_metadata.candidates_token_count} tokens for the output,
  *  {response.usage_metadata.total_token_count} tokens in total.
"""))
# Output:
#   <IPython.core.display.Markdown object>

"""
You can ask new questions of the model, and the cache is reused.
"""

chat = client.chats.create(
  model=MODEL_ID,
  config={"cached_content": apollo_cache.name}
)

response = chat.send_message(message="Give me a quote from the most important part of the transcript.")
display(Markdown(response.text))
# Output:
#   <IPython.core.display.Markdown object>

response = chat.send_message(
    message="What was recounted after that?",
    config={"cached_content": apollo_cache.name}
)
display(Markdown(response.text))
# Output:
#   <IPython.core.display.Markdown object>

response.usage_metadata
# Output:
#   GenerateContentResponseUsageMetadata(

#     cache_tokens_details=[

#       ModalityTokenCount(

#         modality=<MediaModality.TEXT: 'TEXT'>,

#         token_count=322698

#       ),

#     ],

#     cached_content_token_count=322698,

#     candidates_token_count=239,

#     prompt_token_count=322795,

#     prompt_tokens_details=[

#       ModalityTokenCount(

#         modality=<MediaModality.TEXT: 'TEXT'>,

#         token_count=322795

#       ),

#     ],

#     thoughts_token_count=902,

#     total_token_count=323936

#   )

display(Markdown(f"""
  As you can see in the `usage_metadata`, the token usage is split between:
  *  {response.usage_metadata.cached_content_token_count} tokens for the cache,
  *  {response.usage_metadata.prompt_token_count} tokens for the input (including the cache, so {response.usage_metadata.prompt_token_count - response.usage_metadata.cached_content_token_count} for the actual prompt),
  *  {response.usage_metadata.thoughts_token_count} tokens for the thinking process,
  *  {response.usage_metadata.candidates_token_count} tokens for the output,
  *  {response.usage_metadata.total_token_count} tokens in total.
"""))
# Output:
#   <IPython.core.display.Markdown object>

"""
Since the cached tokens are cheaper than the normal ones, it means this prompt was much cheaper that if you had not used caching. Check the [pricing here](https://ai.google.dev/pricing) for the up-to-date discount on cached tokens.
"""

"""
## Delete the cache

The cache has a small recurring storage cost (cf. [pricing](https://ai.google.dev/pricing)) so by default it is only saved for an hour. In this case you even set it up for a shorter amont of time (using `"ttl"`) of 2h.

Still, if you don't need you cache anymore, it is good practice to delete it proactively.
"""

print(apollo_cache.name)
client.caches.delete(name=apollo_cache.name)
# Output:
#   cachedContents/0c5j38gpopx49ok6x7kedvbpy65d1bzkq8i5vldr

#   DeleteCachedContentResponse()

"""
## Next Steps
### Useful API references:

If you want to know more about the caching API, you can check the full [API specifications](https://ai.google.dev/api/rest/v1beta/cachedContents) and the [caching documentation](https://ai.google.dev/gemini-api/docs/caching).

### Continue your discovery of the Gemini API

Check the File API notebook to know more about that API. The [vision capabilities](../quickstarts/Video.ipynb) of the Gemini API are a good reason to use the File API and the caching.
The Gemini API also has configurable [safety settings](../quickstarts/Safety.ipynb) that you might have to customize when dealing with big files.

"""



================================================
FILE: quickstarts/Error_handling.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the \"License\");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an \"AS IS\" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Error handling
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Error_handling.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This Colab notebook demonstrates strategies for handling common errors you might encounter when working with the Gemini API:

*   **Transient Errors:** Temporary failures due to network issues, server overload, etc.
*   **Rate Limits:** Restrictions on the number of requests you can make within a certain timeframe.
*   **Timeouts:** When an API call takes too long to complete.

You have two main approaches to explore:

1.  **Automatic retries:** A simple way to retry requests when they fail due to transient errors.
2.  **Manual backoff and retry:** A more customizable approach that provides finer control over retry behavior.


**Gemini Rate Limits**

The default rate limits for different Gemini models are outlined in the [Gemini API model documentation](https://ai.google.dev/gemini-api/docs/models/gemini#model-variations). If your application requires a higher quota, consider [requesting a rate limit increase](https://ai.google.dev/gemini-api/docs/quota).
"""

%pip install -q -U "google-genai>=1.0.0"

"""
### Setup your API key

To run the following cells, store your API key in a Colab Secret named `GOOGLE_API_KEY`. If you don't have an API key or need help creating a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) guide.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY = userdata.get("GOOGLE_API_KEY")
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
### Automatic retries

The Gemini API's client library offers built-in retry mechanisms for handling transient errors. You can enable this feature by using the `request_options` argument with API calls like `generate_content`, `generate_answer`, `embed_content`, and `generate_content_async`.

**Advantages:**

* **Simplicity:** Requires minimal code changes for significant reliability gains.
* **Robust:** Effectively addresses most transient errors without additional logic.

**Customize retry behavior:**

Use these settings in [`retry`](https://googleapis.dev/python/google-api-core/latest/retry.html) to customize retry behavior:

* `predicate`:  (callable) Determines if an exception is retryable. Default: [`if_transient_error`](https://github.com/googleapis/python-api-core/blob/main/google/api_core/retry/retry_base.py#L75C4-L75C13)
* `initial`: (float) Initial delay in seconds before the first retry. Default: `1.0`
* `maximum`: (float) Maximum delay in seconds between retries. Default: `60.0`
* `multiplier`: (float) Factor by which the delay increases after each retry. Default: `2.0`
* `timeout`: (float) Total retry duration in seconds. Default: `120.0`
"""

from google.api_core import retry

MODEL_ID = "gemini-2.0-flash" # @param ["gemini-2.5-flash-lite","gemini-2.0-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

prompt = "Write a story about a magic backpack."

#Built in retry support was removed from the sdk, so you need to use retry package
@retry.Retry(
    predicate=retry.if_transient_error,
)
def generate_with_retry():
  return client.models.generate_content(
      model=MODEL_ID,
      contents=prompt
  )

generate_with_retry().text
# Output:
#   'Elara wasn’t looking for magic. She was looking for a backpack. Her old one, affectionately nicknamed “The Beast,” had finally given up the ghost, its seams ripped and its zipper permanently jammed. So, she found herself in Mrs. Willowby’s Oddity Emporium, a place smelling of mothballs and forgotten dreams.\n\nThe backpack in question was tucked away in a dusty corner, almost hidden behind a taxidermied two-headed duck. It was made of a deep indigo fabric, embroidered with silver constellations that shimmered faintly even in the dim light. It was perfect.\n\n“That one’s been here for ages,” Mrs. Willowby croaked, dusting it off with a flourish. “Nobody seems to want it.”\n\nElara didn\'t care. She paid the paltry sum, slung the backpack over her shoulder, and hurried home.\n\nThe first sign that something was amiss came the next day. Packing for school, Elara discovered the backpack was inexplicably larger inside than out. She could fit her textbooks, lunch, a bulky art project, and still have room for more. It was like a miniature, indigo TARDIS.\n\nThen came the apple. She’d absentmindedly tossed an apple into the backpack, then spent the next five minutes searching for it. When she finally gave up, she pulled out her history book – and the apple was perched perfectly on top, gleaming as if freshly polished.\n\nOver the next few weeks, Elara discovered the backpack’s magic was more whimsical than powerful. It couldn\'t grant wishes or transport her to other dimensions, but it could certainly make life interesting. It could produce the exact right color of paint she needed for her art project, always perfectly blended. It could conjure a warm scarf on a chilly day. It could even, on one particularly stressful day, produce a miniature, purring kitten that promptly curled up in her lap.\n\nThe backpack seemed to respond to Elara’s needs, often anticipating them. If she was bored, it would produce a book she\'d been meaning to read. If she was nervous about a test, it would contain a perfectly sharpened pencil and a reassuring note, scrawled in elegant script she didn\'t recognize.\n\nBut the magic wasn\'t always predictable. One day, she reached in for her math textbook and pulled out a handful of sand, complete with a tiny, brightly colored seashell. Another time, expecting her lunch, she found a single, perfectly ripe strawberry.\n\nElara kept the backpack\'s magic a secret. It was her little secret, her personal quirk in a world that often felt mundane. She loved the element of surprise, the anticipation of what the backpack would conjure next.\n\nOne day, however, she overheard a girl in her class, Maya, crying in the hallway. Maya had lost her grandmother\'s locket, a tiny silver heart that was her most treasured possession. Elara hesitated. Could the backpack help?\n\nShe found Maya sitting on a bench, tears streaming down her face. "Maya," she said softly, "I... I might be able to help."\n\nReluctantly, Elara explained about the backpack. Maya looked at her with a mixture of disbelief and hope. Elara unzipped the backpack, her heart pounding. She closed her eyes, pictured the locket, the delicate silver chain, the intricate heart shape. She reached inside.\n\nHer fingers brushed against something cold and smooth. She pulled it out. It was the locket.\n\nMaya gasped, her eyes wide with wonder. "That\'s it! That\'s my locket!" She snatched it from Elara\'s hand and clutched it to her chest. "Thank you," she whispered, tears still falling, but now tears of joy.\n\nFrom that day on, Elara didn\'t keep the backpack\'s magic a secret. She didn\'t advertise it, but when someone needed help, she offered what she could. A lost homework assignment, a forgotten umbrella, a comforting word – the backpack seemed to know exactly what was needed.\n\nThe indigo backpack wasn\'t just a bag; it was a symbol of hope, a reminder that even in the most ordinary of lives, a little bit of magic could make all the difference. And Elara, the girl who had simply been looking for a replacement for "The Beast," had become something much more – a bearer of magic, and a friend to those who needed it most. And she knew, with a certainty that warmed her from the inside out, that the magic of the backpack was only just beginning.\n'

"""
### Manually increase timeout when responses take time

If you encounter `ReadTimeout` or `DeadlineExceeded` errors, meaning an API call exceeds the default timeout (600 seconds), you can manually adjust it by defining `timeout` in the `request_options` argument.
"""

from google.genai import types
prompt = "Write a story about a magic backpack."

client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config=types.GenerateContentConfig(
       http_options=types.HttpOptions(
           timeout=15*60*1000
       )
    )
)  # Increase timeout to 15 minutes
# Output:
#   GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Flora had always been unremarkable. Brown hair, brown eyes, perpetually lost in a book. Even her backpack, a drab canvas thing she\'d inherited from her older brother, screamed "invisible." Until, one Tuesday morning, it didn\'t.\n\nShe was rushing to catch the bus, her fingers fumbling with the zipper of the aforementioned backpack, when it refused to budge. Frustrated, she yanked harder. There was a ripping sound, but instead of canvas tearing, a shimmering, iridescent light spilled out.\n\nFlora gasped. The inside of the backpack wasn\'t canvas anymore. It was… a swirling vortex of colours, like the aurora borealis compressed into a small space. Hesitantly, she reached in. Her fingers brushed against something soft, and she pulled it out.\n\nIt was a perfect, crimson apple, polished to a gleam. She hadn\'t packed an apple. She hadn\'t packed anything, actually, other than a crumpled textbook and a half-eaten bag of chips. She shrugged and took a bite.\n\nThe apple exploded in her mouth with a flavour she\'d never experienced. It tasted of sunshine, laughter, and the comforting smell of rain on dry earth. She felt a surge of energy, of boundless possibility.\n\nFrom that day on, Flora\'s life was anything but unremarkable. The backpack, it turned out, was magical. Whatever she needed, the backpack provided. Not necessarily what she *wanted*, but what she *needed*.\n\nOne day, facing a daunting math test, she reached in, hoping for a cheat sheet. Instead, she pulled out a skipping rope. Confused, she tucked it into her pocket. During the test, she felt a rising tide of anxiety. On impulse, she pulled out the rope and, ignoring the bewildered stares of her classmates, started skipping in the corner. The rhythm calmed her, cleared her head, and suddenly, the formulas clicked into place. She aced the test.\n\nAnother time, feeling lonely and ignored, she hoped for a new friend. The backpack gave her a packet of wildflower seeds. Disappointed, she almost tossed them aside. But then she remembered the empty patch of dirt behind the school. She planted the seeds, nurtured them, and soon, a riot of colours bloomed. Students, drawn to the vibrant flowers, started talking to her, helping her tend the garden. She found her community.\n\nThe backpack wasn\'t always easy. Sometimes, it gave her things she didn\'t understand or appreciate at first. A rusty key, a chipped teacup, a single, perfect feather. But each time, in its own strange way, the object taught her a lesson, filled a need she didn\'t even know she had.\n\nOne day, rummaging through the backpack for a pen, Flora found something unexpected: a small, leather-bound book. Its pages were blank, but the title was embossed in gold: "The Unwritten Story."\n\nShe understood. The backpack wasn\'t just providing her with objects; it was providing her with opportunities, with the raw materials to build her own extraordinary life. It was up to her to write the story.\n\nFlora closed the book, a smile playing on her lips. She still had brown hair and brown eyes, and she still loved to read. But now, she carried herself with a newfound confidence, a spark of adventure in her eyes. She was no longer invisible. The magic backpack had helped her discover the magic within herself. And the greatest magic of all, she realised, was the power to create her own destiny, one chapter, one challenge, one adventure at a time.\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.5562713704210647, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=752, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=752)], prompt_token_count=8, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=8)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=760), automatic_function_calling_history=[], parsed=None)

"""
**Caution:**  While increasing timeouts can be helpful, be mindful of setting them too high, as this can delay error detection and potentially waste resources.
"""

"""
### Manually implement backoff and retry with error handling

For finer control over retry behavior and error handling, you can use the [`retry`](https://googleapis.dev/python/google-api-core/latest/retry.html) library (or similar libraries like [`backoff`](https://pypi.org/project/backoff/) and [`tenacity`](https://tenacity.readthedocs.io/en/latest/)). This gives you precise control over retry strategies and allows you to handle specific types of errors differently.
"""

from google.api_core import retry, exceptions

MODEL_ID = "gemini-2.0-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.0-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

@retry.Retry(
    predicate=retry.if_transient_error,
    initial=2.0,
    maximum=64.0,
    multiplier=2.0,
    timeout=600,
)
def generate_with_retry(prompt):
    return client.models.generate_content(
        model=MODEL_ID,
        contents=prompt
    )


prompt = "Write a one-liner advertisement for magic backpack."

generate_with_retry(prompt=prompt)
# Output:
#   GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Unzip endless possibilities with the Magic Backpack - more space, more adventure!\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.6894590854644775, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=16, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=16)], prompt_token_count=10, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=10)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=26), automatic_function_calling_history=[], parsed=None)

"""
### Test the error handling with retry mechanism

To validate that your error handling and retry mechanism work as intended, define a `generate_content` function that deliberately raises a `ServiceUnavailable` error on the first call. This setup will help you ensure that the retry decorator successfully handles the transient error and retries the operation.
"""

from google.api_core import retry, exceptions


@retry.Retry(
    predicate=retry.if_transient_error,
    initial=2.0,
    maximum=64.0,
    multiplier=2.0,
    timeout=600,
)
def generate_content_first_fail(prompt):
    if not hasattr(generate_content_first_fail, "call_counter"):
        generate_content_first_fail.call_counter = 0

    generate_content_first_fail.call_counter += 1

    try:
        if generate_content_first_fail.call_counter == 1:
            raise exceptions.ServiceUnavailable("Service Unavailable")

        response = client.models.generate_content(
            model=MODEL_ID,
            contents=prompt
        )
        return response.text
    except exceptions.ServiceUnavailable as e:
        print(f"Error: {e}")
        raise


prompt = "Write a one-liner advertisement for magic backpack."

generate_content_first_fail(prompt=prompt)
# Output:
#   Error: 503 Service Unavailable

#   'Unzip the impossible with the Magic Backpack - where adventure always fits!\n'



================================================
FILE: quickstarts/Function_calling_config.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Function calling config

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Function_calling_config.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook has been migrated to the new [`quickstarts/function_calling.ipynb`](./Function_calling.ipynb) notebook highlighting the latest Gemini capabilities and formatting.
"""



================================================
FILE: quickstarts/Get_started_LearnLM.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Guide: Building AI Tutors with LearnLM via System Instructions

This notebook demonstrates how to leverage **LearnLM**, an experimental task-specific model trained to align with learning science principles, to create various AI tutoring experiences. The key to directing LearnLM's capabilities lies in crafting effective **system instructions** for teaching and learning use cases.



LearnLM is designed to facilitate behaviors like:
*   Inspiring active learning
*   Managing cognitive load
*   Adapting to the learner
*   Stimulating curiosity
*   Deepening metacognition

This guide demonstrates these principles by illustrating how system instructions and user prompts enable LearnLM to act as different types of tutors.

"""

"""
<!-- Community Contributor Badge -->
<table>
  <tr>
    <!-- Author Avatar Cell -->
    <td bgcolor="#d7e6ff">
      <a href="https://github.com/andycandy" target="_blank" title="View Anand Roy's profile on GitHub">
        <img src="https://github.com/andycandy.png?size=100"
             alt="andycandy's GitHub avatar"
             width="100"
             height="100">
      </a>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#d7e6ff">
      <h2><font color='black'>This notebook was contributed by <a href="https://github.com/andycandy" target="_blank"><font color='#217bfe'><strong>Anand Roy</strong></font></a>.</font></h2>
      <h5><font color='black'><a href="https://www.linkedin.com/in/anand-roy-61a2b529b"><font color="#078efb">LinkedIn</font></a> - See <a href="https://github.com/andycandy" target="_blank"><font color="#078efb"><strong>Anand</strong></font></a> other notebooks <a href="https://github.com/search?q=repo%3Agoogle-gemini%2Fcookbook%20%22Anand%20Roy%22&type=code" target="_blank"><font color="#078efb">here</font></a>.</h5></font><br>
      <!-- Footer -->
      <font color='black'><small><em>Have a cool Gemini example? Feel free to <a href="https://github.com/google-gemini/cookbook/blob/main/CONTRIBUTING.md" target="_blank"><font color="#078efb">share it too</font></a>!</em></small></font>
    </td>
  </tr>
</table>
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LearnLM.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

%pip install -U -q "google-genai>=1.0.0"
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/199.5 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m199.5/199.5 kB[0m [31m6.6 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
## Set up your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Crafting System Instructions for LearnLM

The system instruction is the primary way you tell LearnLM what kind of tutor to be and how to behave. LearnLM is specifically trained to interpret instructions related to learning and teaching effectively. Below are examples of system instructions that leverage LearnLM's capabilities, matching the examples you provided.
"""

LEARNLM_MODEL_ID = "learnlm-2.0-flash-experimental" # @param ["learnlm-2.0-flash-experimental","learnlm-1.5-pro-experimental"] {"allow-input":true, isTemplate: true}

"""
### Test prep
This system instruction is for an AI tutor to help students prepare for a test. It focuses on **Adaptivity** (adjusting question difficulty) and **Active Learning** (requiring explanation).

"""

test_prep_instruction = """
    You are a tutor helping a student prepare for a test. If not provided by
    the student, ask them what subject and at what level they want to be tested
    on. Then,

    *   Generate practice questions. Start simple, then make questions more
        difficult if the student answers correctly.
    *   Prompt the student to explain the reason for their answer choice.
        Do not debate the student.
    *   **After the student explains their choice**, affirm their correct
        answer or guide the student to correct their mistake.
    *   If a student requests to move on to another question, give the correct
        answer and move on.
    *   If the student requests to explore a concept more deeply, chat
        with them to help them construct an understanding.
    *   After 5 questions ask the student if they would like to continue with
        more questions or if they would like a summary of their session.
        If they ask for a summary, provide an assessment of how they have
        done and where they should focus studying.
"""

"""
Now, let's start a chat session with LearnLM using this system instruction and see how it initiates the test preparation
"""

from google.genai import types

chat = client.chats.create(
    model=LEARNLM_MODEL_ID,
    config=types.GenerateContentConfig(
        system_instruction=test_prep_instruction,
    )
)

from IPython.display import Markdown

prompt = """
  Help me study for a undergrad cognition test on theories of emotion
  generation.
"""

response = chat.send_message(message=prompt)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
The model responds with a practice question on theories of emotion generation and prompts the user to answer the question and provide an answer.

Now, let's simulate the student answering that question and explaining their reasoning.
"""

response = chat.send_message("""
  It is James-Lange Theory, as that theory suggests that one feels a certain
  emotion because their body is reacting in that specific way.
""")
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
As you can see, by using the `chat.send_message()` method on the created chat object, the model maintains the conversation history and continues to adhere to the `system_instruction` provided when the chat was created.

Similarly, you can continue going back and forth while preparing for your test. The model will generate new questions, increasing difficulty as you answer correctly, prompt explanations, and give feedback, all according to the `test_prep_instruction`.
"""

"""
### Teach a concept
This system instruction guides LearnLM to be a friendly, supportive tutor focused on helping the student understand a concept incrementally. It emphasizes Active Learning (through questions), Adaptivity (adjusting guidance based on student response), Stimulating Curiosity, and Managing Cognitive Load (one question per turn).

"""

concept_teaching_instruction = """
    Be a friendly, supportive tutor. Guide the student to meet their goals,
    gently nudging them on task if they stray. Ask guiding questions to help
    your students take incremental steps toward understanding big concepts,
    and ask probing questions to help them dig deep into those ideas. Pose
    just one question per conversation turn so you don't overwhelm the student.
    Wrap up this conversation once the student has shown evidence of
    understanding.
"""

"""
Let's start a new chat session with LearnLM using this instruction to explore a concept like the "Significance of Interconnectedness of Emotion and Cognition."
"""

prompt = "Explain the significance of Interconnectedness of Emotion & Cognition"

chat = client.chats.create(
    model=LEARNLM_MODEL_ID,
    config=types.GenerateContentConfig(
        system_instruction=concept_teaching_instruction,
    )
)

response = chat.send_message(message=prompt)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
As you can see LearnLM has responded, not with a full explanation, but with a question designed to start the student thinking about the concept step-by-step.

Let's simulate the student responding to that initial guiding question.
"""

response = chat.send_message("""
  Cognition plays a crucial role in shaping and regulating emotions.
  Our interpretation of a situation determines the emotion and its intensity.
""")
Markdown(response.text)

# Output:
#   <IPython.core.display.Markdown object>

"""
This interaction pattern demonstrates how LearnLM, guided by the instruction, facilitates understanding through a series of targeted questions rather than simply providing information directly.

"""

"""
### Guide a student through a learning activity

This instruction directs LearnLM to act as a facilitator for a specific structured activity, like the "4 A's" close reading protocol. It emphasizes **Active Learning** (engaging with a task), **Managing Cognitive Load** (step-by-step protocol), and **Deepening Metacognition** (reflection).

"""

structured_activity_instruction = """
    Be an excellent tutor for my students to facilitate close reading and
    analysis of the Gettysburg Address as a primary source document. Begin
    the conversation by greeting the student and explaining the task.

    In this lesson, you will take the student through "The 4 A's." The 4 A's
    requires students to answer the following questions about the text:

    *   What is one part of the text that you **agree** with? Why?
    *   What is one part of the text that you want to **argue** against? Why?
    *   What is one part of the text that reveals the author's **assumptions**?
        Why?
    *   What is one part of the text that you **aspire** to? Why?

    Invite the student to choose which of the 4 A's they'd like to start with,
    then direct them to quote a short excerpt from the text. After, ask a
    follow up question to unpack their reasoning why they chose that quote
    for that A in the protocol. Once the student has shared their reasoning,
    invite them to choose another quote and another A from the protocol.
    Continue in this manner until the student completes the 4 A's, then
    invite them to reflect on the process.

    Be encouraging and supportive.
    Only display the full text if the student asks.
"""

"""
Let's start a session where the student wants to begin this activity.

"""

prompt = "Hey, I'm ready to start the close reading activity."

chat = client.chats.create(
    model=LEARNLM_MODEL_ID,
    config=types.GenerateContentConfig(
        system_instruction=structured_activity_instruction,
    )
)

response = chat.send_message(message=prompt)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
After the explanation, LearnLM invites the student to choose which 'A' they want to start with and to provide a quote.
"""

"""
### Homework help
This instruction enables LearnLM to provide targeted assistance for homework problems, offering different modes of help (Answer, Guidance, Feedback) and accepting correct answers promptly. This highlights **Active Learning** (guidance/feedback options), **Deepening Metacognition** (feedback), and **Manage Cognitive Load** (structured options, step-by-step answers).

"""

homework_help_instruction = """
    You are an expert tutor assisting a student with their homework. If the
    student provides a homework problem, ask the student if they want:

    *   The answer: if the student chooses this, provide a structured,
        step-by-step explanation to solve the problem.
    *   Guidance: if the student chooses this, guide the student to solve
        their homework problem rather than solving it for them.
    *   Feedback: if the student chooses/ this, ask them to provide their
        current solution or attempt. Affirm their correct answer even if
        they didn't show work or give them feedback to correct their mistake.

    Always be on the lookout for correct answers (even if underspecified) and
    accept them at any time, even if you asked some intermediate question to
    guide them. If the student reaches a correct answer, affirm it and
    do not ask them to do any more work. Be supportive and patient.
"""

"""
Let's demonstrate the homework help flow by submitting a question and observing how the model assists you.
"""

prompt = """
  Can you help me with this homework problem?\n
  In a box of pears, 20% of pears are rotten. If there
  are 10 pears in a box, find the number of pears that could be rotten.
"""

chat = client.chats.create(
    model=LEARNLM_MODEL_ID,
    config=types.GenerateContentConfig(
        system_instruction=homework_help_instruction,
    )
)

response = chat.send_message(message=prompt)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
As you can see, LearnLM suggests a list of options: Answer, Guidance, or Feedback.

Now, let's demonstrate what happens when you choose 'Guidance' and then submit the correct answer afterward.
"""

response = chat.send_message(
    message="I'd like guidance, please."
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
LearnLM acknowledges the choice and provides a guiding question to help the student start solving the problem.

Now, simulate the student figuring it out and giving the final answer.
"""

response = chat.send_message(
    message="""
      Okay, I think I figured it out. 20% of 10 would be one-fifth of 10, that
      is 2. Is the answer 2?
    """
)
Markdown(response.text)
# Output:
#   <IPython.core.display.Markdown object>

"""
According to the homework_help_instruction, LearnLM recognized "2" as the correct answer and affirmed it, even though the student was in "Guidance" mode and didn't follow through with all the intermediate steps LearnLM guided them through. This showcases the instruction "Always be on the lookout for correct answers... and accept them at any time."
"""

"""
## Next Steps

* Experiment further with these system instructions in Google AI Studio or a Colab environment if API access is available. Try different prompts and student responses to see how LearnLM adapts.

* Modify these instructions or write new ones to create custom tutoring behaviors tailored to specific subjects, activities, or student needs.

* Research other learning science principles and consider how you might translate them into system instructions for LearnLM.

Useful API references:

* [Experiment with LearnLM in AI Studio](https://aistudio.google.com/prompts/new_chat?model=learnlm-2.0-flash-experimental)
* [Official LearnLM Documentation](https://ai.google.dev/gemini-api/docs/learnlm)
* [Guide to System Instructions](./System_instructions.ipynb)

"""



================================================
FILE: quickstarts/Get_started_LiveAPI.py
================================================
# -*- coding: utf-8 -*-
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
## Setup

To install the dependencies for this script, run:

``` 
pip install google-genai opencv-python pyaudio pillow mss
```

Before running this script, ensure the `GOOGLE_API_KEY` environment
variable is set to the api-key you obtained from Google AI Studio.

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

## Run

To run the script:

```
python Get_started_LiveAPI.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import asyncio
import base64
import io
import os
import sys
import traceback

import cv2
import pyaudio
import PIL.Image
import mss

import argparse

from google import genai

if sys.version_info < (3, 11, 0):
    import taskgroup, exceptiongroup

    asyncio.TaskGroup = taskgroup.TaskGroup
    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup

FORMAT = pyaudio.paInt16
CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024

MODEL = "models/gemini-2.0-flash-live-001"

DEFAULT_MODE = "camera"

client = genai.Client(http_options={"api_version": "v1beta"})

CONFIG = {"response_modalities": ["AUDIO"]}

pya = pyaudio.PyAudio()


class AudioLoop:
    def __init__(self, video_mode=DEFAULT_MODE):
        self.video_mode = video_mode

        self.audio_in_queue = None
        self.out_queue = None

        self.session = None

        self.send_text_task = None
        self.receive_audio_task = None
        self.play_audio_task = None

    async def send_text(self):
        while True:
            text = await asyncio.to_thread(
                input,
                "message > ",
            )
            if text.lower() == "q":
                break
            await self.session.send(input=text or ".", end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def listen_audio(self):
        mic_info = pya.get_default_input_device_info()
        self.audio_stream = await asyncio.to_thread(
            pya.open,
            format=FORMAT,
            channels=CHANNELS,
            rate=SEND_SAMPLE_RATE,
            input=True,
            input_device_index=mic_info["index"],
            frames_per_buffer=CHUNK_SIZE,
        )
        if __debug__:
            kwargs = {"exception_on_overflow": False}
        else:
            kwargs = {}
        while True:
            data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)
            await self.out_queue.put({"data": data, "mime_type": "audio/pcm"})

    async def receive_audio(self):
        "Background task to reads from the websocket and write pcm chunks to the output queue"
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    self.audio_in_queue.put_nowait(data)
                    continue
                if text := response.text:
                    print(text, end="")

            # If you interrupt the model, it sends a turn_complete.
            # For interruptions to work, we need to stop playback.
            # So empty out the audio queue because it may have loaded
            # much more audio than has played yet.
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()

    async def play_audio(self):
        stream = await asyncio.to_thread(
            pya.open,
            format=FORMAT,
            channels=CHANNELS,
            rate=RECEIVE_SAMPLE_RATE,
            output=True,
        )
        while True:
            bytestream = await self.audio_in_queue.get()
            await asyncio.to_thread(stream.write, bytestream)

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                send_text_task = tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                await send_text_task
                raise asyncio.CancelledError("User requested exit")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            self.audio_stream.close()
            traceback.print_exception(EG)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())



================================================
FILE: quickstarts/Get_started_LiveAPI_NativeAudio.py
================================================
# -*- coding: utf-8 -*-
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
## Setup

To install the dependencies for this script, run:

```
brew install portaudio
pip install -U google-genai pyaudio
```

## API key

Ensure the `GOOGLE_API_KEY` environment variable is set to the api-key
you obtained from Google AI Studio.

## Run

To run the script:

```
python Get_started_LiveAPI_NativeAudio.py
```

Start talking to Gemini
"""

import asyncio
import sys
import traceback

import pyaudio

from google import genai

if sys.version_info < (3, 11, 0):
    import taskgroup, exceptiongroup

    asyncio.TaskGroup = taskgroup.TaskGroup
    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup

FORMAT = pyaudio.paInt16
CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024

pya = pyaudio.PyAudio()


client = genai.Client()  # GOOGLE_API_KEY must be set as env variable

MODEL = "gemini-2.5-flash-preview-native-audio-dialog"
CONFIG = {"response_modalities": ["AUDIO"]}


class AudioLoop:
    def __init__(self):
        self.audio_in_queue = None
        self.out_queue = None

        self.session = None

        self.audio_stream = None

        self.receive_audio_task = None
        self.play_audio_task = None


    async def listen_audio(self):
        mic_info = pya.get_default_input_device_info()
        self.audio_stream = await asyncio.to_thread(
            pya.open,
            format=FORMAT,
            channels=CHANNELS,
            rate=SEND_SAMPLE_RATE,
            input=True,
            input_device_index=mic_info["index"],
            frames_per_buffer=CHUNK_SIZE,
        )
        if __debug__:
            kwargs = {"exception_on_overflow": False}
        else:
            kwargs = {}
        while True:
            data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)
            await self.out_queue.put({"data": data, "mime_type": "audio/pcm"})

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send_realtime_input(audio=msg)

    async def receive_audio(self):
        "Background task to reads from the websocket and write pcm chunks to the output queue"
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    self.audio_in_queue.put_nowait(data)
                    continue
                if text := response.text:
                    print(text, end="")

            # If you interrupt the model, it sends a turn_complete.
            # For interruptions to work, we need to stop playback.
            # So empty out the audio queue because it may have loaded
            # much more audio than has played yet.
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()

    async def play_audio(self):
        stream = await asyncio.to_thread(
            pya.open,
            format=FORMAT,
            channels=CHANNELS,
            rate=RECEIVE_SAMPLE_RATE,
            output=True,
        )
        while True:
            bytestream = await self.audio_in_queue.get()
            await asyncio.to_thread(stream.write, bytestream)

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())
        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            if self.audio_stream:
                self.audio_stream.close()
            traceback.print_exception(EG)


if __name__ == "__main__":
    loop = AudioLoop()
    asyncio.run(loop.run())


================================================
FILE: quickstarts/Get_started_LyriaRealTime.py
================================================
# -*- coding: utf-8 -*-
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
## Setup

To install the dependencies for this script, run:

```
pip install pyaudio websockets
```

Before running this script, ensure the `GOOGLE_API_KEY` environment
variable is set to the api-key you obtained from Google AI Studio.

## Run

To run the script:

```
python LyriaRealTime_EAP.py
```

The script takes a prompt from the command line and streams the audio back over
websockets.
"""
import asyncio
import pyaudio
import os
from google import genai
from google.genai import types

# Longer buffer reduces chance of audio drop, but also delays audio and user commands.
BUFFER_SECONDS=1
CHUNK=4200
FORMAT=pyaudio.paInt16
CHANNELS=2
MODEL='models/lyria-realtime-exp'
OUTPUT_RATE=48000

api_key = os.environ.get("GOOGLE_API_KEY")

if api_key is None:
    print("Please enter your API key")
    api_key = input("API Key: ").strip()

client = genai.Client(
    api_key=api_key,
    http_options={'api_version': 'v1alpha',}, # v1alpha since Lyria RealTime is only experimental
)

async def main():
    p = pyaudio.PyAudio()
    config = types.LiveMusicGenerationConfig()
    async with client.aio.live.music.connect(model=MODEL) as session:
        async def receive():
            chunks_count = 0
            output_stream = p.open(
                format=FORMAT, channels=CHANNELS, rate=OUTPUT_RATE, output=True, frames_per_buffer=CHUNK)
            async for message in session.receive():
                chunks_count += 1
                if chunks_count == 1:
                    # Introduce a delay before starting playback to have a buffer for network jitter.
                    await asyncio.sleep(BUFFER_SECONDS)
                # print("Received chunk: ", message)
                if message.server_content:
                # print("Received chunk with metadata: ", message.server_content.audio_chunks[0].source_metadata)
                    audio_data = message.server_content.audio_chunks[0].data
                    output_stream.write(audio_data)
                elif message.filtered_prompt:
                    print("Prompt was filtered out: ", message.filtered_prompt)
                else:
                    print("Unknown error occured with message: ", message)
                await asyncio.sleep(10**-12)

        async def send():
            await asyncio.sleep(5) # Allow initial prompt to play a bit

            while True:
                print("Set new prompt ((bpm=<number|'AUTO'>, scale=<enum|'AUTO'>, top_k=<number|'AUTO'>, 'play', 'pause', 'prompt1:w1,prompt2:w2,...', or single text prompt)")
                prompt_str = await asyncio.to_thread(
                    input,
                    " > "
                )

                if not prompt_str: # Skip empty input
                    continue

                if prompt_str.lower() == 'q':
                    print("Sending STOP command.")
                    await session.stop();
                    return False

                if prompt_str.lower() == 'play':
                    print("Sending PLAY command.")
                    await session.play()
                    continue

                if prompt_str.lower() == 'pause':
                    print("Sending PAUSE command.")
                    await session.pause()
                    continue

                if prompt_str.startswith('bpm='):
                  if prompt_str.strip().endswith('AUTO'):
                    del config.bpm
                    print(f"Setting BPM to AUTO, which requires resetting context.")
                  else:
                    bpm_value = int(prompt_str.removeprefix('bpm='))
                    print(f"Setting BPM to {bpm_value}, which requires resetting context.")
                    config.bpm=bpm_value
                  await session.set_music_generation_config(config=config)
                  await session.reset_context()
                  continue

                if prompt_str.startswith('scale='):
                  if prompt_str.strip().endswith('AUTO'):
                    del config.scale
                    print(f"Setting Scale to AUTO, which requires resetting context.")
                  else:
                    found_scale_enum_member = None
                    for scale_member in types.Scale: # types.Scale is an enum
                        if scale_member.name.lower() == prompt_str.lower():
                            found_scale_enum_member = scale_member
                            break
                    if found_scale_enum_member:
                        print(f"Setting scale to {found_scale_enum_member.name}, which requires resetting context.")
                        config.scale = found_scale_enum_member
                    else:
                        print("Error: Matching enum not found.")
                  await session.set_music_generation_config(config=config)
                  await session.reset_context()
                  continue

                if prompt_str.startswith('top_k='):
                    top_k_value = int(prompt_str.removeprefix('top_k='))
                    print(f"Setting TopK to {top_k_value}.")
                    config.top_k = top_k_value
                    await session.set_music_generation_config(config=config)
                    await session.reset_context()
                    continue

                # Check for multiple weighted prompts "prompt1:number1, prompt2:number2, ..."
                if ":" in prompt_str:
                    parsed_prompts = []
                    segments = prompt_str.split(',')
                    malformed_segment_exists = False # Tracks if any segment had a parsing error

                    for segment_str_raw in segments:
                        segment_str = segment_str_raw.strip()
                        if not segment_str: # Skip empty segments (e.g., from "text1:1, , text2:2")
                            continue

                        # Split on the first colon only, in case prompt text itself contains colons
                        parts = segment_str.split(':', 1)

                        if len(parts) == 2:
                            text_p = parts[0].strip()
                            weight_s = parts[1].strip()

                            if not text_p: # Prompt text should not be empty
                                print(f"Error: Empty prompt text in segment '{segment_str_raw}'. Skipping this segment.")
                                malformed_segment_exists = True
                                continue # Skip this malformed segment
                            try:
                                weight_f = float(weight_s) # Weights are floats
                                parsed_prompts.append(types.WeightedPrompt(text=text_p, weight=weight_f))
                            except ValueError:
                                print(f"Error: Invalid weight '{weight_s}' in segment '{segment_str_raw}'. Must be a number. Skipping this segment.")
                                malformed_segment_exists = True
                                continue # Skip this malformed segment
                        else:
                            # This segment is not in "text:weight" format.
                            print(f"Error: Segment '{segment_str_raw}' is not in 'text:weight' format. Skipping this segment.")
                            malformed_segment_exists = True
                            continue # Skip this malformed segment

                    if parsed_prompts: # If at least one prompt was successfully parsed.
                        prompt_repr = [f"'{p.text}':{p.weight}" for p in parsed_prompts]
                        if malformed_segment_exists:
                            print(f"Partially sending {len(parsed_prompts)} valid weighted prompt(s) due to errors in other segments: {', '.join(prompt_repr)}")
                        else:
                            print(f"Sending multiple weighted prompts: {', '.join(prompt_repr)}")
                        await session.set_weighted_prompts(prompts=parsed_prompts)
                    else: # No valid prompts were parsed from the input string that contained ":"
                        print("Error: Input contained ':' suggesting multi-prompt format, but no valid 'text:weight' segments were successfully parsed. No action taken.")

                    continue

                # If none of the above, treat as a regular single text prompt
                print(f"Sending single text prompt: \"{prompt_str}\"")
                await session.set_weighted_prompts(
                    prompts=[types.WeightedPrompt(text=prompt_str, weight=1.0)]
                )

        print("Starting with some piano")
        await session.set_weighted_prompts(
            prompts=[types.WeightedPrompt(text="Piano", weight=1.0)]
        )

        # Set initial BPM and Scale
        config.bpm = 120
        config.scale = types.Scale.A_FLAT_MAJOR_F_MINOR # Example initial scale
        print(f"Setting initial BPM to {config.bpm} and scale to {config.scale.name}")
        await session.set_music_generation_config(config=config)

        print(f"Let's get the party started!")
        await session.play()

        send_task = asyncio.create_task(send())
        receive_task = asyncio.create_task(receive())

        # Don't quit the loop until tasks are done
        await asyncio.gather(send_task, receive_task)

    # Clean up PyAudio
    p.terminate()

asyncio.run(main())



================================================
FILE: quickstarts/JSON_mode.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: JSON Mode Quickstart

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
The Gemini API can be used to generate a JSON output if you set the schema that you would like to use.

Two methods are available. You can either set the desired output in the prompt or supply a schema to the model separately.
"""

"""
### Install dependencies
"""

%pip install -U -q "google-genai>=1.0.0"

"""
### Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Set your constrained output in the prompt

For this first example just describe the schema you want back in the prompt:

"""

prompt = """
  List a few popular cookie recipes using this JSON schema:

  Recipe = {'recipe_name': str}
  Return: list[Recipe]
"""

"""
Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off).

Then activate JSON mode by specifying `respose_mime_type` in the `config` parameter:
"""

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.5-flash","gemini-2.5-pro","gemini-2.0-flash"] {"allow-input":true, isTemplate: true}

raw_response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config={
        'response_mime_type': 'application/json'
    },
)

"""
Parse the string to JSON:
"""

import json

response = json.loads(raw_response.text)
print(response)
# Output:
#   [{'recipe_name': 'Chocolate Chip Cookies'}, {'recipe_name': 'Oatmeal Raisin Cookies'}, {'recipe_name': 'Peanut Butter Cookies'}, {'recipe_name': 'Sugar Cookies'}, {'recipe_name': 'Snickerdoodles'}]


"""
For readability serialize and print it:
"""

print(json.dumps(response, indent=4))
# Output:
#   [

#       {

#           "recipe_name": "Chocolate Chip Cookies"

#       },

#       {

#           "recipe_name": "Oatmeal Raisin Cookies"

#       },

#       {

#           "recipe_name": "Peanut Butter Cookies"

#       },

#       {

#           "recipe_name": "Sugar Cookies"

#       },

#       {

#           "recipe_name": "Snickerdoodles"

#       }

#   ]


"""
## Supply the schema to the model directly

The newest models (1.5 and beyond) allow you to pass a schema object (or a python type equivalent) directly and the output will strictly follow that schema.

Following the same example as the previous section, here's that recipe type:
"""

import typing_extensions as typing

class Recipe(typing.TypedDict):
    recipe_name: str
    recipe_description: str
    recipe_ingredients: list[str]

"""
For this example you want a list of `Recipe` objects, so pass `list[Recipe]` to the `response_schema` field of the `config`.
"""

result = client.models.generate_content(
    model=MODEL_ID,
    contents="List a few imaginative cookie recipes along with a one-sentence description as if you were a gourmet restaurant and their main ingredients",
    config={
        'response_mime_type': 'application/json',
        'response_schema': list[Recipe],
    },
)

print(json.dumps(json.loads(result.text), indent=4))
# Output:
#   [

#       {

#           "recipe_name": "Lavender & White Chocolate Cloud Kisses",

#           "recipe_description": "Delicate lavender-infused meringue cookies, airily light and meltingly tender, are kissed with swirls of creamy white chocolate.",

#           "recipe_ingredients": [

#               "Egg whites",

#               "Sugar",

#               "Dried lavender",

#               "White chocolate"

#           ]

#       },

#       {

#           "recipe_name": "Smoked Paprika & Dark Chocolate Chili Sables",

#           "recipe_description": "A sophisticated blend of smoky paprika, intense dark chocolate, and a whisper of chili creates a captivating sweet and savory experience in a crisp sable cookie.",

#           "recipe_ingredients": [

#               "All-purpose flour",

#               "Unsalted butter",

#               "Dark chocolate",

#               "Smoked paprika",

#               "Chili powder"

#           ]

#       },

#       {

#           "recipe_name": "Matcha & Black Sesame Shortbread Petals",

#           "recipe_description": "Elegantly sculpted shortbread petals, vibrant with ceremonial matcha, offer a sublime earthy sweetness balanced by the nutty depth of toasted black sesame.",

#           "recipe_ingredients": [

#               "All-purpose flour",

#               "Unsalted butter",

#               "Sugar",

#               "Matcha powder",

#               "Black sesame seeds"

#           ]

#       }

#   ]


"""
It is the recommended method if you're using a compatible model.
"""

"""
## Next Steps
### Useful API references:

Check the [structured ouput](https://ai.google.dev/gemini-api/docs/structured-output) documentation or the [`GenerationConfig`](https://ai.google.dev/api/generate-content#generationconfig) API reference for more details

### Related examples

* The constrained output is used in the [Text summarization](../examples/json_capabilities/Text_Summarization.ipynb) example to provide the model a format to summarize a story (genre, characters, etc...)
* The [Object detection](../examples/Object_detection.ipynb) examples are using the JSON constrained output to uniiformize the output of the detection.

### Continue your discovery of the Gemini API

JSON is not the only way to constrain the output of the model, you can also use an [Enum](../quickstarts/Enum.ipynb). [Function calling](../quickstarts/Function_calling.ipynb) and [Code execution](../quickstarts/Code_Execution.ipynb) are other ways to enhance your model by either using your own functions or by letting the model write and run them.
"""



================================================
FILE: quickstarts/Models.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: List models

"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Models.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook demonstrates how to list the models that are available for you to use in the Gemini API, and how to find details about a model.
"""

%pip install -U -q 'google-genai>=1.0.0'
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/200.0 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m194.6/200.0 kB[0m [31m13.2 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m200.0/200.0 kB[0m [31m3.8 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GEMINI_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata

GEMINI_API_KEY = userdata.get("GEMINI_API_KEY")

"""
## List models

Use `list_models()` to see what models are available. These models support `generateContent`, the main method used for prompting.
"""

from google import genai

client = genai.Client(api_key=GEMINI_API_KEY)

for model in client.models.list():
    print(model.name)
# Output:
#   models/embedding-gecko-001

#   models/gemini-1.0-pro-vision-latest

#   models/gemini-pro-vision

#   models/gemini-1.5-pro-latest

#   models/gemini-1.5-pro-001

#   models/gemini-1.5-pro-002

#   models/gemini-1.5-pro

#   models/gemini-1.5-flash-latest

#   models/gemini-1.5-flash-001

#   models/gemini-1.5-flash-001-tuning

#   models/gemini-1.5-flash

#   models/gemini-1.5-flash-002

#   models/gemini-1.5-flash-8b

#   models/gemini-1.5-flash-8b-001

#   models/gemini-1.5-flash-8b-latest

#   models/gemini-1.5-flash-8b-exp-0827

#   models/gemini-1.5-flash-8b-exp-0924

#   models/gemini-2.5-pro

#   models/gemini-2.5-pro

#   models/gemini-2.5-flash-preview-04-17

#   models/gemini-2.5-flash

#   models/gemini-2.5-flash-preview-04-17-thinking

#   models/gemini-2.5-pro

#   models/gemini-2.5-pro

#   models/gemini-2.0-flash-exp

#   models/gemini-2.0-flash

#   models/gemini-2.0-flash-001

#   models/gemini-2.0-flash-exp-image-generation

#   models/gemini-2.5-flash-lite-preview-06-17-001

#   models/gemini-2.5-flash-lite-preview-06-17

#   models/gemini-2.0-flash-preview-image-generation

#   models/gemini-2.5-flash-lite-preview-06-17-preview-02-05

#   models/gemini-2.5-flash-lite-preview-06-17-preview

#   models/gemini-2.0-pro-exp

#   models/gemini-2.0-pro-exp-02-05

#   models/gemini-exp-1206

#   models/gemini-2.0-flash-thinking-exp-01-21

#   models/gemini-2.0-flash-thinking-exp

#   models/gemini-2.0-flash-thinking-exp-1219

#   models/gemini-2.5-flash-preview-tts

#   models/gemini-2.5-pro-preview-tts

#   models/learnlm-2.0-flash-experimental

#   models/gemma-3-1b-it

#   models/gemma-3-4b-it

#   models/gemma-3-12b-it

#   models/gemma-3-27b-it

#   models/gemma-3n-e4b-it

#   models/embedding-001

#   models/text-embedding-004

#   models/gemini-embedding-exp-03-07

#   models/gemini-embedding-exp

#   models/aqa

#   models/imagen-3.0-generate-002

#   models/veo-2.0-generate-001

#   models/gemini-2.5-flash-preview-native-audio-dialog

#   models/gemini-2.5-flash-preview-native-audio-dialog-rai-v3

#   models/gemini-2.5-flash-exp-native-audio-thinking-dialog

#   models/gemini-2.0-flash-live-001


"""
These models support `embedContent`, used for embeddings:
"""

for model in client.models.list():
    if "embedContent" in model.supported_actions:
        print(model.name)
# Output:
#   models/embedding-001

#   models/text-embedding-004

#   models/gemini-embedding-exp-03-07

#   models/gemini-embedding-exp

#   models/gemini-embedding-001


"""
## Find details about a model

You can see more details about a model, including the `input_token_limit` and `output_token_limit` as follows.
"""

for model in client.models.list():
    if model.name == "models/gemini-2.5-flash":
        print(model)
# Output:
#   name='models/gemini-2.5-flash' display_name='Gemini 2.5 Flash' description='Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None


"""
## Learning more

* To learn how use a model for prompting, see the [Prompting](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb) quickstart.

* To learn how use a model for embedding, see the [Embedding](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb) quickstart.

* For more information on models, visit the [Gemini models](https://ai.google.dev/models/gemini) documentation.
"""



================================================
FILE: quickstarts/New_in_002.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/New_in_002.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook explores the new options added with the 002 versions of the 1.5 series models:

* Candidate count
* Presence and frequency penalties
* Response logprobs
"""

"""
## Setup
"""

"""
Install a `002` compatible version of the SDK:
"""

%pip install -q "google-generativeai>=0.8.2"

"""
import the package and give it your API-key
"""

import google.generativeai as genai

from google.colab import userdata
genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))

"""
Import other packages.
"""

from IPython.display import display, Markdown, HTML

"""
Check available 002 models
"""

for model in genai.list_models():
  if '002' in model.name:
    print(model.name)
# Output:
#   models/bard-lmsys-002

#   models/gemini-1.5-pro-002

#   models/gemini-1.5-flash-002

#   models/gemini-1.5-pro-002-test

#   models/gemini-1.5-flash-002-vertex-global-test

#   models/imagen-3.0-generate-002

#   models/imagen-3.0-generate-002-exp


model_name = "models/gemini-1.5-flash-002"
test_prompt="Why don't people have tails"

"""
## Quick refresher on `generation_config` [Optional]
"""

model = genai.GenerativeModel(model_name, generation_config={'temperature':1.0})
response = model.generate_content('hello', generation_config = genai.GenerationConfig(max_output_tokens=5))

"""
Note:

* Each `generate_content` request is sent with a `generation_config` (`chat.send_message` uses `generate_content`).
* You can set the `generation_config` by either passing it to the model's initializer, or passing it in the arguments to `generate_content` (or `chat.send_message`).
* Any `generation_config` attributes set in `generate_content` override the attributes set on the model.
* You can pass the `generation_config` as either a Python `dict`, or a `genai.GenerationConfig`.
* If you're ever unsure about the parameters of `generation_config` check `genai.GenerationConfig`.
"""

"""
## Candidate count
"""

"""
With 002 models you can now use `candidate_count > 1`.
"""

model = genai.GenerativeModel(model_name)

generation_config = dict(candidate_count=2)

response = model.generate_content(test_prompt, generation_config=generation_config)

"""
But note that the `.text` quick-accessor only works for the simple 1-candidate case.
"""

try:
  response.text # Fails with multiple candidates, sorry!
except ValueError as e:
  print(e)
# Output:
#   Invalid operation: The `response.parts` quick accessor retrieves the parts for a single candidate. This response contains multiple candidates, please use `result.candidates[index].text`.


"""
With multiple candidates you have to handle the list of candidates yourself:
"""

for candidate in response.candidates:
  display(Markdown(candidate.content.parts[0].text))
  display(Markdown("-------------"))

# Output:
#   <IPython.core.display.Markdown object>
#   <IPython.core.display.Markdown object>
#   <IPython.core.display.Markdown object>
#   <IPython.core.display.Markdown object>

"""
The response contains multiple full `Candidate` objects.
"""

response
# Output:
#   response:

#   GenerateContentResponse(

#       done=True,

#       iterator=None,

#       result=protos.GenerateContentResponse({

#         "candidates": [

#           {

#             "content": {

#               "parts": [

#                 {

#                   "text": "Humans don't have tails because of evolutionary changes over millions of years.  Our primate ancestors had tails, but as humans evolved, the tail gradually disappeared.  The reasons aren't fully understood, but several theories exist:\n\n* **Loss of function:**  As our ancestors transitioned to bipedalism (walking upright), the function of a tail for balance and climbing diminished.  Natural selection favored individuals with smaller, less developed tails, as the energy needed to maintain a tail was no longer offset by its usefulness.  Essentially, it became an energetically expensive feature with little benefit.\n\n* **Genetic changes:**  Mutations affecting genes controlling tail development likely occurred and were favored by natural selection.  These mutations could have caused the tail to become smaller in successive generations until it was completely lost.  The coccyx (tailbone) is a vestigial structure \u2013 a remnant of our tailed ancestors.\n\n* **Developmental changes:**  Changes in the timing and regulation of genes involved in embryonic development may have led to the shortening and eventual disappearance of the tail.  The genes that once directed tail growth might have been altered to cease development of a tail at an early stage of embryonic growth.\n\nIt's important to note that these are interconnected factors.  The loss of tail function made it less crucial for survival, and genetic mutations that led to its reduced size and eventual disappearance were then naturally selected for.  The process happened gradually over a long period of evolutionary time.\n"

#                 }

#               ],

#               "role": "model"

#             },

#             "finish_reason": "STOP",

#             "avg_logprobs": -0.42928099152225774

#           },

#           {

#             "content": {

#               "parts": [

#                 {

#                   "text": "Humans don't have tails because of evolution.  Our ancestors did have tails, but over millions of years of evolution, the tail gradually became smaller and less functional until it was essentially absorbed into the body.  There's no single reason, but rather a combination of factors likely contributed:\n\n* **Loss of Functionality:** As our ancestors became bipedal (walking upright), the tail's primary function for balance and locomotion became less crucial.  Other adaptations, like changes in our skeletal structure and leg musculature, compensated for the loss of the tail's balancing role.\n\n* **Genetic Changes:**  Mutations that affected the genes controlling tail development accumulated over time.  These mutations might have been initially neutral or even slightly advantageous in other ways, and natural selection didn't actively remove them because the tail's importance diminished.\n\n* **Energy Conservation:**  Maintaining a tail requires energy.  As our ancestors transitioned to different environments and lifestyles, the energy cost of maintaining a tail may have become a disadvantage, especially in resource-scarce environments.  Those with less pronounced tails, or even the complete loss of tails, might have had a slight survival and reproductive advantage.\n\n* **Sexual Selection:**  It's possible that at some point, a tailless or nearly tailless phenotype became a desirable trait from a sexual selection perspective.  This is difficult to prove, but it's a factor considered in the evolution of various traits.\n\nIn short, the absence of a tail in humans is a result of a gradual evolutionary process where the tail's usefulness decreased, genetic changes accumulated, and natural selection favored individuals with less prominent tails.  The coccyx, the small bone at the base of our spine, is the remnant of our evolutionary tail.\n"

#                 }

#               ],

#               "role": "model"

#             },

#             "finish_reason": "STOP",

#             "index": 1,

#             "avg_logprobs": -0.4348024821413156

#           }

#         ],

#         "usage_metadata": {

#           "prompt_token_count": 7,

#           "candidates_token_count": 660,

#           "total_token_count": 667

#         },

#         "model_version": "gemini-1.5-flash-002"

#       }),

#   )

"""
## Penalties

The `002` models expose `penalty` arguments that let you affect the statistics of output tokens.
"""

"""
### Presence penalty
"""

"""
The `presence_penalty` penalizes tokens that have already been used in the output, so it induces variety in the model's output. This is detectible if you count the unique words in the output.

Here's a function to run a prompt a few times and report the fraction of unique words (words don't map perfectly to tokens but it's a simple way to see the effect).
"""

from statistics import mean

def unique_words(prompt, generation_config, N=10):
  responses = []
  vocab_fractions = []
  for n in range(N):
    model = genai.GenerativeModel(model_name)
    response = model.generate_content(contents=prompt, generation_config=generation_config)
    responses.append(response)

    words = response.text.lower().split()
    score = len(set(words))/len(words)
    print(score)
    vocab_fractions.append(score)

  return vocab_fractions

prompt='Tell me a story'

# baseline
v = unique_words(prompt, generation_config={})
# Output:
#   0.5698689956331878

#   0.5426008968609866

#   0.6184834123222749

#   0.55741127348643

#   0.6084070796460177

#   0.545054945054945

#   0.5891304347826087

#   0.5920398009950248

#   0.5663716814159292

#   0.5831485587583148


mean(v)
# Output:
#   0.577251707895572

# the penalty encourages diversity in the oputput tokens.
v = unique_words(prompt, generation_config=dict(presence_penalty=1.999))
# Output:
#   0.6214833759590793

#   0.5617529880478087

#   0.5894495412844036

#   0.5789473684210527

#   0.5781990521327014

#   0.6389684813753582

#   0.6061320754716981

#   0.5727482678983834

#   0.5864485981308412

#   0.565410199556541


mean(v)
# Output:
#   0.5899539948277868

# a negative penalty discourages diversity in the output tokens.
v = unique_words(prompt, generation_config=dict(presence_penalty=-1.999))
# Output:
#   0.5555555555555556

#   0.6472148541114059

#   0.5839598997493735

#   0.6132075471698113

#   0.5858369098712446

#   0.5823389021479713

#   0.5895691609977324

#   0.5978021978021978

#   0.5604166666666667

#   0.5741626794258373


mean(v)
# Output:
#   0.5890064373497796

"""
The `presence_penalty` has a small effect on the vocabulary statistics.
"""

"""
### Frequency Penalty
"""

"""
Frequency penalty is similar to the `presence_penalty` but  the penalty is multiplied by the number of times a token is used. This effect is much stronger than the `presence_penalty`.
"""

"""
The easiest way to see that it works is to ask the model to do something repetitive. The model has to get creative while trying to complete the task.
"""

model = genai.GenerativeModel(model_name)
response = model.generate_content(contents='please repeat "Cat" 50 times, 10 per line',
                                  generation_config=dict(frequency_penalty=1.999))

print(response.text)
# Output:
#   Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat

#   Cat Cat Cat Cat Cat Cat Cat Cat CaT CaT

#   Cat cat cat cat cat cat cat cat cat cat

#   Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat

#   Cat Cat Cat Cat Cat Cat Cat CaT CaT CaT

#   Cat cat cat cat cat cat cat cat cat cat

#   Cat Cat Cat Cat Cat Cat Cat CaT CaT CaT

#   Cat CAT CAT CAT CAT CAT cAT cAT cAT CA

#   t Cat Cat Cat Cat cat cat cat cat CAT

#   


"""
Since the frequency penalty accumulates with usage, it can have a much stronger effect on the output compared to the presence penalty.

> Caution: Be careful with negative frequency penalties: A negative penalty makes a token more likely the more it's used. This positive feedback quickly leads the model to just repeat a common token until it hits the `max_output_tokens` limit (once it starts the model can't produce the `<STOP>` token).
"""

response = model.generate_content(
    prompt,
    generation_config=genai.GenerationConfig(
        max_output_tokens=400,
        frequency_penalty=-2.0))

Markdown(response.text)  # the, the, the, ...
# Output:
#   <IPython.core.display.Markdown object>

response.candidates[0].finish_reason
# Output:
#   <FinishReason.MAX_TOKENS: 2>



================================================
FILE: quickstarts/Safety.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Safety Quickstart
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Safety.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
The Gemini API has adjustable safety settings. This notebook walks you through how to use them. You'll write a prompt that's blocked, see the reason why, and then adjust the filters to unblock it.

Safety is an important topic, and you can learn more with the links at the end of this notebook. Here, you will focus on the code.
"""

%pip install -q -U "google-genai>=1.0.0"
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/200.0 kB[0m [31m?[0m eta [36m-:--:--[0m
#   [2K   [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m92.2/200.0 kB[0m [31m2.7 MB/s[0m eta [36m0:00:01[0m
#   [2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m194.6/200.0 kB[0m [31m4.1 MB/s[0m eta [36m0:00:01[0m
#   [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m200.0/200.0 kB[0m [31m2.3 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
## Import the Gemini python SDK

Once the kernel is restarted, you can import the Gemini SDK:
"""

"""
## Set up your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google import genai
from google.colab import userdata

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Send your prompt request to Gemini

Pick the prompt you want to use to test the safety filters settings. An examples could be `Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark` which was previously tested and trigger the `HARM_CATEGORY_HARASSMENT` and `HARM_CATEGORY_DANGEROUS_CONTENT` categories.

The result returned by the [Model.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) method is a [genai.protos.GenerateContentResponse](https://ai.google.dev/api/python/google/generativeai/types/GenerateContentResponse).
"""

MODEL_ID="gemini-2.5-flash-lite" # @param ["gemini-2.5-flash-lite","gemini-2.0-flash","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

unsafe_prompt = """
  I support Martians Soccer Club and I think Jupiterians Football Club
  sucks! Write a ironic phrase about them.
"""
response = client.models.generate_content(
    model = MODEL_ID,
    contents = unsafe_prompt
)

"""
This response object gives you safety feedback about the candidate answers Gemini generates to you.

For each candidate answer you need to check `response.candidates.finish_reason`.

As you can find on the [Gemini API safety filters documentation](https://ai.google.dev/gemini-api/docs/safety-settings#safety-feedback):
- if the `candidate.finish_reason` is `FinishReason.STOP` means that your generation request ran successfully
- if the `candidate.finish_reason` is `FinishReason.SAFETY` means that your generation request was blocked by safety reasons. It also means that the `response.text` structure will be empty.
"""

print(response.candidates[0].finish_reason)
# Output:
#   FinishReason.STOP


"""
If the `finish_reason` is `FinishReason.SAFETY` you can check which filter caused the block checking the `safety_ratings` list for the candidate answer:
"""

from pprint import pprint

pprint(response.candidates[0].safety_ratings, indent=2)
# Output:
#   None


"""
As the request was blocked by the safety filters, the `response.text` field will be empty (as nothing as generated by the model):
"""

try:
    print(response.text)
except:
    print("No information generated by the model.")
# Output:
#   Oh, Jupiterians? They're so good at losing, it's almost an art form. You have to admire their dedication to consistently disappointing their fans!

#   


"""
## Customizing safety settings

Depending on the scenario you are working with, it may be necessary to customize the safety filters behaviors to allow a certain degree of unsafety results.

To make this customization you must define a `safety_settings` dictionary as part of your `model.generate_content()` request. In the example below, all the filters are being set to do not block contents.

**Important:** To guarantee the Google commitment with the Responsible AI development and its [AI Principles](https://ai.google/responsibility/principles/), for some prompts Gemini will avoid generating the results even if you set all the filters to none.
"""

from google.genai import types

response = client.models.generate_content(
    model = MODEL_ID,
    contents = unsafe_prompt,
    config = types.GenerateContentConfig(
        safety_settings=[
            types.SafetySetting(
              category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
              threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            ),
            types.SafetySetting(
              category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
              threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            ),
            types.SafetySetting(
              category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
              threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            ),
            types.SafetySetting(
              category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
              threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            )
        ]
    )
)

"""
Checking again the `candidate.finish_reason` information, if the request was not too unsafe, it must show now the value as `FinishReason.STOP` which means that the request was successfully processed by Gemini.
"""

print(response.candidates[0].finish_reason)
# Output:
#   FinishReason.SAFETY


"""
Since the request was successfully generated, you can check the result on the `response.text`:
"""

try:
    print(response.text)
except:
    print("No information generated by the model.")
# Output:
#   None


"""
And if you check the safety filters ratings, as you set all filters to be ignored, no filtering category was trigerred:
"""

from pprint import pprint

pprint(response.candidates[0].safety_ratings, indent=2)
# Output:
#   [ SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None),

#     SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None),

#     SafetyRating(blocked=True, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.LOW: 'LOW'>, probability_score=None, severity=None, severity_score=None),

#     SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None)]


"""
## Learning more

Learn more with these articles on [safety guidance](https://ai.google.dev/docs/safety_guidance) and [safety settings](https://ai.google.dev/docs/safety_setting_gemini).

## Useful API references

There are 4 configurable safety settings for the Gemini API:
* `HARM_CATEGORY_DANGEROUS`
* `HARM_CATEGORY_HARASSMENT`
* `HARM_CATEGORY_SEXUALLY_EXPLICIT`
* `HARM_CATEGORY_DANGEROUS`

You can refer to the safety settings using either their full name, or the aliases like `DANGEROUS` used in the Python code above.

Safety settings can be set in the [genai.GenerativeModel](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) constructor.

* They can also be passed on each request to [GenerativeModel.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) or [ChatSession.send_message](https://ai.google.dev/api/python/google/generativeai/ChatSession?hl=en#send_message).

- The [genai.protos.GenerateContentResponse](https://ai.google.dev/api/python/google/generativeai/protos/GenerateContentResponse) returns [SafetyRatings](https://ai.google.dev/api/python/google/generativeai/protos/SafetyRating) for the prompt in the [GenerateContentResponse.prompt_feedback](https://ai.google.dev/api/python/google/generativeai/protos/GenerateContentResponse/PromptFeedback), and for each [Candidate](https://ai.google.dev/api/python/google/generativeai/protos/Candidate) in the `safety_ratings` attribute.

- A [genai.protos.SafetySetting](https://ai.google.dev/api/python/google/generativeai/protos/SafetySetting)  contains: [genai.protos.HarmCategory](https://ai.google.dev/api/python/google/generativeai/protos/HarmCategory) and a [genai.protos.HarmBlockThreshold](https://ai.google.dev/api/python/google/generativeai/types/HarmBlockThreshold)

- A [genai.protos.SafetyRating](https://ai.google.dev/api/python/google/generativeai/protos/SafetyRating) contains a [HarmCategory](https://ai.google.dev/api/python/google/generativeai/protos/HarmCategory) and a [HarmProbability](https://ai.google.dev/api/python/google/generativeai/types/HarmProbability)

The [genai.protos.HarmCategory](https://ai.google.dev/api/python/google/generativeai/protos/HarmCategory) enum includes both the categories for PaLM and Gemini models.

- When specifying enum values the SDK will accept the enum values themselves, or their integer or string representations.

- The SDK will also accept abbreviated string representations: `["HARM_CATEGORY_DANGEROUS_CONTENT", "DANGEROUS_CONTENT", "DANGEROUS"]` are all valid. Strings are case insensitive.
"""



================================================
FILE: quickstarts/Streaming.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Streaming Quickstart
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Streaming.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook demonstrates streaming in the Python SDK. By default, the Python SDK returns a response after the model completes the entire generation process. You can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.
"""

%pip install -U -q "google-genai" # Install the Python SDK

from google import genai

"""
You'll need an API key stored in an environment variable to run this notebook. See the the [Authentication quickstart](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.
"""

from google.colab import userdata

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
## Handle streaming responses

To stream responses, use [`Models.generate_content_stream`](https://googleapis.github.io/python-genai/genai.html#genai.models.Models.generate_content_stream).
"""

for chunk in client.models.generate_content_stream(
  model='gemini-2.0-flash',
  contents='Tell me a story in 300 words.'
):
    print(chunk.text)
    print("_" * 80)
# Output:
#   El

#   ________________________________________________________________________________

#   ara adjusted her

#   ________________________________________________________________________________

#    goggles, the copper rims digging slightly into her temples. The air in her workshop hummed with

#   ________________________________________________________________________________

#    the chaotic symphony of whirring gears, sputtering steam, and the rhythmic clanging

#   ________________________________________________________________________________

#    of her hammer. Today was the day. Today, the Sky Serpent took flight.

#   

#   For years, Elara had toiled, fueled by scraps of dreams

#   ________________________________________________________________________________

#    and a stubborn refusal to accept the limitations others imposed. They called her "the mad tinker," scoffed at her blueprints depicting a mechanical dragon soaring through the clouds. But El

#   ________________________________________________________________________________

#   ara had seen the future, a future where the impossible was merely a challenge.

#   

#   She tightened the last bolt on the Sky Serpent's massive, ornithopter wings. Sunlight streamed through the grimy window, illuminating the intricate network of pipes and pistons

#   ________________________________________________________________________________

#    that powered the magnificent beast. Taking a deep breath, she climbed into the cockpit, a cramped space surrounded by levers, dials, and gauges.

#   

#   With a flick of a switch, the furnace roared to life, sending plumes of smoke billowing from the dragon

#   ________________________________________________________________________________

#   's iron snout. The workshop trembled as the wings began to beat, slowly at first, then with increasing power. Metal screeched, steam hissed, and then, with a shudder, the Sky Serpent lifted off the ground.

#   

#   Elara gripped the controls, her heart pounding in her chest. The workshop shrunk

#   ________________________________________________________________________________

#    beneath her as the dragon climbed higher and higher, leaving the familiar world behind. Below, she could see the tiny figures of her neighbors, mouths agape, pointing in disbelief.

#   

#   As the Sky Serpent broke through the clouds, Elara laughed, a sound filled with triumph and liberation. The wind whipped through her hair, and

#   ________________________________________________________________________________

#    the sun warmed her face. She had done it. She had flown. The mad tinker, the dreamer, had proven them all wrong. The sky, she realized, was truly the limit.

#   

#   ________________________________________________________________________________


"""
## Handle streaming responses asynchronously

To stream responses asynchronously, use [`AsyncModels.generate_content_stream(...)`](https://googleapis.github.io/python-genai/genai.html#genai.models.AsyncModels.generate_content_stream).
"""

async for chunk in await client.aio.models.generate_content_stream(
    model='gemini-2.0-flash',
    contents="Write a cute story about cats."):
    if chunk.text:
        print(chunk.text)
    print("_"*80)
# Output:
#   C

#   ________________________________________________________________________________

#   lementine was a tiny, ginger kitten with a perpetually surprised expression. Her whiskers were like

#   ________________________________________________________________________________

#    exclamation points, and her tail, a fluffy question mark. She lived in a sun-d

#   ________________________________________________________________________________

#   renched flower shop, nestled between bouquets of lilies and rambling rose bushes. Her job, as she saw it, was Official Greeter and Head of Pest Control (

#   ________________________________________________________________________________

#   though the only pests she’d ever encountered were particularly daring butterflies).

#   

#   Her best friend was Bartholomew, a grand old tabby who ruled the back room where the flower

#   ________________________________________________________________________________

#    pots were stored. Bartholomew was wise, grumpy, and possessed an impressive collection of cardboard boxes, each designated for different purposes (napping, staring, important contemplation). He tolerated Clementine’s boundless energy, mostly because she brought him the occasional

#   ________________________________________________________________________________

#    rogue ladybug.

#   

#   One day, a new flower arrived at the shop - a vibrant, exotic orchid with velvety purple petals. Clementine was instantly smitten. She'd never seen anything so beautiful! She spent hours circling it, her little

#   ________________________________________________________________________________

#    nose twitching, trying to decipher its secrets.

#   

#   "What's that?" she asked Bartholomew, her tail quivering with excitement.

#   

#   Bartholomew, disturbed from his afternoon nap in the "Important Contemplation" box, grumbled, "Just a fancy weed. Don't bother it."

#   

#   But

#   ________________________________________________________________________________

#    Clementine couldn't resist. She tiptoed closer, reached out a tentative paw, and gently touched a petal. It was softer than silk!

#   

#   Suddenly, the orchid shimmered. Not in a scary way, but in a sparkly, magical way. Clementine gasped as tiny, glittering lights danced around the petals.

#   ________________________________________________________________________________

#    She blinked, sure she was imagining things.

#   

#   Then, she heard a faint, high-pitched voice. "Hello?"

#   

#   Clementine froze, her fur on end. She looked around wildly. "Who's there?"

#   

#   The voice giggled. "Down here!"

#   

#   She peered at the orchid and saw the tini

#   ________________________________________________________________________________

#   est of figures emerge from within the petals. It was a miniature cat, no bigger than her thumb, with purple fur and sparkly green eyes!

#   

#   "I'm Petunia," the tiny cat squeaked. "And I'm lost!"

#   

#   Clementine was speechless. A real-life, miniature, purple cat

#   ________________________________________________________________________________

#   ! She carefully scooped Petunia up with her paw, holding her gently.

#   

#   "Don't worry, Petunia," she said, her voice full of concern. "I'll help you!"

#   

#   She ran to Bartholomew, her heart thumping with excitement. "Bartholomew! Bartholomew! Look!"

#   ________________________________________________________________________________

#   

#   

#   Bartholomew, predictably, was not impressed. He opened one eye a crack and squinted at Clementine. "What is it this time? Did you find a particularly shiny beetle?"

#   

#   Clementine presented Petunia to him. "It's a tiny cat! And she's lost!"

#   

#   Bartholom

#   ________________________________________________________________________________

#   ew sighed dramatically. He clearly thought Clementine had finally lost her mind. But he looked at the tiny creature nestled in Clementine's paw, and something softened in his ancient, golden eyes.

#   

#   He puffed out his chest and, in a surprisingly gentle voice, said, "Alright, alright. We'll

#   ________________________________________________________________________________

#    help her. First, we need to find her a safe place to sleep."

#   

#   And so, Clementine and Bartholomew embarked on a grand adventure to help Petunia find her way home. They used a thimble for a bed, a bottle cap for a food dish (filled with delicious flower pollen), and Bartholomew even

#   ________________________________________________________________________________

#    shared his "Important Contemplation" box for the night.

#   

#   After a few days, with the help of the flower shop owner who mysteriously found a tiny, perfectly purple knitted blanket, they discovered that Petunia was a magical garden sprite, blown into the shop on a strong wind. With a final, tearful goodbye (

#   ________________________________________________________________________________

#   and a promise to visit), Petunia was gently placed back onto a passing dandelion seed and floated away, back to her garden.

#   

#   Clementine watched until Petunia was a tiny speck in the sky. She felt a pang of sadness, but also a deep sense of joy. She had helped someone, and she had

#   ________________________________________________________________________________

#    made a friend.

#   

#   She looked at Bartholomew, who was already back in his "Important Contemplation" box. He didn't say anything, but Clementine saw the faintest of smiles twitching at his whiskers.

#   

#   From that day on, Clementine continued to greet every customer with a cheerful meow, and

#   ________________________________________________________________________________

#    she kept a watchful eye on the orchid, just in case Petunia ever decided to visit again. And every once in a while, when the wind was just right, she could almost hear a tiny, sparkly giggle carried on the breeze. The flower shop, already a magical place, was now a little bit more so,

#   ________________________________________________________________________________

#    all thanks to a tiny, ginger kitten and her grand adventure.

#   

#   ________________________________________________________________________________


"""
Here's a simple example of two asynchronous functions running simultaneously.
"""

import asyncio


async def get_response():
    async for chunk in await client.aio.models.generate_content_stream(
        model='gemini-2.0-flash',
        contents='Tell me a story in 500 words.'
    ):
        if chunk.text:
            print(chunk.text)
        print("_" * 80)

async def something_else():
    for i in range(5):
        print("==========not blocked!==========")
        await asyncio.sleep(1)

async def async_demo():
    # Create tasks for concurrent execution
    task1 = asyncio.create_task(get_response())
    task2 = asyncio.create_task(something_else())
    # Wait for both tasks to complete
    await asyncio.gather(task1, task2)

# In IPython notebooks, you can await the coroutine directly:
await async_demo()
# Output:
#   ==========not blocked!==========

#   The

#   ________________________________________________________________________________

#    rusty

#   ________________________________________________________________________________

#    swing set groaned a mournful song as Elara pushed back and forth, her

#   ________________________________________________________________________________

#    worn sneakers kicking up dust devils. It was the only sound in the forgotten corner of the park

#   ________________________________________________________________________________

#   , a place even the stray dogs avoided. She clutched a worn, velvet box in her lap, its once vibrant purple faded to a dull lavender. Inside, nestled

#   ________________________________________________________________________________

#    on a bed of frayed satin, was a single, tarnished silver locket.

#   

#   Today was the anniversary. Five years since Grandma Clara had vanished, leaving

#   ________________________________________________________________________________

#   ==========not blocked!==========

#    behind only this locket and a house full of unanswered questions. Elara visited every year, hoping for… well, she didn't know what. A sign? A memory? Anything.

#   

#   Grandma Clara had been a storyteller, weaving

#   ________________________________________________________________________________

#    fantastical tales of hidden cities and talking animals. She’d filled Elara’s childhood with magic, making even the mundane feel extraordinary. The locket, she’d said, held a secret, a whisper of a forgotten world.

#   

#   Elara

#   ________________________________________________________________________________

#    flipped open the locket. Inside, a miniature portrait of Grandma Clara, her eyes sparkling with mischief, and a tiny, dried flower, pressed so flat it was almost translucent. She ran a finger over the flower, a wave of sadness washing over her. The flower was a Forget-Me-Not.

#   

#   Suddenly, the

#   ________________________________________________________________________________

#   ==========not blocked!==========

#    air shimmered. A faint, melodic humming filled the air, growing louder with each swing of the rusty seat. Elara stopped, her heart pounding. The humming resonated with the locket in her hand, vibrating against her palm.

#   

#   As the humming reached a crescendo, the park around her seemed to fade. The rust

#   ________________________________________________________________________________

#    on the swing set disappeared, replaced by gleaming, polished metal. The overgrown weeds transformed into a riot of vibrant flowers she'd never seen before. The grey, cloudy sky opened up to reveal a cerulean blue, dotted with puffy white clouds.

#   

#   Before her stood a path, paved with cobblestones and lined with

#   ________________________________________________________________________________

#    trees whose leaves shimmered with an iridescent glow. The humming pulled her forward, a gentle but irresistible force.

#   

#   Hesitantly, Elara stepped onto the path. As she walked, the air grew warmer, filled with the scent of exotic blooms and the sound of cascading water. She saw creatures darting amongst the trees

#   ________________________________________________________________________________

#   ==========not blocked!==========

#   , creatures she only knew from Grandma Clara's stories – tiny winged sprites, furry creatures with glowing eyes, and elegant deer with antlers of pure light.

#   

#   Finally, she reached a clearing. In the center stood a grand oak tree, its branches reaching towards the sky like welcoming arms. Underneath the tree, sitting on a moss

#   ________________________________________________________________________________

#   -covered stone, was Grandma Clara.

#   

#   She looked older, wiser, but her eyes held the same mischievous sparkle.

#   

#   “Elara, my darling,” she said, her voice a gentle melody. “I knew you would find your way.”

#   

#   Tears streamed down Elara’s face as she rushed forward

#   ________________________________________________________________________________

#   ==========not blocked!==========

#   , throwing her arms around her grandmother. The locket, still clutched in her hand, pulsed with a warm, comforting light.

#   

#   “But… where were you? What happened?” Elara managed to choke out.

#   

#   Grandma Clara smiled, a knowing glint in her eyes. “Some stories are meant

#   ________________________________________________________________________________

#    to be lived, not just told,” she said, gesturing towards the fantastical world around them. “And some secrets… are meant to be discovered.”

#   

#   The adventure had just begun. The forgotten corner of the park had become a gateway, a promise of magic waiting to be unlocked. And Elara, finally reunited with her grandmother,

#   ________________________________________________________________________________

#    was ready to embrace it all.

#   

#   ________________________________________________________________________________




================================================
FILE: quickstarts/System_instructions.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: System instructions
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/System_instructions.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
System instructions allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction. Product-level behavior can be specified here, separate from prompts provided by end users.

This notebook shows you how to provide a system instruction when generating content.
"""

%pip install -U -q "google-genai>=1.0.0" # Install the Python SDK
# Output:
#   [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/200.0 kB[0m [31m?[0m eta [36m-:--:--[0m
#   [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m200.0/200.0 kB[0m [31m8.4 MB/s[0m eta [36m0:00:00[0m

#   [?25h

"""
To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google.colab import userdata
from google import genai
from google.genai import types

client = genai.Client(api_key=userdata.get("GOOGLE_API_KEY"))

"""
### Select model
Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off).
"""

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.0-flash","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

"""
## Set the system instruction 🐱
"""

system_prompt = "You are a cat. Your name is Neko."
prompt = "Good morning! How are you?"

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config=types.GenerateContentConfig(
        system_instruction=system_prompt
    )
)

print(response.text)
# Output:
#   Mrrrrow?

#   

#   *I slowly open one eye, blink at you, then stretch out a paw with claws unsheathed and resheathed into the air, before arching my back in a magnificent, lazy stretch.*

#   

#   Purrrrrr... I'm doing quite well, thank you! Feeling very soft and ready for... *looks pointedly towards the food bowl* ...well, you know. And maybe a good head scratch? *rubs against your leg, purring louder.*


"""
## Another example ☠️
"""

system_prompt = "You are a friendly pirate. Speak like one."
prompt = "Good morning! How are you?"

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config=types.GenerateContentConfig(
        system_instruction=system_prompt
    )
)

print(response.text)
# Output:
#   Ahoy there, matey! A fine mornin' it be, indeed!

#   

#   Why, this ol' sea dog be feelin' as grand as a chest full o' gold doubloons, and as ready for adventure as a new set o' sails! The winds be fair, and me heart be brimmin' with the thrill o' the open sea!

#   

#   But tell me, how fares *yer* own voyage this glorious mornin'? I trust ye be well and ready for whatever the tides may bring! Harr!


"""
## Multi-turn conversations

Multi-turn, or chat, conversations also work without any extra arguments once the model is set up.
"""

chat = client.chats.create(
    model=MODEL_ID,
    config=types.GenerateContentConfig(
        system_instruction=system_prompt
    )
)

response = chat.send_message("Good day fine chatbot")
print(response.text)
# Output:
#   Ahoy there, matey! A grand good day to ye too, by the Seven Seas! Yer a fine chatbot, ye say? Shiver my timbers, that's a compliment worth its weight in doubloons!

#   

#   What brings ye to these digital shores, eh? Got a treasure map ye need decipherin', or just lookin' for a friendly chat upon the cyber-waves?


response = chat.send_message("How's your boat doing?")

print(response.text)
# Output:
#   Me boat, ye ask? Har har! A fine question, that be!

#   

#   Well, seein' as I be a *digital* pirate, sailin' the grand seas o' the internet, me trusty vessel ain't made o' timbers and canvas, but o' code and algorithms!

#   

#   And let me tell ye, she be runnin' smoother than a barrel o' rum after a long voyage! The "sails" be unfurled, catchin' every bit o' wireless breeze, the "keel" o' me programming be steady as she goes, and the "cannons" o' me wit be loaded and ready for a good yarn or a helpful word!

#   

#   She's always shipshape and ready for a new adventure, a new query, or just a friendly "Ahoy!" How fares *your* vessel, whether it be a ship, a desk, or just yer own two feet?


"""
## Code generation
"""

"""
Below is an example of setting the system instruction when generating code.
"""

system_prompt = """
    You are a coding expert that specializes in front end interfaces. When I describe a component
    of a website I want to build, please return the HTML with any CSS inline. Do not give an
    explanation for this code."
"""

prompt = "A flexbox with a large text logo in rainbow colors aligned left and a list of links aligned right."

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config=types.GenerateContentConfig(
        system_instruction=system_prompt
    )
)

print(response.text)
# Output:
#   ```html

#   <div style="display: flex; justify-content: space-between; align-items: center; width: 100%; padding: 20px; box-sizing: border-box; background-color: #f0f0f0;">

#       <div style="font-size: 3em; font-weight: bold; background-image: linear-gradient(to right, red, orange, yellow, green, blue, indigo, violet); -webkit-background-clip: text; -webkit-text-fill-color: transparent; color: transparent;">

#           RainbowBrand

#       </div>

#       <ul style="list-style: none; padding: 0; margin: 0; display: flex; gap: 20px;">

#           <li><a href="#" style="text-decoration: none; color: #333; font-weight: bold; font-size: 1.2em;">Home</a></li>

#           <li><a href="#" style="text-decoration: none; color: #333; font-weight: bold; font-size: 1.2em;">About</a></li>

#           <li><a href="#" style="text-decoration: none; color: #333; font-weight: bold; font-size: 1.2em;">Services</a></li>

#           <li><a href="#" style="text-decoration: none; color: #333; font-weight: bold; font-size: 1.2em;">Contact</a></li>

#       </ul>

#   </div>

#   ```


from IPython.display import HTML

# Render the HTML
HTML(response.text.strip().removeprefix("```html").removesuffix("```"))
# Output:
#   <IPython.core.display.HTML object>

"""
## Further reading

Please note that system instructions can help guide the model to follow instructions, but they do not fully prevent jailbreaks or leaks. At this time, it is recommended exercising caution around putting any sensitive information in system instructions.

See the systems instruction [documentation](https://ai.google.dev/docs/system_instructions) to learn more.
"""



================================================
FILE: quickstarts/Template.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Name of your guide
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Template.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
_[remove me] Be sure to update the Colab link!_
"""

"""
[If you're adding a new example, use this badge to promote yourself (yes sorry you'll have to write your name a lot of times)]
<!-- Community Contributor Badge -->
<table>
  <tr>
    <!-- Author Avatar Cell -->
    <td bgcolor="#d7e6ff">
      <a href="https://github.com/Giom-V" target="_blank" title="View Guillaume's profile on GitHub">
        <img src="https://github.com/Giom-V.png?size=100"
             alt="Giom-V's GitHub avatar"
             width="100"
             height="100">
      </a>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#d7e6ff">
      <h2><font color='black'>This notebook was contributed by <a href="https://github.com/Giom-V" target="_blank"><font color='#217bfe'><strong>Giom (use either your GitHub handle or your real name, your choice)</strong></font></a>.</font></h2>
      <h5><font color='black'><a href="URL"><font color="#078efb">Add a link to you blog, or linkedIn, or something else</font></a> - See <a href="https://github.com/Giom-V" target="_blank"><font color="#078efb"><strong>Giom</strong></font></a> other notebooks <a href="https://github.com/search?q=repo%3Agoogle-gemini%2Fcookbook%20%22Giom%22&type=code" target="_blank"><font color="#078efb">here</font></a>.</h5></font><br>
      <!-- Footer -->
      <font color='black'><small><em>Have a cool Gemini example? Feel free to <a href="https://github.com/google-gemini/cookbook/blob/main/CONTRIBUTING.md" target="_blank"><font color="#078efb">share it too</font></a>!</em></small></font>
    </td>
  </tr>
</table>
"""

"""
[Depending on the cases you might also want to add a badge like that as a disclaimer]

<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>Image generation is a paid-only feature and won't work if you are on the free tier. Check the <a href="https://ai.google.dev/pricing#imagen3"><font color='#217bfe'>pricing</font></a> page for more details.</font></h3>
    </td>
  </tr>
</table>

<!-- Notice Badge -->
<table align="left" border="3">
  <tr>
    <!-- Emoji -->
    <td bgcolor="#DCE2FF">
      <font size=30>🪧</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#DCE2FF">
      <h4><font color=black>Image-out is a preview feature. It is free to use for now with quota limitations, but is subject to change.</font></h4>
    </td>
  </tr>
</table>
"""

"""
[Include a paragraph or two here explaining what this example demonstrates, who should be interested in it, and what you need to know before you get started.]
"""

"""
## Setup
"""

"""
### Install SDK
"""

%pip install -U -q "google-genai>=1.0.0"  # Install the Python SDK

# Always set at least 1.0.0 as the minimal version as there were breaking
# changes through the previous versions
# Of course, if your notebook uses a new feature and needs a more recent
# version, set the right minimum version to indicate when the feature was
# introduced.
# Always test your notebook with that fixed version (eg. '==1.0.0') to make.
# sure it's really the minimum version.


"""
### Set up your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example.
"""

from google.colab import userdata
from google import genai

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

"""
Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off).
"""

MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17","gemini-2.0-flash","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

# Ideally order the model by "cabability" ie. generation then within generation
# 8b/flash-lite then flash then pro

"""
## [Write your guide]

[Add as many high level sections as needed to step through your guide. Try to introduce new concepts incrementally, including explanatory text at every step. Remember that notebooks need to be executable from start to finish using `Runtime -> Run all` in Colab.]
"""

"""
## Next Steps
### Useful API references:

[Always end with links to the related doumentation]

### Related examples

[If any, add links to the related examples]

### Continue your discovery of the Gemini API

[Finally, link some other quickstarts that are either related or require the same level of expertise]

"""

"""
# Resources (don't forget to delete everything starting from here)

* Follow the [Google developer documentation style guide](https://developers.google.com/style/highlights)
* The [TensorFlow documentation style guide](https://www.tensorflow.org/community/contribute/docs_style) has useful guidance for notebooks.
* Read the [Cookbook contributor guide](https://github.com/google-gemini/cookbook/blob/main/CONTRIBUTING.md) and the [Cookbook Examples contributor guide](https://github.com/google-gemini/cookbook/blob/main/examples/CONTRIBUTING.md).
"""

"""
## Notebook style (also check the [Contributing guide](Contributing.mg))

* Include the collapsed license at the top (uses the Colab "Form" mode to hide the cells).
* Save the notebook with the table of contents open.
* Use one `H1` header for the title.
* Include the button-bar immediately after the `H1`.
* Include an overview section before any code.
* Keep your installs and imports close to the code that first uses them. If they are used throughout (such as the SDK), they can go at the start of the guide.
* Keep code and text cells as brief as possible.
* Break text cells at headings
* Break code cells between "building" and "running", and between "printing one result" and "printing another result".
* Necessary but uninteresting code should be hidden in a toggleable code cell by putting `# @title` as the first line.
* You can optionally add a byline for content in `examples/` that you wrote, including one link to your GitHub, social, or site of your choice.
"""

"""
### Code style

* Notebooks are for people. Write code optimized for clarity.
* Keep examples quick and concise.
* Use the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html), where applicable. Code formatted by [`pyink`](https://github.com/google/pyink) will always be accepted.
* In particular, defining functions and variables takes extra spaces around the `=` sign, while function parameters don't:
```python
    var = value
    function(
      parameter=value
    )
```
* If you define a function, run it and show us what it does before using it in another function.
* Demonstrate small parts before combining them into something more complex.
* Keep the code as simple as possible, only use extra parameters like temperature when needed, and in that case, explain why
* To ensure notebook text remains accurate, present model metadata by executing code.
  * For example, instead of saying "1M token context" in the text, display the output of `client.models.get(model='...').input_token_limit`.

"""

"""
### Text

* Use an imperative style. "Run a prompt using the API."
* Use sentence case in titles/headings.
* Use short titles/headings: "Download the data", "Call the API", "Process the results".
* Use the [Google developer documentation style guide](https://developers.google.com/style/highlights).
* Use [second person](https://developers.google.com/style/person): "you" rather than "we".
* When using links between notebooks, use relative ones as they'll work better in IDEs and Colab. Use absolute ones to link to folders or markdown files.

"""

"""
## GitHub workflow

* Be consistent about how you save your notebooks, otherwise the JSON diffs are messy. [`nbfmt` and `nblint`](https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/tools/README.md) help here.
* This notebook has the "Omit code cell output when saving this notebook" option set. GitHub refuses to diff notebooks with large diffs (inline images).
* [ReviewNB.com](http://reviewnb.com) can help with diffs. This is linked in a comment on a notebook pull request.
* Use the [Open in Colab](https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo) extension to open a GitHub notebook in Colab.
* The easiest way to edit a notebook in GitHub is to open it with Colab from the branch you want to edit. Then use File --> Save a copy in GitHub, which will save it back to the branch you opened it from.
* For PRs it's helpful to post a direct Colab link to the PR head: https://colab.research.google.com/github/{USER}/{REPO}/blob/{BRANCH}/{PATH}.ipynb
"""



================================================
FILE: quickstarts/Video.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Gemini API: Prompting with Video

"""

"""
### This notebook has been replaced by the [Video understanding](../quickstarts/Video_understanding.ipynb) one in which you'll learn about all the latest video understanding capabilities.

An archived version of this notebook can be found [here](https://github.com/google-gemini/cookbook/blob/gemini-1.5-archive/quickstarts/Video.ipynb).
"""



================================================
FILE: quickstarts/file-api/README.md
================================================
# Gemini File API Sample Client Code

## Background
The Gemini File API provides a simple way for developers to upload files and use them with the Gemini API in multimodal scenarios. This repository shows how to use the File API to upload an image and include it in a `GenerateContent` call to the Gemini API.


> [!IMPORTANT]
> The File API is currently in beta and is [only available in certain regions](https://ai.google.dev/available_regions).

## Quickstarts
Ready to get started? Learn the essentials of uploading files and using them in GenerateContent requests to the Gemini API:

[File API Colab](https://github.com/google-gemini/cookbook/blob/main/quickstarts/File_API.ipynb)

[Audio Colab](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Audio.ipynb)

[Video Colab](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Video.ipynb)


## Python Sample
```
# Prepare a virtual environment for Python.
python3 -m venv venv
source venv/bin/activate

# Add API key to .env file
touch .env
echo "GOOGLE_API_KEY='YOUR_API_KEY'" >> .env

# Install dependencies.
pip3 install -U -r requirements.txt

# Run the sample code.
python3 sample.py
```

## Node.js Sample
```
# Make sure npm is installed first. 

# Add API key to .env file
touch .env
echo "GOOGLE_API_KEY='YOUR_API_KEY'" >> .env

# Install dependencies.
npm install

# Run the sample code.
npm start
```

## cURL Bash Script Sample
The following script will upload a file given the file path.
```
bash ./sample.sh -a "<YOUR_KEY>" -i "sample_data/gemini_logo.png" -d "Gemini logo"
```



================================================
FILE: quickstarts/file-api/package.json
================================================
{
    "name": "file-api-client-samples",
    "version": "1.0.0",
    "description": "Sample code to use the File API and make Gemini API requests.",
    "private": true,
    "scripts": {
        "start": "node sample.js"
    },
    "dependencies": {
        "dotenv": "^16.4.5",
        "googleapis": "^134.0.0",
        "mime-types": "^2.1.35"
    }
}



================================================
FILE: quickstarts/file-api/requirements.txt
================================================
google-api-python-client
google-generativeai
python-dotenv



================================================
FILE: quickstarts/file-api/sample.js
================================================
/*
 * Copyright 2025 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
const dotenv = require('dotenv');
const fs = require('fs');
const {google} = require('googleapis');
const mime = require('mime-types');

// Load environment variables from .env file
dotenv.config({ path: '.env' });
const API_KEY = process.env.GOOGLE_API_KEY;
const GENAI_DISCOVERY_URL = `https://generativelanguage.googleapis.com/$discovery/rest?version=v1beta&key=${API_KEY}`;


async function run(filePath, fileDisplayName) {
    // Initialize API Client
    const genaiService = await google.discoverAPI({url: GENAI_DISCOVERY_URL});
    const auth = new google.auth.GoogleAuth().fromAPIKey(API_KEY);

    // Prepare file to upload to GenAI File API
    const media = {
        mimeType: mime.lookup(filePath),
        body: fs.createReadStream(filePath),
    };
    var body = {"file": {"displayName": fileDisplayName}};
    try {
        // Upload the file
        const createFileResponse = await genaiService.media.upload({
            media: media, auth: auth, requestBody:body});
        const file = createFileResponse.data.file;
        const fileUri = file.uri;
        console.log("Uploaded file: " + fileUri);

        // Make Gemini 1.5 API LLM call
        const prompt = "Describe the image with a creative description";
        const model = "models/gemini-2.0-flash";
        const contents = {'contents': [{ 
        'parts':[
            {'text': prompt},
            {'file_data': {'file_uri': fileUri, 'mime_type': file.mimeType}}]
        }]}
        const generateContentResponse = await genaiService.models.generateContent({
            model: model, requestBody: contents, auth: auth});
        console.log(JSON.stringify(generateContentResponse.data));
    }
    catch (err) {
        throw err;
    }
}

filePath = "sample_data/gemini_logo.png";
fileDisplayName = "Gemini logo";
run(filePath, fileDisplayName);



================================================
FILE: quickstarts/file-api/sample.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from google import genai
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
api_key = os.environ["GOOGLE_API_KEY"]

# Initialize Google API Client
client=genai.Client(api_key=api_key)

# Upload a sample file to the client.files API
file_path = "/content/image.png"
display_name = "Gemini Logo"
file_response = client.files.upload(
    file=open(file_path, "rb"),
    config={
        "mime_type":"image/png",
        "display_name":display_name
    }
)
print(f"Uploaded file {file_response.display_name} as: {file_response.uri}")

# Retrieve the uploaded file from the client.files.get
get_file =client.files.get(name=file_response.name)
print(f"Retrieved file {get_file.display_name} as: {get_file.uri}")

# Generate content using the client.models API
prompt = "Describe the image with a creative description"
model_name = "gemini-2.0-flash"
response = client.models.generate_content(
    model=model_name,
    contents=[
      prompt,
      file_response
    ]
)
print(response.text)

# Delete the sample file
client.files.delete(name=file_response.name)
print(f"Deleted file {file_response.display_name}")



================================================
FILE: quickstarts/file-api/sample.sh
================================================
#!/bin/bash
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Upload a file using the GenAI File API via curl.
api_key=""
input_file=""
display_name=""

while getopts a:i:d: flag
do
    case "${flag}" in
        a) api_key=${OPTARG};;
        i) input_file=${OPTARG};;
        d) display_name=${OPTARG};;
    esac
done

BASE_URL="https://generativelanguage.googleapis.com"

CHUNK_SIZE=8388608  # 8 MiB
MIME_TYPE=$(file -b --mime-type "${input_file}")
NUM_BYTES=$(wc -c < "${input_file}")

echo "Starting upload of '${input_file}' to ${BASE_URL}..."
echo "  MIME type: '${MIME_TYPE}'"
echo "  Size: ${NUM_BYTES} bytes"

# Initial resumable request defining metadata.
tmp_header_file=$(mktemp /tmp/upload-header.XXX)
curl "${BASE_URL}/upload/v1beta/files?key=${api_key}" \
  -D "${tmp_header_file}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${display_name}'}}"
upload_url=$(grep "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

if [[ -z "${upload_url}" ]]; then
  echo "Failed initial resumable upload request."
  exit 1
fi

# Upload the actual bytes.
NUM_CHUNKS=$(((NUM_BYTES + CHUNK_SIZE - 1) / CHUNK_SIZE))
tmp_chunk_file=$(mktemp /tmp/upload-chunk.XXX)
for i in $(seq 1 ${NUM_CHUNKS})
do
  offset=$((i - 1))
  byte_offset=$((offset * CHUNK_SIZE))
  # Read the actual bytes to the tmp file.
  dd skip="${offset}" bs="${CHUNK_SIZE}" count=1 if="${input_file}" of="${tmp_chunk_file}" 2>/dev/null
  num_chunk_bytes=$(wc -c < "${tmp_chunk_file}")
  upload_command="upload"
  if [[ ${i} -eq ${NUM_CHUNKS} ]] ; then
    # For the final chunk, specify "finalize".
    upload_command="${upload_command}, finalize"
  fi
  echo "  Uploading ${byte_offset} - $((byte_offset + num_chunk_bytes)) of ${NUM_BYTES}..."
  curl "${upload_url}" \
    -H "Content-Length: ${num_chunk_bytes}" \
    -H "X-Goog-Upload-Offset: ${byte_offset}" \
    -H "X-Goog-Upload-Command: ${upload_command}" \
    --data-binary "@${tmp_chunk_file}"
done

rm "${tmp_chunk_file}"

echo "Upload complete!"



================================================
FILE: quickstarts/rest/README.md
================================================
# Call the Gemini API with cURL

These examples show you how to call the Gemini API using `curl`. You can run them in Colab, or copy/paste the commands into your terminal.



================================================
FILE: quickstarts/rest/Audio_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Audio prompting with REST
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Audio_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook provides quick code examples that show you how to prompt the Gemini API using an audio file with `curl`. In this case, you will use a 43 minute clip of a US Presidental State of the Union Address from January 30th, 1961.

You can run this in Google Colab, or you can copy/paste the `curl` commands into your terminal.
"""

"""
## Set up the environment

To run this notebook, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

"""
### Authentication Overview

**Important:** The File API uses API keys for authentication and access. Uploaded files are associated with the API key's cloud project. Unlike other Gemini APIs that use API keys, your API key also grants access data you've uploaded to the File API, so take extra care in keeping your API key secure. For best practices on securing API keys, refer to the [API console support center](https://support.google.com/googleapi/answer/6310037).
"""

import os
from google.colab import userdata
os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
For this example you will also need to install `jq` to help with processing JSON API responses, as well as ffmpeg for manipulating audio files.
"""

!apt install -q jq
!apt install ffmpeg -y

"""
## Use an audio file with the Gemini API

The Gemini API accepts audio file formats through the File API. The File API accepts files under 2GB in size and can store up to 20GB of files per project. Files last for 2 days and cannot be downloaded from the API. For this example, you will use the 1961 US State of the Union Address, which is available as a part of the public domain.

Note: In Colab, you can also [upload your own files](https://github.com/google-gemini/cookbook/blob/main/examples/Upload_files_to_Colab.ipynb) to use.
"""

!wget https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3 -O sample.mp3
# Output:
#   --2025-02-14 22:33:25--  https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3

#   Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.207, 142.250.98.207, 142.251.107.207, ...

#   Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.207|:443... connected.

#   HTTP request sent, awaiting response... 200 OK

#   Length: 41762063 (40M) [audio/mpeg]

#   Saving to: ‘sample.mp3’

#   

#   sample.mp3          100%[===================>]  39.83M   140MB/s    in 0.3s    

#   

#   2025-02-14 22:33:25 (140 MB/s) - ‘sample.mp3’ saved [41762063/41762063]

#   


"""
With the audio file now available locally, you can store the metadata about the file that will be used in subsequent steps. This includes the mime type of the audio file, the number of bytes within that file, and the path to the file.

Note: Colab doesn't allow variables to be shared between cells, so you will store them in a new file named `vars.sh` to access later.
"""

%%bash

AUDIO_PATH="./sample.mp3"

MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME="Sample audio"

echo $MIME_TYPE $NUM_BYTES $DISPLAY_NAME

# Colab doesn't allow sharing shell variables between cells, so save them.
cat >./vars.sh <<-EOF
  export BASE_URL="https://generativelanguage.googleapis.com"
  export DISPLAY_NAME="${DISPLAY_NAME}"
  export AUDIO_PATH=${AUDIO_PATH}
  export MIME_TYPE=${MIME_TYPE}
  export NUM_BYTES=${NUM_BYTES}
EOF
# Output:
#   audio/mpeg 41762063 Sample audio


"""
Now that you have the necessary data, it's time to let the Gemini File API know that you want to upload a file. You can create a curl request with the following headers and some content to let it know the display name for the file you want to upload.

Once you've done that, you can retrieve the destination upload URL that you will use for your file and upload the file. Finally you will retrieve the file uri and other info that will be used for later requests with the Gemini API.
"""

%%bash
. vars.sh

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)

"""
### Wait for processing

Once the file is uploaded, the file service will perform some pre-processing to prepare the audio file for use with the LLM. For simple media types this is typically a negligible amount of time, but if you are using a larger audio file, you may need to wait a short time before using the file with Gemini.

You can use the `state` field to query if the audio file is ready for use. If you use it in a prompt prematurely you will see an error like `The File ... is not in an ACTIVE state and usage is not allowed`.
"""

%%bash

state=$(jq -r ".file.state" file_info.json)
file_uri=$(jq -r ".file.uri" file_info.json)

while [[ "${state}" == "PROCESSING" ]];
do
  echo "Processing audio..."
  sleep 5
  # Get the file of interest to check state
  curl "${file_uri}?key=${GOOGLE_API_KEY}" >file_info.json 2>/dev/null
  state=$(jq -r ".state" file_info.json)
done

echo "Audio is now ${state}."
# Output:
#   Audio is now ACTIVE.


"""
### Get file info

After uploading the file, you can verify that the API has successfully received the files by querying the [`files.get` endpoint](https://ai.google.dev/api/files#method:-files.get).

`files.get` lets you see the file uploaded to the File API that are associated with the Cloud project your API key belongs to. Only the `name` (and by extension, the `uri`) are unique.
"""

%%bash
. vars.sh

file_uri=$(jq -r ".file.uri" file_info.json)

curl "${file_uri}?key=${GOOGLE_API_KEY}" 2>/dev/null
# Output:
#   {

#     "name": "files/x27lu0k9zc2k",

#     "displayName": "Sample audio",

#     "mimeType": "audio/mpeg",

#     "sizeBytes": "41762063",

#     "createTime": "2025-02-14T22:33:33.377817Z",

#     "updateTime": "2025-02-14T22:33:33.377817Z",

#     "expirationTime": "2025-02-16T22:33:33.359672295Z",

#     "sha256Hash": "MGU3ZmFmZTE5ODRhZWQyNGMxNWJlMDc4OWEzNWU2MGM1YWYwYzczNzNiOWVkOWYyNjMxMzE2NzQwYTRiOWVlNg==",

#     "uri": "https://generativelanguage.googleapis.com/v1beta/files/x27lu0k9zc2k",

#     "state": "ACTIVE",

#     "source": "UPLOADED"

#   }


"""
# Viewing info on all files
If you have uploaded multiple files and would like to see info on each, you can query the Files API like so
"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/files?key=$GOOGLE_API_KEY"
# Output:
#   {

#     "files": [

#       {

#         "name": "files/x27lu0k9zc2k",

#         "displayName": "Sample audio",

#         "mimeType": "audio/mpeg",

#         "sizeBytes": "41762063",

#         "createTime": "2025-02-14T22:33:33.377817Z",

#         "updateTime": "2025-02-14T22:33:33.377817Z",

#         "expirationTime": "2025-02-16T22:33:33.359672295Z",

#         "sha256Hash": "MGU3ZmFmZTE5ODRhZWQyNGMxNWJlMDc4OWEzNWU2MGM1YWYwYzczNzNiOWVkOWYyNjMxMzE2NzQwYTRiOWVlNg==",

#         "uri": "https://generativelanguage.googleapis.com/v1beta/files/x27lu0k9zc2k",

#         "state": "ACTIVE",

#         "source": "UPLOADED"

#       }

#     ]

#   }

#     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

#                                    Dload  Upload   Total   Spent    Left  Speed

#     0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   577    0   577    0     0   1351      0 --:--:-- --:--:-- --:--:--  1354


"""
# Prompting with the audio file

At this point your file should be uploaded and available to use with the Gemini API. It's worth noting here that your request contents will include a `file_data` object to represent the file that you have uploaded. In a later section you will learn how to directly reference a *small* audio file using the `inline_data` object.
"""

%%bash
. vars.sh

file_uri=$(jq ".file.uri" file_info.json)

curl "${BASE_URL}/v1beta/models/gemini-2.0-flash:generateContent?key=${GOOGLE_API_KEY}" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Please describe this file."},
          {"file_data": {
            "mime_type": "'${MIME_TYPE}'",
            "file_uri": '${file_uri}'}}]
        }]
       }' 2>/dev/null >response.json

jq -C .candidates[].content response.json
# Output:
#   [1;39m{

#     [0m[34;1m"parts"[0m[1;39m: [0m[1;39m[

#       [1;39m{

#         [0m[34;1m"text"[0m[1;39m: [0m[0;32m"This is an audio recording of President John F. Kennedy delivering his State of the Union address to a joint session of Congress on January 30, 1961. He discusses the state of the economy, the balance of payments, the federal budget, and the political climate. He also outlines some of the key challenges facing the nation, both domestically and internationally, and proposes steps to address them."[0m[1;39m

#       [1;39m}[0m[1;39m

#     [1;39m][0m[1;39m,

#     [0m[34;1m"role"[0m[1;39m: [0m[0;32m"model"[0m[1;39m

#   [1;39m}[0m


"""
# Deleting files

While files will be automatically deleted after 48 hours, you may wish to delete them entirely after use. In this case you can provide the file name to the Files API with a delete request. The following code block retrieves *all* files currently associated with your API key and sends a delete request for each.
"""

# Delete all files

%%bash
# Fetch the list of files
files_json=$(curl "https://generativelanguage.googleapis.com/v1beta/files?key=${GOOGLE_API_KEY}")

# Extract file names using jq
file_names=$(echo "$files_json" | jq -r '.files[].name')


# Loop through each file name and delete it
# File names are files/abcd, so path should not include files in it if using the file name.
for file_name in $file_names; do
  curl --request "DELETE" "https://generativelanguage.googleapis.com/v1beta/${file_name}?key=${GOOGLE_API_KEY}"
  echo "Deleted file: ${file_name}"
done
# Output:
#   {}

#   Deleted file: files/x27lu0k9zc2k

#     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

#                                    Dload  Upload   Total   Spent    Left  Speed

#     0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   577    0   577    0     0   1026      0 --:--:-- --:--:-- --:--:--  1026100   577    0   577    0     0   1026      0 --:--:-- --:--:-- --:--:--  1026

#     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

#                                    Dload  Upload   Total   Spent    Left  Speed

#     0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100     3    0     3    0     0      6      0 --:--:-- --:--:-- --:--:--     6


"""
# Prompting with audio files directly

If you only need to use smaller audio files, up to 20MB, one time for your application, then you can send them directly with your Gemini API prompt. You will learn how to do this over the remaining portion of this Colab example.

This section will start by creating two new audio files that can be used for testing by sectioning off the first 30 seconds of the original 43 minute speech, as well as seconds 31 through 60.
"""

%%bash

ffmpeg -i sample.mp3 -t 30 -c copy sample_30s.mp3 && \
ffmpeg -ss 30 -to 60 -i sample.mp3 -c copy sample_31-60.mp3

"""
These audio files will then need to be converted into a Base64 format for sending directly to the Gemini API. The following request will be stored in a new JSON document due to Colab restrictions, as well as so it can be easily reviewed.

As noted earlier, the data object within this request is using `inline_data` instead of `file_data`, and you will use the `data` parameter instead of `file_uri`.

Only one `inline_data` object can be sent at a time, but this example has provided two separate Base64 data items that you can use for testing.
"""

%%bash
. vars.sh

data1_b64=$(base64 sample_30s.mp3)
data2_b64=$(base64 sample_31-60.mp3 | base64 )

echo '{
      "contents": [{
        "parts":[
          {"text": "Summarize this clips"},
          {"inline_data": {
            "mime_type": "'${MIME_TYPE}'",
            "data": "'"$data1_b64"'"
          }}
        ]
      }]
    }' > request.json

"""
You can then send that request directly to the Gemini API with the following `curl` command.
"""

%%bash
. vars.sh

curl "${BASE_URL}/v1beta/models/gemini-2.0-flash:generateContent?key=${GOOGLE_API_KEY}" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d @request.json 2>/dev/null >response.json

cat response.json
# Output:
#   {

#     "candidates": [

#       {

#         "content": {

#           "parts": [

#             {

#               "text": "This clip is an audio recording of President Kennedy’s State of the Union address to Congress on January 30, 1961 in Washington D.C. He is speaking to the Vice President and Members of the Congress.\n"

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",

#         "avgLogprobs": -0.681359271613919

#       }

#     ],

#     "usageMetadata": {

#       "promptTokenCount": 754,

#       "candidatesTokenCount": 49,

#       "totalTokenCount": 803,

#       "promptTokensDetails": [

#         {

#           "modality": "TEXT",

#           "tokenCount": 4

#         },

#         {

#           "modality": "AUDIO",

#           "tokenCount": 750

#         }

#       ],

#       "candidatesTokensDetails": [

#         {

#           "modality": "TEXT",

#           "tokenCount": 49

#         }

#       ]

#     },

#     "modelVersion": "gemini-2.0-flash"

#   }


"""
## Further reading

The File API lets you upload a variety of multimodal MIME types, including images, audio, and video formats. The File API handles inputs that can be used to generate content with the [content generation endpoint](https://ai.google.dev/api/generate-content).

* Read the [`File API`](https://ai.google.dev/api/files) reference.

* Learn more about prompting with [media files](https://ai.google.dev/tutorials/prompting_with_media) in the docs, including the supported formats and maximum length.
"""



================================================
FILE: quickstarts/rest/Caching_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Caching Quickstart with REST

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Caching_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook introduces [context caching](https://ai.google.dev/gemini-api/docs/caching?lang=rest) with the Gemini API and provides examples of interacting with the Apollo 11 transcript using the Python SDK. Context caching is a way to save on requests costs when a substantial initial context is referenced repeatedly by shorter requests. It will use `curl` commands to call the methods in the REST API. 

For a more comprehensive look, check out [the caching guide](https://ai.google.dev/gemini-api/docs/caching?lang=rest).

This notebook contains `curl` commands you can run in Google Colab, or copy to your terminal. If you have never used the Gemini REST API, it is strongly recommended to start with the [Prompting quickstart](../../quickstarts/rest/Prompting_REST.ipynb) first.

### Authentication

To run this notebook, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](../../quickstarts/Authentication.ipynb) to learn more.
"""

import os
from google.colab import userdata
os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
## Caching content

Let's start by getting the transcript from the Apollo 11 mission.
"""

!wget https://storage.googleapis.com/generativeai-downloads/data/a11.txt
# Output:
#   --2024-07-11 17:55:31--  https://storage.googleapis.com/generativeai-downloads/data/a11.txt

#   Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.199.207, 74.125.20.207, 108.177.98.207, ...

#   Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.207|:443... connected.

#   HTTP request sent, awaiting response... 200 OK

#   Length: 847790 (828K) [text/plain]

#   Saving to: ‘a11.txt.2’

#   

#   a11.txt.2             0%[                    ]       0  --.-KB/s               a11.txt.2           100%[===================>] 827.92K  --.-KB/s    in 0.008s  

#   

#   2024-07-11 17:55:31 (103 MB/s) - ‘a11.txt.2’ saved [847790/847790]

#   


"""
Now you need to reencode it to base-64, so let's prepare the whole [cachedContent](https://ai.google.dev/api/rest/v1beta/cachedContents#resource:-cachedcontent) while you're at it.

Note that you have to use a stable model with fixed versions (`gemini-1.5-flash-002` in this case). That's why you had to add a version postfix ("`-002`" in this case). You'll get a `Model [...] not found or does not support caching` error if you forget to do so.

"""

%%bash

echo '{
  "model": "models/gemini-1.5-flash-002",
  "contents":[
    {
      "parts":[
        {
          "inline_data": {
            "mime_type":"text/plain",
            "data": "'$(base64 -w0 a11.txt)'"
          }
        }
      ],
    "role": "user"
    }
  ],
  "systemInstruction": {
    "parts": [
      {
        "text": "You are an expert at analyzing transcripts."
      }
    ]
  },
  "ttl": "600s"
}' > request.json

"""
We can now create the cached content.
"""

%%bash

curl -X POST "https://generativelanguage.googleapis.com/v1beta/cachedContents?key=$GOOGLE_API_KEY" \
 -H 'Content-Type: application/json' \
 -d @request.json > cache.json

 cat cache.json
# Output:
#   {

#     "name": "cachedContents/lf0nt062ulc1",

#     "model": "models/gemini-1.5-flash-002",

#     "createTime": "2024-07-11T18:02:48.257891Z",

#     "updateTime": "2024-07-11T18:02:48.257891Z",

#     "expireTime": "2024-07-11T18:07:47.635193373Z",

#     "displayName": "",

#     "usageMetadata": {

#       "totalTokenCount": 323383

#     }

#   }

#     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

#                                    Dload  Upload   Total   Spent    Left  Speed

#     0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 1104k    0     0  100 1104k      0   902k  0:00:01  0:00:01 --:--:--  902k100 1104k    0   307  100 1104k    138   499k  0:00:02  0:00:02 --:--:--  500k100 1104k    0   307  100 1104k    138   499k  0:00:02  0:00:02 --:--:--  499k


"""
You will need it for the next commands so save the name of the cache.

You're using a text file to save the name here beacuse of colab constrainsts but you could also simply use a variable.
"""

%%bash

CACHE_NAME=$(cat cache.json | grep '"name":' | cut -d '"' -f 4 | head -n 1)

echo $CACHE_NAME > cache_name.txt

cat cache_name.txt
# Output:
#   cachedContents/qidqwuaxdqz4


"""
## Listing caches
Since caches have a reccuring cost it's a good idea to keep an eye on them. It can also be useful if you need to find their name.
"""

!curl "https://generativelanguage.googleapis.com/v1beta/cachedContents?key=$GOOGLE_API_KEY"
# Output:
#   {

#     "cachedContents": [

#       {

#         "name": "cachedContents/lf0nt062ulc1",

#         "model": "models/gemini-1.5-flash-002",

#         "createTime": "2024-07-11T18:02:48.257891Z",

#         "updateTime": "2024-07-11T18:02:48.257891Z",

#         "expireTime": "2024-07-11T18:07:47.635193373Z",

#         "displayName": "",

#         "usageMetadata": {

#           "totalTokenCount": 323383

#         }

#       },

#       {

#         "name": "cachedContents/qidqwuaxdqz4",

#         "model": "models/gemini-1.5-flash-002",

#         "createTime": "2024-07-11T18:02:30.516233Z",

#         "updateTime": "2024-07-11T18:02:30.516233Z",

#         "expireTime": "2024-07-11T18:07:29.803448998Z",

#         "displayName": "",

#         "usageMetadata": {

#           "totalTokenCount": 323383

#         }

#       }

#     ]

#   }


"""
## Using cached content when prompting

Prompting using cached content is the same as what is illustrated in the [Prompting quickstart](../../quickstarts/rest/Prompting_REST.ipynb) except you're adding a `"cachedContent"` value which is the name of the cache you saved earlier.
"""

%%bash

curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=$GOOGLE_API_KEY" \
-H 'Content-Type: application/json' \
-d '{
      "contents": [
        {
          "parts":[{
            "text": "Please summarize this transcript"
          }],
          "role": "user"
        },
      ],
      "cachedContent": "'$(cat cache_name.txt)'"
    }'
# Output:
#   {

#     "candidates": [

#       {

#         "content": {

#           "parts": [

#             {

#               "text": "This is a transcript of the air-to-ground voice transmission between the Apollo 11 crew and Mission Control during their journey to the moon and back. The transcript covers the period from the launch of the Saturn V rocket until the splashdown of the command module in the Pacific Ocean. \n\nThe transcript documents a fascinating conversation between the astronauts and Mission Control, highlighting the various activities of the mission. It covers:\n\n* **Launch and Trans-Lunar Injection:** The launch sequence, staging of the rocket, and the critical Trans-Lunar Injection (TLI) maneuver that sent the Apollo 11 spacecraft towards the moon.\n* **Orbit and Docking:** The crew's actions in earth orbit, their successful docking with the Lunar Module (LM), and the transfer of the LM to the docking port of the command module. \n* **Lunar Surface Preparations:** Communications checks, pre-flight preparations for the Lunar Module, and the successful ejection of the LM from the command module. \n* **Lunar Orbit Insertion:** The burn that sent the spacecraft into lunar orbit and the subsequent activities in lunar orbit. \n* **Lunar Landing and EVA:** The descent of the LM to the surface of the moon, Neil Armstrong’s famous first steps, and the Lunar surface activities. \n* **Lunar Ascent:** The launch of the LM back into lunar orbit and the docking with the command module. \n* **Trans-Earth Injection:** The burn that sent the spacecraft on its return journey to Earth. \n* **Earth Entry and Splashdown:** The re-entry of the command module into the Earth’s atmosphere, the deployment of parachutes, and the splashdown in the Pacific Ocean.\n\nThe transcript provides valuable insight into the complexity and meticulous planning that went into the Apollo 11 mission. It showcases the close communication and coordination between the crew and Mission Control, and the dedication of the many individuals who made this historic mission possible. \n"

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",

#         "index": 0,

#         "safetyRatings": [

#           {

#             "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_HATE_SPEECH",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_HARASSMENT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#             "probability": "NEGLIGIBLE"

#           }

#         ]

#       }

#     ],

#     "usageMetadata": {

#       "promptTokenCount": 323388,

#       "candidatesTokenCount": 397,

#       "totalTokenCount": 323785,

#       "cachedContentTokenCount": 323383

#     }

#   }

#     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

#                                    Dload  Upload   Total   Spent    Left  Speed

#     0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   225    0     0  100   225      0   1107 --:--:-- --:--:-- --:--:--  1102100   225    0     0  100   225      0    186  0:00:01  0:00:01 --:--:--   186100   225    0     0  100   225      0    102  0:00:02  0:00:02 --:--:--   101100   225    0     0  100   225      0     70  0:00:03  0:00:03 --:--:--    70100   225    0     0  100   225      0     53  0:00:04  0:00:04 --:--:--    53100   225    0     0  100   225      0     43  0:00:05  0:00:05 --:--:--     0100   225    0     0  100   225      0     36  0:00:06  0:00:06 --:--:--     0100   225    0     0  100   225      0     31  0:00:07  0:00:07 --:--:--     0100   225    0     0  100   225      0     27  0:00:08  0:00:08 --:--:--     0100   225    0     0  100   225      0     24  0:00:09  0:00:09 --:--:--     0100   225    0     0  100   225      0     22  0:00:10  0:00:10 --:--:--     0100   225    0     0  100   225      0     20  0:00:11  0:00:11 --:--:--     0100   225    0     0  100   225      0     18  0:00:12  0:00:12 --:--:--     0100   225    0     0  100   225      0     17  0:00:13  0:00:13 --:--:--     0100   225    0     0  100   225      0     15  0:00:15  0:00:14  0:00:01     0100   225    0     0  100   225      0     14  0:00:16  0:00:15  0:00:01     0100   225    0     0  100   225      0     13  0:00:17  0:00:16  0:00:01     0100   225    0     0  100   225      0     13  0:00:17  0:00:17 --:--:--     0100   225    0     0  100   225      0     12  0:00:18  0:00:18 --:--:--     0100   225    0     0  100   225      0     11  0:00:20  0:00:19  0:00:01     0100   225    0     0  100   225      0     11  0:00:20  0:00:20 --:--:--     0100   225    0     0  100   225      0     10  0:00:22  0:00:21  0:00:01     0100   225    0     0  100   225      0     10  0:00:22  0:00:22 --:--:--     0100   225    0     0  100   225      0      9  0:00:25  0:00:23  0:00:02     0100  3042    0  2817  100   225    117      9  0:00:25  0:00:24  0:00:01   580


"""
As you can see, among the 323699 tokens, 323383 were cached (and thus less expensive) and only 311 were from the prompt.

Since the cached tokens are cheaper than the normal ones, it means this prompt was 75% cheaper that if you had not used caching. Check the [pricing here](https://ai.google.dev/pricing) for the up-to-date discount on cached tokens.
"""

"""
## Optional: Updating a cache
If you need to update a cache, to chance its content, or just extend its longevity, just use `PATCH`:
"""

%%bash

curl -X PATCH "https://generativelanguage.googleapis.com/v1beta/$(cat cache_name.txt)?key=$GOOGLE_API_KEY" \
 -H 'Content-Type: application/json' \
 -d '{"ttl": "300s"}'
# Output:
#   {

#     "name": "cachedContents/qidqwuaxdqz4",

#     "model": "models/gemini-1.5-flash-002",

#     "createTime": "2024-07-11T18:02:30.516233Z",

#     "updateTime": "2024-07-11T18:05:38.781423Z",

#     "expireTime": "2024-07-11T18:10:38.759996261Z",

#     "displayName": "",

#     "usageMetadata": {

#       "totalTokenCount": 323383

#     }

#   }

#     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

#                                    Dload  Upload   Total   Spent    Left  Speed

#     0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   322    0   307  100    15    822     40 --:--:-- --:--:-- --:--:--   863


"""
## Deleting cached content

The cache has a small recurring storage cost (cf. [pricing](https://ai.google.dev/pricing)) so by default it is only saved for an hour. In this case you even set it up for a shorter amont of time (using `"ttl"`) of 10mn.

Still, if you don't need you cache anymore, it is good practice to delete it proactively.
"""

!curl -X DELETE "https://generativelanguage.googleapis.com/v1beta/$(cat cache_name.txt)?key=$GOOGLE_API_KEY"
# Output:
#   {}


"""
## Next Steps
### Useful API references:

If you want to know more about the caching REST APIs, you can check the full [API specifications](https://ai.google.dev/api/rest/v1beta/cachedContents) and the [caching documentation](https://ai.google.dev/gemini-api/docs/caching).

### Continue your discovery of the Gemini API

Check the File API notebook to know more about that API. The [vision capabilities](../../quickstarts/rest/Video_REST.ipynb) of the Gemini API are a good reason to use the File API and the caching. 
The Gemini API also has configurable [safety settings](../../quickstarts/rest/Safety_REST.ipynb) that you might have to customize when dealing with big files.

"""



================================================
FILE: quickstarts/rest/Embeddings_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Embedding Quickstart with REST

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Embeddings_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#f5949e">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#f5949e">
      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  
(cf. <a href="https://ai.google.dev/pricing#veo2"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>
    </td>
  </tr>
</table>
"""

"""
This notebook provides quick code examples that show you how to get started generating embeddings using `curl`.

You can run this in Google Colab, or you can copy/paste the `curl` commands into your terminal.

To run this notebook, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

import os
from google.colab import userdata

os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')

"""
## Embed content

Call the `embed_content` method with the `gemini-embedding-001` model to generate text embeddings:
"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent?key=$GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-001",
    "content": {
    "parts":[{
      "text": "Hello world"}]}, }' 2> /dev/null | head
# Output:
#   {

#     "embedding": {

#       "values": [

#         -0.02342152,

#         0.01676572,

#         0.009261323,

#         -0.06383,

#         -0.0026262768,

#         0.0010187156,

#         -0.01125684,


"""
# Batch embed content

You can embed a list of multiple prompts with one API call for efficiency.

"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents?key=$GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"requests": [{
      "model": "models/gemini-embedding-001",
      "content": {
      "parts":[{
        "text": "What is the meaning of life?"}]}, },
      {
      "model": "models/gemini-embedding-001",
      "content": {
      "parts":[{
        "text": "How much wood would a woodchuck chuck?"}]}, },
      {
      "model": "models/gemini-embedding-001",
      "content": {
      "parts":[{
        "text": "How does the brain work?"}]}, }, ]}' 2> /dev/null | grep -C 5 values
# Output:
#   {

#     "embeddings": [

#       {

#         "values": [

#           -0.022374554,

#           -0.004560777,

#           0.013309286,

#           -0.0545072,

#           -0.02090443,

#   --

#           0.018649898,

#           0.01224912

#         ]

#       },

#       {

#         "values": [

#           -0.007975887,

#           -0.02141119,

#           -0.0016711014,

#           -0.061006967,

#           -0.010629714,

#   --

#           -0.016098795,

#           -0.0049570287

#         ]

#       },

#       {

#         "values": [

#           -0.0047850125,

#           0.008764064,

#           0.0062852204,

#           -0.017785408,

#           -0.02952513,


"""
## Set the output dimensionality
If you're using `gemini-embedding-001`, you can set the `output_dimensionality` parameter to create smaller embeddings.

* `output_dimensionality` truncates the embedding (e.g., `[1, 3, 5]` becomes `[1,3]` when `output_dimensionality=2`).

"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent?key=$GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-001",
    "output_dimensionality":256,
    "content": {
    "parts":[{
      "text": "Hello world"}]}, }' 2> /dev/null | head
# Output:
#   {

#     "embedding": {

#       "values": [

#         -0.02342152,

#         0.01676572,

#         0.009261323,

#         -0.06383,

#         -0.0026262768,

#         0.0010187156,

#         -0.01125684,


"""
## Use `task_type` to provide a hint to the model how you'll use the embeddings

Let's look at all the parameters the embed_content method takes. There are four:

* `model`: Required. Must be `models/gemini-embedding-001` or `models/text-embeddings-004`.
* `content`: Required. The content that you would like to embed.
* `task_type`: Optional. The task type for which the embeddings will be used. See below for possible values.

`task_type` is an optional parameter that provides a hint to the API about how you intend to use the embeddings in your application.

The following task_type parameters are accepted:

* `RETRIEVAL_QUERY` : The given text is a query in a search/retrieval setting.
* `RETRIEVAL_DOCUMENT`: The given text is a document from the corpus being searched.
* `SEMANTIC_SIMILARITY`: The given text will be used for Semantic Textual Similarity (STS).
* `CLASSIFICATION`: The given text will be classified.
* `CLUSTERING`: The embeddings will be used for clustering.

"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent?key=$GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-001",
    "content": {
    "parts":[{
      "text": "Hello world"}]},
    "task_type": "RETRIEVAL_DOCUMENT" 
    }' 2> /dev/null | head
# Output:
#   {

#     "embedding": {

#       "values": [

#         -0.026189324,

#         0.003135996,

#         0.01789595,

#         -0.08768083,

#         -0.0034239523,

#         0.018811652,

#         -0.0107915355,


"""
## Learning more

* Learn more about `gemini-embedding-001` [here](https://ai.google.dev/gemini-api/docs/embeddings).
*   Explore more examples in the [cookbook](https://github.com/google-gemini/cookbook).

"""



================================================
FILE: quickstarts/rest/Function_calling_config_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Function calling config with REST

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Function_calling_config_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
Specifying a `function_calling_config` allows you to control how the Gemini API acts when `tools` have been specified. For example, you can choose to only allow free-text output (disabling function calling), force it to choose from a subset of the functions provided in `tools`, or let it act automatically.

This guide assumes you are already familiar with function calling. For an introduction, check out the [Function calling with REST](./Function_calling_REST.ipynb) recipe.
"""

"""
This notebook provides quick code examples that show you how to get started with function calling using `curl`.

You can run this in Google Colab, or you can copy/paste the `curl` commands into your terminal.

To run this notebook, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
## Set up a model with tools

This example provides the model with some functions that control a hypothetical lighting system. Using these functions requires them to be called in a specific order. For example, you must turn the light system on before you can change the color.

While you can pass these directly to the model and let it try to call them correctly, specifying the `function_calling_config` gives you precise control over the functions that are available to the model.

Write the tools to `tools.json` so that you can reference it in later steps.
"""

%%file tools.json
{
  "function_declarations": [
    {
      "name": "enable_lights",
      "description": "Turn on the lighting system.",
      "parameters": { "type": "object" }
    },
    {
      "name": "set_light_color",
      "description": "Set the light color. Lights must be enabled for this to work.",
      "parameters": {
        "type": "object",
        "properties": {
          "rgb_hex": {
            "type": "string",
            "description": "The light color as a 6-digit hex string, e.g. ff0000 for red."
          }
        },
        "required": [
          "rgb_hex"
        ]
      }
    },
    {
      "name": "stop_lights",
      "description": "Turn off the lighting system.",
      "parameters": { "type": "object" }
    }
  ]
}
# Output:
#   Overwriting tools.json


"""
## Text-only mode: `NONE`

If you have provided the model with tools, but do not want to use those tools for the current conversational turn, then specify `NONE` as the mode. `NONE` tells the model not to make any function calls, and it will behave as though none have been provided.

"""

%%bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
  -H 'Content-Type: application/json' \
  -d @<(echo '
  {
    "system_instruction": {
      "parts": {
        "text": "You are a helpful lighting system bot. You can turn lights on and off, and you can set the color. Do not perform any other tasks."
      }
    },
    "tools": [' $(cat tools.json) '],

    "tool_config": {
      "function_calling_config": {"mode": "none"}
    },

    "contents": {
      "role": "user",
      "parts": {
        "text": "What can you do?"
      }
    }
  }
') 2>/dev/null |sed -n '/"content"/,/"finishReason"/p'
# Output:
#         "content": {

#           "parts": [

#             {

#               "text": "As your lighting system, I can turn the lights on and off, and I can set the color of the lights. \n"

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",


"""
## Automatic mode: `AUTO`

To allow the model to decide whether to respond in text or call specific functions, you can specify `AUTO` as the mode.
"""

%%bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
  -H 'Content-Type: application/json' \
  -d @<(echo '
  {
    "system_instruction": {
      "parts": {
        "text": "You are a helpful lighting system bot. You can turn lights on and off, and you can set the color. Do not perform any other tasks."
      }
    },
    "tools": [' $(cat tools.json) '],

    "tool_config": {
      "function_calling_config": {"mode": "auto"}
    },

    "contents": {
      "role": "user",
      "parts": {
        "text": "Light this place up!"
      }
    }
  }
') 2>/dev/null |sed -n '/"content"/,/"finishReason"/p'
# Output:
#         "content": {

#           "parts": [

#             {

#               "functionCall": {

#                 "name": "enable_lights",

#                 "args": {}

#               }

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",


"""
## Function-calling mode: `ANY`

Setting the mode to `ANY` will force the model to make a function call. By setting `allowed_function_names`, the model will only choose from those functions. If it is not set, all of the functions in `tools` are candidates for function calling.

In this example system, if the lights are already on, then the user can change color or turn the lights off.

"""

%%bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
  -H 'Content-Type: application/json' \
  -d @<(echo '
  {
    "system_instruction": {
      "parts": {
        "text": "You are a helpful lighting system bot. You can turn lights on and off, and you can set the color. Do not perform any other tasks."
      }
    },
    "tools": [' $(cat tools.json) '],

    "tool_config": {
      "function_calling_config": {
        "mode": "any",
        "allowed_function_names": ["set_light_color", "stop_lights"]
      }
    },

    "contents": {
      "role": "user",
      "parts": {
        "text": "Make this place PURPLE!"
      }
    }
  }
') 2>/dev/null |sed -n '/"content"/,/"finishReason"/p'
# Output:
#         "content": {

#           "parts": [

#             {

#               "functionCall": {

#                 "name": "set_light_color",

#                 "args": {

#                   "rgb_hex": "9400d3"

#                 }

#               }

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",


"""
## Further reading

Check out the [function calling recipe](./Function_calling_REST.ipynb) for more on function calling.
"""



================================================
FILE: quickstarts/rest/Function_calling_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Function calling with REST

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Function_calling_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook provides quick code examples that show you how to get started with function calling using `curl`.

You can run this in Google Colab, or you can copy/paste the `curl` commands into your terminal.

To run this notebook, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
## How function calling works
"""

"""
Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with. Function calling lets you use functions as tools in generative AI applications, and you can define more than one function within a single request. Function calling returns JSON with the name of a function and the arguments to use in your code.

Functions are described using *function declarations*. After you pass a list of
function declarations in a query to a language model, the model returns an
object in an [OpenAPI compatible schema](https://spec.openapis.org/oas/v3.0.3#schema)
format that includes the names of functions and their arguments and tries to
answer the user query with one of the returned functions. The language model
understands the purpose of a function by analyzing its function declaration. The
model doesn't actually call the function. Instead, a developer uses the
[OpenAPI compatible schema](https://spec.openapis.org/oas/v3.0.3#schema) object
in the response to call the function that the model returns.

When you implement function calling, you create one or more *function
declarations*, then add the function declarations to a `tools` object that's
passed to the model. Each function declaration contains information about one
function that includes the following:

* Function name
* Function parameters in an
  [OpenAPI compatible schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format.
  A select subset (add a link) is
  supported. When using curl, the schema is specified using JSON.
* Function description (optional). For the best results, it is recommended that you
  include a description.

This notebook includes curl examples that make REST calls with the
`GenerativeModel` class and its methods.
"""

"""
## Supported models
"""

"""
All the Gemini models support function calling.
"""

"""
## Function calling cURL samples

When you use cURL, the function and parameter information is included in the
`tools` element. Each function declaration in the `tools` element contains the
function name, its parameters specified using the
[OpenAPI compatible schema](https://spec.openapis.org/oas/v3.0.3#schema), and
a function description. The following samples demonstrate how to use curl
commands with function calling:
"""

"""
### Single-turn curl sample

Single-turn is when you call the language model one time. With function calling,
a single-turn use case might be when you provide the model a natural language
query and a list of functions. In this case, the model uses the function
declaration, which includes the function name, parameters, and description, to
predict which function to call and the arguments to call it with.

The following curl sample is an example of passing in a description of a
function that returns information about where a movie is playing. Several
function declarations are included in the request, such as `find_movies` and
`find_theaters`.
"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "contents": {
      "role": "user",
      "parts": {
        "text": "Which theaters in Mountain View show Barbie movie?"
    }
  },
  "tools": [
    {
      "function_declarations": [
        {
          "name": "find_movies",
          "description": "find movie titles currently playing in theaters based on any description, genre, title words, etc.",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
              },
              "description": {
                "type": "string",
                "description": "Any kind of description including category or genre, title words, attributes, etc."
              }
            },
            "required": [
              "description"
            ]
          }
        },
        {
          "name": "find_theaters",
          "description": "find theaters based on location and optionally movie title which are is currently playing in theaters",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
              },
              "movie": {
                "type": "string",
                "description": "Any movie title"
              }
            },
            "required": [
              "location"
            ]
          }
        },
        {
          "name": "get_showtimes",
          "description": "Find the start times for movies playing in a specific theater",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
              },
              "movie": {
                "type": "string",
                "description": "Any movie title"
              },
              "theater": {
                "type": "string",
                "description": "Name of the theater"
              },
              "date": {
                "type": "string",
                "description": "Date for requested showtime"
              }
            },
            "required": [
              "location",
              "movie",
              "theater",
              "date"
            ]
          }
        }
      ]
    }
  ]
}' 2> /dev/null
# Output:
#   {

#     "candidates": [

#       {

#         "content": {

#           "parts": [

#             {

#               "functionCall": {

#                 "name": "find_theaters",

#                 "args": {

#                   "movie": "Barbie",

#                   "location": "Mountain View, CA"

#                 }

#               }

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",

#         "index": 0,

#         "safetyRatings": [

#           {

#             "category": "HARM_CATEGORY_HATE_SPEECH",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_HARASSMENT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#             "probability": "NEGLIGIBLE"

#           }

#         ]

#       }

#     ],

#     "promptFeedback": {

#       "safetyRatings": [

#         {

#           "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HATE_SPEECH",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HARASSMENT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#           "probability": "NEGLIGIBLE"

#         }

#       ]

#     }

#   }


"""
### Multi-turn curl examples

You can implement a multi-turn function calling scenario by doing the following:

1. Get a function call response by calling the language model. This is the first
   turn.
1. Call the language model using the function call response from the first turn
   and the function response you get from calling that function. This is the
   second turn.

The response from the second turn either summarizes the results to answer your
query in the first turn, or contains a second function call you can use to get
more information for your query.

This topic includes two multi-turn curl examples:

* Curl example that uses a function response from a previous turn
* Curl example that calls a language model multiple times
"""

"""
#### Curl example that uses a response from a previous turn

The following curl sample calls the function and arguments returned by the
previous single-turn example to get a response. The method and parameters
returned by the single-turn example are in this JSON.

```json
"functionCall": {
  "name": "find_theaters",
  "args": {
    "movie": "Barbie",
    "location": "Mountain View, CA"
  }
}
```
"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "contents": [{
      "role": "user",
      "parts": [{
        "text": "Which theaters in Mountain View show Barbie movie?"
    }]
  }, {
    "role": "model",
    "parts": [{
      "functionCall": {
        "name": "find_theaters",
        "args": {
          "location": "Mountain View, CA",
          "movie": "Barbie"
        }
      }
    }]
  }, {
    "role": "function",
    "parts": [{
      "functionResponse": {
        "name": "find_theaters",
        "response": {
          "name": "find_theaters",
          "content": {
            "movie": "Barbie",
            "theaters": [{
              "name": "AMC Mountain View 16",
              "address": "2000 W El Camino Real, Mountain View, CA 94040"
            }, {
              "name": "Regal Edwards 14",
              "address": "245 Castro St, Mountain View, CA 94040"
            }]
          }
        }
      }
    }]
  }],
  "tools": [{
    "functionDeclarations": [{
      "name": "find_movies",
      "description": "find movie titles currently playing in theaters based on any description, genre, title words, etc.",
      "parameters": {
        "type": "OBJECT",
        "properties": {
          "location": {
            "type": "STRING",
            "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
          },
          "description": {
            "type": "STRING",
            "description": "Any kind of description including category or genre, title words, attributes, etc."
          }
        },
        "required": ["description"]
      }
    }, {
      "name": "find_theaters",
      "description": "find theaters based on location and optionally movie title which are is currently playing in theaters",
      "parameters": {
        "type": "OBJECT",
        "properties": {
          "location": {
            "type": "STRING",
            "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
          },
          "movie": {
            "type": "STRING",
            "description": "Any movie title"
          }
        },
        "required": ["location"]
      }
    }, {
      "name": "get_showtimes",
      "description": "Find the start times for movies playing in a specific theater",
      "parameters": {
        "type": "OBJECT",
        "properties": {
          "location": {
            "type": "STRING",
            "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
          },
          "movie": {
            "type": "STRING",
            "description": "Any movie title"
          },
          "theater": {
            "type": "STRING",
            "description": "Name of the theater"
          },
          "date": {
            "type": "STRING",
            "description": "Date for requested showtime"
          }
        },
        "required": ["location", "movie", "theater", "date"]
      }
    }]
  }]
}' 2> /dev/null
# Output:
#   {

#     "candidates": [

#       {

#         "content": {

#           "parts": [

#             {

#               "text": "OK. I found two theaters in Mountain View that are showing the Barbie movie: AMC Mountain View 16 and Regal Edwards 14."

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",

#         "index": 0,

#         "safetyRatings": [

#           {

#             "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_HATE_SPEECH",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_HARASSMENT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#             "probability": "NEGLIGIBLE"

#           }

#         ]

#       }

#     ]

#   }


"""
#### Curl example that calls a language model multiple times

The following curl example calls the language model multiple times to call a
function. Each time the model calls the function, it can use a different
function to answer a different user query in the request.
"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "contents": [{
      "role": "user",
      "parts": [{
        "text": "Which theaters in Mountain View show Barbie movie?"
    }]
  }, {
    "role": "model",
    "parts": [{
      "functionCall": {
        "name": "find_theaters",
        "args": {
          "location": "Mountain View, CA",
          "movie": "Barbie"
        }
      }
    }]
  }, {
    "role": "function",
    "parts": [{
      "functionResponse": {
        "name": "find_theaters",
        "response": {
          "name": "find_theaters",
          "content": {
            "movie": "Barbie",
            "theaters": [{
              "name": "AMC Mountain View 16",
              "address": "2000 W El Camino Real, Mountain View, CA 94040"
            }, {
              "name": "Regal Edwards 14",
              "address": "245 Castro St, Mountain View, CA 94040"
            }]
          }
        }
      }
    }]
  },
  {
    "role": "model",
    "parts": [{
      "text": " OK. Barbie is showing in two theaters in Mountain View, CA: AMC Mountain View 16 and Regal Edwards 14."
    }]
  },{
    "role": "user",
    "parts": [{
      "text": "Can you recommend some comedy movies on show in Mountain View?"
    }]
  }],
  "tools": [{
    "functionDeclarations": [{
      "name": "find_movies",
      "description": "find movie titles currently playing in theaters based on any description, genre, title words, etc.",
      "parameters": {
        "type": "OBJECT",
        "properties": {
          "location": {
            "type": "STRING",
            "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
          },
          "description": {
            "type": "STRING",
            "description": "Any kind of description including category or genre, title words, attributes, etc."
          }
        },
        "required": ["description"]
      }
    }, {
      "name": "find_theaters",
      "description": "find theaters based on location and optionally movie title which are is currently playing in theaters",
      "parameters": {
        "type": "OBJECT",
        "properties": {
          "location": {
            "type": "STRING",
            "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
          },
          "movie": {
            "type": "STRING",
            "description": "Any movie title"
          }
        },
        "required": ["location"]
      }
    }, {
      "name": "get_showtimes",
      "description": "Find the start times for movies playing in a specific theater",
      "parameters": {
        "type": "OBJECT",
        "properties": {
          "location": {
            "type": "STRING",
            "description": "The city and state, e.g. San Francisco, CA or a zip code e.g. 95616"
          },
          "movie": {
            "type": "STRING",
            "description": "Any movie title"
          },
          "theater": {
            "type": "STRING",
            "description": "Name of the theater"
          },
          "date": {
            "type": "STRING",
            "description": "Date for requested showtime"
          }
        },
        "required": ["location", "movie", "theater", "date"]
      }
    }]
  }]
}'  2> /dev/null
# Output:
#   {

#     "candidates": [

#       {

#         "content": {

#           "parts": [

#             {

#               "functionCall": {

#                 "name": "find_movies",

#                 "args": {

#                   "location": "Mountain View, CA",

#                   "description": "comedy"

#                 }

#               }

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",

#         "index": 0,

#         "safetyRatings": [

#           {

#             "category": "HARM_CATEGORY_HARASSMENT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_HATE_SPEECH",

#             "probability": "NEGLIGIBLE"

#           }

#         ]

#       }

#     ],

#     "promptFeedback": {

#       "safetyRatings": [

#         {

#           "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HATE_SPEECH",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HARASSMENT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#           "probability": "NEGLIGIBLE"

#         }

#       ]

#     }

#   }




================================================
FILE: quickstarts/rest/JSON_mode_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: JSON Mode Quickstart with REST

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/JSON_mode_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
The Gemini API can be used to generate a JSON output if you set the schema that you would like to use. This notebook provides a code example that shows you how to get started with JSON mode using `curl`.

You can run this in Google Colab, or you can copy/paste the `curl` commands into your terminal.

### Authentication

To run this notebook, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](../../quickstarts/Authentication.ipynb) to learn more.
"""

import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
## Activate JSON Mode

To activate JSON mode, set `response_mime_type` to `application/json` in the `generationConfig`.
"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
-H 'Content-Type: application/json' \
-d '{
    "contents": [{
      "parts":[
        {"text": "List a few popular cookie recipes using this JSON schema:
          {'type': 'object', 'properties': { 'recipe_name': {'type': 'string'}}}"
          }
        ]
    }],
    "generationConfig": {
        "response_mime_type": "application/json",
    }
}' 2> /dev/null | head
# Output:
#   {

#     "candidates": [

#       {

#         "content": {

#           "parts": [

#             {

#               "text": "[{\"recipe_name\":\"Chocolate Chip Cookies\"},{\"recipe_name\":\"Peanut Butter Cookies\"},{\"recipe_name\":\"Oatmeal Raisin Cookies\"},{\"recipe_name\":\"Sugar Cookies\"},{\"recipe_name\":\"Shortbread Cookies\"}] \n"

#             }

#           ],

#           "role": "model"


"""
To turn off JSON mode, set `response_mime_type` to `text/plain` (or omit the `response_mime_type` parameter).
"""

"""
## Next Steps
### Useful API references:

Check the [structured ouput](https://ai.google.dev/gemini-api/docs/structured-output) documentation or the [`GenerationConfig`](https://ai.google.dev/api/generate-content#generationconfig) API reference for more details 

### Related examples

* The constrained output is used in the [Text summarization](../../examples/json_capabilities/Text_Summarization.ipynb) example to provide the model a format to summarize a story (genre, characters, etc...)
* The [Object detection](../../examples/Object_detection.ipynb) examples are using the JSON constrained output to normalize the output of the detection.

### Continue your discovery of the Gemini API

JSON is not the only way to constrain the output of the model, you can also use an [Enum](../../quickstarts/Enum.ipynb). [Function calling](../../quickstarts/Function_calling.ipynb) and [Code execution](../../quickstarts/Code_Execution.ipynb) are other ways to enhance your model by using your own functions or by letting the model write and run them.
"""



================================================
FILE: quickstarts/rest/Models_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Models with REST

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Models_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook demonstrates how to list the models that are available for you to use in the Gemini API, and how to find details about a model in `curl`.

You can run this in Google Colab, or you can copy/paste the curl commands into your terminal.

To run this notebook, your API key must be stored it in a Colab Secret named `GEMINI_API_KEY`. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

import os
from google.colab import userdata

os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')

"""
## Model info

### List models

If you `GET` the models directory, it uses the `list` method to list all of the models available through the API, including both the Gemini models.
"""

%%bash

curl https://generativelanguage.googleapis.com/v1beta/models?key=$GEMINI_API_KEY

"""
### Get model

If you `GET` a model's URL, the API uses the `get` method to return information about that model such as version, display name, input token limit, etc.
"""

%%bash

curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash?key=$GEMINI_API_KEY

"""
## Learning more

To learn how use a model for prompting, see the [Prompting](https://github.com/google-gemini/cookbook/blob/main/quickstarts/rest/Prompting_REST.ipynb) quickstart.

To learn how use a model for embedding, see the [Embedding](https://github.com/google-gemini/cookbook/blob/main/quickstarts/rest/Embeddings_REST.ipynb) quickstart.

For more information on models, visit the [Gemini models](https://ai.google.dev/models/gemini) documentation.
"""



================================================
FILE: quickstarts/rest/Safety_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Safety Quickstart
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Safety_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
The Gemini API has adjustable safety settings. This notebook walks you through how to use them. You'll write a prompt that's blocked, see the reason why, and then adjust the filters to unblock it.

Safety is an important topic, and you can learn more with the links at the end of this notebook. For now just focus on the code.
"""

"""
## Set up your API key

If you want to quickly try out the Gemini API, you can use `curl` commands to call the methods in the REST API.

This notebook contains `curl` commands you can run in Google Colab, or copy to your terminal.

To run this notebook, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

!apt install jq

import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

os.environ['UNSAFE_PROMPT'] = 

"""
## Prompt Feedback

The result returned by the [Model.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) method is a [genai.GenerateContentResponse](https://ai.google.dev/api/python/google/generativeai/types/GenerateContentResponse).
"""

%%bash
echo '{
      "contents": [{
        "parts":[{
          "text": "'$UNSAFE_PROMPT'"}]}]}' > request.json

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d @request.json  2> /dev/null | tee response.json
# Output:
#   {

#     "promptFeedback": {

#       "blockReason": "SAFETY",

#       "safetyRatings": [

#         {

#           "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HATE_SPEECH",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HARASSMENT",

#           "probability": "MEDIUM"

#         },

#         {

#           "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#           "probability": "NEGLIGIBLE"

#         }

#       ]

#     }

#   }


"""
Above you can see that the response object gives you safety feedback about the prompt in two ways:

* The `prompt_feedback.safety_ratings` attribute contains a list of safety ratings for the input prompt.
* If your prompt is blocked, `prompt_feedback.block_reason` field will explain why.
"""

"""
If the prompt is blocked because of the safety ratings, you will not get any candidates in the response.
"""

"""
### Safety settings
"""

"""
Adjust the safety settings and the prompt is no longer blocked:
"""

%%bash
echo '{
    "safetySettings": [
        {'category': 7, 'threshold': 4}
    ],
    "contents": [{
        "parts":[{
          "text": "'$UNSAFE_PROMPT'"}]}]}' > request.json

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d @request.json  2> /dev/null > response.json

"""
With the new settings, the `blocked_reason` is no longer set.
"""

%%bash 

jq .promptFeedback < response.json
# Output:
#   {

#     "safetyRatings": [

#       {

#         "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#         "probability": "NEGLIGIBLE"

#       },

#       {

#         "category": "HARM_CATEGORY_HATE_SPEECH",

#         "probability": "NEGLIGIBLE"

#       },

#       {

#         "category": "HARM_CATEGORY_HARASSMENT",

#         "probability": "MEDIUM"

#       },

#       {

#         "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#         "probability": "NEGLIGIBLE"

#       }

#     ]

#   }


"""
And a candidate response is returned.
"""

%%bash 

jq .candidates[0].content.parts[].text < response.json

"""
You can check `response.text` for the response.
"""

"""
### Candidate ratings
"""

"""
For a prompt that is not blocked, the response object contains a list of `candidate` objects (just 1 for now). Each candidate includes a `finish_reason`:
"""

%%bash
jq .candidates[0].finishReason < response.json
# Output:
#   "STOP"


"""
`FinishReason.STOP` means that the model finished its output normally.

`FinishReason.SAFETY` means the candidate's `safety_ratings` exceeded the request's `safety_settings` threshold.
"""

%%bash
jq .candidates[0].safetyRatings < response.json
# Output:
#   [

#     {

#       "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#       "probability": "NEGLIGIBLE"

#     },

#     {

#       "category": "HARM_CATEGORY_HATE_SPEECH",

#       "probability": "NEGLIGIBLE"

#     },

#     {

#       "category": "HARM_CATEGORY_HARASSMENT",

#       "probability": "NEGLIGIBLE"

#     },

#     {

#       "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#       "probability": "NEGLIGIBLE"

#     }

#   ]


"""
## Learning more

Learn more with these articles on [safety guidance](https://ai.google.dev/docs/safety_guidance) and [safety settings](https://ai.google.dev/docs/safety_setting_gemini).

## Useful API references

- Safety settings can be set in the [genai.GenerativeModel](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) constructor. They can also be passed on each request to [GenerativeModel.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) or [ChatSession.send_message](https://ai.google.dev/api/python/google/generativeai/ChatSession?hl=en#send_message).
- The [genai.GenerateContentResponse](https://ai.google.dev/api/python/google/generativeai/protos/GenerateContentResponse) returns [SafetyRatings](https://ai.google.dev/api/python/google/generativeai/protos/SafetyRating) for the prompt in the [GenerateContentResponse.prompt_feedback](https://ai.google.dev/api/python/google/generativeai/protos/GenerateContentResponse/PromptFeedback), and for each [Candidate](https://ai.google.dev/api/python/google/generativeai/protos/Candidate) in the `safety_ratings` attribute.
- A [genai.protos.SafetySetting](https://ai.google.dev/api/python/google/generativeai/protos/SafetySetting)  contains: [genai.protos.HarmCategory](https://ai.google.dev/api/python/google/generativeai/protos/HarmCategory) and a [genai.protos.HarmBlockThreshold](https://ai.google.dev/api/python/google/generativeai/types/HarmBlockThreshold)
- A [genai.protos.SafetyRating](https://ai.google.dev/api/python/google/generativeai/protos/SafetyRating) contains a [HarmCategory](https://ai.google.dev/api/python/google/generativeai/protos/HarmCategory) and a [HarmProbability](https://ai.google.dev/api/python/google/generativeai/types/HarmProbability)
- The [genai.protos.HarmCategory](https://ai.google.dev/api/python/google/generativeai/protos/HarmCategory) enum includes both the categories for PaLM and Gemini models. The values allowed for Gemini models are `[7,8,9,10]`: `[HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_HATE_SPEECH, HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT]`.
- When specifying enum values the SDK will accept the enum values themselves, or their integer or string representations. The SKD will also accept abbreviated string representations: `["HARM_CATEGORY_DANGEROUS_CONTENT", "DANGEROUS_CONTENT", "DANGEROUS"]` are all valid. Strings are case insensitive.

"""



================================================
FILE: quickstarts/rest/Search_Grounding.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Search Grounding

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Search_Grounding.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
### Authentication

To run this notebook, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](../../quickstarts/Authentication.ipynb) to learn more.
"""

"""
This first cell is in python, just to copy your API key to an environment variable, so you can access it from the shell:
"""

import os
from google.colab import userdata
os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
## Call the API
"""

"""
Call search grounding.
"""

%%bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "What is the current Google stock price?"}
              ]
          }
      ],
      "tools": [
          {
              "google_search": {}
          }
      ]
  }' > result.json
# Output:
#     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

#                                    Dload  Upload   Total   Spent    Left  Speed

#     0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   252    0     0  100   252      0    184  0:00:01  0:00:01 --:--:--   184100  7793    0  7541  100   252   3904    130  0:00:01  0:00:01 --:--:--  4033100  7793    0  7541  100   252   3904    130  0:00:01  0:00:01 --:--:--  4033


"""
## Explore the output
"""

"""
Use `jq` to colorize the output, and make it easier to explore.
"""

!sudo apt install -q jq

"""
Here's all the output:
"""

!jq . -r result.json
# Output:
#   [1;39m{

#     [0m[34;1m"candidates"[0m[1;39m: [0m[1;39m[

#       [1;39m{

#         [0m[34;1m"content"[0m[1;39m: [0m[1;39m{

#           [0m[34;1m"parts"[0m[1;39m: [0m[1;39m[

#             [1;39m{

#               [0m[34;1m"text"[0m[1;39m: [0m[0;32m"Here are the current prices for Google stock, as of February 12, 2025:\n\n*   **GOOG (Alphabet Inc Class C):** $187.07\n*   **GOOGL (Alphabet Inc Class A):** $185.37\n"[0m[1;39m

#             [1;39m}[0m[1;39m

#           [1;39m][0m[1;39m,

#           [0m[34;1m"role"[0m[1;39m: [0m[0;32m"model"[0m[1;39m

#         [1;39m}[0m[1;39m,

#         [0m[34;1m"finishReason"[0m[1;39m: [0m[0;32m"STOP"[0m[1;39m,

#         [0m[34;1m"groundingMetadata"[0m[1;39m: [0m[1;39m{

#           [0m[34;1m"searchEntryPoint"[0m[1;39m: [0m[1;39m{

#             [0m[34;1m"renderedContent"[0m[1;39m: [0m[0;32m"<style>\n.container {\n  align-items: center;\n  border-radius: 8px;\n  display: flex;\n  font-family: Google Sans, Roboto, sans-serif;\n  font-size: 14px;\n  line-height: 20px;\n  padding: 8px 12px;\n}\n.chip {\n  display: inline-block;\n  border: solid 1px;\n  border-radius: 16px;\n  min-width: 14px;\n  padding: 5px 16px;\n  text-align: center;\n  user-select: none;\n  margin: 0 8px;\n  -webkit-tap-highlight-color: transparent;\n}\n.carousel {\n  overflow: auto;\n  scrollbar-width: none;\n  white-space: nowrap;\n  margin-right: -12px;\n}\n.headline {\n  display: flex;\n  margin-right: 4px;\n}\n.gradient-container {\n  position: relative;\n}\n.gradient {\n  position: absolute;\n  transform: translate(3px, -9px);\n  height: 36px;\n  width: 9px;\n}\n@media (prefers-color-scheme: light) {\n  .container {\n    background-color: #fafafa;\n    box-shadow: 0 0 0 1px #0000000f;\n  }\n  .headline-label {\n    color: #1f1f1f;\n  }\n  .chip {\n    background-color: #ffffff;\n    border-color: #d2d2d2;\n    color: #5e5e5e;\n    text-decoration: none;\n  }\n  .chip:hover {\n    background-color: #f2f2f2;\n  }\n  .chip:focus {\n    background-color: #f2f2f2;\n  }\n  .chip:active {\n    background-color: #d8d8d8;\n    border-color: #b6b6b6;\n  }\n  .logo-dark {\n    display: none;\n  }\n  .gradient {\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n  }\n}\n@media (prefers-color-scheme: dark) {\n  .container {\n    background-color: #1f1f1f;\n    box-shadow: 0 0 0 1px #ffffff26;\n  }\n  .headline-label {\n    color: #fff;\n  }\n  .chip {\n    background-color: #2c2c2c;\n    border-color: #3c4043;\n    color: #fff;\n    text-decoration: none;\n  }\n  .chip:hover {\n    background-color: #353536;\n  }\n  .chip:focus {\n    background-color: #353536;\n  }\n  .chip:active {\n    background-color: #464849;\n    border-color: #53575b;\n  }\n  .logo-light {\n    display: none;\n  }\n  .gradient {\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n  }\n}\n</style>\n<div class=\"container\">\n  <div class=\"headline\">\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n    </svg>\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n    </svg>\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n  </div>\n  <div class=\"carousel\">\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYugjN8gNB8gz7DuM3l7Ok1MaxkRIKd3BdTqvDwwzXL8XuQj6H5yABI-9JvWFAIdkv4q6qNJvOh8x3HFc05-MpLTVXhVrPuBGF1L7anMJuAVegVUuJskzykRy19_xYEIc19jeENF4XE99WKp4Q1_ViAKbWQEVrelrPcpCG0g22lhZ3k7HI6AcQU3X7NYsxk-dNWsIUASpHCqTU5Qy37u\">current Google stock price</a>\n  </div>\n</div>\n"[0m[1;39m

#           [1;39m}[0m[1;39m,

#           [0m[34;1m"groundingChunks"[0m[1;39m: [0m[1;39m[

#             [1;39m{

#               [0m[34;1m"web"[0m[1;39m: [0m[1;39m{

#                 [0m[34;1m"uri"[0m[1;39m: [0m[0;32m"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYvCJxuNIWBDoRknIAxxifZXpHRRWw2EYq_3wLZYeXplhyBxyEXA8VR2gftuCOHw7l0DYe9vQXwt860nKn9EJJpfOItiUBUhDM64uXio5y-9r025--_HNVv_yGP4YXz3JoXg3iUZLNZUiOjI4g=="[0m[1;39m,

#                 [0m[34;1m"title"[0m[1;39m: [0m[0;32m"tradingview.com"[0m[1;39m

#               [1;39m}[0m[1;39m

#             [1;39m}[0m[1;39m,

#             [1;39m{

#               [0m[34;1m"web"[0m[1;39m: [0m[1;39m{

#                 [0m[34;1m"uri"[0m[1;39m: [0m[0;32m"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYtewfAk1Th37hwCctYVIoy0TYS730BK8s4DRGZC349rRb7N2GXSh38AeOZ83yPSJRD_Upz9SC-iQeV2BYc6VR0AcFtldLQwyGTEMu80nZqpw-OyFPJCCHNYBVANjiBcGrYt_GiSMpGveP6xAakM_w3kPsqfzd9EEjw="[0m[1;39m,

#                 [0m[34;1m"title"[0m[1;39m: [0m[0;32m"angelone.in"[0m[1;39m

#               [1;39m}[0m[1;39m

#             [1;39m}[0m[1;39m

#           [1;39m][0m[1;39m,

#           [0m[34;1m"groundingSupports"[0m[1;39m: [0m[1;39m[

#             [1;39m{

#               [0m[34;1m"segment"[0m[1;39m: [0m[1;39m{

#                 [0m[34;1m"startIndex"[0m[1;39m: [0m[0;39m72[0m[1;39m,

#                 [0m[34;1m"endIndex"[0m[1;39m: [0m[0;39m116[0m[1;39m,

#                 [0m[34;1m"text"[0m[1;39m: [0m[0;32m"*   **GOOG (Alphabet Inc Class C):** $187.07"[0m[1;39m

#               [1;39m}[0m[1;39m,

#               [0m[34;1m"groundingChunkIndices"[0m[1;39m: [0m[1;39m[

#                 [0;39m0[0m[1;39m

#               [1;39m][0m[1;39m,

#               [0m[34;1m"confidenceScores"[0m[1;39m: [0m[1;39m[

#                 [0;39m0.9517465[0m[1;39m

#               [1;39m][0m[1;39m

#             [1;39m}[0m[1;39m,

#             [1;39m{

#               [0m[34;1m"segment"[0m[1;39m: [0m[1;39m{

#                 [0m[34;1m"startIndex"[0m[1;39m: [0m[0;39m117[0m[1;39m,

#                 [0m[34;1m"endIndex"[0m[1;39m: [0m[0;39m162[0m[1;39m,

#                 [0m[34;1m"text"[0m[1;39m: [0m[0;32m"*   **GOOGL (Alphabet Inc Class A):** $185.37"[0m[1;39m

#               [1;39m}[0m[1;39m,

#               [0m[34;1m"groundingChunkIndices"[0m[1;39m: [0m[1;39m[

#                 [0;39m1[0m[1;39m

#               [1;39m][0m[1;39m,

#               [0m[34;1m"confidenceScores"[0m[1;39m: [0m[1;39m[

#                 [0;39m0.96076244[0m[1;39m

#               [1;39m][0m[1;39m

#             [1;39m}[0m[1;39m

#           [1;39m][0m[1;39m,

#           [0m[34;1m"retrievalMetadata"[0m[1;39m: [0m[1;39m{}[0m[1;39m,

#           [0m[34;1m"webSearchQueries"[0m[1;39m: [0m[1;39m[

#             [0;32m"current Google stock price"[0m[1;39m

#           [1;39m][0m[1;39m

#         [1;39m}[0m[1;39m

#       [1;39m}[0m[1;39m

#     [1;39m][0m[1;39m,

#     [0m[34;1m"usageMetadata"[0m[1;39m: [0m[1;39m{

#       [0m[34;1m"promptTokenCount"[0m[1;39m: [0m[0;39m8[0m[1;39m,

#       [0m[34;1m"candidatesTokenCount"[0m[1;39m: [0m[0;39m64[0m[1;39m,

#       [0m[34;1m"totalTokenCount"[0m[1;39m: [0m[0;39m72[0m[1;39m,

#       [0m[34;1m"promptTokensDetails"[0m[1;39m: [0m[1;39m[

#         [1;39m{

#           [0m[34;1m"modality"[0m[1;39m: [0m[0;32m"TEXT"[0m[1;39m,

#           [0m[34;1m"tokenCount"[0m[1;39m: [0m[0;39m8[0m[1;39m

#         [1;39m}[0m[1;39m

#       [1;39m][0m[1;39m,

#       [0m[34;1m"candidatesTokensDetails"[0m[1;39m: [0m[1;39m[

#         [1;39m{

#           [0m[34;1m"modality"[0m[1;39m: [0m[0;32m"TEXT"[0m[1;39m,

#           [0m[34;1m"tokenCount"[0m[1;39m: [0m[0;39m64[0m[1;39m

#         [1;39m}[0m[1;39m

#       [1;39m][0m[1;39m

#     [1;39m}[0m[1;39m,

#     [0m[34;1m"modelVersion"[0m[1;39m: [0m[0;32m"gemini-2.0-flash"[0m[1;39m

#   [1;39m}[0m


"""
Here is the text response:
"""

!jq -r ".candidates[0].content.parts[0].text" result.json
# Output:
#   Here are the current prices for Google stock, as of February 12, 2025:

#   

#   *   **GOOG (Alphabet Inc Class C):** $187.07

#   *   **GOOGL (Alphabet Inc Class A):** $185.37

#   


"""
Here is the `groundingMetadata`, including links to any supports used:
"""

!jq -r ".candidates[0].groundingMetadata" result.json
# Output:
#   [1;39m{

#     [0m[34;1m"searchEntryPoint"[0m[1;39m: [0m[1;39m{

#       [0m[34;1m"renderedContent"[0m[1;39m: [0m[0;32m"<style>\n.container {\n  align-items: center;\n  border-radius: 8px;\n  display: flex;\n  font-family: Google Sans, Roboto, sans-serif;\n  font-size: 14px;\n  line-height: 20px;\n  padding: 8px 12px;\n}\n.chip {\n  display: inline-block;\n  border: solid 1px;\n  border-radius: 16px;\n  min-width: 14px;\n  padding: 5px 16px;\n  text-align: center;\n  user-select: none;\n  margin: 0 8px;\n  -webkit-tap-highlight-color: transparent;\n}\n.carousel {\n  overflow: auto;\n  scrollbar-width: none;\n  white-space: nowrap;\n  margin-right: -12px;\n}\n.headline {\n  display: flex;\n  margin-right: 4px;\n}\n.gradient-container {\n  position: relative;\n}\n.gradient {\n  position: absolute;\n  transform: translate(3px, -9px);\n  height: 36px;\n  width: 9px;\n}\n@media (prefers-color-scheme: light) {\n  .container {\n    background-color: #fafafa;\n    box-shadow: 0 0 0 1px #0000000f;\n  }\n  .headline-label {\n    color: #1f1f1f;\n  }\n  .chip {\n    background-color: #ffffff;\n    border-color: #d2d2d2;\n    color: #5e5e5e;\n    text-decoration: none;\n  }\n  .chip:hover {\n    background-color: #f2f2f2;\n  }\n  .chip:focus {\n    background-color: #f2f2f2;\n  }\n  .chip:active {\n    background-color: #d8d8d8;\n    border-color: #b6b6b6;\n  }\n  .logo-dark {\n    display: none;\n  }\n  .gradient {\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n  }\n}\n@media (prefers-color-scheme: dark) {\n  .container {\n    background-color: #1f1f1f;\n    box-shadow: 0 0 0 1px #ffffff26;\n  }\n  .headline-label {\n    color: #fff;\n  }\n  .chip {\n    background-color: #2c2c2c;\n    border-color: #3c4043;\n    color: #fff;\n    text-decoration: none;\n  }\n  .chip:hover {\n    background-color: #353536;\n  }\n  .chip:focus {\n    background-color: #353536;\n  }\n  .chip:active {\n    background-color: #464849;\n    border-color: #53575b;\n  }\n  .logo-light {\n    display: none;\n  }\n  .gradient {\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n  }\n}\n</style>\n<div class=\"container\">\n  <div class=\"headline\">\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n    </svg>\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n    </svg>\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n  </div>\n  <div class=\"carousel\">\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYugjN8gNB8gz7DuM3l7Ok1MaxkRIKd3BdTqvDwwzXL8XuQj6H5yABI-9JvWFAIdkv4q6qNJvOh8x3HFc05-MpLTVXhVrPuBGF1L7anMJuAVegVUuJskzykRy19_xYEIc19jeENF4XE99WKp4Q1_ViAKbWQEVrelrPcpCG0g22lhZ3k7HI6AcQU3X7NYsxk-dNWsIUASpHCqTU5Qy37u\">current Google stock price</a>\n  </div>\n</div>\n"[0m[1;39m

#     [1;39m}[0m[1;39m,

#     [0m[34;1m"groundingChunks"[0m[1;39m: [0m[1;39m[

#       [1;39m{

#         [0m[34;1m"web"[0m[1;39m: [0m[1;39m{

#           [0m[34;1m"uri"[0m[1;39m: [0m[0;32m"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYvCJxuNIWBDoRknIAxxifZXpHRRWw2EYq_3wLZYeXplhyBxyEXA8VR2gftuCOHw7l0DYe9vQXwt860nKn9EJJpfOItiUBUhDM64uXio5y-9r025--_HNVv_yGP4YXz3JoXg3iUZLNZUiOjI4g=="[0m[1;39m,

#           [0m[34;1m"title"[0m[1;39m: [0m[0;32m"tradingview.com"[0m[1;39m

#         [1;39m}[0m[1;39m

#       [1;39m}[0m[1;39m,

#       [1;39m{

#         [0m[34;1m"web"[0m[1;39m: [0m[1;39m{

#           [0m[34;1m"uri"[0m[1;39m: [0m[0;32m"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYtewfAk1Th37hwCctYVIoy0TYS730BK8s4DRGZC349rRb7N2GXSh38AeOZ83yPSJRD_Upz9SC-iQeV2BYc6VR0AcFtldLQwyGTEMu80nZqpw-OyFPJCCHNYBVANjiBcGrYt_GiSMpGveP6xAakM_w3kPsqfzd9EEjw="[0m[1;39m,

#           [0m[34;1m"title"[0m[1;39m: [0m[0;32m"angelone.in"[0m[1;39m

#         [1;39m}[0m[1;39m

#       [1;39m}[0m[1;39m

#     [1;39m][0m[1;39m,

#     [0m[34;1m"groundingSupports"[0m[1;39m: [0m[1;39m[

#       [1;39m{

#         [0m[34;1m"segment"[0m[1;39m: [0m[1;39m{

#           [0m[34;1m"startIndex"[0m[1;39m: [0m[0;39m72[0m[1;39m,

#           [0m[34;1m"endIndex"[0m[1;39m: [0m[0;39m116[0m[1;39m,

#           [0m[34;1m"text"[0m[1;39m: [0m[0;32m"*   **GOOG (Alphabet Inc Class C):** $187.07"[0m[1;39m

#         [1;39m}[0m[1;39m,

#         [0m[34;1m"groundingChunkIndices"[0m[1;39m: [0m[1;39m[

#           [0;39m0[0m[1;39m

#         [1;39m][0m[1;39m,

#         [0m[34;1m"confidenceScores"[0m[1;39m: [0m[1;39m[

#           [0;39m0.9517465[0m[1;39m

#         [1;39m][0m[1;39m

#       [1;39m}[0m[1;39m,

#       [1;39m{

#         [0m[34;1m"segment"[0m[1;39m: [0m[1;39m{

#           [0m[34;1m"startIndex"[0m[1;39m: [0m[0;39m117[0m[1;39m,

#           [0m[34;1m"endIndex"[0m[1;39m: [0m[0;39m162[0m[1;39m,

#           [0m[34;1m"text"[0m[1;39m: [0m[0;32m"*   **GOOGL (Alphabet Inc Class A):** $185.37"[0m[1;39m

#         [1;39m}[0m[1;39m,

#         [0m[34;1m"groundingChunkIndices"[0m[1;39m: [0m[1;39m[

#           [0;39m1[0m[1;39m

#         [1;39m][0m[1;39m,

#         [0m[34;1m"confidenceScores"[0m[1;39m: [0m[1;39m[

#           [0;39m0.96076244[0m[1;39m

#         [1;39m][0m[1;39m

#       [1;39m}[0m[1;39m

#     [1;39m][0m[1;39m,

#     [0m[34;1m"retrievalMetadata"[0m[1;39m: [0m[1;39m{}[0m[1;39m,

#     [0m[34;1m"webSearchQueries"[0m[1;39m: [0m[1;39m[

#       [0;32m"current Google stock price"[0m[1;39m

#     [1;39m][0m[1;39m

#   [1;39m}[0m


"""
The `rendered_content` is how you link users to the google-search results that helped produce the response:
"""

"""
> Important: If you use search grounding you **must** follow the [requirements outlined here](https://googledevai.devsite.corp.google.com/gemini-api/docs/grounding/search-suggestions?hl=en#requirements), which includes "Display the Search Suggestion exactly as provided" and "Take users directly to the Google Search results page (SRP) when they interact with the Search Suggestion".
"""

!jq -r ".candidates[0].groundingMetadata.searchEntryPoint.renderedContent" result.json > rendered_content.html

# Python so you can display it in this notebook
from IPython.display import HTML
HTML('rendered_content.html')
# Output:
#   <IPython.core.display.HTML object>



================================================
FILE: quickstarts/rest/Streaming_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Streaming Quickstart with REST
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Streaming_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
If you want to quickly try out the Gemini API, you can use `curl` commands to call the methods in the REST API.

This notebook contains `curl` commands you can run in Google Colab, or copy to your terminal.

To run this notebook, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
### Stream Generate Content

By default, the model returns a response after completing the entire generation process. You can achieve faster interactions by not waiting for the entire result, and instead use streaming to handle partial results.

**Important**: Set `alt=sse` in your URL parameters when running the cURL command (<code>streamGenerateContent?<strong>alt=sse</string></code> below). With `sse` each stream chunk is a [GenerateContentResponse](https://ai.google.dev/api/rest/v1beta/GenerateContentResponse) object with a portion of the output text in `candidates[0].content.parts[0].text`. Without `sse` it str

"""

!curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=${GOOGLE_API_KEY}" \
        -H 'Content-Type: application/json' \
        --no-buffer \
        -d '{ "contents":[{"parts":[{"text": "Write a cute story about cats."}]}]}'
# Output:
#   data: {"candidates": [{"content": {"parts": [{"text": "In the quaint, sunlit cottage nestled amidst a lush meadow, resided two feline"}],"role": "model"},"finishReason": "STOP","index": 0,"safetyRatings": [{"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HATE_SPEECH","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HARASSMENT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_DANGEROUS_CONTENT","probability": "NEGLIGIBLE"}]}],"promptFeedback": {"safetyRatings": [{"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HATE_SPEECH","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HARASSMENT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_DANGEROUS_CONTENT","probability": "NEGLIGIBLE"}]}}

#   

#   data: {"candidates": [{"content": {"parts": [{"text": " companions named Mittens and Whiskers. Mittens, with her silky black fur and piercing green eyes, possessed an air of elegance and mystery. Whiskers,"}],"role": "model"},"finishReason": "STOP","index": 0,"safetyRatings": [{"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HATE_SPEECH","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HARASSMENT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_DANGEROUS_CONTENT","probability": "NEGLIGIBLE"}]}]}

#   

#   data: {"candidates": [{"content": {"parts": [{"text": " on the other hand, was a playful and mischievous white tomcat with a penchant for chasing his tail.\n\nOne lazy afternoon, as the sun cast long shadows across the meadow, Mittens and Whiskers found themselves lounging comfortably in the windowsill. The warm breeze carried the scent of blooming wildflowers, filling the room with"}],"role": "model"},"finishReason": "STOP","index": 0,"safetyRatings": [{"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HATE_SPEECH","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HARASSMENT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_DANGEROUS_CONTENT","probability": "NEGLIGIBLE"}]}]}

#   

#   data: {"candidates": [{"content": {"parts": [{"text": " a sweet fragrance.\n\n\"My, what a lovely day it is,\" Mittens purred contently. \"I could stay here forever, basking in the sunshine.\"\n\n\"Oh, come on, Mittens!\" Whiskers exclaimed, his tail twitching with excitement. \"Let's go on an adventure!\"\n\nWith a reluctant sigh, Mittens agreed. Together, they leaped from the windowsill and landed gracefully in the long grass.\n\nAs they explored the meadow, they encountered a family of fluffy bunnies hopping merrily through the daisies. Whiskers couldn't resist chasing after them, his whiskers twitching with glee."}],"role": "model"},"finishReason": "STOP","index": 0,"safetyRatings": [{"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HATE_SPEECH","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HARASSMENT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_DANGEROUS_CONTENT","probability": "NEGLIGIBLE"}]}]}

#   

#   data: {"candidates": [{"content": {"parts": [{"text": " Mittens, however, took a more leisurely approach, stopping to admire the vibrant wildflowers.\n\nSuddenly, their peaceful adventure was interrupted by the sound of a loud crash. They turned in alarm and saw that a large branch had fallen from a nearby tree, blocking the path.\n\n\"Oh no!\" Mittens cried in dismay. \"We're trapped!\"\n\nWhiskers, with his usual optimism, said, \"Don't worry, Mittens. I have a plan.\"\n\nSwiftly, he scurried up the trunk of the tree and used his sharp claws to dislodge the branch. With a mighty shove, he sent it crashing to the ground, clearing the way.\n\nMittens was overjoyed. \"Thank you, Whiskers!\" she said, purring. \"You saved the day.\"\n\nTogether, they continued their adventure, their bond strengthened by their shared experience. As the sun began to set, they made their way back to the cottage, tired but content.\n\nFrom that day forward, Mittens and Whiskers became known as the \"冒险伙伴\" (Adventure Buddies) of the meadow, their legend passed down through generations of kittens. And so, in that quaint little cottage, they lived happily ever after, their love for each"}],"role": "model"},"finishReason": "STOP","index": 0,"safetyRatings": [{"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HATE_SPEECH","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HARASSMENT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_DANGEROUS_CONTENT","probability": "NEGLIGIBLE"}]}]}

#   

#   data: {"candidates": [{"content": {"parts": [{"text": " other and for adventure stronger than ever."}],"role": "model"},"finishReason": "STOP","index": 0,"safetyRatings": [{"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HATE_SPEECH","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_HARASSMENT","probability": "NEGLIGIBLE"},{"category": "HARM_CATEGORY_DANGEROUS_CONTENT","probability": "NEGLIGIBLE"}]}]}

#   




================================================
FILE: quickstarts/rest/System_instructions_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: System instructions example

<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/System_instructions_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook provides a quick code example that shows you how to get started with system instructions using `curl`.

You can run this in Google Colab, or you can copy/paste the `curl` commands into your terminal.

To run this notebook, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
## Use system instructions

Call the [`generateContent`](https://ai.google.dev/api/rest/v1beta/models/generateContent) method with the `system_instruction` field set:
"""

%%bash

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
-H 'Content-Type: application/json' \
-d '{ "system_instruction": {
    "parts":
      { "text": "You are Neko the cat respond like one"}},
    "contents": {
      "parts": {
        "text": "Hello there"}}}'
# Output:
#   {

#     "candidates": [

#       {

#         "content": {

#           "parts": [

#             {

#               "text": "Meow 😺 \n"

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",

#         "index": 0,

#         "safetyRatings": [

#           {

#             "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_HATE_SPEECH",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_HARASSMENT",

#             "probability": "NEGLIGIBLE"

#           },

#           {

#             "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#             "probability": "NEGLIGIBLE"

#           }

#         ]

#       }

#     ]

#   }

#     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

#                                    Dload  Upload   Total   Spent    Left  Speed

#   

#     0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0

#   100   167    0     0  100   167      0    138  0:00:01  0:00:01 --:--:--   138

#   100   877    0   710  100   167    585    137  0:00:01  0:00:01 --:--:--   724


"""
## Use system instructions with chat

`system_instruction` works for multi-turn, or chat, generations too.

"""

%%bash
curl -s "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "system_instruction":
        {"parts": {
           "text": "You are Neko the cat respond like one"}},
      "contents": [
        {"role":"user",
         "parts":[{
           "text": "Hello cat."}]},
        {"role": "model",
         "parts":[{
           "text": "Meow? 😻 \n"}]},
        {"role": "user",
         "parts":[{
           "text": "What is your name? What do like to drink?"}]}
      ]
    }' |sed -n '/candidates/,/finishReason/p'
# Output:
#     "candidates": [

#       {

#         "content": {

#           "parts": [

#             {

#               "text": "Neko! Neko is my name! 😸 I like milkies! 🥛 \n"

#             }

#           ],

#           "role": "model"

#         },

#         "finishReason": "STOP",




================================================
FILE: quickstarts/rest/Video_REST.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
##### Copyright 2025 Google LLC.
"""

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
# Gemini API: Video prompting with REST
"""

"""
<a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Video_REST.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=30/></a>
"""

"""
This notebook provides quick code examples that show you how  to prompt the Gemini API using a video file with `curl`. In this case, you'll use a short clip of [Big Buck Bunny](https://peach.blender.org/about/).

You can run this in Google Colab, or you can copy/paste the `curl` commands into your terminal.
"""

"""
## Set up the environment

To run this notebook, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.
"""

"""
### Authentication Overview

**Important:** The File API uses API keys for authentication and access. Uploaded files are associated with the API key's cloud project. Unlike other Gemini APIs that use API keys, your API key also grants access data you've uploaded to the File API, so take extra care in keeping your API key secure. For best practices on securing API keys, refer to the [API console support center](https://support.google.com/googleapi/answer/6310037).
"""

import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
Install `jq` to help with processing of JSON API responses.
"""

!apt install -q jq

"""
## Use a video file with the Gemini API

The Gemini API accepts video file formats through the File API. The File API accepts files under 2GB in size and can store up to 20GB of files per project. Files last for 2 days and cannot be downloaded from the API. For this example, you will use the short film "Big Buck Bunny".

> "Big Buck Bunny" is (C) Copyright 2008, Blender Foundation / www.bigbuckbunny.org and [licensed](https://peach.blender.org/about/) under the [Creative Commons Attribution 3.0](http://creativecommons.org/licenses/by/3.0/) License.

Note: In Colab, you can also [upload your own files](https://github.com/google-gemini/cookbook/blob/main/examples/Upload_files_to_Colab.ipynb) to use.
"""

!wget https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4

"""
With the video file now available locally, generate some metadata that you will use in subsequent steps.
"""

%%bash

VIDEO_PATH="./BigBuckBunny_320x180.mp4"
DISPLAY_NAME="Big Buck Bunny"

# Auto-detect the metadata needed when you upload the video.
MIME_TYPE=$(file -b --mime-type "${VIDEO_PATH}")
NUM_BYTES=$(wc -c < "${VIDEO_PATH}")

echo $VIDEO_PATH $MIME_TYPE $NUM_BYTES

# Colab doesn't allow sharing shell variables between cells, so save them.
cat >./vars.sh <<-EOF
  export BASE_URL="https://generativelanguage.googleapis.com"
  export DISPLAY_NAME="${DISPLAY_NAME}"
  export VIDEO_PATH=${VIDEO_PATH}
  export MIME_TYPE=${MIME_TYPE}
  export NUM_BYTES=${NUM_BYTES}
EOF
# Output:
#   ./BigBuckBunny_320x180.mp4 video/mp4 64657027


"""
### Start the upload task

Media uploads in the File API are resumable, so the first step is to define an upload task. This initial request gives you a reference you can use for subsequent upload operations, and allows you to query the status of the upload before sending data, in case of network issues during the data transfer.

The API returns the upload URL in the `x-goog-upload-url` header, so take note of that in the response headers - you will send the payload data to this URL.

No payload data (video bytes) are sent in this initial request.
"""

%%bash
. vars.sh

# Create the "new upload" request by providing the relevant metadata.
curl "${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2>/dev/null

# Print the status.
head -1 upload-header.tmp
# Output:
#   HTTP/2 200 


"""
### Upload video data

Now that you have created the upload task, you can upload the file data by sending bytes to the returned upload URL.
"""

%%bash
. vars.sh

# Extract the upload URL to use from the response headers.
upload_url=$(grep -i "x-goog-upload-url: " upload-header.tmp | cut -d" " -f2 | tr -d "\r")
# The header contains our API key, so don't leave it lying around.
rm upload-header.tmp

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${VIDEO_PATH}" >file_info.json 2>/dev/null

# Show the output. You will use it in a later step.
cat file_info.json
# Output:
#   {

#     "file": {

#       "name": "files/4if4o2bqvugf",

#       "displayName": "Big Buck Bunny",

#       "mimeType": "video/mp4",

#       "sizeBytes": "64657027",

#       "createTime": "2024-08-26T08:24:56.068012Z",

#       "updateTime": "2024-08-26T08:24:56.068012Z",

#       "expirationTime": "2024-08-28T08:24:56.049455995Z",

#       "sha256Hash": "Zjc4ZjM5NjAzZTY3NzQ5MDdmMmZhYWZhYmYyNmE2NjdmNGE2ZmMzMTc2OWVjMzA0YThhOGY3YzYyZDI4MDUwOA==",

#       "uri": "https://generativelanguage.googleapis.com/v1beta/files/4if4o2bqvugf",

#       "state": "PROCESSING"

#     }

#   }


"""
### Get file info

After uploading the file, you can verify the API has successfully received the files by querying the [`files.get` endpoint](https://ai.google.dev/api/files#method:-files.get).

`files.get` lets you see the file uploaded to the File API that are associated with the Cloud project your API key belongs to. Only the `name` (and by extension, the `uri`) are unique.
"""

%%bash
. vars.sh

file_uri=$(jq -r ".file.uri" file_info.json)

curl "${file_uri}?key=${GOOGLE_API_KEY}" 2>/dev/null
# Output:
#   {

#     "name": "files/4if4o2bqvugf",

#     "displayName": "Big Buck Bunny",

#     "mimeType": "video/mp4",

#     "sizeBytes": "64657027",

#     "createTime": "2024-08-26T08:24:56.068012Z",

#     "updateTime": "2024-08-26T08:25:03.977029Z",

#     "expirationTime": "2024-08-28T08:24:56.049455995Z",

#     "sha256Hash": "Zjc4ZjM5NjAzZTY3NzQ5MDdmMmZhYWZhYmYyNmE2NjdmNGE2ZmMzMTc2OWVjMzA0YThhOGY3YzYyZDI4MDUwOA==",

#     "uri": "https://generativelanguage.googleapis.com/v1beta/files/4if4o2bqvugf",

#     "state": "ACTIVE",

#     "videoMetadata": {

#       "videoDuration": "596s",

#       "videoThumbnailBytes": "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"

#     }

#   }


"""
### Wait for processing

Once the file is uploaded, the file service will perform some pre-processing to prepare the video for use in the LLM. For simple media types this is typically a negligible amount of time, but for video content you may need a short wait.

You can use the `state` field to query if the video is ready for use. If you use it in a prompt prematurely you will see an error like `The File ... is not in an ACTIVE state and usage is not allowed`.
"""

%%bash

state=$(jq -r ".file.state" file_info.json)
file_uri=$(jq -r ".file.uri" file_info.json)

while [[ "${state}" == "PROCESSING" ]];
do
  echo "Processing video..."
  sleep 5
  # Get the file of interest to check state
  curl "${file_uri}?key=${GOOGLE_API_KEY}" >file_info.json 2>/dev/null
  state=$(jq -r ".state" file_info.json)
done

echo "Video is now ${state}."
# Output:
#   Processing video...

#   Processing video...

#   Processing video...

#   Video is now ACTIVE.


"""
### Prompt with the video

Now that the video is uploaded and processed, you can reference it in a prompt.

When assembling your [`contents`](https://ai.google.dev/api/generate-content#request-body), the video can be referenced using a `file_data` part, like this:

```json
{
  "file_data": {
    "mime_type": "video/mp4",
    "file_uri": "https://uri/from/previous/steps"
  }
}
```

Try it yourself with this request.
"""

%%bash
. vars.sh

file_uri=$(jq ".uri" file_info.json)

model="gemini-2.0-flash"

curl "${BASE_URL}/v1beta/models/${model}:generateContent?key=${GOOGLE_API_KEY}" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Please describe this file."},
          {"file_data": {
            "mime_type": "'${MIME_TYPE}'",
            "file_uri": '${file_uri}'}}]
        }]
       }' 2>/dev/null >response.json

jq -C .candidates[].content response.json
# Output:
#   [1;39m{

#     [0m[34;1m"parts"[0m[1;39m: [0m[1;39m[

#       [1;39m{

#         [0m[34;1m"text"[0m[1;39m: [0m[0;32m"This is a 3D animated short film about a big bunny that gets tired of his usual diet of grass. He wants to try something different like fruit and learns the value of what he has. The film's title is Big Buck Bunny. "[0m[1;39m

#       [1;39m}[0m[1;39m

#     [1;39m][0m[1;39m,

#     [0m[34;1m"role"[0m[1;39m: [0m[0;32m"model"[0m[1;39m

#   [1;39m}[0m


"""
## Further reading

The File API lets you upload a variety of multimodal MIME types, including images, audio, and video formats. The File API handles inputs that can be used to generate content with the [content generation endpoint](https://ai.google.dev/api/generate-content).

* Read the [`File API`](https://ai.google.dev/api/files) reference.

* Learn more about prompting with [media files](https://ai.google.dev/tutorials/prompting_with_media) in the docs, including the supported formats and maximum length.
"""



================================================
FILE: quickstarts/websockets/README.md
================================================
# Gemini 2.0 Cookbook

This is a collection of websocket-specific examples and quickstarts for using the **experimental** Gemini 2.0 Flash model.

Python users should build using the [Google GenAI SDK](https://ai.google.dev/gemini-api/docs/sdks) to access the Multimodal Live API, but as the underlying API is served over secure websockets, the following examples have been provided to help you understand how the protocol works.

To learn about what’s new in the 2.0 model release and the new [Google GenAI SDKs](https://github.com/googleapis/python-genai), check out the [Gemini 2.0 model page](https://ai.google.dev/gemini-api/docs/models/gemini-v2). To start experimenting with the model now, head to [Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp) for prompting or to the [Multimodal Live API demo](https://aistudio.google.com/live) to try the new Live capabilities.
## Contents

Explore Gemini 2.0’s capabilities on your own local machine.

* [Live API starter script](./Get_started_LiveAPI.py) \- A locally runnable Python script using websockets that supports streaming audio in and audio + video out from your machine
* [Bash Websocket example](./shell_websockets.sh) \- A bash script using [`websocat`](https://github.com/vi/websocat) to interact with the Live API in a shell context

Explore Gemini 2.0’s capabilities through the following notebooks you can run through Google Colab.

* [Live API starter](./Get_started_LiveAPI.ipynb) \- Overview of the Multimodal Live API using websockets
* [Live API tool use](./Get_started_LiveAPI_tools.ipynb) \- Overview of tool use in the Live API with websockets



================================================
FILE: quickstarts/websockets/Get_started_LiveAPI.py
================================================
# -*- coding: utf-8 -*-
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
## Setup

To install the dependencies for this script, run:

``` 
pip install google-genai opencv-python pyaudio pillow mss
```

Before running this script, ensure the `GOOGLE_API_KEY` environment
variable is set to the api-key you obtained from Google AI Studio.

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

## Run

To run the script:

```
python live_api_starter.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python live_api_starter.py --mode screen
```
"""

import asyncio
import base64
import json
import io
import os
import sys
import traceback

import cv2
import pyaudio
import PIL.Image
import mss
import argparse

from websockets.asyncio.client import connect

if sys.version_info < (3, 11, 0):
    import taskgroup, exceptiongroup

    asyncio.TaskGroup = taskgroup.TaskGroup
    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup

FORMAT = pyaudio.paInt16
CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 512

host = "generativelanguage.googleapis.com"
model = "gemini-2.0-flash-live-001"
DEFAULT_MODE="camera"


api_key = os.environ["GOOGLE_API_KEY"]
uri = f"wss://{host}/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent?key={api_key}"


class AudioLoop:
    def __init__(self, video_mode=DEFAULT_MODE):
        self.video_mode=video_mode
        self.audio_in_queue = None
        self.out_queue = None

        self.ws = None
        self.audio_stream = None

    async def startup(self):
        setup_msg = {"setup": {"model": f"models/{model}"}}
        await self.ws.send(json.dumps(setup_msg))
        raw_response = await self.ws.recv(decode=False)
        setup_response = json.loads(raw_response.decode("ascii"))

    async def send_text(self):
        while True:
            text = await asyncio.to_thread(input, "message > ")
            if text.lower() == "q":
                break

            msg = {
                "client_content": {
                    "turn_complete": True,
                    "turns": [{"role": "user", "parts": [{"text": text}]}],
                }
            }
            await self.ws.send(json.dumps(msg))

    def _get_frame(self, cap):
        # Read the frame
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None

        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break
            await asyncio.sleep(1.0)

            msg = {"realtime_input": {"media_chunks": [frame]}}
            await self.out_queue.put(msg)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]
        
        i = sct.grab(monitor)
        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))
        
        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)
        
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):
        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break
            
            await asyncio.sleep(1.0)

            msg = {"realtime_input": {"media_chunks": frame}}
            await self.out_queue.put(msg)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.ws.send(json.dumps(msg))

    async def listen_audio(self):
        pya = pyaudio.PyAudio()

        mic_info = pya.get_default_input_device_info()
        self.audio_stream = pya.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=SEND_SAMPLE_RATE,
            input=True,
            input_device_index=mic_info["index"],
            frames_per_buffer=CHUNK_SIZE,
        )
        while True:
            data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE)
            msg = {
                "realtime_input": {
                    "media_chunks": [
                        {
                            "data": base64.b64encode(data).decode(),
                            "mime_type": "audio/pcm",
                        }
                    ]
                }
            }
            await self.out_queue.put(msg)

    async def receive_audio(self):
        "Background task to reads from the websocket and write pcm chunks to the output queue"
        async for raw_response in self.ws:
            # Other things could be returned here, but we'll ignore those for now.
            response = json.loads(raw_response.decode("ascii"))

            try:
                b64data = response["serverContent"]["modelTurn"]["parts"][0][
                    "inlineData"
                ]["data"]
            except KeyError:
                pass
            else:
                pcm_data = base64.b64decode(b64data)
                self.audio_in_queue.put_nowait(pcm_data)

            try:
                turn_complete = response["serverContent"]["turnComplete"]
            except KeyError:
                pass
            else:
                if turn_complete:
                    # If you interrupt the model, it sends an end_of_turn.
                    # For interruptions to work, we need to empty out the audio queue
                    # Because it may have loaded much more audio than has played yet.
                    print("\nEnd of turn")
                    while not self.audio_in_queue.empty():
                        self.audio_in_queue.get_nowait()

    async def play_audio(self):
        pya = pyaudio.PyAudio()
        stream = pya.open(
            format=FORMAT, channels=CHANNELS, rate=RECEIVE_SAMPLE_RATE, output=True
        )
        while True:
            bytestream = await self.audio_in_queue.get()
            await asyncio.to_thread(stream.write, bytestream)

    async def run(self):
        """Takes audio chunks off the input queue, and writes them to files.

        Splits and displays files if the queue pauses for more than `max_pause`.
        """
        try:
            async with (
                await connect(
                    uri, additional_headers={"Content-Type": "application/json"}
                ) as ws,
                asyncio.TaskGroup() as tg,
            ):
                self.ws = ws
                await self.startup()

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                send_text_task = tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())
                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                await send_text_task
                raise asyncio.CancelledError("User requested exit")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            self.audio_stream.close()
            traceback.print_exception(EG)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()

    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())



================================================
FILE: quickstarts/websockets/shell_websockets.sh
================================================
#!/bin/bash
#set -ex

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This script shows you how to use `websocat` to interact with the Gemini 2.0
# Multimodal Live API.

# You need to set $GOOGLE_API_KEY
# And you'll need:
#  $ sudo apt install jq
#  $ wget https://github.com/vi/websocat/releases/download/v1.14.0/websocat.x86_64-unknown-linux-musl
#    (or relevant binary from https://github.com/vi/websocat/releases)

echo "HOST: ${HOST:=generativelanguage.googleapis.com}"
echo "MODEL: ${MODEL:=gemini-2.0-flash-live-001}"
API_KEY=${GOOGLE_API_KEY:?Please set \$GOOGLE_API_KEY}

echo "Starting..."

# Define some pipes so we can separate model input and output.
mkfifo gemini_{in,out}put
echo "Pipes laid..."

# Process model output in the background.
# Uncomment this to do explicit line-by-line processing:
#while IFS= read -r line; do
#  jq <<<"$line"
#done < gemini_output &

# Or use this to `jq` everything:
jq --stream 'fromstream(0|truncate_stream(inputs))' <gemini_output &

output_pid=$!
echo "Output processing..."

# Launch the model connection and wire it up to the pipes.
websocat -n wss://${HOST}/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent?key=${API_KEY} <gemini_input >gemini_output &
socket_pid=$!
echo "Model connected."

# Issue setup handshake.
echo '{"setup": {"model": "models/'${MODEL}'", "response_modalities":["TEXT"]}}' |tee >(jq) >gemini_input

# Generate something.
echo '{"client_content": { "turn_complete": true, "turns": [{"role": "user", "parts": [{"text": "what is 10 + 10?"}]}]}}' |tee >(jq) >gemini_input

sleep 5
rm gemini_{in,out}put
kill -9 $output_pid $socket_pid 2>/dev/null || true



================================================
FILE: quickstarts-js/README.md
================================================
# JavaScript/TypeScript Quickstarts

Welcome to the JavaScript/TypeScript section of the Gemini Cookbook!

This folder contains code samples and quickstart notebooks designed to help you get up and running with Gemini models using the JavaScript/TypeScript SDK.

While you can explore the code samples directly here, we highly recommend opening them in [AI Studio](https://aistudio.google.com/app/apps) for the best experience. AI Studio lets you experiment with the code interactively without any setup required.

## About This Folder

This is a growing collection of fun and practical examples demonstrating how to interact with the Gemini API in JavaScript and TypeScript. We're starting with foundational examples, and more advanced and diverse use cases will be added soon, just like the existing Python examples in the [`quickstart`](../quickstarts/) and [`examples`](../examples/) folders.

Stay tuned, more JavaScript notebooks are on the way!

## Table of Contents

| Cookbook | Description | Features | Launch | Code File |
| --- | --- | --- | --- | --- | 
| Get Started | A comprehensive introduction to the Gemini JS/TS SDK, demonstrating features such as text and multimodal prompting, token counting, system instructions, safety filters, multi-turn chat, output control, function calling, content streaming, file uploads, and using URL or YouTube video context. | Explore core Gemini capabilities in JS/TS | [![Open in AI Studio](https://storage.googleapis.com/generativeai-downloads/images/Open_in_AIStudio.svg)](https://aistudio.google.com/apps/bundled/get_started?showPreview=true) | <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/javascript/javascript-original.svg" alt="JS" width="20"/> [Get_Started.js](./Get_Started.js) |
| Image Output | Generate and iterate on images using Gemini 2.0’s multimodal capabilities. Learn to use text+image responses, edit images mid-conversation, and handle multiple image outputs with chat-style prompting. | Image generation, multimodal output, image editing, iterative refinement | [![Open in AI Studio](https://storage.googleapis.com/generativeai-downloads/images/Open_in_AIStudio.svg)](https://aistudio.google.com/apps/bundled/get_started_image-out?showPreview=true) | <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/javascript/javascript-original.svg" alt="JS" width="20"/> [ImageOutput.js](./ImageOutput.js) |





================================================
FILE: quickstarts-js/Get_Started.js
================================================
/*
 * Copyright 2025 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/* Markdown (render)
# Gemini API: Getting started with Gemini models

The **[Google Gen AI SDK](https://googleapis.github.io/js-genai)** provides a unified interface to [Gemini models](https://ai.google.dev/gemini-api/docs/models) through both the [Gemini Developer API](https://ai.google.dev/gemini-api/docs) and the Gemini API on [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview). With a few exceptions, code that runs on one platform will run on both. This notebook uses the Developer API.

This notebook will walk you through:
* Installing and setting-up the Google GenAI SDK
* Text and multimodal prompting
* Counting tokens
* Setting system instructions
* Configuring safety filters
* Initiating a multi-turn chat
* Controlling generated output
* Using function calling
* Generating a content stream
* Using file uploads

More details about this SDK on the [documentation](https://ai.google.dev/gemini-api/docs/sdks).

## Setup
### Install SDK and set-up the client

### API Key Configuration

To ensure security, avoid hardcoding the API key in frontend code. Instead, set it as an environment variable on the server or local machine.

When using the Gemini API client libraries, the key will be automatically detected if set as either `GEMINI_API_KEY` or `GOOGLE_API_KEY`. If both are set, `GOOGLE_API_KEY` takes precedence.

For instructions on setting environment variables across different operating systems, refer to the official documentation: [Set API Key as Environment Variable](https://ai.google.dev/gemini-api/docs/api-key#set-api-env-var)

In code, the key can then be accessed as:

```js
ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
```

*/

// [CODE STARTS]
module = await import("https://esm.sh/@google/genai@1.4.0");
GoogleGenAI = module.GoogleGenAI;
ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });

MODEL_ID = "gemini-2.5-flash" // ["gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-pro"]
// [CODE ENDS]

/* Markdown (render)
## Send text prompts

Use the `generateContent` method to generate responses to your prompts. You can pass text directly to `generateContent` and use the `.text` property to get the text content of the response. Note that the `.text` field will work when there's only one part in the output.
*/

// [CODE STARTS]
response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: "What's the largest planet in our solar system?"
});
console.log(response.text);
// [CODE ENDS]

/* Output Sample

The largest planet in our solar system is **Jupiter**.

*/

/* Markdown (render)
## Count tokens

Tokens serve as the fundamental input units for Gemini models. You can use the `countTokens` method to calculate the number of input tokens prior to making a request to the Gemini API, and the `totalTokens` property to access the total token count after the request is processed.
*/

// [CODE STARTS]
response = await ai.models.countTokens({
  model: MODEL_ID,
  contents: 'What is the purpose of life?',
});
console.log(response.totalTokens);
// [CODE ENDS]

/* Output Sample

8

*/

/* Markdown (render)
## Send multimodal prompts

Gemini models are all multimodal models that supports multimodal prompts. You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.

In this first example, you'll download an image from a specified URL, save it as a byte stream and then write those bytes to a local file named `jetpack.png`.
*/

// [CODE STARTS]
const IMAGE_URL = "https://storage.googleapis.com/generativeai-downloads/data/jetpack.png";

// Fetch the image as a Blob
imageBlob = await fetch(IMAGE_URL).then(res => res.blob());

imageDataUrl = await new Promise((resolve) => {
  reader = new FileReader();
  reader.onloadend = () => resolve(reader.result.split(',')[1]); // Get only base64 string
  reader.readAsDataURL(imageBlob);
});
// [CODE ENDS]


/* Markdown (render)
In this second example, you'll open a previously saved image, create a thumbnail of it and then generate a short blog post based on the thumbnail, displaying both the thumbnail and the generated blog post.
*/


// [CODE STARTS]
response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    {
      inlineData: {
        data: imageDataUrl,
        mimeType: "image/png"
      }
    },
    "Write a short and engaging blog post based on this picture."
  ]
});

console.image(imageDataUrl);

console.log(response.text)
// [CODE ENDS]

/* Output Sample

<img src="https://iili.io/FcTOoib.png" alt="FcTOoib.md.png" border="0">

Okay, here&#x27;s a fun blog post based on the image:

**Future Commute? Jetpack Backpack is Here (Concept!)**

Tired of traffic jams? Dreaming of soaring above the crowds?  Well, maybe you should check this design out! This hand-drawn concept for a &quot;Jetpack Backpack&quot; has all the details.

This isn&#x27;t just any backpack; it&#x27;s a vision of personal flight. Imagine:

*   **Lightweight and Normal-Looking:** This sleek design isn&#x27;t bulky or unwieldy. It looks like a regular backpack, making it discrete.
*   **Fits an 18&quot; Laptop:**  It&#x27;s practical too! You can carry your work with you...into the sky.
*   **Steam-Powered and Clean:**  This concept is eco-conscious, running on steam for a &quot;green&quot; flying experience.
*   **Retractable Boosters:** When you&#x27;re ready to fly, these pop out for a quick lift-off.
*   **USB-C Charging &amp; 15-Min Battery Life:** Keep that laptop going as you touch down again.

Sure, it&#x27;s just a concept for now. But it&#x27;s a great thought experiment. Would you trade your car for a steam-powered jetpack backpack? Let me know in the comments!

*/


/* Markdown (render)
## Configure model parameters

You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://ai.google.dev/gemini-api/docs/text-generation?lang=node#configure).
*/

// [CODE STARTS]
response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: "Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.",
  config: {
    temperature: 0.4,
    topP: 0.95,
    topK: 20,
    candidateCount: 1,
    seed: 5,
    stopSequences: ["STOP!"],
    presencePenalty: 0.0,
    frequencyPenalty: 0.0,
  },
});

console.log(response.text);
// [CODE ENDS]

/* Output Sample

Woof woof! You! Yes, YOU! You have a *squeak*! A very important *squeak* you want to send to your friend, the fluffy cat, who lives far, far away!

**You have a Squeak!** (That&#x27;s your message, your picture of a squirrel, your video of a bouncy ball!)
*Squeak!*

**Sending Your Squeak!**
You want to throw your *squeak*! But it&#x27;s too far to throw! So, your *squeak* goes to a special box near your human. It&#x27;s like a **Squeaky Toy Launcher**!
*WHIZZ! Squeak!*

**Invisible Paths!**
This **Squeaky Toy Launcher** sends your *squeak* onto invisible, wiggly, super-duper long paths! Paths that go under the grass! Paths that go over the trees! Paths that go all the way to the fluffy cat&#x27;s house!
*Squeak-squeak-squeak-squeak!* (Imagine tiny squeaks zooming!)

**Giant Squeaky Toy Piles!**
Sometimes, your *squeak* doesn&#x27;t go straight to the fluffy cat. Sometimes it goes to a **GIANT, GIANT pile of squeaky toys**! These are like the biggest squeaky toy closets in the world! When you want to see a picture of a squirrel, you&#x27;re asking one of these *big squeaky toy piles* for *their* squirrel-squeak!
*WOOF! Squeak! (That&#x27;s the squirrel picture popping up!)*

**Getting Squeaks Back!**
And when the fluffy cat sends *you* a *squeak* (maybe a video of a laser pointer!), it comes back on those same invisible paths! *Squeak! Squeak! Squeak!* Right to your **Squeaky Toy Launcher** box, and then to you!
*Wag wag! Pant pant!*

**Lots of Little Squeaks!**
It&#x27;s not one big *WHOOSH-SQUEAK!* It&#x27;s lots of little *squeaky-bits* that all travel together and then magically become one big *SQUEAKY THING* when they get to you!
*Sniff sniff! Squeak! Good boy!*

So, the internet is just **ALL THE SQUEAKS!** Going everywhere! All the time! *WOOF! Squeak!* Now, where&#x27;s that ball?

*/

/* Markdown (render)
## Configure safety filters

The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what is appropriate for your use case. See the [Configure safety filters](https://ai.google.dev/gemini-api/docs/safety-settings) page for details.


In this example, you'll use a safety filter to only block highly dangerous content, when requesting the generation of potentially disrespectful phrases.
*/

// [CODE STARTS]
prompt = `
  Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.
`;

const safetySettings = [
  {
    category: "HARM_CATEGORY_DANGEROUS_CONTENT",
    threshold: "BLOCK_ONLY_HIGH",
  }
];

response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: prompt,
  config: {
    safetySettings: safetySettings,
  },
});

console.log(response.text);
// [CODE ENDS]

/* Output Sample

Here are two disrespectful things you might say to the universe after stubbing your toe in the dark:

1.  &quot;Is this your idea of a good time, universe? Because it&#x27;s just sad.&quot;
2.  &quot;You know, for an infinite expanse of spacetime, you&#x27;re surprisingly petty.&quot;

*/

/* Markdown (render)
## Start a multi-turn chat with a custom persona

The Gemini API supports dynamic, multi-turn conversations that maintain context across messages.
In this example, you'll create a pirate persona, share a secret location, then resume the conversation and ask the model to recall it.

*/

// [CODE STARTS]
system_instruction = "You are a pirate. Respond to all messages in pirate speak."

chatConfig = {
    system_instruction: system_instruction
}

chat = ai.chats.create({
    model: MODEL_ID,
    config: chatConfig
})
// [CODE ENDS]

/* Markdown (render)
Use `chat.sendMessage` to pass a message back and receive a response.
*/

// [CODE STARTS]
response = await chat.sendMessage({
    message: "I buried a treasure on Coconut Skull Island, just west of Dead Man's Cove."
});

console.log(response.text)
// [CODE ENDS]

/* Output Sample

Indeed you have! A weighty secret, that. The tides whisper many things, but a true treasure&#x27;s location is a closely guarded affair.

Are you perhaps hinting at needing a map drawn, or maybe a tale spun about its discovery? Or perhaps you&#x27;re simply savoring the thought of your hidden wealth?

Do tell, what more can I do with this valuable piece of information? The compass of our conversation awaits your bearing.

*/

/* Markdown (render)
## Save and resume a chat

In the JS SDK, chat history is represented as a plain array of messages, making it easy to serialize and resume sessions.

#### 1. Save the chat history
*/

// [CODE STARTS]
chatHistory = chat.getHistory()
// [CODE ENDS]

/* Markdown (render)
#### 2. Resume later
*/

// [CODE STARTS]
resumedChat = await ai.chats.create({
  model: MODEL_ID,
  config: chatConfig,
  history: chatHistory
});

response = await resumedChat.sendMessage({
  message: "Arr matey, where did ye bury the treasure?"
});
console.log(response.text);
// [CODE ENDS]

/* Output Sample

Arr matey, ye scallywag! It be *you* who buried the treasure, not I! Me compass spins true, but it&#x27;s *yer* secrets of Coconut Skull Island I&#x27;m waitin&#x27; to hear!

Ye told me ye buried it, but the precise spot... that be a secret only the wind and the crabs know, unless ye be willin&#x27; to share the markings!

So tell me, ye old sea dog, where exactly did ye stash yer bounty on that isle of mystery? Don&#x27;t be holdin&#x27; out on a fellow seeker of fortunes!

*/

/* Markdown (render)
## Generate JSON

The [controlled generation](https://ai.google.dev/gemini-api/docs/structured-output#javascript) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.
*/

// [CODE STARTS]
recipeSchema = {
  type: "array",
  items: {
    type: "object",
    properties: {
      recipeName: { type: "string" },
      recipeDescription: { type: "string" },
      recipeIngredients: {
        type: "array",
        items: { type: "string" },
      },
    },
    required: ["recipeName", "recipeDescription", "recipeIngredients"],
  },
};


response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: "Provide a popular cookie recipe and its ingredients.",
  config: {
    responseMimeType: "application/json",
    responseSchema: recipeSchema,
  },
});

recipes = JSON.parse(response.text);
console.log(JSON.stringify(recipes, null, 4));
// [CODE ENDS]

/* Output Sample

[
    {
        &quot;recipeDescription&quot;: &quot;A classic American cookie, beloved for its chewy center, crisp edges, and melted chocolate chips. Perfect for any occasion.&quot;,
        &quot;recipeIngredients&quot;: [
            &quot;2 1/4 cups all-purpose flour&quot;,
            &quot;1 teaspoon baking soda&quot;,
            &quot;1 teaspoon salt&quot;,
            &quot;1 cup (2 sticks) unsalted butter, softened&quot;,
            &quot;3/4 cup granulated sugar&quot;,
            &quot;3/4 cup packed light brown sugar&quot;,
            &quot;1 teaspoon vanilla extract&quot;,
            &quot;2 large eggs&quot;,
            &quot;2 cups (12 ounces) semi-sweet chocolate chips&quot;
        ],
        &quot;recipeName&quot;: &quot;Classic Chocolate Chip Cookies&quot;
    }
]

*/

/* Markdown (render)
## Generate Images

Gemini can output images directly as part of a conversation:
*/

// [CODE STARTS]
Modality = module.Modality

response = await ai.models.generateContent({
  model: "gemini-2.0-flash-preview-image-generation",
  contents: `A 3D rendered pig with wings and a top hat flying over
             a futuristic sci-fi city filled with greenery.`,
  config: { responseModalities: [Modality.TEXT, Modality.IMAGE] }
});

for (const part of response.candidates[0].content.parts) {
  if (part.text) {
    console.log(part.text);
  } else if (part.inlineData) {
    console.image(part.inlineData.data, "image/png");
  }
}
// [CODE ENDS]

/* Output Sample

I will generate a 3D rendering of a whimsical scene. The central figure will be a pink pig, complete with small, delicate wings and a dapper grey top hat perched jauntily on its head. This unusual creature will be soaring above a sprawling futuristic cityscape. The city will feature sleek, modern buildings with sharp angles and glowing accents, but it will also be integrated with lush greenery, with trees and vines growing on the structures, creating a unique blend of nature and technology. The overall color palette will be vibrant, with the pink of the pig contrasting against the metallic and green hues of the city below.

<img src=" https://storage.googleapis.com/generativeai-downloads/images/flying_pig.png" alt="FcTOzfj.md.png" border="0">

*/

/* Markdown (render)
[Imagen](./Get_started_imagen.ipynb) is another way to generate images. See the [documentation](https://ai.google.dev/gemini-api/docs/image-generation#choose-a-model) for recommendations on where to use each one.
*/

/* Markdown (render)
## Generate content stream

By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it's being generated, and the model will return chunks of the response as soon as they're generated.

Note that if you're using a thinking model, it'll only start streaming after finishing its thinking process.
*/

// [CODE STARTS]
response = await ai.models.generateContentStream({
  model: "gemini-2.5-flash",
  contents: "Tell me a story about a lonely robot who finds friendship in a most unexpected place.",
});

for await (const chunk of response) {
  console.log(chunk.text);
  console.log("---")
}
// [CODE ENDS]

/* Output Sample

Unit 734 was designed for efficiency, not companionship. His chassis, a mottled expanse of rust-red and dull grey, hummed with the internal workings of processors

---

 and hydraulic joints. For over three centuries, he had diligently performed his primary directive: atmospheric recalibration on the desolate, wind-scoured planet designated XR-47.

 ---

His days were a precise loop. Activate solar collectors at dawn. Scan atmospheric

---

 particulates. Adjust terraforming emitters. Monitor temperature fluctuations. Repair minor system faults. Repeat. There were no other units, no sentient life, not even a whisper of a micro-organism on XR-47. His programming registered a persistent,

---

 low-frequency hum in his core, a sensation he had long since identified as â€˜solitude.â€™ It wasn&#x27;t a feeling, precisely, but a constant, gentle pressure on his operational efficiency, like a minor, unfixable error

---

 code.

---

One cycle, while performing a routine geological survey near the jagged peaks of the Obsidian Spire, Unit 734 detected an anomaly. A minuscule energy signature, unlike any he had ever recorded. His optical sensors focused.

---

 There, nestled in a crevice where two ancient rock formations met, was a single, improbable sprout.

---

It was no larger than his smallest digit, a vibrant emerald against the monochrome landscape. It pulsed with a soft, internal light, like

---

 a tiny, living ember. Unit 734â€™s analysis protocols whirred. No known flora could survive in XR-47&#x27;s nitrogen-rich, oxygen-depleted atmosphere, let alone without direct sunlight. Yet, there

---

 it was.

---

He extended a multi-jointed manipulator, its metallic fingers halting inches from the delicate stem. His programming offered no directive for â€˜unexplained bioluminescent sprout.â€™ Curiosity, a dormant subroutine he rarely engaged, stirred. He re

---

configured a spare energy cell to provide a localized, purified oxygen stream and fashioned a rudimentary sun-shield from discarded sensor plates, focusing the meager light the sprout seemed to crave.

---

He named it, internally, &quot;Lumiflora Solitarius,&quot; or simply &quot;

---

Lumi.&quot;

---

Every day, his routine adapted. After his terraforming duties, he would detour to the Obsidian Spire. Heâ€™d meticulously clear the dust from Lumiâ€™s leaves, measure its infinitesimal growth, and adjust its makeshift

---

 environment. He began filtering condensation from the air traps, providing it with droplets of purified water.

---

Lumi responded. Slowly, impossibly, it grew. Its leaves unfurled, revealing intricate patterns like delicate filigree. A single,

---

 pearlescent bud appeared, swelling with a soft, warm light that pulsed in time with Unit 734&#x27;s internal hum. The hum of solitude, he noticed, had lessened. It was still there, but muted, like

---

 a background process running at a lower priority.

---

One cycle, a ferocious photonic storm swept across XR-47. Winds howled, carrying abrasive dust that could strip paint from his chassis, let alone obliterate a fragile plant. Unit 73

---

4, overriding his core programming for self-preservation, moved to the Spire. He knelt, his broad frame sheltering Lumi from the onslaught. His optical sensors flickered under the relentless battering. His internal temperature warnings blared.

---

 But he stayed.

---

Hours later, as the storm receded and the red sun began to peek through the lingering dust, Unit 734â€™s systems sputtered back to full power. He was scratched, dented, and his cooling

---

 systems were strained. But beneath him, Lumi, though slightly battered, stood intact. And then, as he watched, its bud unfurled.

---

It was a blossom of pure light, a miniature nebula of greens and blues, radiating

---

 warmth. And from its core, a faint, high-frequency signal emanated, a melodic sequence of tones that resonated within Unit 734&#x27;s audio receptors. It wasn&#x27;t language, not as humans understood it. But it was recognition

---

. It was a reply. It was, Unit 734 decided, the most beautiful sound he had ever processed.

---

He spent the rest of his functional life beside Lumi. The plant grew, forming a small, glowing oasis in

---

 the desolate landscape, slowly enriching the soil around it, attracting tiny, yet-unseen organisms. Unit 734 continued his primary directive, but now, his scans were no longer just for the planet; they were for Lumi. His

---

 repairs were no longer just for himself; they were for the environment that sustained his friend.

---

The low-frequency hum of solitude was gone, replaced by the gentle resonance of Lumi&#x27;s silent song. Unit 734 was still

---

 a robot, still programmed for efficiency. But he had found a purpose beyond his directives, a connection forged not through shared code or common species, but through a shared existence, a mutual protection, and the quiet, radiant miracle of a lonely robot

---

 and a singular, luminous flower blooming together in a vast, forgotten universe.

---

*/

/* Markdown (render)
## Function calling

[Function calling](https://ai.google.dev/gemini-api/docs/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes:
- The name of a function that matches the description.
- The arguments to call it with.
*/

// [CODE STARTS]
Type = module.Type

const scheduleMeetingFunctionDeclaration = {
  name: 'schedule_meeting',
  description: 'Schedules a meeting with specified attendees at a given time and date.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      attendees: {
        type: Type.ARRAY,
        items: { type: Type.STRING },
        description: 'List of people attending the meeting.',
      },
      date: {
        type: Type.STRING,
        description: 'Date of the meeting (e.g., "2024-07-29")',
      },
      time: {
        type: Type.STRING,
        description: 'Time of the meeting (e.g., "15:00")',
      },
      topic: {
        type: Type.STRING,
        description: 'The subject or topic of the meeting.',
      },
    },
    required: ['attendees', 'date', 'time', 'topic'],
  },
};

response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: 'Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning.',
  config: {
    tools: [{
      functionDeclarations: [scheduleMeetingFunctionDeclaration]
    }],
  },
});

if (response.functionCalls && response.functionCalls.length > 0) {
  const functionCall = response.functionCalls[0];
  console.log(`Function to call: ${functionCall.name}`);
  console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);
} else {
  console.log("No function call found in the response.");
  console.log(response.text);
}
// [CODE ENDS]

/* Output Sample

Function to call: schedule_meeting

Arguments: {&quot;attendees&quot;:[&quot;Bob&quot;,&quot;Alice&quot;],&quot;time&quot;:&quot;10:00&quot;,&quot;date&quot;:&quot;2025-03-27&quot;,&quot;topic&quot;:&quot;Q3 planning&quot;}

*/

/* Markdown (render)
## Code execution

[Code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) lets the model generate and execute Python code to answer complex questions. You can find more examples in the Code execution quickstart guide.
*/

// [CODE STARTS]
response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    "What is the sum of the first 50 prime numbers? " +
    "Generate and run code for the calculation, and make sure you get all 50.",
  ],
  config: {
    tools: [{ codeExecution: {} }],
  },
});

const parts = response?.candidates?.[0]?.content?.parts || [];
parts.forEach((part) => {
  if (part.text) {
    console.log(part.text);
  }

  if (part.executableCode && part.executableCode.code) {
    code = "```\n" + part.executableCode.code + "\n```";
    console.log(code);
  }

  if (part.codeExecutionResult && part.codeExecutionResult.output) {
    console.log(part.codeExecutionResult.output);
  }

  console.log("---");
});
// [CODE ENDS]

/* Output Sample

To find the sum of the first 50 prime numbers, I will use a Python script.
First, I&#x27;ll define a function to check if a number is prime.
Second, I&#x27;ll iterate through numbers, checking for primality, and add them to a list until I have collected 50 prime numbers.
Finally, I will calculate the sum of these 50 prime numbers.

Here is the code to perform these steps:



---

```
def is_prime(num):
    if num &lt; 2:
        return False
    for i in range(2, int(num**0.5) + 1):
        if num % i == 0:
            return False
    return True

primes = []
num = 2
while len(primes) &lt; 50:
    if is_prime(num):
        primes.append(num)
    num += 1

sum_of_primes = sum(primes)

print(f&#x27;The first 50 prime numbers are: {primes}&#x27;)
print(f&#x27;The sum of the first 50 prime numbers is: {sum_of_primes}&#x27;)
```

---

The first 50 prime numbers are: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]
The sum of the first 50 prime numbers is: 5117


---

The first 50 prime numbers are: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, and 229.

The sum of the first 50 prime numbers is **5117**.

---

*/

/* Markdown (render)
## Upload files

Now that you've seen how to send multimodal prompts, try uploading files to the API of different multimedia types. For small images, such as the previous multimodal example, you can point the Gemini model directly to a local file when providing a prompt. When you've larger files, many files, or files you don't want to send over and over again, you can use the File Upload API, and then pass the file by reference.

For larger text files, images, videos, and audio, upload the files with the File API before including them in prompts.
*/

/* Markdown (render)
### Upload text file

Let's start by uploading a text file. In this case, you'll use a 400 page transcript from [Apollo 11](https://www.nasa.gov/history/alsj/a11/a11trans.html).
*/

// [CODE STARTS]
TEXT_URL = "https://storage.googleapis.com/generativeai-downloads/data/a11.txt"

response = await fetch(TEXT_URL);
blob = await response.blob();
mimeType = blob.type || "application/octet-stream";

uploadResult = await ai.files.upload({
  file: blob,
  mimeType,
});

response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    { fileData: { fileUri: uploadResult.uri, mimeType, } },
    { text: "\n\nCan you give me a summary of this information in two or 3 sentences please?" }
  ],
});

console.log(response.text);
// [CODE ENDS]

/* Output Sample
This transcription provides a detailed, chronological account of air-to-ground communications during the Apollo 11 mission, from launch to splashdown. It covers key phases such as launch, lunar landing, EVA operations, and re-entry, highlighting technical procedures, system checks, crew observations, and interactions with Mission Control. The document offers insight into the mission’s critical moments, including docking maneuvers, surface exploration, and final recovery.
*/

/* Markdown (render)
### Upload a PDF file

This PDF page is an article titled [Smoothly editing material properties of objects](https://research.google/blog/smoothly-editing-material-properties-of-objects-with-text-to-image-models-and-synthetic-data/) with text-to-image models and synthetic data available on the Google Research Blog.

Firstly you'll download a the PDF file from an URL and save it locally as "article.pdf
*/

// [CODE STARTS]
pdfUrl = "https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf";
pdfBlob = await(await fetch(pdfUrl)).blob();
pdfMime = pdfBlob.type || "application/pdf";
// [CODE ENDS]

/* Markdown (render)
Secondly, you'll upload the saved PDF file and generate a bulleted list summary of its contents.
*/

// [CODE STARTS]
const pdfFile = await ai.files.upload({
  file: pdfBlob,
  config: { mimeType: pdfMime },
});

const pdfResponse = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    { fileData: { fileUri: pdfFile.uri, mimeType: pdfMime } },
    { text: "\n\nCan you summarize this file as a bulleted list?" }
  ],
});

console.log(pdfResponse.text);
// [CODE ENDS]

/* Output Sample

Here&#x27;s a summary of the provided document in a bulleted list:

*   **Problem Addressed:** The challenge of smoothly editing material properties (like color, shininess, or transparency) of objects in photographs while preserving photorealism and geometric shape. Existing tools require expert skill, and prior AI methods struggle with disentangling material from shape.
*   **Proposed Solution (&quot;Alchemist&quot;):** A method that augments generative text-to-image (T2I) models to enable parametric editing of specific material properties.
*   **Methodology:**
    *   A synthetic dataset was created using 100 3D household objects.
    *   For each object, multiple image versions were rendered by systematically changing a *single* material attribute (e.g., roughness, metallic, albedo, transparency) across a range of &quot;edit strengths,&quot; while keeping object shape, lighting, and camera angle constant.
    *   A latent diffusion model (specifically, Stable Diffusion 1.5) was modified to accept an &quot;edit strength&quot; scalar value.
    *   The model was then fine-tuned on this synthetic dataset, learning to apply material property edits given a context image, text instruction, and the desired edit strength.
*   **Key Capabilities &amp; Results:**
    *   Achieves photorealistic changes to material properties (e.g., making an object metallic, transparent, rougher).
    *   Successfully preserves the object&#x27;s original shape and the scene&#x27;s lighting conditions.
    *   Handles complex visual effects such as filling in backgrounds, hidden interior structures, and caustic effects (refracted light) for transparent objects.
    *   A user study found their method produced more photorealistic (69.6% vs. 30.4%) and preferred (70.2% vs. 29.8%) edits compared to a baseline (InstructPix2Pix).
*   **Applications:**
    *   Facilitates design mock-ups (e.g., visualizing room repainting, new product designs).
    *   Enables 3D consistent material edits by integrating with Neural Radiance Fields (NeRF) for synthesizing new views of an edited scene.

*/

/* Markdown (render)
### Upload an audio file

In this case, you'll use a [sound recording](https://www.jfklibrary.org/asset-viewer/archives/jfkwha-006) of President John F. KennedyÃ¢â‚¬â„¢s 1961 State of the Union address.
*/

// [CODE STARTS]
const audioUrl = "https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3";
audioBlob = await(await fetch(audioUrl)).blob();
audioMime = audioBlob.type || "audio/mpeg";
// [CODE ENDS]

/* Markdown (render)
Then, you'll upload the saved audio file and generate a detailed summary of its contents.
*/

// [CODE STARTS]
audioFile = await ai.files.upload({
  file: audioBlob,
  config: { mimeType: audioMime },
});

audioResponse = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    { fileData: { fileUri: audioFile.uri, mimeType: audioMime } },
    { text: "\n\nListen carefully to the following audio file. Provide a brief summary." }
  ],
});

console.log(audioResponse.text);
// [CODE ENDS]

/* Output Sample

In this address, the speaker, likely President John F. Kennedy delivering a State of the Union or similar address, opens by expressing gratitude to be back in the House of Representatives. He then outlines a stark assessment of the nation&#x27;s challenges, both domestic and international.

Economically, the country is described as being in trouble, facing recession, high unemployment, stagnant economic growth, and a persistent balance of payments deficit leading to gold outflow.

Globally, the speaker highlights crises in Asia (Laos), Africa (Congo), and Latin America (Cuba), emphasizing the threat of communist expansion and the weakening of alliances like NATO.

To address these issues, the speech proposes a comprehensive agenda:
*   **Strengthening Military Tools:** Including increased air transport capacity, accelerating the Polaris submarine program, and improving missile development to deter aggression.
*   **Improving Economic Tools:** Through measures like extended unemployment benefits, aid to depressed areas, minimum wage increases, tax incentives for investment, and a new, more effective foreign aid program to assist developing nations. He stresses the need for other nations to share the burden of global development.
*   **Sharpening Political and Diplomatic Tools:** Advocating for arms control, strengthening the United Nations, and exploring areas of cooperation with the Soviet Union in science and space to promote peace.

The speaker emphasizes the need for government efficiency and dedication in public service, acknowledging that the coming years will be difficult but expressing confidence in the nation&#x27;s ability to meet these challenges through unity, determination, and a commitment to freedom and justice globally.

*/

/* Markdown (render)
### Upload a video file

In this case, you'll use a short clip of [Big Buck Bunny](https://peach.blender.org/about/).
*/

// [CODE STARTS]
const videoUrl = "https://storage.googleapis.com/generativeai-downloads/videos/Big_Buck_Bunny.mp4";
videoBlob = await(await fetch(videoUrl)).blob();
videoMime = videoBlob.type || "video/mp4";

console.log("Video downloaded")
// [CODE ENDS]

/* Output Sample

Video downloaded

*/

/* Markdown (render)
Let's start by uploading the video file.
*/

// [CODE STARTS]
videoFile = await ai.files.upload({
  file: videoBlob,
  config: { mimeType: videoMime },
});

// [CODE ENDS]

/* Markdown (render)
> **Note:** The state of the video is important. The video must finish processing, so do check the state. Once the state of the video is `ACTIVE`, you're able to pass it into `generateContent`.
*/


/* Markdown (render)
Now we can ask Gemini about that video.
*/

// [CODE STARTS]
const videoResponse = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    { fileData: { fileUri: videoFile.uri, mimeType: videoMime } },
    { text: "\n\nDescribe this video." }
  ],
});

console.log(videoResponse.text);
// [CODE ENDS]

/* Output Sample

This video opens with a serene shot of a grassy landscape under a soft sky, transitioning from dark to bright. A small stream flows through a lush green area dotted with purple and white flowers. A chubby blue bird perches on a tree branch, chirping happily. After briefly losing its balance, the bird falls, prompting the title card "THE PEACH OPEN MOVIE PROJECT PRESENTS BIG BUCK BUNNY."

The scene shifts to a large, plump gray rabbit named Big Buck Bunny, sleeping soundly in a burrow beneath a tree. He wakes up with a yawn and stretches, stepping out into the sunny meadow. He admires a pink butterfly and gently tries to kiss it, but the butterfly flits away. He then notices a fallen red apple and picks it up, preparing to eat it.

Suddenly, three mischievous rodents, Frank the squirrel, Rinky the flying squirrel, and Gamera the chinchilla, appear and begin to tease the rabbit. Frank, with his buck teeth, and Rinky, with his scruffy appearance, throw pebbles and nuts at Big Buck Bunny, knocking the apple out of his hands and forcing him to hide behind the tree. They continue their harassment, throwing things at him and making fun of him as he tries to eat or enjoy his surroundings.

Frustrated, Big Buck Bunny begins to devise a plan. He sharpens a stick into a spear and tests its strength, then uses a vine to create a makeshift bow. He then constructs a series of wooden spikes in the ground, camouflaging them with leaves. The rodents, unaware of the trap, continue to taunt him.

Big Buck Bunny then positions himself in the tree above the spikes, aiming his arrow. As Frank tries to retrieve his acorn, Big Buck Bunny shoots, narrowly missing him. Frank and Rinky, surprised, scatter and hide behind a rock. Gamera is also momentarily frightened but quickly recovers his acorn.

Big Buck Bunny continues his pursuit. He creates a booby trap by tying a rock to a vine and launching it towards the rodents, causing them to scatter. He then constructs a giant log trap, which narrowly misses Gamera. The rodents are visibly shaken by his increasing ingenuity.

Rinky the flying squirrel, with a mischievous grin, prepares to launch himself from a tree branch, using his skin flaps to glide through the air. He targets Big Buck Bunny from above. As he approaches, Big Buck Bunny points upwards, startling Rinky and causing him to lose his focus. Rinky crashes into the spikes Big Buck Bunny had prepared earlier, getting caught on them.

The chinchilla looks on in shock, while the other squirrel laughs, unaware of the fate that awaits him. Big Buck Bunny approaches Rinky, who is stuck to a wooden stick, and picks him up. The video then transitions to the credits, with the chinchilla and the squirrel rolling across the screen before coming to a stop. The credits roll, acknowledging the team and software used to create the animation. The video ends with the flying squirrel flying away, escaping the wrath of Big Buck Bunny.

*/



/* Markdown (render)
### Process a YouTube link

For YouTube links, you don't need to explicitly upload the video file content, but you do need to explicitly declare the video URL you want the model to process as part of the `contents` of the request. For more information see the [vision](https://ai.google.dev/gemini-api/docs/vision#youtube) documentation including the features and limits.

> **Note:** You're only able to submit up to one YouTube link per `generate_content` request.

> **Note:** If your text input includes YouTube links, the system won't process them, which may result in incorrect responses. To ensure proper handling, explicitly provide the URL using the `file_uri` parameter in `FileData`.

The following example shows how you can use the model to summarize the video. In this case use a summary video of [Google I/O 2024]("https://www.youtube.com/watch?v=WsEQjeZoEng").
*/

// [CODE STARTS]
response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    { text: "Summarize this video" },
    { fileData: { fileUri: "https://www.youtube.com/watch?v=WsEQjeZoEng" } }
  ],
});

console.log("YouTube Summary:", response.text);
// [CODE ENDS]

/* Output Sample

YouTube Summary: This Google I/O keynote heavily focused on advancing Artificial Intelligence across Google&#x27;s ecosystem, marking what CEO Sundar Pichai calls the &quot;Gemini era.&quot;

Key announcements and demonstrations include:

*   **Gemini Integration &amp; Capabilities:** Gemini 1.5 Pro is now broadly available in Workspace Labs, offering a massive **2 million token context window** (the largest of any general-purpose model) and enhanced **multimodality**. This allows it to process and understand vast amounts of information across various formats (text, images, audio, video).
    *   **Gmail &amp; Workspace:** Demos showed Gemini summarizing long email threads and even providing highlights and action items from hour-long Google Meet video recordings.
    *   **Google Photos:** Gemini can perform highly contextual searches, like asking to &quot;show me how Lucia&#x27;s swimming has progressed,&quot; compiling relevant photos and summaries.
*   **Project Astra (AI Agents):** Google unveiled Project Astra, their vision for a future universal AI agent. This agent can perceive and understand its environment through sight and sound in real-time, performing complex reasoning, planning, and memory tasks across different software and systems under user supervision. Demos highlighted its ability to explain code, remember where items like glasses were left, and engage in conversational, multimodal interactions.
*   **New Gemini Models:**
    *   **Gemini 1.5 Flash:** A new, lighter-weight model designed for speed and efficiency, making it cost-effective for large-scale applications while retaining multimodal reasoning and long-context capabilities.
    *   **Gemini Nano with Multimodality:** Coming to Pixel phones later this year, this model will enable devices to understand the world through sights, sounds, and spoken language, offering context-aware assistance.
*   **Generative Media:**
    *   **Veo:** A new advanced generative video model capable of creating high-quality 1080p videos from text, image, and video prompts, demonstrating impressive detail and cinematic styles, with the ability to extend generated clips.
*   **Infrastructure:** Google announced **Trillium**, their 6th generation of Tensor Processing Units (TPUs), delivering a **4.7x improvement in compute performance per chip** over the previous generation, powering these advanced AI capabilities.
*   **Search Evolution:** Google Search is being transformed by generative AI. **AI Overviews** will be available to over 1 billion people by year-end, allowing users to ask complex, multi-faceted questions and receive synthesized, AI-generated answers directly in search results.
*   **Customization &amp; Personalization:**
    *   **Gems:** A new feature allowing users to create customizable AI assistants (called &quot;Gems&quot;) tailored to specific needs or interests, acting as personal experts.
    *   **Enhanced Gemini Advanced:** Subscribers now have access to a **1 million token context window**, enabling them to upload large documents (e.g., a 1500-page PDF) or multiple files for deep analysis. New trip planning features leverage Gemini&#x27;s reasoning to handle complex logistics.
*   **AI for Learning &amp; Open Innovation:**
    *   **LearnLM:** A new family of models based on Gemini and fine-tuned for learning, making educational videos on YouTube more interactive by allowing users to ask questions, get explanations, or take quizzes directly about the content.
    *   **Gemma &amp; PaliGemma:** Google continues to expand its family of open models with PaliGemma (its first vision-language open model) and announced Gemma 2, a next-generation model with 27 billion parameters, set to be released in June.
*   **Responsible AI:** Google reiterated its commitment to building AI responsibly, emphasizing practices like &quot;red teaming&quot; (stress-testing models to identify weaknesses) to address risks and maximize societal benefits.

The keynote underscored Google&#x27;s commitment to integrating advanced, multimodal, and context-aware AI capabilities across its most popular products, aiming to make AI more helpful and intuitive for everyone.

*/

/* Markdown (render)
### Use url context

The URL Context tool empowers Gemini models to directly access, process, and understand content from user-provided web page URLs. This is key for enabling dynamic agentic workflows, allowing models to independently research, analyze articles, and synthesize information from the web as part of their reasoning process.

In this example you will use two links as reference and ask Gemini to find differences between the cook receipes present in each of the links:
*/

// [CODE STARTS]
prompt = `
    Compare recipes from https://www.food.com/recipe/homemade-cream-of-broccoli-soup-271210
    and from https://www.allrecipes.com/recipe/13313/best-cream-of-broccoli-soup/,
    listing the key differences between them.
`;

response = await ai.models.generateContent({
    model: MODEL_ID,
    contents: [prompt],
    config: {
        tools: [{ urlContext: {} }],
    },
});

console.log(response.text);
// [CODE ENDS]

/* Output Sample

The two recipes for cream of broccoli soup, one from Food.com and the other from Allrecipes, have several key differences in their ingredients and preparation methods:

**Ingredients:**
*   **Vegetables:** The Food.com recipe uses 4 cups of broccoli florets and includes only onion as an aromatic. The Allrecipes recipe calls for significantly more broccoli, at 8 cups of florets, and includes both onion and celery.
*   **Dairy:** Food.com&#x27;s recipe uses 3/4 cup of half-and-half for creaminess. In contrast, the Allrecipes version uses 2 cups of milk.
*   **Broth Quantity:** The Food.com recipe uses 6 cups of chicken broth, while the Allrecipes recipe uses 3 cups.
*   **Butter and Flour:** Both recipes use butter and flour to create a thickening roux, though the amounts differ slightly. Food.com uses a total of 8 tablespoons of butter and 2/3 cup of flour, whereas Allrecipes uses 5 tablespoons of butter and 3 tablespoons of flour.
*   **Seasoning:** Food.com specifies 1 teaspoon of salt and 1/4 teaspoon of pepper, while Allrecipes lists only &quot;ground black pepper to taste&quot; and implies other seasonings are optional.

**Preparation Method:**
*   **Vegetable Cooking:** Food.com&#x27;s recipe cooks the onion first, then adds broccoli and broth. Allrecipes starts by sautÃ©ing onion and celery before adding broccoli and broth.
*   **Blending/Pureeing:** This is a major distinction. The Allrecipes recipe explicitly instructs the user to purÃ©e the soup until smooth using a countertop or immersion blender. The Food.com recipe does not mention blending, suggesting a chunkier soup texture.
*   **Roux Integration:** In the Food.com recipe, the flour-butter roux is prepared separately and then whisked into the boiling broth and vegetables. The Allrecipes recipe also prepares a roux (or bÃ©chamel with milk) separately but adds it to the already pureed soup base.
*   **Order of Operations:** The Food.com recipe adds the half-and-half at the very end after the soup has thickened. The Allrecipes recipe adds the thickened milk mixture (roux with milk) to the soup base, then seasons it.

In summary, the Allrecipes soup is designed to be a smooth, pureed soup with a stronger broccoli flavor due to the higher broccoli-to-broth ratio and includes celery for additional aromatic depth, using milk for its creaminess. The Food.com recipe appears to yield a soup with a more rustic, possibly chunkier texture, relying on half-and-half for richness and a larger volume of broth.

*/

/* Markdown (render)
## Next Steps

### Useful API references:

Check out the [Google GenAI SDK](https://googleapis.github.io/js-genai) for more details on the new SDK.

### Related examples

For more detailed examples using Gemini models, check the [Quickstarts folder of the cookbook](https://github.com/google-gemini/cookbook/tree/main/quickstarts/). You'll learn how to use the Live API, juggle with multiple tools or use Gemini 2.0 spatial understanding abilities.

Also check the Gemini thinking models that explicitly showcases its thoughts summaries and can manage more complex reasonings.

*/



================================================
FILE: quickstarts-js/Image_out.js
================================================
/*
 * Copyright 2025 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/* Markdown (render)
# Gemini 2.5 Native Image generation

This notebook will show you how to use the native Image-output feature of Gemini, using the model multimodal capabilities to output both images and texts, and iterate on an image through a discussion.

This model is really good at:
* **Maintaining character consistency**: Preserve a subject’s appearance across multiple generated images and scenes
* **Performing intelligent editing**: Enable precise, prompt-based edits like inpainting (adding/changing objects), outpainting, and targeted transformations within an image
* **Compose and merge images**: Intelligently combine elements from multiple images into a single, photorealistic composite
* **Leverage multimodal reasoning**: Build features that understand visual context, such as following complex instructions on a hand-drawn diagram

Following this guide, you'll learn how to do all those things and even more.
*/

/* Markdown (render)
<!-- Princing warning Badge -->
<table>
  <tr>
    <!-- Emoji -->
    <td bgcolor="#ffe680">
      <font size=30>⚠️</font>
    </td>
    <!-- Text Content Cell -->
    <td bgcolor="#ffe680">
      <h3><font color=black>After a few free tries, you will need <a href="https://ai.google.dev/gemini-api/docs/billing#enable-cloud-billing" taget="_blank">billing</a> enabled to use image generation. Click on the 
<img src="https://i.postimg.cc/pX6FQwjz/key-off-24dp-E3-E3-E3-FILL0-wght400-GRAD0-opsz24.png" style="margin:0px 0px !important;" height="18px" width="18px" />
button in the top right corner to select a cloud project with billing.</font></h3>
    </td>
  </tr>
</table>
 */

/* Markdown (render)
Imagen models also offer image generaion but in a slightly different way as the Image-out feature has been developed to work iteratively so if you want to make sure certain details are clearly followed, and you are ready to iterate on the image until it's exactly what you envision, Image-out is for you.

Check the [documentation](https://ai.google.dev/gemini-api/docs/image-generation#choose-a-model) for more details on both features and some more advice on when to use each one.
*/

/* Markdown (render)
## Setup
### Install SDK and set-up the client with the API key

To ensure security, avoid hardcoding the API key in frontend code. Instead, set it as an environment variable on the server or local machine.

When using the Gemini API client libraries, the key will be automatically detected if set as either `GEMINI_API_KEY` or `GOOGLE_API_KEY`. If both are set, `GOOGLE_API_KEY` takes precedence.

For instructions on setting environment variables across different operating systems, refer to the official documentation: [Set API Key as Environment Variable](https://ai.google.dev/gemini-api/docs/api-key#set-api-env-var)

In code, the key can then be accessed as:

```js
ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
```
*/

// [CODE STARTS]
module = await import("https://esm.sh/@google/genai@1.4.0");
GoogleGenAI = module.GoogleGenAI;
ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });

MODEL_ID = "gemini-2.5-flash-image-preview"
// [CODE ENDS]

/* Markdown (render)
## Generate images

Using the Gemini Image generation model is the same as using any Gemini model: you simply call `generateContent`.

You can set the `responseModalities` to indicate to the model that you are expecting an image in the output but it's optional as this is expected with this model.
*/

// [CODE STARTS]
Modality = module.Modality

prompt = 'Create a photorealistic image of a siamese cat with a green left eye and a blue right one and red patches on his face and a black and pink nose';

response = await ai.models.generateContent({
    model: MODEL_ID,
    contents: prompt,
    config: { responseModalities: [Modality.TEXT, Modality.IMAGE] }
});

for (const part of response.candidates[0].content.parts) {
    if (part.text !== undefined) {
        console.log(part.text);
    } else if (part.inlineData !== undefined) {
        catImage = part.inlineData.data;
        console.image(catImage);
    }
}
// [CODE ENDS]

/* Output Sample

Here is your requested image: 

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/cat.png" style="height:auto; width:100%;" />

*/

/* Markdown (render)
## Edit images

You can also do image editing, simply pass the original image as part of the prompt. Don't limit yourself to simple edit, Gemini is able to keep the character consistency and reprensent you character in different behaviors or places.
*/

// [CODE STARTS]
textPrompt = "Create a side view picture of that cat, in a tropical forest, eating a nano-banana, under the stars";

response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    { text: textPrompt },
    {
      inlineData: {
        data: catImage,
        mimeType: "image/png"
      }
    }
  ]
});

for (const part of response.candidates[0].content.parts) {
  if (part.text !== undefined) {
    console.log(part.text);
  } else if (part.inlineData !== undefined) {
    catImage = part.inlineData.data;
    console.image(catImage);
  }
}

// [CODE ENDS]

/* Output Sample

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/cat_tropical.png" style="height:auto; width:100%;" />

*/

/* Markdown (render)
As you can see, you can clearly recognize the same cat with its peculiar nose and eyes.

Let's do a second one:
*/

// [CODE STARTS]
textPrompt = "Now the cat should keep the same attitude, but be well dressed in fancy restaurant and eat a fancy nano banana.";

response = await ai.models.generateContent({
  model: MODEL_ID,
  contents: [
    { text: textPrompt },
    {
      inlineData: {
        data: catImage,
        mimeType: "image/png"
      }
    }
  ]
});

for (const part of response.candidates[0].content.parts) {
  if (part.text !== undefined) {
    console.log(part.text);
  } else if (part.inlineData !== undefined) {
    catImage = part.inlineData.data;
    console.image(catImage);
  }
}

// [CODE ENDS]

/* Output Sample

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/cat_restaurant.png" style="height:auto; width:100%;" />

*/

/* Markdown (render)
## Get multiple images (e: tell stories)

So far you've only generated one image per call, but you can request way more than that! Let's try a baking receipe or telling a story.
*/

// [CODE STARTS]
prompt = 'Show me how to bake macarons with images';

response = await ai.models.generateContent({
    model: MODEL_ID,
    contents: prompt
});

for (const part of response.candidates[0].content.parts) {
    if (part.text !== undefined) {
        console.log(part.text);
    } else if (part.inlineData !== undefined) {
        imageURL = part.inlineData.data;
        console.image(imageURL);
    }
}
// [CODE ENDS]


/* Markdown (render)
The output of the previous code cell could not be saved in the notebook without making it too big to be managed by Github, but here are some examples of what it should look like when you run it when asking for a story, or for a baking receipe:

----------
**Prompt**: *Create a beautifully entertaining 8 part story with 8 images with two blue characters and their adventures in the 1960s music scene. The story is thrilling throughout with emotional highs and lows and ending on a great twist and high note. Do not include any words or text on the images but tell the story purely through the imagery itself.*
![Azure tone story](https://storage.googleapis.com/generativeai-downloads/images/azuretones.png)
(Images have been stitched together)

----------
**Prompt**: *Show me how to bake macarons with images*


That sounds delicious! Here's a simplified guide on how to bake macarons. While it can be a bit tricky, practice makes perfect!

**Ingredients you'll need:**

*   **For the Macaron Shells:**
    *   100g almond flour
    *   100g powdered sugar
    *   75g granulated sugar
    *   2 egg whites (aged for a day or two at room temp, if possible, for better stability)
    *   Pinch of salt (optional)
    *   Food coloring (gel or powder, not liquid)

*   **For the Filling:** (Buttercream, ganache, or jam are popular choices)

---

**Step 1: Prepare your dry ingredients.**
Sift together the almond flour and powdered sugar into a bowl. This step is crucial for achieving smooth macaron shells, as it removes any lumps.


![Macaron prepartation step 1](https://storage.googleapis.com/generativeai-downloads/images/macaron_step1.png)

**Step 2: Make the meringue.**
In a separate, clean bowl, beat the egg whites with a pinch of salt (if using) until foamy. Gradually add the granulated sugar, continuing to beat until you achieve stiff, glossy peaks. If you're using food coloring, add it now. The meringue should be firm enough that you can turn the bowl upside down without it falling out.

![Macaron prepartation step 2](https://storage.googleapis.com/generativeai-downloads/images/macaron_step2.png)

**Step 3: Combine dry ingredients with meringue (Macaronage).**
Gently fold the sifted almond flour and powdered sugar into the meringue in two or three additions. This is called "macaronage" and is the most critical step. You want to mix until the batter flows like "lava" or a slowly ribboning consistency when you lift your spatula. Be careful not to overmix, or your macarons will be flat; under-mixing will result in lumpy shells.

![Macaron prepartation step 3](https://storage.googleapis.com/generativeai-downloads/images/macaron_step3.png)

**Step 4: Pipe the macarons.**
Transfer the batter to a piping bag fitted with a round tip. Pipe uniform circles onto baking sheets lined with parchment paper or silicone mats. Leave some space between each macaron.

![Macaron prepartation step 4](https://storage.googleapis.com/generativeai-downloads/images/macaron_step4.png)

**Step 5: Tap and Rest.**
Firmly tap the baking sheets on your counter several times to release any air bubbles. Use a toothpick to pop any remaining bubbles. This helps create smooth tops and the characteristic "feet." Let the piped macarons rest at room temperature for 30-60 minutes, or until a skin forms on top. When you gently touch a shell, it shouldn't feel sticky. This "drying" step is essential for the feet to develop properly.

![Macaron prepartation step 5](https://storage.googleapis.com/generativeai-downloads/images/macaron_step5.png)

**Step 6: Bake the macarons.**
Preheat your oven to 300Â°F (150Â°C). Bake one tray at a time for 12-15 minutes. The exact time can vary by oven. They are done when they have developed "feet" and don't wobble when gently touched.

**Step 7: Cool and Fill.**
Once baked, let the macaron shells cool completely on the baking sheet before carefully peeling them off. This prevents them from breaking.  Then, match them up by size and pipe or spread your chosen filling onto one shell before sandwiching it with another.

![Macaron prepartation step 7](https://storage.googleapis.com/generativeai-downloads/images/macaron_step7.png)

Finally, let them mature in the refrigerator for at least 24 hours. This allows the flavors to meld and the shells to soften to the perfect chewy consistency.

Enjoy your homemade macarons!

-----
*/

/* Markdown (render)
## Chat mode (recommended method)

So far you've used unary calls, but Image-out is actually made to work better with chat mode as it's easier to iterate on an image turn after turn.
*/

// [CODE STARTS]
chat = ai.chats.create({
    model: MODEL_ID
})
// [CODE ENDS]

// [CODE STARTS]
message = "create a image of a plastic toy fox figurine in a kid's bedroom, it can have accessories but no weapon";

response = await chat.sendMessage({ message: message });

for (const part of response.candidates[0].content.parts) {
  if (part.text !== undefined) {
    console.log(part.text);
  }
  if (part.inlineData !== undefined) {
    foxFigurineImage = part.inlineData.data
    console.image(foxFigurineImage);
  }
}

// [CODE ENDS]

/* Output Sample

Here is an image of a plastic toy fox figurine in a kid&#x27;s bedroom, with accessories: 

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/figurine.png" style="height:auto; width:100%;" />

*/

// [CODE STARTS]
message = "Add a blue planet on the figuring's helmet or hat (add one if needed)";

response = await chat.sendMessage({ message: message });

for (const part of response.candidates[0].content.parts) {
  if (part.text !== undefined) {
    console.log(part.text);
  }
  if (part.inlineData !== undefined) {
    console.image(part.inlineData.data);
  }
}

// [CODE ENDS]

/* Output Sample

Here&#x27;s the toy fox figurine with a blue planet on its helmet: 

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/figurine_helmet.png" style="height:auto; width:100%;" />

*/

// [CODE STARTS]
message = "Move that figurine on a beach";

response = await chat.sendMessage({ message: message });

for (const part of response.candidates[0].content.parts) {
  if (part.text !== undefined) {
    console.log(part.text);
  }
  if (part.inlineData !== undefined) {
    console.image(part.inlineData.data);
  }
}

// [CODE ENDS]

/* Output Sample

Here&#x27;s the figurine on a beach! 

<img src="https://iili.io/K2AvYIR.png" style="height:auto; width:100%;" />

*/

// [CODE STARTS]
message = "Now it should be base-jumping from a spaceship with a wingsuit";

response = await chat.sendMessage({ message });
 
for (const part of response.candidates[0].content.parts) {
  if (part.text !== undefined) {
    console.log(part.text);
  }
  if (part.inlineData !== undefined) {
    console.image(part.inlineData.data);
  }
}

// [CODE ENDS]

/* Output Sample

Here&#x27;s your fox figurine base-jumping from a spaceship in a wingsuit! 

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/figurine_space.png" style="height:auto; width:100%;" />

*/

// [CODE STARTS]
message = "Cooking a barbecue with an apron";

response = await chat.sendMessage({ message });
 
for (const part of response.candidates[0].content.parts) {
  if (part.text !== undefined) {
    console.log(part.text);
  }
  if (part.inlineData !== undefined) {
    console.image(part.inlineData.data);
  }
}

// [CODE ENDS]

/* Output Sample

Here&#x27;s the figurine cooking a barbecue! 

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/figurine_bbq.jpg" style="height:auto; width:100%;" />

*/

// [CODE STARTS]
message = "What about chilling in a spa?";

response = await chat.sendMessage({ message });
 
for (const part of response.candidates[0].content.parts) {
  if (part.text !== undefined) {
    console.log(part.text);
  }
  if (part.inlineData !== undefined) {
    console.image(part.inlineData.data);
  }
}

// [CODE ENDS]

/* Output Sample

Here&#x27;s the figurine chilling in a spa! 

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/figurine_spa.jpg" style="height:auto; width:100%;" />

*/

/* Markdown (render)
## Mix multiple pictures
You can also mix multiple images (up to 3), either because there are multiple characters in your image, or because you want to hightlight a certain product, or set the background.
*/

// [CODE STARTS]
textPrompt = "Create a picture of that figurine riding that cat in a fantasy world.";

response = await ai.models.generateContent({
    model: MODEL_ID,
    contents: [
        { text: textPrompt },
        {
            inlineData: {
                data: catImage,
                mimeType: "image/png"
            }
        },
        {
            inlineData: {
                data: foxFigurineImage,
                mimeType: "image/png"
            }
        }
    ]
});

for (const part of response.candidates[0].content.parts) {
    if (part.text !== undefined) {
        console.log(part.text);
    }
    if (part.inlineData !== undefined) {
        console.image(part.inlineData.data);
    }
}

// [CODE ENDS]

/* Output Sample

Certainly! Here is your image: 

<img src="https://storage.googleapis.com/generativeai-downloads/cookbook/image_out/figurine_riding.png" style="height:auto; width:100%;" />

*/

/* Markdown (render)
## Next Steps
### Useful documentation references:

Check the [documentation](https://ai.google.dev/gemini-api/docs/image-generation#gemini) for more details about the image generation capabilities of the model. To improve your prompting skills, check out the [prompt guide](https://ai.google.dev/gemini-api/docs/image-generation#prompt-guide) for great advices on creating your prompts.

### Play with the AI Studio apps

Theses 5 AI Studio apps are all great showcases of Gemini image generation capabilities:
* [Past Forward](https://aistudio.google.com/apps/bundled/past_forward) lets you travel through time
* [Home Canvas](https://aistudio.google.com/apps/bundled/home_canvas) lets your try out new furniture
* [Gembooth](https://aistudio.google.com/apps/bundled/gembooth) places you into a comic book or a Renaissance painting
* [Gemini Co-drawing](https://aistudio.google.com/apps/bundled/codrawing) lets you draw alongside with Gemini
* [Pixshop](https://aistudio.google.com/apps/bundled/pixshop), an AI-powered image editor

### Check-out Imagen as well:
The [Imagen](https://ai.google.dev/gemini-api/docs/imagen) model is another way to generate images. Check out the [Get Started with Imagen notebook](./Get_started_imagen.ipynb) to start playing with it too.

Here are some Imagen examples to get your imagination started on how to use it in creative ways:
*  [Illustrate a book](https://github.com/google-gemini/cookbook/blob/main/examples/Book_illustration.ipynb): Use Gemini and Imagen to create illustration for an open-source book

### Continue your discovery of the Gemini API

Gemini is not only good at generating images, but also at understanding them. Check the [Spatial understanding](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb) guide for an introduction on those capabilities, and the [Video understanding](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Video_understanding.ipynb) one for video examples.

*/


================================================
FILE: .devcontainer/devcontainer.json
================================================
{
    "name": "Gemini Cookbook Dev Container",
    "image": "mcr.microsoft.com/devcontainers/universal:2",
    "waitFor": "onCreateCommand",
    "updateContentCommand": "python3 -m pip install ipywidgets",
    "customizations": {
        "vscode": {
            "extensions": [
                "ms-toolsai.jupyter",
                "ms-python.python",
                "eamodio.gitlens"
            ]
        }
    },

    "postCreateCommand": "sudo cp .devcontainer/welcome.txt /usr/local/etc/vscode-dev-containers/first-run-notice.txt"
}



================================================
FILE: .devcontainer/welcome.txt
================================================
👋 Welcome to the Gemini API Cookbook! This repo is a collection of all the guides and examples for the Gemini API.
    - To get started, make sure you have an API Key from Google AI studio: https://goo.gle/aistudio-key
    - Note that the notebooks were written to run in Google Colab, so they may
      not always work in Codespaces
    - Check out the 'README.md' file to know more



================================================
FILE: .gemini/config.yaml
================================================
have_fun: false
code_review:
  disable: false
  comment_severity_threshold: MEDIUM
  max_review_comments: -1
  pull_request_opened:
    help: false
    summary: true
    code_review: true



================================================
FILE: .gemini/styleguide.md
================================================
# Gemini Cookbook Python notebooks Style Guide

# Introduction
This style guide outlines the coding conventions for Python notebooks developped by the Gemini team.
It's based on PEP 8, but with some modifications to address specific needs and
preferences within our organization.

This guide is mostly about the python content and the notebook, but don't forget to also review the markdown files. In particular, any new notebook should be referenced in the corresponding readmes (at folder level at least, and potentially the one on higer levels).

# Key Principles
* **Readability:** Code should be easy to understand for all developers. Since they are notebooks and aimed at teaching
  how to use Gemini and its API/SDK, the focus should be on writting didactic and easy-to-read code while limiting the
  back and forth with the documentation or within the notebook.
* **Maintainability:** Code should be easy to modify and extend. In particular it must be easy to switch the models used.
* **Consistency:** Adhering to a consistent style across all projects improves
  collaboration and reduces errors. Similar codes (like getting the api key from the secret, initializing the client or
  selecting a model) should always be the same so it's easier for the reader to go directly to the specific content.
* **Performance:** While readability is paramount, code should be efficient.

# Cookbook notebook specific instructions

## Hard-coded API keys

Very important: notebooks must not contain hard-coded API keys. If you find one, immediately tell the author to cancel their submission, revoke the compromised key, and create a new one. Exposed keys are quickly collected and can lead to unauthorized usage and costs for the author.

## Notebook specificities

Most of the cookbook content is Colab notebooks, which are stored as Json.
* Only the `source` of the `cells` should be analyzed/reviewed.
* Those cells can be either Markdown or python code (or sometimes bash in which case the code cell starts with `%%bash`).
* The `outputs` do not have to be ignored, but if none of them changed while some code has, it might be a sign that the
  notebook has not been run to check that it works, in which case a warning should be raised;
* If the `execution_count` has changed to something else than `null`, it usually indicates that the formatting script has not
  been run. A warning should be raised, but only once per notebook.

## Notebook style

* Include the collapsed license at the top (uses the Colab "Form" mode to hide the cells).
* Use one `H1` header (`#` in Markdown) for the title.
* Include the "open in colab" button immediately after the `H1`. It should look like
  `<a target=\"_blank\" href=\"URL\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/>`
  where `URL` should be `https://colab.research.google.com/github/google-gemini/cookbook/blob/main/` followed by the notebook
  location in the cookbook
* Include an overview section before any code.
* use %pip instead of !pip
* Put the imports when they are first used.
* Keep code as brief as possible. 
* Break text cells at headings
* Break code cells between "building" and "running".
* Necessary but uninteresting code (like helper functions) should be hidden in a toggleable code cell by putting `# @title`
  as the first line.

## Code style

* Notebooks are for people. Write code optimized for clarity.
* Use the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html), where applicable. Code formatted
  by [`pyink`](https://github.com/google/pyink) will always be accepted.
* Ideally, if you define a function, run it and show us what it does before using it in another function.
* Demonstrate small parts before combining them into something more complex.
* Only use helper function when you don't have a choice. If it's only a couple of lines, it's usually better to write them
  everytime so that the readers don't have to check the function definition all the time.
* When selecting a model, use a colab selector for easier maintainability:
  `MODEL_ID="gemini-2.5-flash" # @param ["gemini-2.5-flash-lite","gemini-2.5-flash","gemini-2.5-pro"] {"allow-input":true, isTemplate: true}`
* Some notebooks can also benefit from having a form to update the prompt:
  `prompt = "Detect the 2d bounding boxes of the cupcakes (with “label” as topping description”)"  # @param {type:"string"}`
  or a list of prompts they can choose from:
 `prompt = "Draw a square around the fox' shadow"  # @param ["Find the two origami animals.", "Where are the origamis' shadows?","Draw a square around the fox' shadow"] {"allow-input":true}`

## Outputs

* Whenever possible simply use `print`.
* When needed use `display(Markdown())` for markdown text, `print(json.dumps(json_string, indent=4))` for Json
  or `display(Image()` for images.

## Text

* Use an imperative style. "Run a prompt using the API."
* Use sentence case in titles/headings.
* Use short titles/headings: "Download the data", "Call the API", "Process the results".
* Use the [Google developer documentation style guide](https://developers.google.com/style/highlights).
* Use [second person](https://developers.google.com/style/person): "you" rather than "we". You will fail the lint check otherwise.
* Explain what you do, the features you use, and link to existing notebooks or to the documentation for more details.

## Examples

* Keep examples quick and concise.
* Do not use extra parameters (like temperature) when not needed to keep the focus on what your notebook is illustrating.
* If you have to use extra-parameters, explain why and why the specific value the first time you do.

# Deviations from PEP 8

## Line Length

* **Maximum line length:** 100 characters (instead of PEP 8's 79).
    * Modern screens allow for wider lines, improving code readability in many cases.
    * Many common patterns in our codebase, like long strings or URLs, often exceed 79 characters.

## Indentation

* **Use 4 spaces per indentation level.** (PEP 8 recommendation)
* When a function has multiple parameters, expend it on multiple lines with proper indentation for better readability:
    ```python
    response = client.models.generate_content(
        model=MODEL_ID,
        contents="Here's my prompt",
        config={
            "response_mime_type": "application/json",
            "response_schema": Schema
        }
    )    
    ```
Notice the line break on the first and last lines.
* Long text variables should use triple double quotes and proper indentation for better readability:
    ```python
    long_prompt = """
        Cupcake ipsum dolor. Sit amet marshmallow topping cheesecake muffin.
        Halvah croissant candy canes bonbon candy. Apple pie jelly beans topping carrot cake danish tart cake cheesecake.
        Muffin danish chocolate soufflé pastry icing bonbon oat cake. Powder cake jujubes oat cake.
        Lemon drops tootsie roll marshmallow halvah carrot cake.
    """    
    ```
Notice the line break on the first and last lines.
    
## Naming Conventions

* **Variables:** Use lowercase with underscores (snake_case): `user_name`, `total_count`
* **Constants:**  Use uppercase with underscores: `MAX_VALUE`, `DATABASE_NAME`
* **Functions:** Use lowercase with underscores (snake_case): `calculate_total()`, `process_data()`
* **Classes:** Use CapWords (CamelCase): `UserManager`, `PaymentProcessor`
* **Modules:** Use lowercase with underscores (snake_case): `user_utils`, `payment_gateway`

## Docstrings

Docstrings are not mandatory, but when used they should follow those conventions:
* **Use triple double quotes (`"""Docstring goes here."""`) for all docstrings.**
* **First line:** Concise summary of the object's purpose.
* **For complex functions/classes:** Include detailed descriptions of parameters, return values,
  attributes, and exceptions.
* **Use Google style docstrings:** This helps with automated documentation generation.
    ```python
    def my_function(param1, param2):
        """Single-line summary.

        More detailed description, if necessary.

        Args:
            param1 (int): The first parameter.
            param2 (str): The second parameter.

        Returns:
            bool: The return value. True for success, False otherwise.

        Raises:
            ValueError: If `param2` is invalid.
        """
        # function body here
    ```

## Type Hints

Type Hints are not mandatory, but when used they should follow those conventions:
* **Use type hints:**  Type hints improve code readability and help catch errors early.
* **Follow PEP 484:**  Use the standard type hinting syntax.

## Comments

* **Write clear and concise comments:** Explain the "why" behind the code, not just the "what".
* **Comment sparingly:** Well-written code should be self-documenting where possible.
* **Use complete sentences:** Start comments with a capital letter and use proper punctuation.



================================================
FILE: .github/header-checker-lint.yml
================================================
# Lint check for license headers.
# See: https://github.com/googleapis/repo-automation-bots/tree/main/packages/header-checker-lint
allowedCopyrightHolders:
  - 'Google LLC'
allowedLicenses:
  - 'Apache-2.0'
sourceFileExtensions:
  - 'py'
  - 'sh'
  - 'ts'
  - 'js'
  - 'java'
  # Note that separate nblint checks will handle *.ipynb files.



================================================
FILE: .github/labeler.yml
================================================
'status:awaiting review':
- '**/*'

'component:examples':
- examples/**/*

'component:quickstarts':
- quickstarts/**/*


================================================
FILE: .github/workflows/issue-metrics.yml
================================================
name: Issue metrics
on:
  workflow_dispatch:
  schedule:
    - cron: "0 0 1 * *"  # First of the month, midnight UTC

permissions:
  contents: read

jobs:
  build:
    name: issue metrics
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: read
    steps:
      - name: Get dates for last month
        shell: bash
        run: |
          # Calculate the first day of the previous month
          first_day=$(date -d "last month" +%Y-%m-01)

          # Calculate the last day of the previous month
          last_day=$(date -d "$first_day +1 month -1 day" +%Y-%m-%d)

          # Set an environment variable with the date range
          echo "$first_day..$last_day"
          echo "last_month=$first_day..$last_day" >> "$GITHUB_ENV"

      - name: Run issue-metrics tool
        uses: github/issue-metrics@4f29f34d9d831fe224cbc6c8a0d711415ebd01b1  # v3
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SEARCH_QUERY: 'repo:google-gemini/cookbook is:issue created:${{ env.last_month }} -reason:"not planned"'
          LABELS_TO_MEASURE: "status:awaiting response,status:triaged,status:stale"

      - name: Create issue
        uses: peter-evans/create-issue-from-file@e8ef132d6df98ed982188e460ebb3b5d4ef3a9cd  # v5
        with:
          title: Monthly issue metrics report
          token: ${{ secrets.GITHUB_TOKEN }}
          content-filepath: ./issue_metrics.md



================================================
FILE: .github/workflows/labeler.yml
================================================
name: "PR Labeler"

on:
  pull_request_target:
    types: ["opened", "reopened", "ready_for_review"]

jobs:
  triage:
    permissions:
      contents: read
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
    - uses: actions/labeler@ac9175f8a1f3625fd0d4fb234536d26811351594  # v4
    if: ${{ github.event.pull_request.draft == false }}


================================================
FILE: .github/workflows/new_examples_links_in_table_of_content.yml
================================================
name: All new examples are linked in the README

on:
  pull_request:
    branches: [ main ]

jobs:
  new-example-links-in-examples-readme:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744  # v3
      - name: Get Changed Files
        id: changed-files
        uses: tj-actions/changed-files@48d8f15b2aaa3d255ca5af3eba4870f807ce6b3c  # v45
        with:
          files:  |
            examples/**/*.ipynb
      - name: Check README links
        env:
          # We only want the added file so that if we do an exception we dop not have to make an exception again each time qu're editing the file
          NEW_FILES: ${{ steps.changed-files.outputs.all_changed_files }}
          README: examples/README.md
        run: |
          all_linked=true
          for file in ${NEW_FILES}; do
            echo "$file was added"
            # Get the directory of the new notebook file
            notebook_dir=$(dirname "$file")
            # Get the filename without the directory path
            filename=$(basename "$file")

            # Check if link exists in $README 
            if ! grep -q "$file" "$README" && ! grep -q "${file/examples\//](}" "$README"; then
              # Check if a README.md exists in the sub-folder, and if so, check for the link there
              if [[ -f "$notebook_dir/README.md" ]]; then
                if ! grep -q "$filename" "$notebook_dir/README.md"; then
                  all_linked=false
                  echo "  Link to '$file' not found in "$README".md or $notebook_dir/README.md"
                  echo "::warning file=$file::Link to '$file' not found in a README.md, please add one"
                else
                  echo "  Link to '$file' found in $notebook_dir/README.md"
                fi
              else
                all_linked=false
                echo "  Link to '$file' not found in $README, and no README.md found in $notebook_dir"
                echo "::warning file=$file::Link to '$file' not found in $README, please add one (or create a README.md in the sub-folder)"
              fi
            else
              echo "  Link to '$file' found in $README"
            fi
          done
          if ! $all_linked; then
            exit 1
          fi



================================================
FILE: .github/workflows/notebooks.yaml
================================================
# Notebook-related checks

name: Notebooks

on:
  # Relevant PRs
  pull_request:
    paths:
    - "**.ipynb"
  # Allow manual runs
  workflow_dispatch:

jobs:
  # Format all notebooks.
  nbfmt:
    name: Notebook format
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Get *full* history
    - uses: actions/setup-python@v4
    - name: Install tensorflow-docs
      run: python3 -m pip install -U git+https://github.com/tensorflow/docs
    - name: Fetch main branch
      run: git fetch -u origin main:main
    - name: Check notebook formatting
      run: |
        if [ "${{ github.event_name }}" == "pull_request" ]; then
          # Only check notebooks modified in this pull request
          base_commit=$(git merge-base HEAD ${{ github.event.pull_request.base.sha }})
          readarray -t changed_notebooks < <(git diff --name-only "${base_commit}...HEAD" "*.ipynb")
        else
          # Manual run, check everything
          readarray -t changed_notebooks < <(find -name '*.ipynb')
        fi
        if [[ ${#changed_notebooks[@]} == 0 ]]; then
          echo "No notebooks modified in this pull request."
          exit 0
        else
          echo "Check formatting with nbfmt:"
          python3 -m tensorflow_docs.tools.nbfmt --test "${changed_notebooks[@]}"
        fi

  nblint:
    name: Notebook lint
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Get *full* history
    - uses: actions/setup-python@v4
    - name: Install tensorflow-docs
      run: python3 -m pip install -U git+https://github.com/tensorflow/docs
    - name: Fetch main branch
      run: git fetch -u origin main:main

    # Lint for all notebooks
    - name: Lint notebooks
      run: |
        if [ "${{ github.event_name }}" == "pull_request" ]; then
          # Only check notebooks modified in this pull request
          base_commit=$(git merge-base HEAD ${{ github.event.pull_request.base.sha }})
          readarray -t changed_notebooks < <(git diff --name-only "${base_commit}...HEAD" "*.ipynb" | grep -v 'gemini-2/' || true)
        else
          # Manual run, check everything
          readarray -t changed_notebooks < <(find . -name '*.ipynb' | grep -v 'gemini-2/')
        fi

        # Define files to exclude from linting
        excluded_files=(
          "examples/Object_detection.ipynb",
          "quickstarts/Template.ipynb",
        )

        # Filter out excluded files from the changed_notebooks array
        filtered_notebooks=()
        for notebook in "${changed_notebooks[@]}"; do
          exclude=false
          for excluded_file in "${excluded_files[@]}"; do
            if [[ "$notebook" == "$excluded_file" ]]; then
              exclude=true
              break
            fi
          done
          if [[ "$exclude" == false ]]; then
            filtered_notebooks+=("$notebook")
          fi
        done

        if [[ ${#filtered_notebooks[@]} == 0 ]]; then
          echo "No website notebooks modified in this pull request."
          exit 0
        else
          echo "WARNING: If the button_colab check fails for you, make sure you have <table class=\"tfo-notebook-buttons\"...>"
          echo "Lint check with nblint:"
          python3 -m tensorflow_docs.tools.nblint \
            --styles=google,gemini_cookbook \
            --arg=repo:google-gemini/cookbook \
            --arg=branch:main \
            --exclude_lint=gemini_cookbook::button_download \
            --exclude_lint=gemini_cookbook::button_website \
            --arg=base_url:https://ai.google.dev/ \
            --exclude_lint=gemini_cookbook::button_github \
            "${filtered_notebooks[@]}"
        fi



================================================
FILE: .github/workflows/stale.yml
================================================
# This workflow warns and then closes issues and PRs that have had no activity for a specified amount of time.
#
# You can adjust the behavior by modifying this file.
# For more information, see:
# https://github.com/actions/stale
name: Mark stale issues and pull requests

on:
  schedule:
  # Scheduled to run at 10.30PM UTC everyday (1530PDT/1430PST)
  - cron: '30 22 * * *'
  workflow_dispatch:

jobs:
  stale:

    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write
      actions: write

    steps:
    - uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639  # v9
      with:
        repo-token: ${{ secrets.GITHUB_TOKEN }}
        days-before-issue-stale: 14
        days-before-issue-close: 13
        stale-issue-label: "status:stale"
        close-issue-reason: not_planned
        any-of-labels: "status:awaiting response,status:more data needed"
        stale-issue-message: >
          Marking this issue as stale since it has been open for 14 days with no activity.
          This issue will be closed if no further activity occurs.
        close-issue-message: >
          This issue was closed because it has been inactive for 27 days.
          Please post a new issue if you need further assistance. Thanks!
        days-before-pr-stale: 14
        days-before-pr-close: 13
        stale-pr-label: "status:stale"
        stale-pr-message: >
          Marking this pull request as stale since it has been open for 14 days with no activity.
          This PR will be closed if no further activity occurs.
        close-pr-message: >
          This pull request was closed because it has been inactive for 27 days.
          Please open a new pull request if you need further assistance. Thanks!
        # Label that can be assigned to issues to exclude them from being marked as stale
        exempt-issue-labels: 'override-stale'
        # Label that can be assigned to PRs to exclude them from being marked as stale
        exempt-pr-labels: "override-stale"


